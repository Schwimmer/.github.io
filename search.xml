<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[【转】了解JanusGraph]]></title>
    <url>%2F2019%2F07%2F09%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%2FJanusGraph%E7%89%B9%E6%80%A7%2F</url>
    <content type="text"><![CDATA[JanusGraph特性开源图数据库Titan项目已经停止更新，JanusGraph可以说复活了Titan项目，到目前为止，JanusGraph与Titan在核心机制上相差不大。JanusGraph/Titan有如下关键设计： 支持大规模图数据存储，Titan图数据库是建立在分布式集群上，数据存储容量和集群节点数量成正比 支持弹性和线性扩展，高可用，高容错 支持Gremlin图查询语言 支持利用Hadoop计算框架对图数据进行分析 支持外部索引：ElasticSearch、Solr、Lucene 支持多储存引擎：Cassandra、HBase、Berkeley DB和InMemory模式; 基于Apache License 2.0 基本概念同大多数图数据库一样，JanusGraph采用属性图进行建模。基于属性图的模型，JanusGraph有如下基本概念： Vertex Label：节点的类型，用于表示现实世界中的实体类型，比如”人”，“车”。在JanusGraph中，每一个节点有且只有一个Vertex Label。当不显式指定Vertex Label时，采用默认的Vertex Label。 Vertex：节点/顶点，用于表示现实世界中的实体对象。 Edge Label：边的类型，用于表示现实世界中的关系类型，比如“通话关系”，“转账关系”，“微博关注关系”等； Edge: 边，用于表示一个个具体的联系。JanusGraph的边都是单向边。如果需要双向边，则通过两条相反方向的单向边组成。JanusGraph不存在无向边。 Property Key：属性的类型，比如“姓名”，“年龄”，“时间”等。Property Key有Cardinality的概念。Cardinality有SINGLE、LIST和SET三种选项。这三种选项分别用于表示一个Property中，对于同一个Property Key是只允许有一个值、允许多个可重复的值，还是多个不可重复的值。 Property：属性，用于表示一个个具体的附加信息，采用Key-Value结构。Key就是Property Key，Value就是具体的值。 属性图举例张三与李四是同事关系，他们从2017年开始成为同事，用属性图表达： 参考 图解JanusGraph内部数据存储结构]]></content>
      <categories>
        <category>知识图谱</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[【转】基于Neo4j的推荐系统]]></title>
    <url>%2F2019%2F07%2F08%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%2F%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[获取数据从官网获取电影数据库https://neo4j.com/sandbox-v2/。登录后会有这样的页面 选择右边这个推荐系统的。然后给了这样的链接：https://10-0-1-218-33967.neo4jsandbox.com/browser/ 进入后就是一个Neo4j的web端，包含了完整的数据 跑一个简单查询测试一下数据 1match (n) return n limit 10 返回 基于内容的过滤基于内容的过滤(Content-based filtering)：该技术通过比较商品之间的相似性或者相关性进行推荐。这种方式忽略用户的购买行为，只考虑商品之间的相似关系。 12MATCH p=(m:Movie &#123;title: &quot;Net, The&quot;&#125;)-[:ACTED_IN|:IN_GENRE|:DIRECTED*2]-()RETURN p LIMIT 25 Cypher 语句的意思是：找出25条记录，该记录满足以下条件之一： 该记录中的电影和电影m 有相同的流派（IN_GENRE） 出演过电影m的演员，出演过该电影（ACTED_IN） 执导过电影m的导演，执导过该电影（DIRECTED） 注意：此语句返回的记录P，是一条关系链，并不仅仅是某一部电影。 另外，关系的运算，用到了 | 表示或。:DIRECTED2 中的 2 表示关系长度为2的关系。 参考 Neo4j 做推荐 （1）—— 基础数据 Neo4j 做推荐 （2）—— 基于内容的过滤]]></content>
      <categories>
        <category>知识图谱</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F07%2F08%2Fhadoop-spark%2Fspark%2F%E5%AE%BD%E4%BE%9D%E8%B5%96%E7%AA%84%E4%BE%9D%E8%B5%96%E8%AE%B2%E7%9A%84%E6%AF%94%E8%BE%83%E5%A5%BD%E7%9A%84%E6%96%87%E7%AB%A0%2F</url>
    <content type="text"><![CDATA[https://blog.csdn.net/silviakafka/article/details/54574653]]></content>
  </entry>
  <entry>
    <title><![CDATA[美团知识图谱]]></title>
    <url>%2F2019%2F07%2F08%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%2F%E7%BE%8E%E5%9B%A2%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%2F</url>
    <content type="text"><![CDATA[参考 美团大脑：知识图谱的建模方法及其应用 美团餐饮娱乐知识图谱——美团大脑揭秘]]></content>
      <categories>
        <category>知识图谱</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[【转】高能NLP之路]]></title>
    <url>%2F2019%2F07%2F07%2F%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90%2F%E9%AB%98%E8%83%BDNLP%E4%B9%8B%E8%B7%AF%2F</url>
    <content type="text"><![CDATA[转自知乎专栏：高能NLP之路]]></content>
      <categories>
        <category>算法学习资源</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F07%2F07%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%93%E9%A2%98%2F01SoftMax%2F%E7%99%BE%E9%97%AE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-Softmax%2F</url>
    <content type="text"><![CDATA[问题1：多分类下为什么用Softmax而不是其他的归一化方法转自知乎：多类分类下为什么用softmax而不是用其他归一化方法? 1.softmax设计的初衷，是希望特征对概率的影响是乘性的。即，$exp(\sumd w{id} xd)$ 可以看做是类别i的未归一化概率。$x_d$ 加上某个数，会导致这个概率乘上某个数，乘多少由 $w{id}$ 来控制。 2.多类分类问题的目标函数常常选为cross-entropy。即 L = -\sum_i t_i \cdot lnP(y=i)其中目标类的$t_i$为1，其余类的$t_i$为0。在神经网络模型中，输出层第i个神经元的输入为 a_i = \sum_d w_{id} x_d神经网络是用error back-propagation训练的，这个过程中有一个关键的量是 \frac {\partial L}{\partial a_i}可以算出，同时使用softmax和cross-entropy时， \frac {\partial L}{\partial a_i}=P(y=i)-t_i这个形式非常简洁，而且与线性回归（采用最小均方误差目标函数）、两类分类（采用cross-entropy目标函数）时的形式一致。 还是没看懂。]]></content>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法推导（一）Softmax]]></title>
    <url>%2F2019%2F07%2F07%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%93%E9%A2%98%2F01SoftMax%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%8E%A8%E5%AF%BC%EF%BC%88%E4%BA%8C%EF%BC%89Softmax%2F</url>
    <content type="text"><![CDATA[Softmax是否包含两个用途： 1）用于多分类 2）用于输出结果的概率化。 用作输出结果假设有一个数组V，$V_i$表示V中的第i个元素，那么这个元素的softmax值为: S_i = \frac{e^i}{\sum_j e^j}加上参数等的写法是 该元素的softmax值，就是该元素的指数与所有元素指数和的比值。 这个定义可以说很简单，也很直观。那为什么要定义成这个形式呢？原因主要如下。 Softmax求偏导在分类问题中，会使用交叉熵作为损失函数，可以表示为 Loss=-\sum_it_i\ln y_i参考小白都能看懂的softmax详解]]></content>
      <categories>
        <category>机器学习算法专题</category>
      </categories>
      <tags>
        <tag>Softmax</tag>
        <tag>算法推导</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[极大似然估计]]></title>
    <url>%2F2019%2F07%2F06%2F%E7%AE%97%E6%B3%95%E8%83%8C%E5%90%8E%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86%2F%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[通俗的讲，最大似然估计，就是利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。]]></content>
      <categories>
        <category>算法背后的数学原理</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F07%2F06%2F%E7%AE%97%E6%B3%95%E8%83%8C%E5%90%8E%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86%2F%E4%BA%8C%E9%A1%B9%E5%88%86%E5%B8%83%E5%92%8C%E5%87%A0%E4%BD%95%E5%88%86%E5%B8%83%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[百问机器学习-逻辑回归]]></title>
    <url>%2F2019%2F07%2F06%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%93%E9%A2%98%2F00%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F%E7%99%BE%E9%97%AE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[问题1：LR和线性回归，有何异同1、LR是分类问题，线性回归是回归问题，这是最本质的区别。 具体来说，LR中，y取值是一个几何分布，模型学习得出的是 E[y|x;\theta]这个公式的解释是，给定x和参数后，得到y的期望。 而线性回归求解的是 {y}' = \theta^Tx是我们对于假设的真实关系 {y} = \theta^Tx+\epsilon的一个近似。其中$\epsilon$代表误差项。我们用这个近似项来处理回归问题。 正因为是分类和回归问题，LR的因变量是离散的，线性回归的是连续的。 2、在x和参数确定的情况下，LR可以看做是广义线性模型（Generalized Linear Model）在y服从几何分布时的一个特殊情况。 暂时不理解，需要查一下，广义线性模型一般是什么样的。 而使用最小二乘求线性回归时，我们认为y服从正态分布。 3、都使用了极大似然估计来对训练样本进行建模。但是， 线性回归使用最小二乘法，就是在x和参数确定，y服从正态分布的假设下，使用极大似然估计的一个化简。 是不是个化简呢？这两个都不是一种求值的思路。 最大似然估计：现在已经拿到了很多个样本（你的数据集中所有因变量），这些样本值已经实现，最大似然估计就是去找到那个（组）参数估计值，使得前面已经实现的样本值发生概率最大。因为你手头上的样本已经实现了，其发生概率最大才符合逻辑。这时是求样本所有观测的联合概率最大化，是个连乘积，只要取对数，就变成了线性加总。此时通过对参数求导数，并令一阶导数为零，就可以通过解方程（组），得到最大似然估计值。最小二乘：找到一个（组）估计值，使得实际值与估计值的距离最小。本来用两者差的绝对值汇总并使之最小是最理想的，但绝对值在数学上求最小值比较麻烦，因而替代做法是，找一个（组）估计值，使得实际值与估计值之差的平方加总之后的值最小，称为最小二乘。“二乘”的英文为least square，其实英文的字面意思是“平方最小”。这时，将这个差的平方的和式对参数求导数，并取一阶导数为零，就是OLSE。作者：稻花香链接：https://www.zhihu.com/question/20447622/answer/23848605来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 问题2：Softmax和逻辑回归问题3：为什么要用Sigmoid因为线性回归模型的预测值为实数，而样本的类标记为（0,1），我们需要将分类任务的真实标记y与线性回归模型的预测值联系起来，也就是找到广义线性模型中的联系函数。如果选择单位阶跃函数的话，它是不连续的不可微。而如果选择sigmoid函数，它是连续的，而且能够将z转化为一个接近0或1的值。 问题4：为什么叫对数几率函数Sigmoid的定义是 y=\frac {1} {1+e^{-(w^Tx+b)}}可以推导出 \ln \frac y {1-y} = w^Tx+b若将y视为x作为正例的可能性，那么1-y是其反例的可能性，两者的比值就称为“几率”。反映了x作为正例的相对可能性。然后再取对数，可以看出，上式其实是在用线性回归模型的预测结果去逼近真实标记的对数几率。 问题5：逻辑回归的假设假设数据服从伯努利分布。 不同文章说法不一，伯努利、二项、几何分布都有。 问题6：为什么LR用最大似然而不是最小二乘作为损失函数简单的说，如果用最小二乘会导致损失函数是一个非凸函数。具体参考机器学习算法推导（一）逻辑回归。 问题7：逻辑回归在训练的过程当中，如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？ 先说结论，如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果。 但是对特征本身来说的话，假设只有一个特征，在不考虑采样的情况下，你现在将它重复100遍。训练以后完以后，数据还是这么多，但是这个特征本身重复了100遍，实质上将原来的特征分成了100份，每一个特征都是原来特征权重值的百分之一。 如果在随机采样的情况下，其实训练收敛完以后，还是可以认为这100个特征和原来那一个特征扮演的效果一样，只是可能中间很多特征的值正负相消了。 问题8：LR的缺点 很难处理数据不平衡的问题。举个例子：如果我们对于一个正负样本非常不平衡的问题比如正负样本比 10000:1.我们把所有样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好。 处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据，或者进一步说，处理二分类的问题 。 逻辑回归本身无法筛选特征。有时候，我们会用gbdt来筛选特征，然后再上逻辑回归。 问题9：LR和神经网络的关系LR可以看做是没有隐层的神经网络。因为LR有激活函数，有梯度下降更新权重，只是少了一层全连接的隐层。]]></content>
      <categories>
        <category>机器学习算法专题</category>
      </categories>
      <tags>
        <tag>逻辑回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法推导（一）逻辑回归]]></title>
    <url>%2F2019%2F07%2F06%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%93%E9%A2%98%2F00%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%8E%A8%E5%AF%BC%EF%BC%88%E4%B8%80%EF%BC%89%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[前言我们知道，线性回归的公式是 h_{\theta}(x)={\theta}^Tx如果想实现分类功能，就需要把输出变成一个阶跃函数，比如 y=\left\{\begin{matrix} 0; & z\leqslant 0 \\ 1; & z \geqslant 0 \end{matrix}\right. ,z=h_{\theta}(x)但是这样就太粗糙了，而且不连续不可微。我们希望有一个理想的阶跃函数，而且是单调可微的函数。于是找到了对数几率函数（Logistic Function）。因为曲线像S，又称Sigmoid。 定义Sigmoid函数的定义是： y=\frac {1} {1+e^{-(\theta^Tx+b)}}接下来就是根据给定的训练集，求出参数w。首先要找到目标函数（代价函数/损失函数）。 目标函数、代价函数、损失函数，一般而言，这三个概念是一回事。 如果深究的话，有细微的差别。参考深入理解机器学习中的：目标函数，损失函数和代价函数 损失函数：计算的是一个样本的误差 代价函数：是整个训练集上所有样本误差的平均 目标函数：代价函数 + 正则化项 首先想到的就是模仿线性回归的思路，用最小二乘当损失函数。 J(\theta)=\sum_{i} \frac 1 2 (h_{\theta}(x^i),\ y^i)^2但是，把Sigmoid带入后，会发现这是一个非凸函数，这就意味着代价函数有着许多的局部最小值，不利于求解。 后来人们想到，用极大似然函数作为损失函数，这样就是一个凸函数。 损失函数如果将$h_{\theta}(x)$看做是正类的后验概率，则有 h_{\theta}(x;\theta)=p(y=1|x;\theta)=\phi(x)=\frac {1} {1+e^{-(\theta^Tx+b)}}那么，有 p(y=0|x;\theta) = 1-\phi(x)将上面两式写成一般形式（概率分布函数） p(y|x;\theta)=h_{\theta}(x;\theta)^y(1-h_{\theta}(x;\theta))^{1-y}接下来用对数极大似然估计（加上对数是为了方便求导）根据训练集估计出参数$\theta$。 \begin{aligned} l(\theta) &= Log\ L(\theta)\\ &=\sum_{j=1}^M(y_jln\ h(x_j)+(1-y_j)ln\ (1-h(x_j))) \end{aligned}其中M是样本个数。对其求关于$\theta$的偏导。 1）首先求$h_{\theta}(x)$，即Sigmoid的偏导。 h_{\theta}^{'}(\mathbf X) = h_{\theta}(\mathbf X)(1-h_{\theta}(\mathbf X))推导过程如下： 2）有了第一步得出的公式后，得到 3）利用梯度下降法求解参数。由于损失函数是凸函数，沿着梯度下降方向找到最小点。 ① 批量梯度下降是： Repeat\ until\ convergence \{ \\ \theta^{(t+1)} := \theta^t - \eta^t\triangledown _{\theta}l(\theta^{t},Z) \\ \}\\ \tag{1-2}② 随机梯度下降（SGD）： Repeat\ until\ convergence \{ \\ for\ j=1\ to\ M, \{ \\ \theta^{(t+1)} := \theta^t - \eta^t\triangledown _{\theta}l(\theta^{t},Z_j) \\ \}两者的区别是： 前者每次更新$\theta$都需要遍历一次整个样本集合；而后者在遍历样本集合的时候，每个样本都能改变$\theta$ ，有更快的收敛速度 。由于SGD针对观测到的随机一条数据进行权重的更新，很适合进行增量计算，实现梯度下降的online模式。 ③ small batch梯度下降 结合了上述两点的优点，每次更新参数时仅使用一部分样本，减少了参数更新的次数，可以达到更加稳定的结果，一般在深度学习中采用这种方法。 参考： Logistic Regression——逻辑回归算法推导 逻辑回归与最大似然估计推导]]></content>
      <categories>
        <category>机器学习算法专题</category>
      </categories>
      <tags>
        <tag>算法推导</tag>
        <tag>逻辑回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习资源]]></title>
    <url>%2F2019%2F07%2F06%2F%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90%2F</url>
    <content type="text"><![CDATA[手写基于numpy的神经网络，全中文解析，重点：中文简单https://github.com/yuanxiaosc/Theoretical-Proof-of-Neural-Network-Model-and-Implementation-Based-on-Numpy]]></content>
      <categories>
        <category>算法学习资源</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习资源]]></title>
    <url>%2F2019%2F07%2F06%2F%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90%2F</url>
    <content type="text"><![CDATA[NumPy手写全部主流机器学习模型https://github.com/ddbourgin/numpy-ml]]></content>
      <categories>
        <category>算法学习资源</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F07%2F06%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%93%E9%A2%98%2F00%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84Python%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[统计学习方法和西瓜书学习资源]]></title>
    <url>%2F2019%2F07%2F05%2F%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90%2F%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E5%92%8C%E8%A5%BF%E7%93%9C%E4%B9%A6%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90%2F</url>
    <content type="text"></content>
      <categories>
        <category>算法学习资源</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F07%2F05%2F%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90%2FNLP%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90%2F</url>
    <content type="text"><![CDATA[用百行代码搞定各类NLP模型https://github.com/graykode/nlp-tutorial 韩国人写的，没有中文 项目目录 下面为项目的基本框架以及每个模型的功能： 1、基本嵌入模型 NNLM - 预测下一个单词 Word2Vec(Skip-gram) - 训练词嵌入并展示词的类推图 FastText(Application Level) - 情感分类 2、CNN TextCNN - 二元情感分类 DCNN（进行中……） 3、RNN TextRNN - 预测下一步 TextLSTM - 自动完成 Bi-LSTM - 在长句子中预测下一个单词 4、注意力机制 Seq2Seq - 同类词转换 Seq2Seq with Attention - 翻译 Bi-LSTM with Attention - 二元情感分类 5、基于 Transformer 的模型 Transformer - 翻译 BERT - 分类是否是下一句和预测 Mask 掉的词 模型示例 在这一部分中，我们将以带注意力机制的 Bi-LSTM 与 Transformer 为例分别介绍 TensorFlow 和 PyTorch 的代码实现。当然我们也只会介绍模型部分的核心代码，其它训练迭代和可视化等过程可以查阅原项目。 基于注意力机制的双向 LSTM 作者用不到 90 行代码简单介绍了如何用双向 LSTM 与注意力机制构建情感分析模型，即使使用 TensorFlow 这种静态计算图，Tae Hwan Jung 借助高级 API 也能完成非常精简代码。总的而言，模型先利用双向 LSTM 抽取输入词嵌入序列的特征，再使用注意力机制选择不同时间步上比较重要的信息，最后用这些信息判断输入句子的情感倾向。 首先对于构建双向 LSTM，我们只需要定义前向和后向 LSTM 单元（lstm_fw_cell 与 lstm_bw_cell），并传入高级 API tf.nn.bidirectional_dynamic_rnn() 就行了： 12345678910111213# LSTM ModelX = tf.placeholder(tf.int32, [None, n_step])Y = tf.placeholder(tf.int32, [None, n_class])out = tf.Variable(tf.random_normal([n_hidden * 2, n_class]))embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_dim]))input = tf.nn.embedding_lookup(embedding, X) # [batch_size, len_seq, embedding_dim]lstm_fw_cell = tf.nn.rnn_cell.LSTMCell(n_hidden)lstm_bw_cell = tf.nn.rnn_cell.LSTMCell(n_hidden)# output : [batch_size, len_seq, n_hidden], states : [batch_size, n_hidden]output, final_state = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell,lstm_bw_cell, input, dtype=tf.float32) 第二个比较重要的步骤是构建注意力模块，注意力机制其实就是衡量不同时间步（不同单词）对最终预测的重要性，它的过程也就计算重要性并根据重要性合成上下文语义特征两部分。下图展示了全局注意力的具体过程，它确定不同时间步的权重（alpha），并加权计算得出上下文向量（context vextor）。如果读者希望详细了解 Attention，查阅下图的来源论文就好了，当然也可以跳过原理直接进入实战部分～ 如下所示，模型主要根据前面双向 LSTM 输出的结果（output）与最终隐藏状态之间的余弦相似性计算怎样为输出结果 output 加权，加权得到的上下文向量 context 可进一步用于计算最终的预测结果。 1234567891011# Attentionoutput = tf.concat([output[0], output[1]], 2) # output[0] : lstm_fw, output[1] : lstm_bwfinal_hidden_state = tf.concat([final_state[1][0], final_state[1][1]], 1) # final_hidden_state : [batch_size, n_hidden * num_directions(=2)]final_hidden_state = tf.expand_dims(final_hidden_state, 2) # final_hidden_state : [batch_size, n_hidden * num_directions(=2), 1]attn_weights = tf.squeeze(tf.matmul(output, final_hidden_state), 2) # attn_weights : [batch_size, n_step]soft_attn_weights = tf.nn.softmax(attn_weights, 1)context = tf.matmul(tf.transpose(output, [0, 2, 1]), tf.expand_dims(soft_attn_weights, 2)) # context : [batch_size, n_hidden * num_directions(=2), 1]context = tf.squeeze(context, 2) # [batch_size, n_hidden * num_directions(=2)]model = tf.matmul(context, out) 当然，实际上这个模型还有更多关于损失函数、最优化器和训练过程等模块的定义，感兴趣的读者可以在 Colab 上跑一跑。 Transformer 机器之心曾解读过基于 TensorFlow 的 Transformer 代码，总体而言代码量还是比较大的，其中包括了各模块的可视化与预处理过程。对 Transformer 原理及实现代码感兴趣的读者可查阅以下文章： 基于注意力机制，机器之心带你理解与训练神经机器翻译系统 Transformer 比较重要的结构主要是经过缩放的点乘注意力和 Multi-head 注意力，其它前馈网络、位置编码等结构主要起到协助作用，它们共同可以构建 Transformer。在 Tae Hwan Jung 的实现中，他只使用了两百行代码就完成了核心过程，而且大量使用类和实例的结构更能理清整体架构。这一部分主要介绍点乘注意力和 Multi-head 注意力两个类。 首先对于点乘注意力，它率先形式化地定义了整个注意力过程，过程和上面双向 LSTM 案例使用的注意力机制基本差不多，只不过 Transformer 会有一个缩放过程。如下所示，scores 即表示模型对输入（Value/V）所加的权重，最后算出来的为上下文信息 context。 1234567891011class ScaledDotProductAttention(nn.Module): def __init__(self): super(ScaledDotProductAttention, self).__init__() def forward(self, Q, K, V, attn_mask=None): scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)] if attn_mask is not None: scores.masked_fill_(attn_mask, -1e9) attn = nn.Softmax(dim=-1)(scores) context = torch.matmul(attn, V) return context, attn 最后，下图展示了 Transformer 中所采用的 Multi-head Attention 结构，它其实就是多个点乘注意力并行地处理并最后将结果拼接在一起。一般而言，我们可以对三个输入矩阵 Q、V、K 分别进行 h 个不同的线性变换，然后分别将它们投入 h 个点乘注意力函数并拼接所有的输出结果。 最后核心的 MultiHeadAttention 同样很精简，读者可以感受一下： 123456789101112131415161718192021class MultiHeadAttention(nn.Module): def __init__(self): super(MultiHeadAttention, self).__init__() self.W_Q = nn.Linear(d_model, d_k * n_heads) self.W_K = nn.Linear(d_model, d_k * n_heads) self.W_V = nn.Linear(d_model, d_v * n_heads) def forward(self, Q, K, V, attn_mask=None): # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model] residual, batch_size = Q, Q.size(0) # (B, S, D) -proj-&gt; (B, S, D) -split-&gt; (B, S, H, W) -trans-&gt; (B, H, S, W) q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2) # q_s: [batch_size x n_heads x len_q x d_k] k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2) # k_s: [batch_size x n_heads x len_k x d_k] v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2) # v_s: [batch_size x n_heads x len_k x d_v] if attn_mask is not None: # attn_mask : [batch_size x len_q x len_k] attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k] # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)] context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask=attn_mask) context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size x len_q x n_heads * d_v] output = nn.Linear(n_heads * d_v, d_model)(context) return nn.LayerNorm(d_model)(output + residual), attn # output: [batch_size x len_q x d_model]]]></content>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow学习资源]]></title>
    <url>%2F2019%2F07%2F05%2F%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90%2FTensorflow%E8%B5%84%E6%BA%90%2F</url>
    <content type="text"><![CDATA[Tensorflowhttps://github.com/osforscience/TensorFlow-Course tensorflow清晰教程 https://zhuanlan.zhihu.com/p/47136826 TensorFlow-Examples 对初学者友好 https://github.com/aymericdamien/TensorFlow-Examples Tensorflow-101 用Jupyter Notebook编写 https://github.com/sjchoi86/Tensorflow-101 TensorFlow_Exercises 从其他TensorFlow示例重新创建代码 https://github.com/terryum/TensorFlow_Exercises LSTM-Human-Activity-Recognition 基于LSTM的TensorFlow在手机传感器数据上的递归神经网络分类 [https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition](]]></content>
      <categories>
        <category>算法学习资源</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark flatMap]]></title>
    <url>%2F2019%2F07%2F04%2Fhadoop-spark%2Fspark%2FSpark%20flatmap%2F</url>
    <content type="text"><![CDATA[一直以来没有用到flatMap，遂不清楚它的具体用法，今天遇到一个具体例子，需要将 RDD[Array[Row]]转为RDD[Row]，然后才能转为DataFrame。 用flatMap就可以解决该问题。在RDD的最后加上 1.flatMap(x =&gt; x)]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>Spark算子</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark读取数据、聚类、再转DataFrame]]></title>
    <url>%2F2019%2F07%2F04%2Fhadoop-spark%2Fspark%2FSpark%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E5%86%8D%E8%81%9A%E7%B1%BB%E5%86%8D%E8%BD%ACDataFrame%2F</url>
    <content type="text"></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>自用代码记录</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HanLP使用]]></title>
    <url>%2F2019%2F07%2F04%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FNLP%2FHanLP%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[项目在https://github.com/hankcs/HanLP 安装和配置从git上，采用第二种安装方式，即下载jar包和数据包。下载后存放的目录在/Users/david/david/code/nlp 由于自己的是Maven项目，先将jar包安装到本地的maven仓库。 12345cd /Users/david/david/code/nlp/hanlp-1.7.4-releasemvn install:install-file -DgroupId=com.hankcs -DartifactId=hanlp -Dversion=1.7.4 -Dpackaging=jar -Dfile=hanlp-1.7.4.jarmvn install:install-file -DgroupId=com.hankcs -DartifactId=hanlp -Dversion=1.7.4 -Dpackaging=jar -Dfile=hanlp-1.7.4-sources.jar -Dclassifier=sources API使用]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[【转】活体检测解决方案-简版]]></title>
    <url>%2F2019%2F07%2F04%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%93%E9%A2%98%2F%E6%B4%BB%E4%BD%93%E6%A3%80%E6%B5%8B%2F%E6%B4%BB%E4%BD%93%E6%A3%80%E6%B5%8B%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88-%E7%AE%80%E7%89%88%2F</url>
    <content type="text"><![CDATA[Liveness Detection with OpenCV 除了代码，还有一份计算机视觉方面的PDF文档，良心的。 代码下载后，在/Users/david/david/code/deep_learning/image/liveness/liveness-detection-opencv.zip PDF讲义在/Users/david/david/code/deep_learning/image/liveness/cv_dl_resource_guide.pdf www.pyimagesearch.com 这个网站也是专门用python深度学习实现各种人脸识别等图像算法的，推荐。]]></content>
      <categories>
        <category>机器学习和深度学习算法理论</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>图像相关</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F07%2F03%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%2FNeo4j%E5%A4%87%E4%BB%BD%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[Neo4j 企业版提供了一个在线备份（完整备份和增量备份）功能。 社区版不知道]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F07%2F03%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%2FNeo4j%E5%8D%95%E5%90%91%E5%92%8C%E5%8F%8C%E5%90%91%E5%85%B3%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[https://www.jianshu.com/p/a25a1907b926 Neo4j的API允许开发人员在查询的时候完全忽略关系的方向 比如，在Neo4j的查询语言Cypher中，查询Neo Technology的全部合作公司的语句可以大致如下： 1MATCH (neo)-[:PARTNER]-(partner) 查询结果会和下面两条语句执行并合并后的结果完全一致： 12MATCH (neo)-[:PARTNER]-&gt;(partner)and MATCH (neo)&lt;-[:PARTNER]-(partner) 因此，构建合作关系正确（至少是最有效）的方式就是用一个单向的关系，而且方向可以任意。]]></content>
  </entry>
  <entry>
    <title><![CDATA[【转】图数据库Neo4j实现人脉推荐——二度人脉]]></title>
    <url>%2F2019%2F07%2F02%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%2F%E5%9B%BE%E6%95%B0%E6%8D%AE%E5%BA%93Neo4j%E5%AE%9E%E7%8E%B0%E4%BA%BA%E8%84%89%E6%8E%A8%E8%8D%90%E2%80%94%E2%80%94%E4%BA%8C%E5%BA%A6%E4%BA%BA%E8%84%89%2F</url>
    <content type="text"><![CDATA[https://blog.csdn.net/LoveJavaYDJ/article/details/53491346 业务需求：通过现有系统“好友关系”和“用户通讯录”数据，实现人脉推荐——二度人脉….六度人脉 技术实现分析：关系数据库（深度关联表,算死人）图数据库（天然图关系，选择Neo4j）业务实体模型及规模 ：实体——顶点：用户（数百万）、手机号码（数亿）关系——边：好友双向关系（数亿）、手机属于某个注册用户（数百万）、用户拥有手机号码（数亿） 事例(Neo4j中只存储关系相关数据)： 12345678910111213141516171819202122232425CREATE (p1:Person &#123;uid:122622,identityType:1,personIUCode:&apos;0101&apos;&#125;)CREATE (p2:Person &#123;uid:122623,identityType:1,personIUCode:&apos;0201&apos;&#125;)CREATE (p3:Person &#123;uid:122624,identityType:2,personIUCode:&apos;0301&apos;&#125;)CREATE (m1:Mobile &#123;num:&apos;1580084xxxx&apos;&#125;)CREATE (m2:Mobile &#123;num:&apos;1680084xxxx&apos;&#125;)CREATE (m3:Mobile &#123;num:&apos;1780084xxxx&apos;&#125;)CREATE /*p1分别和p2、p3互为双向好友/ (p1)-[:Fellow]-&gt;(p2), (p2)-[:Fellow]-&gt;(p1), (p1)-[:Fellow]-&gt;(p3), (p3)-[:Fellow]-&gt;(p1), /*用户p1通讯录中有m1、m2、m3三个联系人/ (p1)-[:Has]-&gt;(m1), (p1)-[:Has]-&gt;(m2), (p1)-[:Has]-&gt;(m3), (p2)-[:Has]-&gt;(m3), /*用户p1使用手机m1注册/ (m1)-[:Belong]-&gt;(p1) 实施方案&amp;计划：第一数据：]]></content>
      <categories>
        <category>知识图谱</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F07%2F02%2Fhadoop-spark%2Fspark%2FSpark%20SQL%E4%B8%ADJoin%E5%B8%B8%E7%94%A8%E7%9A%84%E5%87%A0%E7%A7%8D%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[https://www.iteblog.com/archives/2086.html 引言Join是SQL语句中的常用操作，良好的表结构能够将数据分散在不同的表中，使其符合某种范式，减少表冗余、更新容错等。而建立表和表之间关系的最佳方式就是Join操作。 SparkSQL作为大数据领域的SQL实现，自然也对Join操作做了不少优化，今天主要看一下在SparkSQL中对于Join，常见的3种实现。 Spark SQL中Join常用的实现Broadcast Join大家知道，在数据库的常见模型中（比如星型模型或者雪花模型），表一般分为两种：事实表和维度表。维度表一般指固定的、变动较少的表，例如联系人、物品种类等，一般数据有限。而事实表一般记录流水，比如销售清单等，通常随着时间的增长不断膨胀。 因为Join操作是对两个表中key值相同的记录进行连接，在SparkSQL中，对两个表做Join最直接的方式是先根据key分区，再在每个分区中把key值相同的记录拿出来做连接操作。但这样就不可避免地涉及到shuffle，而shuffle在Spark中是比较耗时的操作，我们应该尽可能的设计Spark应用使其避免大量的shuffle。 当维度表和事实表进行Join操作时，为了避免shuffle，我们可以将大小有限的维度表的全部数据分发到每个节点上，供事实表使用。executor存储维度表的全部数据，一定程度上牺牲了空间，换取shuffle操作大量的耗时，这在SparkSQL中称作Broadcast Join，如下图所示： 如果想及时了解Spark、Hadoop或者Hbase相关的文章，欢迎关注微信公共帐号：iteblog_hadoop Table B是较小的表，黑色表示将其广播到每个executor节点上，Table A的每个partition会通过block manager取到Table A的数据。根据每条记录的Join Key取到Table B中相对应的记录，根据Join Type进行操作。这个过程比较简单，不做赘述。 Broadcast Join的条件有以下几个： 被广播的表需要小于 spark.sql.autoBroadcastJoinThreshold 所配置的值，默认是10M （或者加了broadcast join的hint） 基表不能被广播，比如 left outer join 时，只能广播左表 看起来广播是一个比较理想的方案，但它有没有缺点呢？也很明显。这个方案只能用于广播较小的表，否则数据的冗余传输就远大于shuffle的开销；另外，广播时需要将被广播的表现collect到driver端，当频繁有广播出现时，对driver的内存也是一个考验。 Shuffle Hash Join当一侧的表比较小时，我们选择将其广播出去以避免shuffle，提高性能。但因为被广播的表首先被collect到driver段，然后被冗余分发到每个executor上，所以当表比较大时，采用broadcast join会对driver端和executor端造成较大的压力。 但由于Spark是一个分布式的计算引擎，可以通过分区的形式将大批量的数据划分成n份较小的数据集进行并行计算。这种思想应用到Join上便是Shuffle Hash Join了。利用key相同必然分区相同的这个原理，SparkSQL将较大表的join分而治之，先将表划分成n个分区，再对两个表中相对应分区的数据分别进行Hash Join，这样即在一定程度上减少了driver广播一侧表的压力，也减少了executor端取整张被广播表的内存消耗。其原理如下图： 如果想及时了解Spark、Hadoop或者Hbase相关的文章，欢迎关注微信公共帐号：iteblog_hadoop Shuffle Hash Join分为两步： 对两张表分别按照join keys进行重分区，即shuffle，目的是为了让有相同join keys值的记录分到对应的分区中 对对应分区中的数据进行join，此处先将小表分区构造为一张hash表，然后根据大表分区中记录的join keys值拿出来进行匹配 Shuffle Hash Join的条件有以下几个： 分区的平均大小不超过spark.sql.autoBroadcastJoinThreshold所配置的值，默认是10M 基表不能被广播，比如left outer join时，只能广播右表 一侧的表要明显小于另外一侧，小的一侧将被广播（明显小于的定义为3倍小，此处为经验值） 我们可以看到，在一定大小的表中，SparkSQL从时空结合的角度来看，将两个表进行重新分区，并且对小表中的分区进行hash化，从而完成join。在保持一定复杂度的基础上，尽量减少driver和executor的内存压力，提升了计算时的稳定性。 Sort Merge Join上面介绍的两种实现对于一定大小的表比较适用，但当两个表都非常大时，显然无论适用哪种都会对计算内存造成很大压力。这是因为join时两者采取的都是hash join，是将一侧的数据完全加载到内存中，使用hash code取join keys值相等的记录进行连接。 当两个表都非常大时，SparkSQL采用了一种全新的方案来对表进行Join，即Sort Merge Join。这种实现方式不用将一侧数据全部加载后再进星hash join，但需要在join前将数据排序，如下图所示： 如果想及时了解Spark、Hadoop或者Hbase相关的文章，欢迎关注微信公共帐号：iteblog_hadoop 可以看到，首先将两张表按照join keys进行了重新shuffle，保证join keys值相同的记录会被分在相应的分区。分区后对每个分区内的数据进行排序，排序后再对相应的分区内的记录进行连接，如下图示： 如果想及时了解Spark、Hadoop或者Hbase相关的文章，欢迎关注微信公共帐号：iteblog_hadoop 看着很眼熟吧？也很简单，因为两个序列都是有序的，从头遍历，碰到key相同的就输出；如果不同，左边小就继续取左边，反之取右边。 可以看出，无论分区有多大，Sort Merge Join都不用把某一侧的数据全部加载到内存中，而是即用即取即丢，从而大大提升了大数据量下sql join的稳定性。 总结本文介绍了SparkSQL中的3中Join实现，其实这也不是什么新鲜玩意儿。传统DB也有这也的玩法儿，SparkSQL只是将其做成分布式的实现。 本文仅仅从大的理论方面介绍了这几种实现，具体到每个join type是怎么遍历、没有join keys时应该怎么做、这些实现对join keys有什么具体的需求，这些细节都没有展现出来。感兴趣的话，可以去翻翻源码。]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F07%2F01%2F%E6%80%BB%E7%BB%93%E4%B8%8E%E6%80%9D%E8%80%83%2F%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98-BI%E5%B7%A5%E7%A8%8B%E5%B8%88%2F</url>
    <content type="text"><![CDATA[数仓1.请简单说一下过往做过的数据仓库项目，在仓库内部数据分层是如何做的，每层的出发点是什么。2.说说你过往在做数据仓库过程中碰到的最大困难，你最终是是如何应对的？3.星型模型和雪花模型的优劣各是什么？你日常是如何使用这两种模型的？data vault4.聊聊你对于数据监控的认知和实践经验？监控的目的？监控的逻辑？监控的手段？5.任务调度工具使用过哪些，你觉得里面有没有什么不顺手的地方？如果需要改进，怎么改进？如何避免环状调度依赖？ hive1、decimal做group by2、锁表问题，锁表的表现是什么？如何解决？3、如果对任务资源做限制，用哪些参数？这些参数值调高调低的依据是什么4、hive的数据倾斜问题。数据倾斜在任务执行时的表现是什么，你一般会从什么角度来排查数据倾斜的原因？ spark1、spark常用的配置参数有哪些，设置某个参数值的依据是什么，如何验证你的设置2、spark里，聊聊你常用的算子，哪些是Transformation类型，哪些是Action类型。3、spark里，如果有一些变量要做全局告知或重复使用，该怎么做。cache和persist的区别4、有没有用过spark sql。join时小表放前面。为什么？5、GC的原因6、数据落地后小文件过多。7、并行task的数量8、宽依赖、窄依赖 streaming1、中间变量保存2、如何停止3、Structured streaming4、SparkStreaming中的Job和Spark core的JobSparkStreaming中的Job是一个应用程序，不同于Spark core中的Job。从Job的的定义来看，类似于一个Java Bean，核心是其run方法，相当于Java中线程要处理的Runnable逻辑的封装。Job是基于DStreams生成，更准确的说，基于DStreams的依赖关系graph来产生Jobs。在Spark Streaming中以时间方式触发Job，主要采用定时方式生成，也包涵其他方式(比如状态操作state对很多Batch Duration做汇总处理)。5、如何保证在遇到问题的时候数据不丢失不重复消费checkpoint的最大的弊端在于，一旦你的流式程序代码或配置改变了，或者更新迭代新功能了，这个时候，你先停旧的sparkstreaming程序，然后新的程序打包编译后执行运行，会发现两种情况： （1）启动报错，反序列化异常 （2）启动正常，但是运行的代码仍然是上一次的程序的代码。 neo4j1、分布式还是单机2、数据导入怎么做3、对比过其他图数据库 hbase1、RIT(Region in Transaction)2、region空洞3、如何查看HFile kafka问题1、zookeeper的作用，可以在没有zookeeper的情况下使用kafka吗2、什么是Rebalance，什么情况下会触发rebalance，consumer group怎么执行Rebalance的 沟通和影响力1、数仓工作需要清除了解业务需求，但业务部门也有自己的工作，如果他们表现出为你解释业务问题不是很耐烦，你要怎么解决？2、与业务部门沟通的时候遇到分歧，如何看待这种分歧，怎么解决？ 团队协作能力1、是否带领团队，如何进行任务分工？你承担的是什么角色？2、如果你来带领团队，你能举一个例子来说明你打算如何进行分工和项目推进吗3、能否描述你在什么情况下独立工作效率最高，什么情况下你在团队中效率最高？ 技术规划和推动能力1、你觉得这个岗位需要具备哪些能力，自己目前还有什么不足？你对自己未来的发展有什么规划吗？]]></content>
  </entry>
  <entry>
    <title><![CDATA[Neo4j安装配置]]></title>
    <url>%2F2019%2F07%2F01%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%2F%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Neo4j安装下载地址https://neo4j.com/download-center/#releases 解压后，放在/Users/david/david/neo4j-community-3.5.6 1、配置环境变量 12export NEO4J_HOME=&quot;/Users/david/david/neo4j-community-3.5.6&quot;export PATH=&quot;$NEO4J_HOME/bin:$PATH&quot; 2、启动服务 1neo4j start 3、浏览器访问 1http://localhost:7474/browser/ 首次进入时用户名密码都是neo4j，然后会提示输入新密码，新密码是123456 重启服务 1neo4j restart]]></content>
      <categories>
        <category>知识图谱</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F07%2F01%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%2FNeo4j%E4%BD%BF%E7%94%A8%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[如何在neo4j中创建新数据库如图片中所示，默认的DB名称是Graph.db。 在Neo4j的文档中 Community Edition is a fully functional edition of Neo4j, suitable for single instance deployments. It has full support for key Neo4j features, such as ACID compliance, Cypher, and programming APIs. It is ideal for learning Neo4j, for do-it-yourself projects, and for applications in small workgroups. 所以你只有一个数据库实例。 需在$NEO4J_HOME的conf的目录编辑neo4j.conf。 搜寻dbms.active_database=，其默认值应为graph.db。用其他名称替换它，然后再次启动neo4j。现在，将在该目录名下创建一个新数据库。若要切换回以前的db，请重复这些步骤，只需将新值替换为graph.db在配置文件中。]]></content>
  </entry>
  <entry>
    <title><![CDATA[【转】如何将大规模数据导入Neo4j]]></title>
    <url>%2F2019%2F07%2F01%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%2F%E5%A6%82%E4%BD%95%E5%B0%86%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5Neo4j%2F</url>
    <content type="text"><![CDATA[http://paradoxlife.me/how-to-insert-bulk-data-into-neo4j 项目需要基于Neo4j开发，由于数据量较大（数千万节点），因此对当前数据插入的方法进行了分析和对比。 常见数据插入方式概览 Neo4j Version Language Driver Community 3.0.2 Python neo4j-driver 1.0.0 目前主要有以下几种数据插入方式： Cypher CREATE 语句，为每一条数据写一个CREATE Cypher LOAD CSV 语句，将数据转成CSV格式，通过LOAD CSV读取数据。 官方提供的Java API —— Batch Inserter 大牛编写的 Batch Import 工具 官方提供的 neo4j-import 工具 这些工具有什么不同呢？速度如何？适用的场景分别是什么？我这里根据我个人理解，粗略地给出了一个结果： CREATE语句 LOAD CSV语句 Batch Inserter Batch Import Neo4j-import 适用场景 1 ~ 1w nodes 1w ~ 10 w nodes 千万以上 nodes 千万以上 nodes 千万以上 nodes 速度 很慢 (1000 nodes/s) 一般 (5000 nodes/s) 非常快 (数万 nodes/s) 非常快 (数万 nodes/s) 非常快 (数万 nodes/s) 优点 使用方便，可实时插入。 使用方便，可以加载本地/远程CSV；可实时插入。 速度相比于前两个，有数量级的提升 基于Batch Inserter，可以直接运行编译好的jar包；可以在已存在的数据库中导入数据 官方出品，比Batch Import占用更少的资源 缺点 速度慢 需要将数据转换成CSV 需要转成CSV；只能在JAVA中使用；且插入时必须停止neo4j 需要转成CSV；必须停止neo4j 需要转成CSV；必须停止neo4j；只能生成新的数据库，而不能在已存在的数据库中插入数据。 速度测试下面是我自己做的一些性能测试： 1. CREATE 语句这里每1000条进行一次Transaction提交 1CREATE (:label &#123;property1:value, property2:value, property3:value&#125; ) 11.5w nodes 18.5w nodes 100 s 160 s 2. LOAD CSV 语句123using periodic commit 1000load csv from &quot;file:///fscapture_screencapture_syscall.csv&quot; as linecreate (:label &#123;a:line[1], b:line[2], c:line[3], d:line[4], e:line[5], f:line[6], g:line[7], h:line[8], i:line[9], j:line[10]&#125;) 这里使用了语句USING PERIODIC COMMIT 1000，使得每1000行作为一次Transaction提交。 11.5w nodes 18.5w nodes 21 s 39 s 3. Batch Inserter、Batch Import、Neo4j-import我只测试了Neo4j-import，没有测试Batch Inserter和Batch Import，但是我估计他们的内部实现差不多，速度也处于一个数量级别上，因此这里就一概而论了。 neo4j-import需要在Neo4j所在服务器执行，因此服务器的资源影响数据导入的性能，我这里为JVM分配了16G的heap资源，确保性能达到最好。 1sudo ./bin/neo4j-import --into graph.db --nodes:label path_to_csv.csv 11.5w nodes 18.5w nodes 150w nodes + 1431w edges 3113w nodes + 7793w edges 3.4 s 3.8 s 26.5 s 3 m 48 s 结论 如果项目刚开始，想要将大量数据导入数据库，Neo4j-import是最好的选择。 如果数据库已经投入使用，并且可以容忍Neo4j关闭一段时间，那么Batch Import是最好的选择，当然如果你想自己实现，那么你应该选择Batch Inserter 如果数据库已经投入使用，且不能容忍Neo4j的临时关闭，那么LOAD CSV是最好的选择。 最后，如果只是想插入少量的数据，且不怎么在乎实时性，那么请直接看Cypher语言。 其它的Tips 在LOAD CSV前面加上USING PERIODIC COMMIT 1000，1000表示每1000行的数据进行一次Transaction提交，提升性能。 建立index可以使得查询性能得到巨大提升。如果不建立index，则需要对每个node的每一个属性进行遍历，所以比较慢。 并且index建立之后，新加入的数据都会自动编入到index中。 注意index是建立在label上的，不是在node上，所以一个node有多个label，需要对每一个label都建立index。]]></content>
      <categories>
        <category>知识图谱</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[cypher基本概念和语法]]></title>
    <url>%2F2019%2F07%2F01%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%2Fcypher%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[官方文档：https://neo4j.com/docs/cypher-refcard/3.0/ 快捷技巧结合表达式： match(n)-[r] -(b) 如果查询节点n 就 return n 如果查询关系r 就 return r 如果查询节点b 就 return b 如果查询节点n和b之间的关系r 就 return n，r，b 如果查询带条件 就 where n.x = x，r.xx = xx，b.xxx = xxx 如果修改属性 就 where..... set .... 如果删除属性 就 where..... remove ..... 如果删除节点或关系 就 where..... delete n 或者 delete r 或者 delete b 或者 delete n , r , b 检索节点12345678MATCH (n:Movie) RETURN n LIMIT 25// 这种查询时不限节点类型MATCH (tom &#123;name: &quot;冒险&quot;&#125;) RETURN tomMATCH (tom &#123;title: &quot;一曲凤求凰&quot;&#125;) RETURN tom// where条件match (p:Movie) 创建节点123// 前面这个david的部分没有意义，随便起名字// 属性中，最好有个叫name或者title的字段，这样在检索时会默认显示这个值。CREATE (david:Person &#123;name:&apos;david&apos;, borns:&apos;19869623&apos;&#125;) 建立连接12345// 没有效果，为什么呢CREATE (david) - [:ACT_I &#123;roles:[&apos;Neo&apos;]&#125;] -&gt; (珠海三雄)// 导入数据时用的方式merge (from)-[r:actedin&#123;pid:toInteger(line.pid),mid:toInteger(line.mid)&#125;]-&gt;(to) 删除所有节点和关系12MATCH (n)-[r]-(b)DELETE n,r,b Node语法在cypher里面通过用一对小括号()表示一个节点，它在cypher里面查询形式如下： 1，() 代表匹配任意一个节点 2, (node1) 代表匹配任意一个节点，并给它起了一个别名 3, (:Lable) 代表查询一个类型的数据 4, (person:Lable) 代表查询一个类型的数据，并给它起了一个别名 5, (person:Lable {name:”小王”}) 查询某个类型下，节点属性满足某个值的数据 6, (person:Lable {name:”小王”,age:23}) 节点的属性可以同时存在多个，是一个AND的关系 关系语法关系用一对-组成，关系分有方向的进和出，如果是无方向就是进和出都查询 1,—&gt; 指向一个节点 2,-[role]-&gt; 给关系加个别名 3,-[:acted_in]-&gt; 访问某一类关系 4,-[role:acted_in]-&gt; 访问某一类关系，并加了别名 5,-[role:acted_in {roles:[“neo”,”hadoop”]}]-&gt; 访问某一类关系下的某个属性的关系的数据 带属性的关系123match(n),(b) where n.label=&apos;科比&apos; and b.label=&apos;奥尼尔&apos;create(n)-[r:搭档&#123;since:1996,des:&apos;NBA史上最强OK组合&apos;,champion:3&#125;]-&gt;(b) return n,r,b 模式语法模式语法是节点和关系查询语法的结合，通过模式语法我们可以进行我们想要的任意复杂的查询 12// 这里的Actor是什么(p1: Person:Actor &#123;name:&quot;tom&quot;&#125;)-[role:acted_in &#123;roles:[&quot;neo&quot;,&quot;actor&quot;]&#125;]-(m1:Movie &#123;title:&quot;water&quot;&#125;) 模式变量为了增加模块化和减少重复，cypher允许把模式的结果指定在一个变量或者别名中，方便后续使用或操作 1path = (: Person)-[:ACTED_IN]-&gt;(:Movie) path是结果集的抽象封装，有多个函数可以直接从path里面提取数据如： 12345nodes(path)：提取所有的节点rels(path): 提取所有的关系 和relationships(path)相等length(path): 获取路径长度 条件cypher语句也是由多个关键词组成，像SQL的 select name, count() from talbe where age=24 group by name having count() &gt;2 order by count(*) desc多个关键字组成的语法，cypher也非常类似，每个关键词会执行一个特定的task来处理数据 match: 查询的主要关键词 create: 类似sql里面的insert filter，project，sort，page等都有对应的功能语句 通过组合上面的一些语句，我们可以写出非常强大复杂的语法，来查询我们想要检索的内容，cypher会 自动解析语法并优化执行。 创建1create (:Movie &#123;title:&quot;驴得水&quot;,released:2016&#125;) return p; 执行成功，在neo4j的web页面我们能看到下面的信息 +—————————-+| No data returned. |+—————————-+Nodes created: 1Properties set: 2Labels added: 1当然cypher也可以一次创建多个数据，并同时添加关系 查询1234567match (p: Person) return p; 查询Person类型的所有数据match (p: Person &#123;name:&quot;sun&quot;&#125;) return p; 查询名字等于sun的人match( p1: Person &#123;name:&quot;sun&quot;&#125; )-[rel:friend]-&gt;(p2) return p2.name , p2.age 查询sun的朋友的名字和年龄match (old) ... create (new) create (old)-[rel:dr]-&gt;(new) return new 对已经存在的节点和新建的节点建立关系 用ID查询12// 不能用n.id。这里的ID是neo4j默认的match(n) where ID(n) = 21798 return n 模糊查询1match(n) where n.label=~&apos;科*.&apos; return n 更新merge 语法可以对已经存在的节点做改变，对变化的部分会合并 1234567MERGE (m:Movie &#123; title:&quot;Cloud Atlas&quot; &#125;) ON CREATE SET m.released = 2012 RETURN m// 执行以下语句没效果，没有增加realeased属性。把on CREATE去掉就可以了。MERGE (m:Person &#123; person_id:&quot;111&quot; &#125;) ON CREATE SET m.released = 2012 RETURN mMERGE (m:Person &#123; person_id:&quot;111&quot; &#125;) SET m.released = 2012 RETURN m merge …. on create set … return 语法支持合并更新 更新属性1match(n) where ID(n) = 21798 set n.label=&apos;科比&apos;,n.height=198,n.position=&apos;得分后卫&apos; return n 修改关系属性12match(n)-[r]-(b) where ID(r) = 12513 set r.des=&apos;小飞侠&amp;大鲨鱼&apos; return n,r,b 删除属性1match(n) where ID(n) = 21798 remove n.height return n 删除节点1match(n:洛杉矶湖人) where ID(n) = 21836 delete n 删除关系123match(n)-[r]-(b) where n.label=&apos;科比&apos; and b.label=&apos;奥尼尔&apos;delete rreturn r 现有节点创建关系https://www.quackit.com/neo4j/tutorial/neo4j_create_a_relationship_using_cypher.cfm 1234MATCH (a:APP),(b:Person)WHERE a.app_code = &quot;F19999&quot; AND b.person_id = &quot;113&quot;CREATE (a)-[r:app_person]-&gt;(b)RETURN r 多个关系的创建1234MATCH (a:Artist),(b:Album),(p:Person)WHERE a.Name = &quot;Strapping Young Lad&quot; AND b.Name = &quot;Heavy as a Really Heavy Thing&quot; AND p.Name = &quot;Devin Townsend&quot; CREATE (p)-[pr:PRODUCED]-&gt;(b), (p)-[pf:PERFORMED_ON]-&gt;(b), (p)-[pl:PLAYS_IN]-&gt;(a)RETURN a,b,p 筛选过滤cypher过滤也是用的和SQL一样的关键词where 1match (p1: Person) where p1.name=&quot;sun&quot; return p1; 等同下面的 1match (p1: Person &#123;name:&quot;sun&quot;&#125;) return p1 注意where条件里面支持 and ， or ，xor，not等boolean运算符，在json串里面都是and 除此之外，where里面查询还支持正则查询 123match (p1: Person)-[r:friend]-&gt;(p2: Person)where p1.name=~&quot;K.+&quot; or p2.age=24 or &quot;neo&quot; in r.relsreturn p1,r,p2 关系过滤匹配使用not 1MATCH (p:Person)-[:ACTED_IN]-&gt;(m) WHERE NOT (p)-[:DIRECTED]-&gt;() RETURN p,m 结果集返回12// 结果集返回做去重MATCH (p:Person) RETURN p, p.name AS name, upper(p.name), coalesce(p.nickname,&quot;n/a&quot;) AS nickname, &#123; name: p.name, label:head(labels(p))&#125; AS person 1match (n) return distinct n.name; 聚合函数cypher支持count,sum,avg,min,max 1match (: Person) return count(*) 聚合的时候null会被跳过 count 语法 支持 count( distinct role ) 1MATCH (actor:Person)-[:ACTED_IN]-&gt;(movie:Movie)&lt;-[:DIRECTED]-(director:Person)RETURN actor,director,count(*) AS collaborations 排序和分页1MATCH (a:Person)-[:ACTED_IN]-&gt;(m:Movie) RETURN a,count() AS appearances ORDER BY appearances DESC SKIP 3 LIMIT 10; 收集聚合结果12MATCH (m:Movie)&lt;-[:ACTED_IN]-(a:Person)RETURN m.title AS movie, collect(a.name) AS cast, count(*) AS actors union 联合支持两个查询结构集一样的结果合并 12345MATCH (actor:Person)-[r:ACTED_IN]-&gt;(movie:Movie)RETURN actor.name AS name, type(r) AS acted_in, movie.title AS titleUNION （ALL）MATCH (director:Person)-[r:DIRECTED]-&gt;(movie:Movie)RETURN director.name AS name, type(r) AS acted_in, movie.title AS title withwith语句给cypher提供了强大的pipeline能力，可以一个或者query的输出，或者下一个query的输入 和return语句非常类似，唯一不同的是，with的每一个结果，必须使用别名标识。 通过这个功能，我们可以轻而易举的做到在查询结果里面在继续嵌套查询。 123MATCH (person:Person)-[:ACTED_IN]-&gt;(m:Movie)WITH person, count(*) AS appearances, collect(m.title) AS moviesWHERE appearances &gt; 1 RETURN person.name, appearances, movies 注意在SQL里面，我们想过滤聚合结果，需要使用having语句但是在cypher里面我们可以配合with语句使用 where关键词来完成过滤 添加约束或者索引唯一约束(使用merge来实现) 1CREATE CONSTRAINT ON (movie:Movie) ASSERT movie.title IS UNIQUE 添加索引(在图谱遍历时，快速找到开始节点),大幅提高查询遍历性能 CREATE INDEX ON :Actor(name) 添加测试数据： 12CREATE (actor:Actor &#123; name:&quot;Tom Hanks&quot; &#125;),(movie:Movie &#123; title:‘Sleepless IN Seattle‘ &#125;),(actor)-[:ACTED_IN]-&gt;(movie); 使用索引查询: 1MATCH (actor:Actor &#123; name: &quot;Tom Hanks&quot; &#125;) RETURN actor; 地理信息转自：neo4j 3.4根据经纬度计算两点之间的距离 https://neo4j.com/docs/developer-manual/current/cypher/functions/spatial/ 123456789create(l:Location) set l.latitude=toFloat(&apos;55.612151&apos;),l.longitude=toFloat(&apos;12.995090&apos;) return lwith point(&#123;latitude:55.612,longitude:12.995&#125;) as poimatch (l:Location) where l.latitude=55.612151return distance(point(&#123;latitude:l.latitude,longitude: l.longitude&#125;),poi) as distincewith point(&#123;latitude:55.612,longitude:12.995&#125;) as poimatch (l:Location) where l.latitude=55.612151 and distance(point(&#123;latitude:l.latitude,longitude: l.longitude&#125;),poi)&lt;20return distance(point(&#123;latitude:l.latitude,longitude: l.longitude&#125;),poi) as distince cypher 实现， 节点上存lat，lng属性，我用cypher重写了 谷歌的 地理位置距离算法，你可以在cypher的where语句里，用下面方法来找 500米以内的 节点 12345678// cypher有自带的，目前应该用不到下面的round( 6378.137 *1000*2* asin(sqrt( sin((radians(start.lat)-radians(&#123;start_lat&#125;))/2)^2+cos(radians(start.lat))*cos(radians(&#123;start_lat&#125;))* sin((radians(start.lng)-radians(&#123;start_lng&#125;))/2)^2 )) )&lt;500]]></content>
      <categories>
        <category>知识图谱</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[【转】Neo4j以电影数据为例的教程]]></title>
    <url>%2F2019%2F06%2F29%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%2F%E6%95%99%E7%A8%8B-%E7%94%B5%E5%BD%B1%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[关于数据执行/Users/david/david/code/neo4j/movie_sample/movie_data_import.sql 表格有 genre电影类型和id movie每个电影数据 movie_to_genre电影和类型的映射关系 person电影演员信息 person_to_movie电影演员和电影的映射关系 2、导出CSV 3、导入neo4j neo4j的默认导入路径是/Users/david/david/neo4j-community-3.5.6/import 把csv复制到这里。 123456789101112131415161718192021222324252627282930313233343536//导入节点 电影类型 == 注意类型转换LOAD CSV WITH HEADERS FROM &quot;file:///genre.csv&quot; AS lineMERGE (p:Genre&#123;gid:toInteger(line.gid),name:line.gname&#125;) //导入节点 演员信息 LOAD CSV WITH HEADERS FROM &apos;file:///person.csv&apos; AS lineMERGE (p:Person &#123; pid:toInteger(line.pid),birth:line.birth,death:line.death,name:line.name,engname:line.engname,biography:line.biography,birthplace:line.birthplace&#125;)// 导入节点 电影信息LOAD CSV WITH HEADERS FROM &quot;file:///movie.csv&quot; AS line MERGE (p:Movie&#123;mid:toInteger(line.mid),title:line.title,introduction:line.introduction,rating:toFloat(line.rating),releasedate:line.releasedate&#125;)// 导入关系 actedin 电影是谁参演的 1对多LOAD CSV WITH HEADERS FROM &quot;file:///person_to_movie.csv&quot; AS line match (from:Person&#123;pid:toInteger(line.pid)&#125;),(to:Movie&#123;mid:toInteger(line.mid)&#125;) merge (from)-[r:actedin&#123;pid:toInteger(line.pid),mid:toInteger(line.mid)&#125;]-&gt;(to) //导入关系 电影是什么类型 == 1对多LOAD CSV WITH HEADERS FROM &quot;file:///movie_to_genre.csv&quot; AS linematch (from:Movie&#123;mid:toInteger(line.mid)&#125;),(to:Genre&#123;gid:toInteger(line.gid)&#125;) merge (from)-[r:is&#123;mid:toInteger(line.mid),gid:toInteger(line.gid)&#125;]-&gt;(to)-- 问：章子怡都演了哪些电影？match(n:Person)-[:actedin]-&gt;(m:Movie) where n.name=&apos;章子怡&apos; return m.title-- 删除所有的节点及关系MATCH (n)-[r]-(b)DELETE n,r,b 导入时可能会报错 1Neo.ClientError.Statement.SemanticError: Cannot merge node using null property value for name 可能是csv中这一列有空值，我的做法是把空值改成空格，解决。 查询我们利用cypher语句查询一下关系actedin有哪些 1match (n)-[r:actedin]-(b) return n,r,b limit 10 参考： 基于电影知识图谱的智能问答系统（二） — Neo4j导入CSV文件 基于电影知识图谱的智能问答系统（一） — Mysql数据准备 基于电影知识图谱的智能问答系统（三） — Spark环境搭建 基于电影知识图谱的智能问答系统（四） —HanLP分词器]]></content>
      <categories>
        <category>知识图谱</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F06%2F27%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%93%E9%A2%98%2F%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%2F%E5%BC%80%E6%BA%90%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[人脸验证Face Identification / Face Verification 查阅开源的人脸识别技术方案，暂定使用如下三种方案： ​ https://github.com/cmusatyalab/openface 基于FaceNet- A Unified Embedding for Face Recognition and Clustering ​ https://github.com/ageitgey/face_recognition#face-recognition ​ https://github.com/deepinsight/insightface 基于ArcFace: Additive Angular Margin Loss for Deep Face Recognition]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F06%2F26%2Fhadoop-spark%2Fspark%2F%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F%E5%92%8Clocal%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[local模式可用于debug 1234567891011121314151617181920212223242526272829// 集群模式@transientval conf = new SparkConf().setAppName(&quot;MetricSimilarity&quot;).set(&quot;spark.sql.shuffle.partitions&quot;, &quot;500&quot;).set(&quot;spark.default.parallelism&quot;, &quot;500&quot;).set(&quot;spark.shuffle.consolidateFiles&quot;, &quot;true&quot;)@transientval sc = new SparkContext(conf)@transientval hiveContext = new HiveContext(sc)hiveContext.sql(&quot;SET mapreduce.job.reduces=40&quot;)val hiveSql = HiveHandler.getExecuteSql(&quot;wx.trajectory.select&quot;, List(currentDateStr))println(hiveSql)val df = hiveContext.sql(hiveSql)// local模式// val conf = new SparkConf().setAppName(&quot;LocalSpark&quot;).set(&quot;spark.default.parallelism&quot;, &quot;8&quot;).setMaster(&quot;local[1]&quot;)// val sc = new SparkContext(conf)// val hiveContext = new HiveContext(sc)// val df = readCsv(sc, &quot;/Users/david/Downloads/app_traj.csv&quot;)//读取csv，由于HUE下载的数据表头带有表名，需要去掉。例如stg_d001_t_jxl_behavior.call_cnt --&gt;&gt; call_cnt def readCsv(sc:SparkContext, csvPath:String)=&#123; val sqlContext = new SQLContext(sc) //这里如果在csv第一行有属性的话，没有就是&quot;false&quot; val dataset = sqlContext.read.format(&quot;com.databricks.spark.csv&quot;) .option(&quot;header&quot;,&quot;true&quot;) //.option(&quot;inferSchema&quot;,true.toString)//这是自动推断属性列的数据类型。不要打开 .load(csvPath)//文件的路径 dataset &#125;]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F06%2F26%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%93%E9%A2%98%2FOPTICS%2F%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[OPTICS聚类算法 代码：https://github.com/amyxzhang/OPTICS-Automatic-Clustering 核心距离给若干个邻域参数，找到满足这个参数的最小半径？ 可达距离]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F06%2F24%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%93%E9%A2%98%2F%E5%9B%BE%E7%AE%97%E6%B3%95%2F%E7%A4%BE%E5%8C%BA%E5%8F%91%E7%8E%B0%2F%E6%96%87%E7%AB%A0%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[Fast Unfolding 社区发现算法FastUnfolding的GraphX实现]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F06%2F22%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%93%E9%A2%98%2FBP%2F%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%B2%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[转自：神经网络BP反向传播算法原理和详细推导流程 这篇还算讲的比较清楚的。后面我需要手动推一遍公式 本文的记号说明： 下面以三层感知器(即只含有一个隐藏层的多层感知器)为例介绍“反向传播算法(BP 算法)”。 2 信息前向传播 3 误差反向传播 3.1 输出层的权重参数更新 3.2 隐藏层的权重参数更新 3.3输出层和隐藏层的偏置参数更新 3.4 BP算法四个核心公式 3.5 BP 算法计算某个训练数据的代价函数对参数的偏导数]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F06%2F22%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2FPytorch%E7%AC%94%E8%AE%B0%2F%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5-Tensor%2F</url>
    <content type="text"><![CDATA[张量操作 其实 torch 中 tensor 的运算和 numpy array 的如出一辙, 我们就以对比的形式来看. 如果想了解 torch 中其它更多有用的运算符, API就是你要去的地方. 12345678910111213141516171819202122# abs 绝对值计算data = [-1, -2, 1, 2]tensor = torch.FloatTensor(data) # 转换成32位浮点 tensorprint( '\nabs', '\nnumpy: ', np.abs(data), # [1 2 1 2] '\ntorch: ', torch.abs(tensor) # [1 2 1 2])# sin 三角函数 sinprint( '\nsin', '\nnumpy: ', np.sin(data), # [-0.84147098 -0.90929743 0.84147098 0.90929743] '\ntorch: ', torch.sin(tensor) # [-0.8415 -0.9093 0.8415 0.9093])# mean 均值print( '\nmean', '\nnumpy: ', np.mean(data), # 0.0 '\ntorch: ', torch.mean(tensor) # 0.0) 123456789101112131415# 若obj是一个pytorch的tensor，则返回truetorch.is_tensor(obj)# 判断obj是否是storage# torch.Storage是单个数据类型的连续的一维数组，每个torch.Tensor都具有相同数据类型的相应存储。他是torch.tensor底层数据结构,他除了像Tensor一样定义数值，还可以直接把文件映射到内存中进行操作torch.is_storage(obj)# 设置默认的tensor的数据类型torch.set_default_tensor_type(torch.DoubleTensor)# 返回input张量中的元素个数torch.numel(input) -&gt; int# 设置打印选项，与numpy一致torch.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None)]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F06%2F22%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2FPytorch%E7%AC%94%E8%AE%B0%2F%E9%9D%99%E6%80%81%E5%9B%BE%E5%92%8C%E5%8A%A8%E6%80%81%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[深度学习框架PyTorch-入门与实践_陈云(著) .pdf Pytorch实现 tensorflow实现]]></content>
  </entry>
  <entry>
    <title><![CDATA[用户行为模式挖掘方案]]></title>
    <url>%2F2019%2F06%2F21%2F%E5%9C%B0%E7%90%86%E4%BF%A1%E6%81%AF%E6%8C%96%E6%8E%98%2F%E8%A1%8C%E4%B8%BA%E6%A8%A1%E5%BC%8F%E6%8C%96%E6%8E%98%2F%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%A8%A1%E5%BC%8F%E6%8C%96%E6%8E%98%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[用户行为模式挖掘方案目的不同于异常检测，行为模式侧重于挖掘用户的行驶或停留习惯，可用于客户分类、人群画像、以及对个人生活习惯的描述。这些信息同时也能作为反欺诈和异常检测的有效手段。 1、第一层模型：停留点识别（已实现）有线用停留区表，无线用每天上报的轨迹点。 2、第二层模型：用户热点地址识别[1,2]将分散的停留点聚合为有意义的地点，对所有合同形成维护一张停留地址表，包括停留点编号、聚类出的每个停留中心点、停留区的凸包边界和地址信息。 这里的关键问题是聚类算法。 1）DBSCAN是一种参数敏感的聚类算法，不同参数会对结果有不同影响。论文中主流的方案是采用OPTICS聚类。 2）因为数据每天在更新，要实现一种增量聚类的策略。 比如可以设置邻域半径100米，MinPts是5个，且停留次数超过5次的作为参数，挖掘用户热点地址。 3、第三层模型：基于热点地址的行为模式3.1 个人兴趣点和公共兴趣点[2]所有的用户热点地址可以分为两类： 1）个人兴趣点：例如居住地、工作地。 2）公共兴趣点：例如饭店、车行等POI。 个人兴趣点采用一种新的居住地工作地算法。 其中，$i$表示某个用户，$SPk^i$是i用户的第k个停留地址，$s{kj}^i$是第k个停留地址中的第j个停留点。$STIp=[t{arrive}, t_{leave}]$是自定义的标准时间间隔（standard time intervals）。 比如，可以定义居住地是$STIp=[00:00, 07:00] \cup [20:00, 24:00] $，工作地是$STI_p=[07:00, 20:00]{workday}$。 这个公式的好处是，对居住和工作的具体时间段不敏感，而之前的算法需要定一个严格的时间范围，再找常停留地。 公共兴趣点排除个人兴趣点后，剩余的就是公共兴趣点。对$SP_k^i$，找出属于它的停留点的POI，选频度最高的。但是由于地图API的限制，用$SP_k^i$的中心点来找POI也可以。 3.2 用户相似性[2]基于的假设是，如果两个用户的热点地址在时间和空间上相似，则这两个用户也是相似的。 相似度按照不同的兴趣点类型分别对比，比如居住地和居住地比， 不同类别的热点权重不同。 用户相似性计算公式 其中，$w{ps}$是权重，不同类别的热点权重不同（或者相同，看分析的目的）。$Sim{ps}(a, b)$是a和b用户在时间和空间上的相似度的综合。 其中，$w_1$和$w_2$是空间相似度和时间相似度的权重。时空相似度定义为 分母是两个用户的热点地址数之和。分子是 即同类型的热点地址，比较距离是否小于阈值。这里又分为： 1）空间距离：求热点地址中心点的经纬度距离。阈值假设可以设为400米。 2）时间距离：求同类型热点地址在时间上分布的KL散度。阈值假设可以设为0.3 热点地址的时间分布例如 3.3 周期行为分析[1]可以生成某合同在最近一期的行驶报告。 在上一期， 高频地点是XX，有k%的日期会去，经常在几点到几点去，每次去平均都要停留N小时。该地点在历史上出现次数为M次。 工作日和周末有不同的偏好，工作日更常去XX，XX，周末更常去XX 有k%的日期，都会去A、B、C。（FP-Growth） 有k%的日期，行驶路线包括A-&gt;B-&gt;C，从A出发的时间大多在几点到几点，到达的C的时间大多在几点到几点。 3.4 频繁区域发现[3] 系统中，会与地图结合起来展示。 再通过社区发现（Community Detection）算法，找到用户出现最频繁的区域（Activity Area）。 参考文献： [1] Ye Y, Zheng Y, Chen Y, et al. Mining individual life pattern based on location history[C]//2009 tenth international conference on mobile data management: Systems, services and middleware. IEEE, 2009: 1-10. [2] Yang M, Cheng C, Chen B. Mining Individual Similarity by Assessing Interactions with Personally Significant Places from GPS Trajectories[J]. ISPRS International Journal of Geo-Information, 2018, 7(3): 126. [3] Zhao S, Zhao Z, Huang R, et al. Discovering Individual Life Style From Anonymized WiFi Scan Lists on Smartphones[J]. IEEE Access, 2019, 7: 22698-22709]]></content>
      <categories>
        <category>地理信息处理和挖掘</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[行为模式挖掘论文整理]]></title>
    <url>%2F2019%2F06%2F20%2F%E5%9C%B0%E7%90%86%E4%BF%A1%E6%81%AF%E6%8C%96%E6%8E%98%2F%E8%A1%8C%E4%B8%BA%E6%A8%A1%E5%BC%8F%E6%8C%96%E6%8E%98%2F%E8%AE%BA%E6%96%87%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[综述 吉根林, 赵斌. 时空轨迹大数据模式挖掘研究进展[J]. 数据采集与处理, 2015, 30(1): 47-58. 提到了频繁模式挖掘，伴随模式挖掘，聚集模式挖掘等。 行为模式挖掘Ye Y, Zheng Y, Chen Y, et al. Mining individual life pattern based on location history[C]//2009 tenth international conference on mobile data management: Systems, services and middleware. IEEE, 2009: 1-10. 被引244次。讲通过找兴趣点等。 1）第一层建模，获取停留点 2）第二层建模，停留点=&gt; 历史位置序列 3）时序抽样，找到特定日期的行为 4）挖掘非时序的行为模式，频繁项集。 5）挖掘时序的行为模式。从频繁项目里面找出有时序的 6）找出一些规则 如何通过增量聚类来更新历史区域。 Zhao S, Zhao Z, Huang R, et al. Discovering Individual Life Style From Anonymized WiFi Scan Lists on Smartphones[J]. IEEE Access, 2019, 7: 22698-22709. 用图来找出用户经过的区域的关系 用户在地点之间的变迁可以表示为无向图中的权重。 从P到P+1的点，构成一条弧，次数就是弧的权重。点的大小是平均停留时长。点的编号就是停留区域的编号。 这张图可以得到如下信息： 1）少数点size大，说明A到过很多地方，但经常待的只有几个。 2）71和14最大，说明最常停留的是这两个点。 3）很多点会连接到71（71的degree最大），说明71是核心停留点。 4）71和66，71和14的edge最粗，说明是最频繁的往返点。 把这个图跟地图结合起来？ 接下来用社区发现（Community Detection ）算法这样可以知道每个人的activity area的数量。 再从activity area发现行为模式。 1）计算活跃度。统计在activity area中次数的均值。 搞了一堆统计 Andrade T, Gama J. Identifying Points of Interest and Similar Individuals from Raw GPS Data[J]. arXiv preprint arXiv:1904.09357, 2019.用户相似性的定义：太简单了，就是看A和B的停留点的重复数，然后算jaccard系数 Yang M, Cheng C, Chen B. Mining Individual Similarity by Assessing Interactions with Personally Significant Places from GPS Trajectories[J]. ISPRS International Journal of Geo-Information, 2018, 7(3): 126.SCI 关于Individual Significant Places，大多数用逆编码。文章除了POI，还考虑了用户的个人兴趣点（比如家）。 个人兴趣点，比如居住地和工作地，有高频的访问和长时间停留。 提出居住地工作地的新算法 提出一种相似度计算的算法 Naserian E, Wang X, Dahal K, et al. Personalized location prediction for group travellers from spatial–temporal trajectories[J]. Future Generation Computer Systems, 2018, 83: 278-292. Parent C, Spaccapietra S, Renso C, et al. Semantic trajectories modeling and analysis[J]. ACM Computing Surveys (CSUR), 2013, 45(4): 42. 被引381 轨迹的语义 完成常驻地列表、中心经纬度的指标加工 其他文献]]></content>
      <categories>
        <category>地理信息处理和挖掘</category>
      </categories>
      <tags>
        <tag>论文整理</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F06%2F20%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%93%E9%A2%98%2F%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%2F%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%9F%BA%E6%9C%AC%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[隐马尔可夫模型的三个基本问题https://zhuanlan.zhihu.com/p/35169365 隐马尔科夫模型的三个基本问题是： （1）评估问题；（2）模型学习问题；（3）解码/预测问题 不同的问题有不同的算法！ （1）评估问题：也即概率计算问题。在给定模型和观测序列的条件下，计算在给定模型下观测序列出现的概率。 前向、后向算法用于解决评估问题，也就是说，在给定模型下，求某观测序列出现的概率，用于评估该观测序列最匹配的模型。 （2）模型学习问题：也即参数估计问题，具体是指，用已知的观测序列去估计模型中的参数，使得在给定模型下观测序列出现的概率最大，采用的方法为极大似然估计方法。 Baum-Welchs算法用于解决模型学习问题，即参数估计，它是zy一种无监督的训练方法，主要通过EM迭代来实现。 （3）解码/预测问题：已知模型和观测序列，在给定的观测序列下，求其最可能对应的状态序列。 维特比算法用于解决的是给定一个模型和某个特定的输出序列下，求最可能产生这个特定的输出序列对应的状态序列。 参考 一文搞懂HMM（隐马尔可夫模型）和HMM模型相关的算法主要分为三类，分别解决三种问题： 1）知道骰子有几种（隐含状态数量），每种骰子是什么（转换概率），根据掷骰子掷出的结果（可见状态链），我想知道每次掷出来的都是哪种骰子（隐含状态链）。 这个问题呢，在语音识别领域呢，叫做解码问题。这个问题其实有两种解法，会给出两个不同的答案。每个答案都对，只不过这些答案的意义不一样。第一种解法求最大似然状态路径，说通俗点呢，就是我求一串骰子序列，这串骰子序列产生观测结果的概率最大。第二种解法呢，就不是求一组骰子序列了，而是求每次掷出的骰子分别是某种骰子的概率。比如说我看到结果后，我可以求得第一次掷骰子是D4的概率是0.5，D6的概率是0.3，D8的概率是0.2.第一种解法我会在下面说到，但是第二种解法我就不写在这里了，如果大家有兴趣，我们另开一个问题继续写吧。 2）还是知道骰子有几种 （隐含状态数量） ，每种骰子是什么 （转换概率） ，根据掷骰子掷出的结果 （可见状态链） ，我想知道掷出这个结果的概率。 看似这个问题意义不大，因为你掷出来的结果很多时候都对应了一个比较大的概率。问这个问题的目的呢，其实是检测观察到的结果和已知的模型是否吻合。如果很多次结果都对应了比较小的概率，那么就说明我们已知的模型很有可能是错的，有人偷偷把我们的骰子給换了。 3）知道骰子有几种 （隐含状态数量） ，不知道每种骰子是什么 （转换概率） ，观测到很多次掷骰子的结果 （可见状态链） ，我想反推出每种骰子是什么 （转换概率）** 。 这个问题很重要，因为这是最常见的情况。很多时候我们只有可见结果，不知道HMM模型里的参数，我们需要从可见结果估计出这些参数，这是建模的一个必要步骤。]]></content>
  </entry>
  <entry>
    <title><![CDATA[【转】Spark TopK问题解决]]></title>
    <url>%2F2019%2F06%2F19%2Fhadoop-spark%2Fspark%2FTOP%20K%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[Spark TopK 问题解决-使用最小堆https://blog.csdn.net/yuzx2008/article/details/50732151 整个排序取 TopK 的实现： 12345678910111213141516171819202122object TopK0 &#123; val K = 3 def main(args: Array[String]) &#123; // 执行 wordcount val conf = new SparkConf().setAppName(&quot;TopK0&quot;) val spark = new SparkContext(conf) val textRDD = spark.textFile(&quot;hdfs://10.0.8.162:9000/home/yuzx/input/wordcount.txt&quot;) textRDD.flatMap(line =&gt; line.split(&quot; &quot;)) .map(word =&gt; (word, 1)) .reduceByKey(_ + _) .map &#123; pair =&gt; pair.swap &#125; .sortByKey(true, 2) .top(3) .foreach(println) spark.stop() &#125;&#125; 基于最小堆的实现： 最大/小堆，对应的数据结构优先级队列，PriorityQueue，不光 Java 中有，Scala 中也有，当然 c++ 中也有 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263object TopK &#123; val K = 3 val ord = Ordering.by[(String, Int), Int](_._2).reverse def main(args: Array[String]) &#123; // 执行 wordcount val conf = new SparkConf().setAppName("TopK") val spark = new SparkContext(conf) val textRDD = spark.textFile("hdfs://10.0.8.162:9000/home/yuzx/input/wordcount.txt") val countRes = textRDD.flatMap(line =&gt; line.split(" ")).map(word =&gt; (word, 1)).reduceByKey(_ + _) // debug mapreduce 的结果 countRes.foreach(println) /* 每个 RDD 分区内进行 TOP K 计算 需要每个分区内有自己的桶，如果整个程序使用一个 heap（将 heap 设定为成员变量） 会不正确 为什么呢？ */ val topk = countRes.mapPartitions(iter =&gt; &#123; val heap = new mutable.PriorityQueue[(String, Int)]()(ord) while (iter.hasNext) &#123; val n = iter.next println("分区计算：" + n) putToHeap(heap, n) &#125; heap.iterator &#125;).collect() println("分区结果：") topk.foreach(println) // 每个分区的 TOP K 合并，计算总的 TopK val heap = new mutable.PriorityQueue[(String, Int)]()(ord) val iter = topk.iterator while (iter.hasNext) &#123; putToHeap(heap, iter.next) &#125; println("最终结果：") while (heap.nonEmpty) &#123; println(heap.dequeue()) &#125; spark.stop() &#125; def putToHeap(heap: mutable.PriorityQueue[(String, Int)], iter: (String, Int)): Unit = &#123; if (heap.nonEmpty &amp;&amp; heap.size &gt;= K) &#123; if (heap.head._2 &lt; iter._2) &#123; heap += iter heap.dequeue() &#125; &#125; else &#123; heap += iter &#125; &#125;&#125;]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>数据结构算法</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F06%2F18%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%2FCH3%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[代码在 /Users/david/david/git/dl/deep-learning-from-scratch 激活函数sigmoid12def sigmoid(x): return 1 / (1 + np.exp(-x)) x可以是标量，可以是np.array]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F06%2F18%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%2FCH2%20%E6%84%9F%E7%9F%A5%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[代码在 /Users/david/david/git/dl/deep-learning-from-scratch 实现与门、与非门和或门，在此基础上实现了感知机。 与门、与非门和或门 与感知机的思想的类似的，对任意的输入，最终变成0和1的输出，在输入到输出的时候还会带上权重和偏置。 感知机和异或门感知机的局限性，单层感知机是无法实现异或门的。 why？ 比如与门、与非门和或门，在空间的划分都是直线，例如 可以用单层感知机实现对0和1的分类，只要定义好公式和权重。 而异或门对于空间的划分 是非线性的。 若要实现异或门，必须上多层感知机。 12345def XOR(x1, x2): s1 = NAND(x1, x2) s2 = OR(x1, x2) y = AND(s1, s2) return y P59]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F06%2F16%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2FLeeCode%2Fweek4-1%20%E9%AA%8C%E8%AF%81%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%2F</url>
    <content type="text"><![CDATA[https://blog.csdn.net/qq_30490125/article/details/53135274 错误解答 12345678910def isValidBST(self, root: TreeNode) -&gt; bool: if root is None: return True if root.left is not None and root.left.val &gt; root.val: return False if root.right is not None and root.right.val &lt; root.val: return False if not self.isValidBST(root.left) or not self.isValidBST(root.right): return False return True 反例 方法1： 中序遍历，判断是否是升序]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F06%2F16%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F%E4%BC%98%E8%B4%A8git%E8%B5%84%E6%BA%90%2F</url>
    <content type="text"><![CDATA[深度学习500问以问答形式对常用的概率知识、线性代数、机器学习、深度学习、计算机视觉等热点问题进行阐述，以帮助自己及有需要的读者。 全书分为18个章节，50余万字。由于水平有限，书中不妥之处恳请广大读者批评指正。 https://github.com/scutan90/DeepLearning-500-questions]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F06%2F16%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2FPytorch%E7%AC%94%E8%AE%B0%2FPytorch%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[1、libiomp5.dylib already initialized12OMP: Error #15: Initializing libiomp5.dylib, but found libomp.dylib already initialized.OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://www.intel.com/software/products/support/. 解决方案 1os.environ[&quot;KMP_DUPLICATE_LIB_OK&quot;]=&quot;TRUE&quot;]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F06%2F15%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2FPytorch%E8%AE%AD%E7%BB%83%E8%90%A5%2FD5%20%E6%9E%84%E5%BB%BA%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[第二周Day11-Day12 任务名称：构建网络模型(包括torch.nn和torch.nn.functional操作[参考文档：(1)pytorch官方文档torch.nn和torch.nn.function(2)PyTorchtutorial_0.0.5余霆嵩文档17~29页) 任务简介：构建网络模型是学习pytorch框架最重要的内容 详细说明： Day11~Day12的任务是构建网络模型，在学习构建网络模型之前需要我们了解官方文档9~10的资料，这里包括了基本的模块，卷积操作、BN操作、池化操作、激活函数等等在构建模型中最基层的方法。学习初期我们可以先学习经典的网络模型，Lenet,Alxnet,Vgg,Resnet等网络模型，后面随着学习的深入自己搭建网络模型或者在经典网络模型的基础上增加或修改其网络，使其达到更好的性能； Torch.nn.function中的激活函数和池化是我们常用的，一般不使用torch.nn中的激活函数和池化操作，因为网络模型反向传播时不计算池化层的梯度，使用在torch.nn.function中重新封装了部分接口，当然也可以不使用torch.nn.function。 作业资料包下载链接：（学习资料里包含了老师对官方文档的重组文件，前面一周的文档内容也都有更新，对前面内容还有所疑惑的小伙伴可以回头看看文档资料） 链接：https://pan.baidu.com/s/17xW8rfG-14nu6vc9vjeBlQ 提取码：34k3 作业名称（详解）：（1）手敲官方文档9，10中的基本操作3遍；（2）自己尝试写一个5层的网络模型； 作业提交形式：打卡提交文字或图片，总结内容不少于20字 打卡截止时间：6/14]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F06%2F14%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%93%E9%A2%98%2FCNN%2F%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[same padding和valid padding首先，padding的含义 https://zhuanlan.zhihu.com/p/36278093 padding是增加各个边的pixels的数量，目的是保持feature map 不要太小（不要被卷积越卷越小），但也没必要超过原图的大小，所以不可以任意数量。 padding的上限是维持feature map 大小与原图大小一致，具体增加pixel的数量多少，由filter的尺寸和stride大小共同决定； padding 存在的意义在于 为了不丢弃原图信息为了保持feature map 的大小与原图一致为了让更深层的layer的input依旧保持有足够大的信息量为了实现上述目的，且不做多余的事情，padding出来的pixel的值都是0，不存在噪音问题 计算输出尺寸 过滤器数量-输出卷的深度与过滤器的数量成正比。请记住该如何堆叠每个过滤器的输出以形成激活映射。激活图的深度等于过滤器的数量。 步幅（Stride）-如果步幅是 1，那么我们处理图片的精细度就进入单像素级别了。更高的步幅意味着同时处理更多的像素，从而产生较小的输出量。 零填充（zero padding）-这有助于我们保留输入图像的尺寸。如果添加了单零填充，则单步幅过滤器的运动会保持在原图尺寸。 我们可以应用一个简单的公式来计算输出尺寸。输出图像的空间尺寸可以计算为（[W-F + 2P] / S）+1。在这里，W 是输入尺寸，F 是过滤器的尺寸，P 是填充数量，S 是步幅数字。假如我们有一张32*32*3 的输入图像，我们使用 10 个尺寸为3*3*3的过滤器，单步幅和零填充。 那么 W=32，F=3，P=0，S=1。输出深度等于应用的滤波器的数量，即 10，输出尺寸大小为([32-3+0]/1)+1 = 30。因此输出尺寸是 30*30*10。 卷积核相关概念如Pytorch中，conv1.weight.shape() = [6,3,5,5]，输入通道数为 3，卷积核个数为 6，则 feature map 数为 6，卷积核大小为 5*5。 这些概念分别代表什么 通道数【CNN】理解卷积神经网络中的通道 channel tensorflow 中给出的，对于输入样本中 channels 的含义。一般的RGB图片，channels数量是 3 （红、绿、蓝）；而monochrome图片，channels 数量是 1 。 特征图feature map]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F06%2F14%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2FPytorch%E7%AC%94%E8%AE%B0%2F%E5%AE%8C%E6%95%B4%E7%9A%84%E4%B8%80%E6%AC%A1%E4%BB%BB%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[《PyTorch模型训练实用教程》 代码在：/Users/david/david/code/deep_learning/PyTorch_Tutorial_yuting 预先安装 requirement.txt 运行时报错 12OMP: Error #15: Initializing libiomp5.dylib, but found libomp.dylib already initialized.OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://www.intel.com/software/products/support/. 解决方案 1os.environ[&quot;KMP_DUPLICATE_LIB_OK&quot;]=&quot;TRUE&quot; 第一章 数据1.1 Cifar10 转 pngcifar-10 的测试集，10000张图片。 官网:http://www.cs.toronto.edu/~kriz/cifar.html 读取并保存为图片图片是序列化的，不能直接读取。 运行代码:Code/1_data_prepare/1_1_cifar10_to_png.py 可在文件夹 Data/cifar-10-png/raw_test/下看到 0-9 个文件夹，对应 9 个类别。 将 测试集中的 10000 张图片解压出来，作为原始图片，将从这 10000 张图片中划分出训练集 (train)，验证集(valid)，测试集(test)。 1.2 训练集、验证集、测试集的划分把原始数据按 8:1:1 的比例划分为训练集(train set)、验证集(valid/dev set)和测试集(test set) 运行代码：Code/1_data_prepare/1_2_split_dataset.py 数据划分完毕，下一步是制作存放有图片路径及其标签的 txt。pytorch会根据txt的信息寻找图片，并读取图片数据和标签数据。 1.3 Pytorch读图片数据集Dataset类PyTorch 读取图片，主要是通过 Dataset 类，所以先简单了解一下 Dataset 类。抽象类 1234567891011121314151617class Dataset(object): """An abstract class representing a Dataset. All other datasets should subclass it. All subclasses should override ``__len__``, that provides the size of the dataset, and ``__getitem__``, supporting integer indexing in range from 0 to len(self) exclusive. """ # 接收一个 index，然后返回图片数据和标签 def __getitem__(self, index): raise NotImplementedError def __len__(self): raise NotImplementedError def __add__(self, other): return ConcatDataset([self, other]) 需要将list放到txt里面，读取txt，获取list。那么读取自己数据的基本流程就是: 制作存储了图片的路径和标签信息的 txt 将这些信息转化为 list，该 list 每一个元素对应一个样本 通过 getitem 函数，读取数据和标签，并返回数据和标签 在训练代码里是感觉不到这些操作的，只会看到通过 DataLoader 就可以获取一个batch 的数据，其实触发去读取图片这些操作的是 DataLoader 里的iter(self)，后面会详细讲解读取过程。在本小节，主要讲 Dataset 子类。 要让 PyTorch 能读取自己的数据集，只需要两步: 制作图片数据的索引 构建 Dataset 子类 1、制作图片索引就是获取图片路径和标签，保存到txt。 运行代码 Code/1_data_prepare/1_3_generate_txt.py 2、构建Dataset子类构建了MyDataset类 12345678910111213141516171819202122232425262728class MyDataset(Dataset): # 初始化中，从txt读到imgs对象 def __init__(self, txt_path, transform = None, target_transform = None): fh = open(txt_path, 'r') imgs = [] for line in fh: line = line.rstrip() words = line.split() imgs.append((words[0], int(words[1]))) # 最主要就是要生成这个list， 然后DataLoader中给index，通过getitem读取图片数据 self.imgs = imgs # Compose 类型，里边有一个 list，list定义了对图像的各种操作，如减均值，除标准差，随机裁剪，仿射变换等。 self.transform = transform self.target_transform = target_transform # python内建的魔法方法，访问索引就会触发 def __getitem__(self, index): fn, label = self.imgs[index] img = Image.open(fn).convert('RGB') # 像素值 0~255，在transfrom.totensor会除以255，使像素值变成 0~1 if self.transform is not None: img = self.transform(img) # 在这里做transform，转为tensor等等 return img, label def __len__(self): return len(self.imgs) Pytorch对图片的处理不能生成新图，而是覆盖原图（不是真正的覆盖，就是对象赋值）。当采用randomcrop之类的随机操作时，每个 epoch 输入进来的图片几乎不会是一模一样的，这达到了样本多样性的功能。 当 Mydataset 构建好，剩下的操作就交给 DataLoder，在 DataLoder 中，会触发Mydataset 中的 getiterm 函数读取一张图片的数据和标签，并拼接成一个 batch 返回，作为模型真正的输入。 1.4 DataLoder加载图片getitem是在DataLoader中触发的， 1234567891011121314151617181920212223# 构建MyDataset实例train_data = MyDataset(txt_path=train_txt_path, transform=trainTransform)# 构建DataLodertrain_loader = DataLoader(dataset=train_data, batch_size=train_bs, shuffle=True)# 这里的data就是__getitem__返回的img, labelfor i, data in enumerate(train_loader):"""-&gt; def __iter__(self): return _DataLoaderIter(self) class _DataLoaderIter(object): def __next__(self): # collate_fn (callable, optional): merges a list of samples to form a mini-batch. # 这里调用__getitem__ batch = self.collate_fn([self.dataset[i] for i in indices])""" # 获取图片和标签 inputs, labels = data inputs, labels = Variable(inputs), Variable(labels) 图片是通过 Image.open()函数读取进来的，当涉及如下问题： 图片的通道顺序(RGB ? BGR ?) 图片是w*h*c ? c*w*h ? 像素值范围[0-1] or [0-255] ? 就要查看 MyDataset()类中 getitem()下读取图片用的是什么方法 1.5 数据增强和数据标准化在 PyTorch 中，数据增强方法放在了 transforms.py 文件中。 这一节主要介绍transforms的操作。 1.6 transforms 的二十二个方法 第二章 模型2.1 模型的搭建2.1.1 模型定义的三要素1）首先，必须继承 nn.Module 这个类，要让 PyTorch 知道这个类是一个 Module。 2）其次，在init(self)中设置好需要的“组件”(如 conv、pooling、Linear、BatchNorm等）。 3）最后，在 forward(self, x)中用定义好的“组件”进行组装，就像搭积木，把网络结构搭建 在/Code/maintraining/main.py 中可以看到定义了一个类class Net(nn.Module)，集成了nn.Module，先看_init(self)函数 12345678910111213def __init__(self): # 初始化 super(Net, self).__init__() # 定义了一系列组件 self.conv1 = nn.Conv2d(3, 6, 5) self.pool1 = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 3) self.pool2 = nn.MaxPool2d(2, 2) self.conv3 = nn.Conv2d(16, 64, 3) self.pool3=nn.MaxPool2d(2,1) self.fc1 = nn.Linear(64 * 3 * 3, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) 当这些组件定义好之后，就可以定义 forward()函数，用来搭建网络结构 1234567891011121314def forward(self, x): # x作为模型的输入，x 经过 conv1，然后经过激活函数 relu，再经过 pool1 操作 x = self.pool1(F.relu(self.conv1(x))) # 再做一轮 x = self.pool2(F.relu(self.conv2(x))) # 再做一轮 x=self.pool3(F.relu(self.conv3(x))) # 将 x 进行 reshape，为了后面做为全连接层的输入 x = x.view(-1, 64 * 3* 3) # 先经过全连接层 fc，然后经过 relu x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x 2.1.2 一个更复杂的模型来看一个更复杂的模型，看Resnet网络的定义方法https://github.com/yuanlairuci110/pytorch-best-practice-master/blob/master/models/ResNet34.py 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677#coding:utf8from .BasicModule import BasicModulefrom torch import nnfrom torch.nn import functional as Fclass ResidualBlock(nn.Module): ''' 实现子module: Residual Block ''' def __init__(self, inchannel, outchannel, stride=1, shortcut=None): super(ResidualBlock, self).__init__() self.left = nn.Sequential( nn.Conv2d(inchannel, outchannel, 3, stride, 1, bias=False), nn.BatchNorm2d(outchannel), nn.ReLU(inplace=True), nn.Conv2d(outchannel, outchannel, 3, 1, 1, bias=False), nn.BatchNorm2d(outchannel) ) self.right = shortcut def forward(self, x): out = self.left(x) residual = x if self.right is None else self.right(x) out += residual return F.relu(out)class ResNet34(BasicModule): ''' 实现主module：ResNet34 ResNet34包含多个layer，每个layer又包含多个Residual block 用子module来实现Residual block，用_make_layer函数来实现layer ''' def __init__(self, num_classes=2): super(ResNet34, self).__init__() self.model_name = 'resnet34' # 前几层: 图像转换 self.pre = nn.Sequential( nn.Conv2d(3, 64, 7, 2, 3, bias=False), nn.BatchNorm2d(64), nn.ReLU(inplace=True), nn.MaxPool2d(3, 2, 1)) # 重复的layer，分别有3，4，6，3个residual block self.layer1 = self._make_layer( 64, 128, 3) self.layer2 = self._make_layer( 128, 256, 4, stride=2) self.layer3 = self._make_layer( 256, 512, 6, stride=2) self.layer4 = self._make_layer( 512, 512, 3, stride=2) #分类用的全连接 self.fc = nn.Linear(512, num_classes) def _make_layer(self, inchannel, outchannel, block_num, stride=1): ''' 构建layer,包含多个residual block ''' shortcut = nn.Sequential( nn.Conv2d(inchannel,outchannel,1,stride, bias=False), nn.BatchNorm2d(outchannel)) layers = [] layers.append(ResidualBlock(inchannel, outchannel, stride, shortcut)) for i in range(1, block_num): layers.append(ResidualBlock(outchannel, outchannel)) return nn.Sequential(*layers) def forward(self, x): x = self.pre(x) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) x = self.layer4(x) x = F.avg_pool2d(x, 7) x = x.view(x.size(0), -1) return self.fc(x) 这里用到了torch.nn.Sequential 2.1.3 nn.Sequential这个是Sequential容器，将一系列操作包起来。例如Resnet有很多重复的block，就可以包起来。 官方文档中给了两个例子 123456789# Example of using Sequential model = nn.Sequential(nn.Conv2d(1,20,5), nn.ReLU(), nn.Conv2d(20,64,5), nn.ReLU())# Example of using Sequential with OrderedDictmodel = nn.Sequential(OrderedDict([('conv1', nn.Conv2d(1,20,5)), ('relu1', nn.ReLU()),('conv2', nn.Conv2d(20,64,5)), ('relu2', nn.ReLU())])) 总结：模型的定义就是先继承，再构建组件，最后组装(forward)。 2.2 权值初始化的十种方法初始化方法会直接影响模型的收敛与否 2.2.1 权重初始化流程总共两步， 1）先设定什么层用什么初始化方法，初始化方法在torch.nn.init中给出； 2）实例化一个模型之后，执行该函数，即可完成初始化。 重点是第一步，看Main的方法 1234567891011121314# 定义权值初始化 def initialize_weights(self): for m in self.modules(): if isinstance(m, nn.Conv2d): torch.nn.init.xavier_normal_(m.weight.data) if m.bias is not None: # 若有bias，初始化全为0 m.bias.data.zero_() elif isinstance(m, nn.BatchNorm2d): m.weight.data.fill_(1) m.bias.data.zero_() elif isinstance(m, nn.Linear): torch.nn.init.normal_(m.weight.data, 0, 0.01) m.bias.data.zero_() 2.2.2 常用初始化方法1）Xavier，kaiming系列 2）其他方法分布 Xavier 初始化方法，论文在《Understanding the difficulty of training deep feedforward neural networks》 公式推导是从“方差一致性”出发，初始化的分布有均匀分布和正态分布两种。 1、Xavier均匀分布1torch.nn.init.xavier_uniform_(tensor, gain=1) 服从均匀分布U(-a, a)，分布的参数$a=gain * sqrt(6/fan_in+fan_out)$ 这里有一个gain，增益的大小是依据激活函数类型来设定。如 1nn.init.xavier_uniform_(w, gain=nn.init.calculate_gain(&apos;relu&apos;)) 上述方法也成为Glorot initialization 2、Xavier正态分布1torch.nn.init.xavier_normal_(tensor, gain=1) 3、kaiming均匀分布论文在《 Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification》。公式推导同样从“方差一致性”出法，kaiming是针对 xavier 初始化方法在 relu 这一类激活函数表现不佳而提出的改进 1torch.nn.init.kaiming_uniform_(tensor, a=0, mode=&apos;fan_in&apos;, nonlinearity=&apos;leaky_relu&apos;) 其中，a是激活函数的负半轴的斜率，relu是0 mode可选为fan_in或fan_out，前者使正向传播时方差一致，后者使反向传播时方差一致。 nonlinearity可选relu和leaky_relu，默认值为leaky_relu。 4、kaiming正态分布 1torch.nn.init.kaiming_normal_(tensor, a=0, mode=&apos;fan_in&apos;, nonlinearity=&apos;leaky_relu&apos;) 5、其他方法 参见文档 权值初始化杂谈 1、从代码中发现，即使不进行初始化，模型的权重也不为空，而是有值的，这些值是什么时候赋给的呢？ 其实，在创建网络实例的过程中，一旦调用nn.Conv2d的时候就会对权值进行初始化。 初始化过程是在Conv2d的基类_ConvNd中进行的 123456&gt; class Conv2d(_ConvNd):&gt; --&gt; 在_ConvNd 中:&gt; --&gt; self.reset_parameters()&gt; ---&gt; def reset_parameters(self)&gt; ---&gt; self.weight.data.uniform_(-stdv, stdv)&gt; &gt; 可以看出这里是均匀分布，其中-stdv与kernel的size有关。 补充：在Pytorch1.0版本中，这里改用了kaiminguniform()进行初始化。 2、按需定义初始化方法，例如： 123if isinstance(m, nn.Conv2d): n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels m.weight.data.normal_(0, math.sqrt(2. / n)) 2.3 模型Finetune实际应用中，通常采用一个已经训练模型的权值参数作为我们模型的初始化参数，也称之为finetune—迁移学习。迁移学习中的 Finetune 技术，本质上就是让我们新构建的模型，拥有一个较好的权值初始值。 finetune 权值初始化三步曲，finetune 就相当于给模型进行初始化，其流程共用三步:第一步:保存模型，拥有一个预训练模型;第二步:加载模型，把预训练模型中的权值取出来;第三步:初始化，将权值对应的“放”到新模型中 2.3.1 权值初始化在进行 finetune 之前我们需要拥有一个模型或者是模型参数，因此需要了解如何保存 模型。官方文档中介绍了两种保存模型的方法，一种是保存整个模型，另外一种是仅保存 模型参数(官方推荐用这种方法)，这里采用官方推荐的方法。 1、保存模型参数若拥有模型参数，可跳过这一步。假设创建了一个 net = Net()，并且经过训练，通过以下方式保存: 1torch.save(net.state_dict(), &apos;net_params.pkl&apos;) 2、加载模型进行三步曲中的第二步，加载模型，这里只是加载模型的参数: 1pretrained_dict = torch.load(&apos;net_params.pkl&apos;) 3、初始化进行三步曲中的第三步，将取到的权值，对应的放到新模型中: 首先我们创建新模型，并且获取新模型的参数字典 net_state_dict: net=Net()# 创建net 1net_state_dict = net.state_dict() # 获取已创建 net 的 state_dict 接着将 pretrained_dict 里不属于 net_state_dict 的键剔除掉: 1pretrained_dict_1 = &#123;k: v for k, v in pretrained_dict.items() if k in net_state_dict&#125; 然后，用预训练模型的参数字典 对 新模型的参数字典 net_state_dict 进行更新: 1net_state_dict.update(pretrained_dict_1) 最后，将更新了参数的字典 “放”回到网络中: 1net.load_state_dict(net_state_dict) 采用 finetune 的训练过程中，有时候希望前面层的学习率低一些，改变不要太大，而 后面的全连接层的学习率相对大一些。这时就需要对不同的层设置不同的学习率，下面就 介绍如何为不同层配置不同的学习率。 2.3.2 不同层设置不同学习率在利用 pre-trained model 的参数做初始化之后，我们可能想让 fc 层更新相对快一些，而希望前面的权值更新小一些，这就可以通过为不同的层设置不同的学习率来达到此目的。 为不同层设置不同的学习率，主要通过优化器对多个参数组进行设置不同的参数。所以，只需要将原始的参数组，划分成两个，甚至更多的参数组，然后分别进行设置学习率。 这里将原始参数“切分”成 fc3 层参数和其余参数，为 fc3 层设置更大的学习率。 12345678910# 返回的是 parameters 的 内存地址 # 将fc3层的参数从原始参数net.parameters中剥离出来。# base_params是剥离了fc3层参数的其他参数。ignored_params = list(map(id, net.fc3.parameters())) base_params = filter(lambda p: id(p) not in ignored_params, net.parameters())# 然后优化器中为fc3的单独设置学习率optimizer = optim.SGD([ &#123;'params': base_params&#125;, &#123;'params': net.fc3.parameters(), 'lr': 0.001*10&#125;], 0.001, momentum=0.9, weight_decay=1e-4)]) 完整代码在/Code/2_model/2_finetune.py 第三章 损失函数和优化器Pytorch中十七个损失函数，十个优化器和六个学习率调整方法。 3.1 十七个损失函数我们所说的优化，即优化网络权值使得损失函数值变小。但是，损失函数值变小是否能代表模型的分类/回归精度变高呢?那么多种损失函数，应该如何选择呢?请来了解PyTorch 中给出的十七种损失函数吧。 3.1.1 L1loss1class torch.nn.L1Loss(size_average=None, reduce=None) 官方文档中仍有 reduction=’elementwise_mean’参数，但代码实现中已经删除该参数 功能： 计算output和target之差的绝对值，可选返回同维度的tensor或一个标量。 公式： 参数: reduce(bool)- 返回值是否为标量，默认为 Truesize_average(bool)- 当 reduce=True 时有效。为 True 时，返回的 loss 为平均值;为 False 时，返回的各样本的 loss 之和。实例: /Code/3_optimizer/3_1_lossFunction/1_L1Loss.py 第四章 监控模型-可视化4.1 TensorBoardX流行的有两种方法，本文重点介绍第二种。 1、构建Logger类 Logger 类中“包”了 tf.summary.FileWriter ，截至目前(2018.10.17)，只有三种操作，分别是 scalar_summary(), image_summary(), histo_summary()。 优点:轻便，可满足大部分需求 参考github：https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/04-utils/tensorboard 2、借助TensorBoardX包 TensorBoardX 包的功能就比较全，截至目前(2018.10.17)，支持除beholder 之外的所有 tensorboard 的记录类型。 github：https://github.com/lanpa/tensorboardX API文档：https://tensorboard-pytorch.readthedocs.io/en/latest/tutorial_zh.html 代码实现: tensorboardX 提供 13 个函数，可以记录标量、图像、语音、文字等等，功能十分丰富。本节将对这些函数进行介绍，所用代码为 tensorboardX 的官方 demo.py，放在： /Code/4_viewer/1_tensorboardX_demo.py 运行该文件，再打开一个terminal，进入/Result/，执行 1tensorboard --logdir=runs 然后浏览器打开 1localhost:6006 可以看到显示界面如下： 4.2 TensorBoardX的函数4.2.1 add_scalar()1add_scalar(tag, scalar_value, global_step=None, walltime=None) 功能： 在一个图表中记录一个标量的变化，常用于Loss和Accuracy曲线的记录。 参数: tag(string)- 该图的标签，类似于 polt.title。 scalar_value(float or string/blobname)- 用于存储的值，曲线图的 y 坐标 global_step(int)- 曲线图的 x 坐标 walltime(float)- 为 event 文件的文件名设置时间，默认为 time.time() 运行 demo 中的: 用 github 首页 demo 运行这一行: 1writer.add_scalar(&apos;data/scalar1&apos;, dummy_s1[0], n_iter) 可以得到下图:]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F06%2F12%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2FPytorch%E8%AE%AD%E7%BB%83%E8%90%A5%2FD4%20%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E3%80%81%E6%95%B0%E6%8D%AE%E6%89%A9%E5%A2%9E%2F</url>
    <content type="text"><![CDATA[任务名称：1.数据读取；2.数据扩增[参考资料：(1)PyTorchtutorial_0.0.5余霆嵩文档1~16页(2)pytorch官方文档16.torch.utils.data;17.torch.utils.model_zoo;18.torchvision.datasets;19.torchvision.models;20.torchvision.transforms;21.torchvision.utils] 任务简介：数据读取和自定义数据集的读取操作；数据集的扩增的方法； 详细说明： Day9~Day10的任务是数据的读取和数据的扩增，在学习之前需要我们把官方文档16~21的资料学习，然后学习余霆嵩大神整理的资料，这里包括了数据读取和数据扩增的方法基本操作的API。数据扩增在我们数据有限的情况下可以通过数据扩增得到更多的数据，一方面可以抑制过拟合一方面可以提高模型泛化性，pytorch中封装了22种数据扩增的方法，基本包括了我们常用扩增方法，可以根据需求调用各自方法。 作业资料包下载链接： 链接：https://pan.baidu.com/s/17xW8rfG-14nu6vc9vjeBlQ 提取码：34k3 作业名称（详解）：（1）使用提供的网络模型读取自己数据进行训练；（2）使用22中数据扩增的方法进行组合，测试其效果； 作业提交形式：打卡提交文字或图片，不少于20字 打卡截止时间：6/12]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F06%2F12%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2Fcolab%20faceswap%2F</url>
    <content type="text"><![CDATA[报错1、 1!python tools.py effmpeg -a extract -i ./video/zhubo.mp4 -o ./video/zhubo20190611/ 报错 123456Traceback (most recent call last): File &quot;tools.py&quot;, line 5, in &lt;module&gt; import tools.cli as cli File &quot;/content/gdrive/My Drive/tensorflow/faceswap-master/tools.py&quot;, line 5, in &lt;module&gt; import tools.cli as cliModuleNotFoundError: No module named &apos;tools.cli&apos;; &apos;tools&apos; is not a package 2、 1!python faceswap.py extract -i video/wangwei2/ -o data/wangwei3 报错 123456789101112Traceback (most recent call last): File &quot;faceswap.py&quot;, line 36, in &lt;module&gt; ARGUMENTS.func(ARGUMENTS) File &quot;/content/gdrive/My Drive/tensorflow/faceswap-master/lib/cli.py&quot;, line 105, in execute_script log_setup(arguments.loglevel, arguments.logfile, self.command) File &quot;/content/gdrive/My Drive/tensorflow/faceswap-master/lib/logger.py&quot;, line 93, in log_setup f_handler = file_handler(numeric_loglevel, logfile, log_format, command) File &quot;/content/gdrive/My Drive/tensorflow/faceswap-master/lib/logger.py&quot;, line 115, in file_handler log_file.doRollover() File &quot;/usr/lib/python3.6/logging/handlers.py&quot;, line 172, in doRollover os.remove(dfn)PermissionError: [Errno 1] Operation not permitted: &apos;/content/gdrive/My Drive/tensorflow/faceswap-master/faceswap.log.1&apos; 参考https://www.deepfakescn.com/?p=295 https://github.com/dream80/DeepFaceLab_Colab https://github.com/dream80/DeepFaceLab_Colab/blob/master/DeepFaceLab_Colab_zh_cn.ipynb]]></content>
  </entry>
  <entry>
    <title><![CDATA[Vultr配置]]></title>
    <url>%2F2019%2F06%2F11%2F%E5%B7%A5%E5%85%B7%E5%92%8C%E7%8E%AF%E5%A2%83%2FVultr%2F</url>
    <content type="text"><![CDATA[https://github.com/Alvin9999/new-pac]]></content>
      <categories>
        <category>工具和环境</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F06%2F10%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2FPytorch%E8%AE%AD%E7%BB%83%E8%90%A5%2FD3%20storage%E5%92%8Ccuda%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[任务名称：Pytorch官方文档(参考资料：Pytorch官方文档 8.torch.Storage操作；14.torch.cuda操作） 任务简介：torch.Storage 主要是Tensor数据类型的转换；torch.cuda； 详细说明： Day8的任务两块，第一块是Storage操作，主要包括数据类型转换的接口；第二块是如何使用判断是否有显卡，以及如何把数据和模型在显卡上运行。 作业资料包下载链接： 链接：https://pan.baidu.com/s/17xW8rfG-14nu6vc9vjeBlQ 提取码：34k3 作业名称（详解）：（1）自己练习数据类型之间的转换方法（2）测试在显卡上训练和在cpu上训练的速度差多少倍； 作业提交形式：打卡提交文字或图片 打卡截止时间：6/11 https://blog.csdn.net/Mr_JP/article/details/81906616 torch.Storagetorch.Storage 是一个连续的，一维的，单一数据类型的数组。每一个torch.Tensor有一个对应的torch.storage,并且二者都有相同的数据类型。 123456789101112a = torch.FloatTensor([1, 2, 3])b = torch.FloatStorage([1, 2, 3])print (a)print (type(a))print (type(a[0]))print (a.shape)print (b)print (type(b))print (type(b[0]))#print (b.shape) 报错 1234567891011&gt; tensor([1., 2., 3.])&gt; &lt;class &apos;torch.Tensor&apos;&gt;&gt; &lt;class &apos;torch.Tensor&apos;&gt;&gt; torch.Size([3])&gt; 1.0&gt; 2.0&gt; 3.0&gt; [torch.FloatStorage of size 3]&gt; &lt;class &apos;torch.FloatStorage&apos;&gt;&gt; &lt;class &apos;float&apos;&gt;&gt; FloatTensor可以接受FloatStorage类型进行初始化，若对FloatStorage进行修改时，FloatTensor也会被修改: 123456b = torch.FloatStorage([1, 2, 3])a = torch.FloatTensor(b)b[0] = 10print(b)print(a) 123456&gt; 10.0&gt; 2.0&gt; 3.0&gt; [torch.FloatStorage of size 3]&gt; tensor([10., 2., 3.])&gt; torch.from_numpytorch.from_numpy与用FloatStorage初始化类似，不是直接复制数据初始化。修改numpy类型数据时，tensor数据会发生改变. 1234a = np.array([1,2,3])b = torch.from_numpy(a)a[0] = 10print (b) 输出内容： 1tensor([10, 2, 3], dtype=torch.int32) 结论当张量需要被多处共享使用时，使用FloatStorage初始化FloatTensor.torch.from_numpy和torch.tensor(FloatStorage)都是浅拷贝。 torch.cuda]]></content>
  </entry>
  <entry>
    <title><![CDATA[Pyspark技巧总结]]></title>
    <url>%2F2019%2F06%2F10%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FPYTHON%2Fpyspark%E5%90%84%E7%A7%8D%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[spark跟pandas数据转换不支持spark 1.6 pandas dataframe转 spark dataframe, 1234567891011121314151617import pandas as pdfrom pyspark.sql import SparkSession#pandas读取cvs,形成dataframe,userDF = pd.read_csv(&quot;src/main/resources/upload.csv&quot;)#启动sparkspark = SparkSession \ .builder \ .appName(&quot;Python Spark SQL Hive integration example&quot;) \ .enableHiveSupport() \ .getOrCreate()#spark读取pandas dataframe,形成spark dataframesparkDF = spark.createDataFrame(userDF)sparkDF.show() spark dataframe 转 pandas data,download.py 12345678910111213141516171819from pyspark.sql import SparkSessionspark = SparkSession \ .builder \ .appName(&quot;Python Spark SQL Hive integration example&quot;) \ .enableHiveSupport() \ .getOrCreate()spark.sql(&quot;CREATE TABLE IF NOT EXISTS user (userid int, name string)&quot;)spark.sql(&quot;LOAD DATA LOCAL INPATH &apos;src/main/resources/user.txt&apos; INTO TABLE user&quot;)userSparkDF = spark.sql(&quot;select * from user&quot;)userPandasDF = userSparkDF.toPandas()print userPandasDFspark.stop() pyspark.sql.functionshttp://spark.apache.org/docs/1.5.2/api/python/_modules/pyspark/sql/functions.html http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.to_json 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354_functions = &#123; &apos;lit&apos;: &apos;Creates a :class:`Column` of literal value.&apos;, &apos;col&apos;: &apos;Returns a :class:`Column` based on the given column name.&apos;, &apos;column&apos;: &apos;Returns a :class:`Column` based on the given column name.&apos;, &apos;asc&apos;: &apos;Returns a sort expression based on the ascending order of the given column name.&apos;, &apos;desc&apos;: &apos;Returns a sort expression based on the descending order of the given column name.&apos;, &apos;upper&apos;: &apos;Converts a string expression to upper case.&apos;, &apos;lower&apos;: &apos;Converts a string expression to upper case.&apos;, &apos;sqrt&apos;: &apos;Computes the square root of the specified float value.&apos;, &apos;abs&apos;: &apos;Computes the absolute value.&apos;, &apos;max&apos;: &apos;Aggregate function: returns the maximum value of the expression in a group.&apos;, &apos;min&apos;: &apos;Aggregate function: returns the minimum value of the expression in a group.&apos;, &apos;first&apos;: &apos;Aggregate function: returns the first value in a group.&apos;, &apos;last&apos;: &apos;Aggregate function: returns the last value in a group.&apos;, &apos;count&apos;: &apos;Aggregate function: returns the number of items in a group.&apos;, &apos;sum&apos;: &apos;Aggregate function: returns the sum of all values in the expression.&apos;, &apos;avg&apos;: &apos;Aggregate function: returns the average of the values in a group.&apos;, &apos;mean&apos;: &apos;Aggregate function: returns the average of the values in a group.&apos;, &apos;sumDistinct&apos;: &apos;Aggregate function: returns the sum of distinct values in the expression.&apos;,&#125;_functions_1_4 = &#123; # unary math functions &apos;acos&apos;: &apos;Computes the cosine inverse of the given value; the returned angle is in the range&apos; + &apos;0.0 through pi.&apos;, &apos;asin&apos;: &apos;Computes the sine inverse of the given value; the returned angle is in the range&apos; + &apos;-pi/2 through pi/2.&apos;, &apos;atan&apos;: &apos;Computes the tangent inverse of the given value.&apos;, &apos;cbrt&apos;: &apos;Computes the cube-root of the given value.&apos;, &apos;ceil&apos;: &apos;Computes the ceiling of the given value.&apos;, &apos;cos&apos;: &apos;Computes the cosine of the given value.&apos;, &apos;cosh&apos;: &apos;Computes the hyperbolic cosine of the given value.&apos;, &apos;exp&apos;: &apos;Computes the exponential of the given value.&apos;, &apos;expm1&apos;: &apos;Computes the exponential of the given value minus one.&apos;, &apos;floor&apos;: &apos;Computes the floor of the given value.&apos;, &apos;log&apos;: &apos;Computes the natural logarithm of the given value.&apos;, &apos;log10&apos;: &apos;Computes the logarithm of the given value in Base 10.&apos;, &apos;log1p&apos;: &apos;Computes the natural logarithm of the given value plus one.&apos;, &apos;rint&apos;: &apos;Returns the double value that is closest in value to the argument and&apos; + &apos; is equal to a mathematical integer.&apos;, &apos;signum&apos;: &apos;Computes the signum of the given value.&apos;, &apos;sin&apos;: &apos;Computes the sine of the given value.&apos;, &apos;sinh&apos;: &apos;Computes the hyperbolic sine of the given value.&apos;, &apos;tan&apos;: &apos;Computes the tangent of the given value.&apos;, &apos;tanh&apos;: &apos;Computes the hyperbolic tangent of the given value.&apos;, &apos;toDegrees&apos;: &apos;Converts an angle measured in radians to an approximately equivalent angle &apos; + &apos;measured in degrees.&apos;, &apos;toRadians&apos;: &apos;Converts an angle measured in degrees to an approximately equivalent angle &apos; + &apos;measured in radians.&apos;, &apos;bitwiseNOT&apos;: &apos;Computes bitwise not.&apos;,&#125; sparkDF.columns 把某一行转成json123456789101112def row_2_json(row): return json.dumps(row.asDict(),ensure_ascii=False,indent=2)row_2_json = F.udf(row_2_json, StringType())spark_df = hiveCtx.createDataFrame(install_df) event_df = spark_df.withColumn('evnet_id', F.lit(6)) \ .withColumn('event_name', F.lit('安装信息异常')) \ .withColumn('event_lonlat', F.col('install_lonlat')) \ .withColumn('anomaly_json', row_2_json(F.struct([F.col(x) for x in spark_df.columns]))) \ .withColumn('data_date', F.lit(data_param)) \ .select('app_code', 'evnet_id', 'event_name', 'event_lonlat', 'anomaly_json', 'data_date')]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F06%2F08%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%93%E9%A2%98%2F01SoftMax%2F%E8%BE%93%E5%87%BA%E5%B1%82%E7%9A%84Softmax%2F</url>
    <content type="text"><![CDATA[https://cloud.tencent.com/developer/news/307323 Softmax是假设不同特征是相互独立的 然而，这可能在许多情况下不成立，因为特征之间可能存在协同作用或冗余，种协同或者作用会直接影响输出概率。 解决方案可以是： 1）去除有协同作用或冗余的特征，如x3 =X1⋅x2x3=x1⋅x2（但是如果我们不知道哪些特征值是相关的，我们可能会引入更多无用的特征！ 2）当两个特征经常一起被激活时，训练过程将学习较小的权重W1和W2，使得它们的联合效果更接近真实效果 如何判断两个特征是否同时被激活]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F06%2F05%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%93%E9%A2%98%2FEM%2FEM%E7%AE%97%E6%B3%95%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[https://zhuanlan.zhihu.com/p/61768577?utm_source=ZHShareTargetIDMore&amp;utm_medium=social&amp;utm_oi=28206795063296 这个推导啥子的也太难了把。不过经过我不停不停不停不停的看这个算法，到今天我突然觉得自己好像明白了，然后我决定把我的理解写成一篇文章，毕竟只有给别人讲明白了才能算自己真正的明白。那么就进入我们这篇文章的主题:EM算法。 我们先讲一下极大似然估计法，然后再引申出EM算法 1.极大似然估计法 假设我们有如下的一维高斯分布 X的概率密度函数为: 其似然函数为 求对数为 对其求导，可以得到如下似然方程组 我们可以使用 梯度下降法 极大似然估计法 这两种方法来根据样本估计高斯分布的参数，具体代码如下: 1234567891011121314151617181920def cal_guassian_theta(): # u=1 siga=4 y = 1 + 2*np.random.randn(1000,1) n,m = y.shape # 梯度下降 u1 = np.random.randn(1)[0] siga1 = np.random.randn(1)[0] lr = 0.001 for i in range(1000): lu = (np.sum(y,axis=0) - n*u1)/(siga1) lsiga = (np.sum((y-u1)**2, axis=0)/siga1 - n)/(2*siga1) u1 = u1+lr*lu siga1 = siga1+lr*lsiga print("u1: %lf, siga1: %lf"%(u1, siga1)) # 解析解 u2 = np.sum(y, axis = 0)/n siga2 = np.sum((y-u2)**2, axis = 0)/n print("u2: %lf, siga2: %lf"%(u2, siga2)) 我们使用均值为1，标准差为2的高斯分布随机生成了1000个样本，然后分别使用梯度下降和极大似然估计法两种方式来估计参数，得到的参数如下: 两种方法得到的结果还是挺不错的。 2. EM算法 极大似然算法确实可以很方便的根据样本估算模型的参数，如果样本来自一个以上的模型，我们又不知道某个样本点到底是来自某个模型的，那么此时极大似然算法就无能为力了。 我们依旧用高斯分布来举例子，混合高斯分布的模型如下: 其中 是系数， ，同时 ； 是高斯分布密度函数， 。 这个时候我们需要估计的参数有 此时阻挡我们使用极大似然法的原因就是:我们不知道到底哪些样本点由哪个模型生成。 现在假设我们有1000个样本点，由两个独立的高斯分布生成。我们知道其中第一类有300个，第二类有700个，那么我们就可以对两个高斯分布分别使用极大似然法估计他们的参数了。 但事实上我们知道的只有一堆样本点以及其可能的类别数 ，至于某个样本到底属于那个模型我们是不知道。此时就要到EM算法登场的时候了，EM算法的主要思想如下： E步:先随便设置一下各种参数，然后再算一下在当前情况下每个样本点属于哪一个模型的概率值； M步:此时我们知道了一个样本点属于某个模型的概率，然后再次计算各个模型的参数（具体计算方法在下面）；然后返回上一步，直至算法收敛。 现在我们知道了EM算法的思想，那么EM算法是怎么在第二步估算出各个模型的参数呢。 我们先介绍一些概念：用 来表示观测随机变量的数据， 表示隐随机变量的数据（比如上述混合高斯分布样本点属于某个模型的概率）， 和 连在一起称为完全数据(这个我们是没法知道的)，观测数据 也被称为不完全数据(这个我们知道)， 被称为隐变量(我们不知道)，假定给定观测数据 ，其概率分布是 ，其中 是需要估计的模型参数，那么不完全数据 的似然函数是 ，对数似然函数为 ； 和 的联合概率分布是 ，其对数似然函数是 。 EM算法是通过迭代来求 的极大似然估计，也就是在估计出来的参数条件下，模型产生给定样本点的概率最大~。因此我们要最大化下式。 上式中最右边的 指的的一个模型被选择的概率， 是指我们选定了一个模型，此模型产生这个样本点的概率。 上图是一维高斯混合分布，黄色的那个高斯分布均值为0，方差为1，被选择的概率为0.3；红色的那个高斯分布均值为3，方差为4，被选择的概率为0.7。 (下面部分参考《李航统计学习》P.159，推导更详细了一点) EM是通过迭代的方法来逐步逼近近似极大化 ，假设某一次我们得到了模型的参数估计值为 (是一个我们知道的值)，我们要求估计新的 可以时 增大，即 。我们计算两者的差 利用Jenson不等式 第一步到第二步除了使用了Jenson不等式，还使用了 ，其中的 都被约去。 令 则有 ，为了使 尽可能的大，我们应该选择 使 达到极大值。即 通过省去对 极大化是常数的项。 其中 指的是当我们知道模型的参数和样本的分布情况时，此时隐变量的状态。如果模型为高斯分布，那么 指的就是样本点属于某个模型的概率； 就是我们要找到 使得在当前 的情况下，获得函数的一个极大值。 EM算法的流程如下: (1)随机选择参数 ,开始迭代 (2)E步:计算 (3)M步:最大化 (4)重复(2),(3)步直到收敛 对高斯混合模型使用EM算法估计参数，其中第一个高斯分布均值为3，方差为4，系数为0.7；第二个高斯分布均值为0，方差为1，系数为0.3。 1234567891011121314151617181920212223242526272829def cal_mix_guassian_theta(): # u = 3, siga = 4, alpha = 0.7 f1 = 3+2*np.random.randn(700,1) # u = 0, siga = 1, alpha = 0.3 f2 = 1+np.random.rand(300,1) f = np.concatenate((f1,f2),axis=0) n,m = f.shape alpha = np.random.rand(1,2) alpha = alpha/np.sum(alpha, axis=1) u = np.random.rand(1,2) siga = np.random.rand(1,2) # EM算法求解 for i in range(100): #1.E步 gamma1 = gaussian(f, u[0][0], siga[0][0]) gamma2 = gaussian(f, u[0][1], siga[0][1]) gamma = np.concatenate((gamma1, gamma2), axis = 1) gamma = alpha*gamma/np.sum(alpha*gamma, axis = 1, keepdims=True) #2.M步 u = np.sum(gamma*f, axis = 0, keepdims=True)/np.sum(gamma, axis = 0,keepdims=True) siga = np.sum(gamma*((f-u)**2), axis = 0, keepdims=True)/np.sum(gamma, axis = 0,keepdims=True) siga = siga**(1/2) alpha = np.sum(gamma, axis = 0, keepdims=True)/n print("alpha1: %lf,alpha2: %lf"%(alpha[0][0], alpha[0][1])) print("u1: %lf,u2: %lf"%(u[0][0], u[0][1])) print("siga1: %lf,siga2: %lf"%(siga[0][0], siga[0][1])) 算法的估计值如下:]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F06%2F05%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%93%E9%A2%98%2FTransformer%2FAttention%2F</url>
    <content type="text"><![CDATA[《Attention is All You Need》浅读（简介+代码） 2017年中，有两篇类似同时也是笔者非常欣赏的论文，分别是FaceBook的《Convolutional Sequence to Sequence Learning》和Google的《Attention is All You Need》，它们都算是Seq2Seq上的创新，本质上来说，都是抛弃了RNN结构来做Seq2Seq任务。 这篇博文中，笔者对《Attention is All You Need》做一点简单的分析。当然，这两篇论文本身就比较火，因此网上已经有很多解读了（不过很多解读都是直接翻译论文的，鲜有自己的理解），因此这里尽可能多自己的文字，尽量不重复网上各位大佬已经说过的内容。 序列编码 #深度学习做NLP的方法，基本上都是先将句子分词，然后每个词转化为对应的词向量序列。这样一来，每个句子都对应的是一个矩阵X=(x1,x2,…,xt)X=(x1,x2,…,xt)，其中xixi都代表着第ii个词的词向量（行向量），维度为dd维，故X∈ℝn×dX∈Rn×d。这样的话，问题就变成了编码这些序列了。 第一个基本的思路是RNN层，RNN的方案很简单，递归式进行： yt=f(yt−1,xt)yt=f(yt−1,xt) 不管是已经被广泛使用的LSTM、GRU还是最近的SRU，都并未脱离这个递归框架。RNN结构本身比较简单，也很适合序列建模，但RNN的明显缺点之一就是无法并行，因此速度较慢，这是递归的天然缺陷。另外我个人觉得 RNN无法很好地学习到全局的结构信息，因为它本质是一个马尔科夫决策过程。 第二个思路是CNN层，其实CNN的方案也是很自然的，窗口式遍历，比如尺寸为3的卷积，就是 yt=f(xt−1,xt,xt+1)yt=f(xt−1,xt,xt+1) 在FaceBook的论文中，纯粹使用卷积也完成了Seq2Seq的学习，是卷积的一个精致且极致的使用案例，热衷卷积的读者必须得好好读读这篇文论。 CNN方便并行，而且容易捕捉到一些全局的结构信息，笔者本身是比较偏爱CNN的，在目前的工作或竞赛模型中，我都已经尽量用CNN来代替已有的RNN模型了，并形成了自己的一套使用经验 ，这部分我们以后再谈。 Google的大作提供了第三个思路：纯Attention！单靠注意力就可以！RNN要逐步递归才能获得全局信息，因此一般要双向RNN才比较好；CNN事实上只能获取局部信息，是通过层叠来增大感受野；Attention的思路最为粗暴，它一步到位获取了全局信息！它的解决方案是： yt=f(xt,A,B)yt=f(xt,A,B) 其中 A,BA,B 是另外一个序列（矩阵）。如果都取 A=B=XA=B=X ，那么就称为Self Attention， 它的意思是直接将xtxt与原来的每个词进行比较，最后算出ytyt ！ Attention层 #]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F06%2F04%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%93%E9%A2%98%2F%E4%BC%98%E8%B4%A8git%E8%B5%84%E6%BA%90%2F</url>
    <content type="text"><![CDATA[AI Learning汇集了30多名贡献者的集体智慧，把学习机器学习的路线图、视频、电子书、学习建议等中文资料全部都整理好了。 https://github.com/apachecn/AiLearning]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F06%2F04%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2FPytorch%E8%AE%AD%E7%BB%83%E8%90%A5%2FD2%20%E5%BA%8F%E5%88%97%E5%8C%96%E6%A8%A1%E5%9E%8B%EF%BC%8Ctorch%E6%8E%A5%E5%8F%A3%2F</url>
    <content type="text"><![CDATA[任务：Pytorch官方文档（参考资料：Pytorch官方文档 5.序列化模型；6.torch接口） 任务简介：《Pytorch官方文档》 学习时长：6/4 详细说明： 本节任务资料包下载： 链接：https://pan.baidu.com/s/1U7KBXRv4V5rkvMjWQqULnw 提取码：0b3u 本节内容包括如何保存和载入模型，我们一般情况下载训练阶段保存在预测阶段载入，同时需要了解两种方法保存模型的差异。下面是pytorch最重要的部分，对Tensor的操作，由于本节内容较多，我们分为七个部分讲解，今天主要是熟悉torch接口中1~9页的API,能够知道如何使用，知道每个函数的意义和参数的意义。 作业名称（详解）：保存模型的两种形式以及他们的区别？手敲今天所学API三遍 作业提交形式：打卡提交文字或图片，不少于20字 打卡截止时间：6/5 1、保存模型的两种形式以及他们的区别？第一种，只保存和加载模型参数 12345torch.save(the_model.state_dict(), PATH)# 加载时the_model = TheModelClass(*args, **kwargs)the_model.load_state_dict(torch.load(PATH)) 第二种，保存和加载整个模型 12345# 没有state_dicttorch.save(the_model, PATH)# 加载时the_model = torch.load(PATH) 第二种的不足是，序列化的数据被绑定到特定的类和固定的目录结构。缺少灵活性，可能有各种break的隐患。 2、Pytorch各种API张量操作123456789101112131415# 若obj是一个pytorch的tensor，则返回truetorch.is_tensor(obj)# 判断obj是否是storage# torch.Storage是单个数据类型的连续的一维数组，每个torch.Tensor都具有相同数据类型的相应存储。他是torch.tensor底层数据结构,他除了像Tensor一样定义数值，还可以直接把文件映射到内存中进行操作torch.is_storage(obj)# 设置默认的tensor的数据类型torch.set_default_tensor_type(torch.DoubleTensor)# 返回input张量中的元素个数torch.numel(input) -&gt; int# 设置打印选项，与numpy一致torch.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None) 创建操作Creation Ops12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879# 返回一个二维张量，对角线都是1，其他是0torch.eye(n, m=None, out=None)# 用一个值填充tensorend = torch.Tensor(4).fill_(10)"""&gt;&gt;&gt; end 10 10 10 10[torch.FloatTensor of size 4]"""# 将numpy.ndarray转为tensor，共享同一内存空间，修改一个会导致修改另一个。torch.from_numpy(ndarray) -&gt; Tensor# 返回一维张量，包含在start和end上均匀间隔的steps个点, out是结果张量torch.linspace(start, end, steps=100, out=None) -&gt; Tensor# 返回一维张量，包含在区间10^start和10^end上以对数刻度均匀间隔的steps个点。torch.logspace(start, end, steps=100, out=None) -&gt; Tensor&gt;&gt;&gt; torch.logspace(start=-10, end=10, steps=5) 1.0000e-10 1.0000e-05 1.0000e+00 1.0000e+05 1.0000e+10 [torch.FloatTensor of size 5] # 返回一个全为1的张量，形状由可变参数sizes定义torch.ones(*size, out=None) -&gt; Tensor&gt;&gt;&gt; torch.ones(2, 3)111111[torch.FloatTensor of size 2x3]# 从区间(0,1)的均匀分布中抽取一组随机数torch.rand(*sizes, out=None) -&gt; Tensor# 下面函数也可以实现这个效果torch.Tensor(3, 3).uniform_(0,1)# 等同于torch.rand(3,3)# 从标准正态分布（均值为0，方差为1，即高斯白噪声）中抽取一组随机数。torch.randn(*size, out=None) -&gt; Tensor# 给定参数n，返回一个从0到n-1的随机整数排列torch.randperm(n, out=None) -&gt; LongTensor&gt;&gt;&gt; torch.randperm(4)2 1 3 0[torch.LongTensor of size 4]# 返回一维张量，长度为floor((end-start)/step)。包含从start到end，以step为步长的torch.arange(start, end, step=1, out=None) -&gt; Tensor&gt;&gt;&gt; torch.arange(1, 2.5, 0.5) 1.0000 1.5000 2.0000[torch.FloatTensor of size 3]# 有floor((end-start)/step)+1个元素，包含在[start, end)# WARNING: 建议使用函数torch.arangetorch.range(start, end, step=1, out=None) -&gt; Tensor# 返回全为标量0的张量torch.zeros(*size, out=None)&gt;&gt;&gt; torch.zeros(2, 3)000000[torch.FloatTensor of size 2x3] 数学操作torch.log1p1torch.log1p(input, out=None) → Tensor 计算 input+1input+1的自然对数 yi=log(xi+1)yi=log(xi+1) 注意：对值比较小的输入，此函数比torch.log()更准确。 https://blog.csdn.net/qq_36523839/article/details/82422865 优点： 在数据预处理时首先可以对偏度比较大的数据用log1p函数进行转化，使其更加服从高斯分布，此步处理可能会使我们后续的分类结果得到一个更好的结果； 平滑处理很容易被忽略掉，导致模型的结果总是达不到一定的标准，同样使用逼格更高的log1p能避免复值得问题——复值指一个自变量对应多个因变量；]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F06%2F04%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2FPytorch%E7%AC%94%E8%AE%B0%2F%E5%87%BD%E6%95%B0%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[torch.ones]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F06%2F03%2F%E6%80%BB%E7%BB%93%E4%B8%8E%E6%80%9D%E8%80%83%2F%E5%A6%82%E4%BD%95%E5%90%91%E5%AE%A2%E6%88%B7%E6%8F%8F%E8%BF%B0%E4%BA%A7%E5%93%81%2F</url>
    <content type="text"><![CDATA[对于一个系统，如何从业务的角度去描述它，能讲一个故事让用户听懂，让用户感兴趣，而不是从模型到模型。 —首先，这个系统服务于什么领域 ​ 目前这个领域存在什么问题， ​ 有哪些因素影响到这些问题， ​ 我们的系统能够做到什么。 这样了解业务逻辑。自己清楚，也能跟别人讲。 了解不同类型的客户，知道这些客户关注的是什么问题。吃透行业标准。总结项目中吸收到的新的知识和方法。 跟客户谈的时候： —先说当前存在什么问题 ​ 我们有什么方法； ​ 能支持，能推动，能帮你们，能做到……； 不能让客户觉得系统就是个模型的入口跟数据的出口。 对于项目中不确定的模糊的问题，要尽快跟客户商定，不能成为项目隐藏的风险！ 跟客户探口风，把握需求的上限。哪些功能要做，哪些不要做，要有理有据。 思考客户提出需求的深层次原因，比如说领导重视。 沟通时，如想表达这个功能可以实现，但是因为钱不够不想做，可以说：这个系统的实现上，重点和难点在于XX。我们可以花工作量在XX功能，但是这会对最关键的内容产生影响。如果能给我们更多一些时间和预算，我们可以保质保量把这个新提出的功能做好。 同样的预算，要让不断提出新需求的客户认识到，做A还是做B，不能盲目接受客户需求，也不能因为合同没写或钱不够而直接拒绝。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Colab使用]]></title>
    <url>%2F2019%2F06%2F03%2F%E5%B7%A5%E5%85%B7%E5%92%8C%E7%8E%AF%E5%A2%83%2Fcolab%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[挂载google driver12from google.colab import drivedrive.mount(&apos;/content/gdrive&apos;) 运行之后： 123Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&amp;redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&amp;scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&amp;response_type=codeEnter your authorization code: 登录后获得授权码，填写后 1Mounted at /content/gdrive 桌面版工具 https://www.insynchq.com/ 跑fast.aihttps://segmentfault.com/a/1190000018580340?utm_source=tag-newest fast.ai B站教程https://www.bilibili.com/video/av42079986?from=search&amp;seid=1436832162873131397 公众号文章-colab汇总3 Colab的缺点？ 12小时连续连接的限制（可以用 暂存深度学习权重的方法来解决）； 需要科学上网，请查看KK大佬编写的视频教材 “如何优雅地使用Google系产品？” https://www.bilibili.com/video/av43034822?from=search&amp;seid=14859222855758483769 或者 更简单的方法 如果你有朋友已经搭建好了 你可以蹭一下^_^ 4 Colab中 如何装包？ !pip 5 如何导入数据？ 使用Google Drive,方法见: https://colab.research.google.com/notebooks/io.ipynb 用！wget直接从数据源读入； 用!git clone直接从Github读取整个项目文件（这个方法用于学习或者测试 特别赞） 提示： Google Drive只有15G 不适合放置大量数据，我的经验是： 数据从源文件处直接用!wget读取，但是将程序文件 和结果文件保存在网盘（特别是前面提到的 暂存的中间权重数据，以便后期恢复继续执行的文件 都要存入网盘 ，否则12小时限制到了 这些数据就没有了） 6 能否直接在Colab执行.py文件？ 可以 用!run 或者!python3 但是要注意 这样执行文件如果设计到模块的调用 非常容易出错 另外 在Colab中调用.py文件中定义的模块 也是很方便的 7 如果12小时 无法训练完整的结果，如何暂存结果 并在重新连接服务器后继续恢复执行？ 见 如何在TensorFlow中保存和恢复深度学习模型训练8 能否跑比较大的数据集？ 测试过wikidata,这个数据集 对于学习阶段来说 算比较大的数据了 见 在Google Colab复现一篇EMNLP 2017会议论文中的源代码当然 这个程序无法在12小时内完成 需要结合前面提到的暂存权重参数 重新恢复训练的方式； 个人觉得 学习阶段的使用Google Colab完全可以满足需求 9 Colab对于知名公开课的支持有哪些？ 见 怎样用Google Colab完成Stanford CS231n的作业1怎样用Google Colab完成Stanford CS224n的作业吴恩达老师又又又出新课了，这次是TensorFlow如何从0开始 在Google Colab上训练一个图片分类的卷积神经网络TensorFlow图片分类实战（吴恩达TensorFlow系列新课第一课完结篇）10 Colab对于知名教材的支持？ 见 在Google Colab中测试李航《统计学习方法》Python代码怎样用Google Colab完成周志华西瓜书的习题11 Colab也可以跑竞赛的数据？ 见 如何通过参加 数据分析竞赛 来提高算法水平（以”达观杯” 文本挖掘比赛为例）12 Colab支持TensorFlow2.0吗？ 见 用Google Colab尝鲜测试TensorFlow 2.013 也可以用Colab进行Python的基本学习吗？ 可以 如用Colab学习Matploylib库 见 用matplotlib可视化初步14 Colab支持GAN吗？ 见 如何在Google Colab运行你的第一个GAN模型这里只测试了最简单的GAN模型，Colab也支持更加复杂的GAN模型训练，如 https://github.com/shaoanlu/faceswap-GAN 15 如何搜索支持Colab的源代码？ Github,Seedbank项目 jupyter notebook格式的均支持 上传并使用数据文件 我们一般都需要在 Colab 笔记本中使用数据，对吧？你可以使用 wget 之类的工具从网络上获取数据，但是如果你有一些本地文件，想上传到你的谷歌硬盘中的 Colab 环境里并使用它们，该怎么做呢？ 很简单，只需 3 步即可实现！ 首先使用以下命令调用笔记本中的文件选择器： 12from google.colab import filesuploaded = files.upload() 运行之后，我们就会发现单元 cell 下出现了“选择文件”按钮： 这样就可以直接选择你想上传的文件啦！ 选择文件后，使用以下迭代方法上传文件以查找其键名，命令如下： 12for fn in uploaded.keys(): print('User uploaded file "&#123;name&#125;" with length &#123;length&#125; bytes'.format(name=fn, length=len(uploaded[fn]))) 例如待上传的是 iris.csv 文件，若运行没有问题的话，应该出现类似下面的提示语句： User uploaded file “iris.csv” with length 3716 bytes 最后，就使用以下命令将文件的内容加载到 Pandas 的 DataFrame 中了： 1234import pandas as pdimport iodf = pd.read_csv(io.StringIO(uploaded['iris.csv'].decode('utf-8')))print(df)]]></content>
      <categories>
        <category>工具和环境</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F06%2F02%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2FPytorch%E7%AC%94%E8%AE%B0%2FAutograd%2F</url>
    <content type="text"><![CDATA[PyTorch学习笔记(12)——PyTorch中的Autograd机制介绍 Pytorch从入门到放弃（5）——取消测试与验证阶段的梯度]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F06%2F02%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2FPytorch%E8%8E%AB%E7%83%A6%2F%E5%BB%BA%E9%80%A0%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[https://morvanzhou.github.io/tutorials/machine-learning/torch/3-01-regression/ 关系拟合 (回归)要点我会这次会来见证神经网络是如何通过简单的形式将一群数据用一条线条来表示. 或者说, 是如何在数据当中找到他们的关系, 然后用神经网络模型来建立一个可以代表他们关系的线条. 建立数据集我们创建一些假数据来模拟真实的情况. 比如一个一元二次函数: y = a * x^2 + b, 我们给 y 数据加上一点噪声来更加真实的展示它. 123456789import torchimport matplotlib.pyplot as pltx = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1) # x data (tensor), shape=(100, 1)y = x.pow(2) + 0.2*torch.rand(x.size()) # noisy y data (tensor), shape=(100, 1)# 画图plt.scatter(x.data.numpy(), y.data.numpy())plt.show() 建立神经网络建立一个神经网络我们可以直接运用 torch 中的体系. 先定义所有的层属性(__init__()), 然后再一层层搭建(forward(x))层于层的关系链接. 建立关系的时候, 我们会用到激励函数, 如果还不清楚激励函数用途的同学, 这里有非常好的一篇动画教程. 12345678910111213141516171819202122232425import torchimport torch.nn.functional as F # 激励函数都在这class Net(torch.nn.Module): # 继承 torch 的 Module def __init__(self, n_feature, n_hidden, n_output): super(Net, self).__init__() # 继承 __init__ 功能 # 定义每层用什么样的形式 self.hidden = torch.nn.Linear(n_feature, n_hidden) # 隐藏层线性输出 self.predict = torch.nn.Linear(n_hidden, n_output) # 输出层线性输出 def forward(self, x): # 这同时也是 Module 中的 forward 功能 # 正向传播输入值, 神经网络分析出输出值 x = F.relu(self.hidden(x)) # 激励函数(隐藏层的线性值) x = self.predict(x) # 输出值 return xnet = Net(n_feature=1, n_hidden=10, n_output=1)print(net) # net 的结构"""Net ( (hidden): Linear (1 -&gt; 10) (predict): Linear (10 -&gt; 1))""" 训练网络训练的步骤很简单, 如下: 123456789101112# optimizer 是训练的工具optimizer = torch.optim.SGD(net.parameters(), lr=0.2) # 传入 net 的所有参数, 学习率loss_func = torch.nn.MSELoss() # 预测值和真实值的误差计算公式 (均方差)for t in range(100): prediction = net(x) # 喂给 net 训练数据 x, 输出预测值 loss = loss_func(prediction, y) # 计算两者的误差 optimizer.zero_grad() # 清空上一步的残余更新参数值 loss.backward() # 误差反向传播, 计算参数更新值 optimizer.step() # 将参数更新值施加到 net 的 parameters 上 可视化训练过程为了可视化整个训练的过程, 更好的理解是如何训练, 我们如下操作: 1234567891011121314151617181920import matplotlib.pyplot as pltplt.ion() # 画图plt.show()for t in range(200): prediction = net(x) # 喂给 net 训练数据 x, 输出预测值 loss = loss_func(prediction, y) # 计算两者的误差 optimizer.zero_grad() # 清空上一步的残余更新参数值 loss.backward() optimizer.step() # 接着上面来 if t % 5 == 0: # plot and show learning process plt.cla() plt.scatter(x.data.numpy(), y.data.numpy()) plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5) plt.text(0.5, 0, 'Loss=%.4f' % loss.data.numpy(), fontdict=&#123;'size': 20, 'color': 'red'&#125;) plt.pause(0.1) 区分类型要点这次我们也是用最简单的途径来看看神经网络是怎么进行事物的分类. 建立数据集我们创建一些假数据来模拟真实的情况. 比如两个二次分布的数据, 不过他们的均值都不一样. 1234567891011121314151617181920import torchimport torch.nn.functional as Fimport matplotlib.pyplot as plt# torch.manual_seed(1) # reproducible# make fake datan_data = torch.ones(100, 2)x0 = torch.normal(2*n_data, 1) # class0 x data (tensor), shape=(100, 2)y0 = torch.zeros(100) # class0 y data (tensor), shape=(100, 1)x1 = torch.normal(-2*n_data, 1) # class1 x data (tensor), shape=(100, 2)y1 = torch.ones(100) # class1 y data (tensor), shape=(100, 1)x = torch.cat((x0, x1), 0).type(torch.FloatTensor) # shape (200, 2) FloatTensor = 32-bit floatingy = torch.cat((y0, y1), ).type(torch.LongTensor) # shape (200,) LongTensor = 64-bit integer# The code below is deprecated in Pytorch 0.4. Now, autograd directly supports tensors# x, y = Variable(x), Variable(y)plt.scatter(x.data.numpy()[:, 0], x.data.numpy()[:, 1], c=y.data.numpy(), s=100, lw=0, cmap='RdYlGn')plt.show() 建立神经网络建立一个神经网络我们可以直接运用 torch 中的体系. 先定义所有的层属性(__init__()), 然后再一层层搭建(forward(x))层于层的关系链接. 这个和我们在前面 regression 的时候的神经网络基本没差. 建立关系的时候, 我们会用到激励函数, 如果还不清楚激励函数用途的同学, 这里有非常好的一篇动画教程. 123456789101112131415161718192021222324import torchimport torch.nn.functional as F # 激励函数都在这class Net(torch.nn.Module): # 继承 torch 的 Module def __init__(self, n_feature, n_hidden, n_output): super(Net, self).__init__() # 继承 __init__ 功能 self.hidden = torch.nn.Linear(n_feature, n_hidden) # 隐藏层线性输出 self.out = torch.nn.Linear(n_hidden, n_output) # 输出层线性输出 def forward(self, x): # 正向传播输入值, 神经网络分析出输出值 x = F.relu(self.hidden(x)) # 激励函数(隐藏层的线性值) x = self.out(x) # 输出值, 但是这个不是预测值, 预测值还需要再另外计算 return xnet = Net(n_feature=2, n_hidden=10, n_output=2) # 几个类别就几个 outputprint(net) # net 的结构"""Net ( (hidden): Linear (2 -&gt; 10) (out): Linear (10 -&gt; 2))""" 训练网络训练的步骤很简单, 如下: 1234567891011121314# optimizer 是训练的工具optimizer = torch.optim.SGD(net.parameters(), lr=0.02) # 传入 net 的所有参数, 学习率# 算误差的时候, 注意真实值!不是! one-hot 形式的, 而是1D Tensor, (batch,)# 但是预测值是2D tensor (batch, n_classes)loss_func = torch.nn.CrossEntropyLoss()for t in range(100): out = net(x) # 喂给 net 训练数据 x, 输出分析值 loss = loss_func(out, y) # 计算两者的误差 optimizer.zero_grad() # 清空上一步的残余更新参数值 loss.backward() # 误差反向传播, 计算参数更新值 optimizer.step() # 将参数更新值施加到 net 的 parameters 上 可视化训练过程为了可视化整个训练的过程, 更好的理解是如何训练, 我们如下操作: 12345678910111213141516171819202122232425import matplotlib.pyplot as pltplt.ion() # 画图plt.show()for t in range(100): ... loss.backward() optimizer.step() # 接着上面来 if t % 2 == 0: plt.cla() # 过了一道 softmax 的激励函数后的最大概率才是预测值 prediction = torch.max(F.softmax(out), 1)[1] pred_y = prediction.data.numpy().squeeze() target_y = y.data.numpy() plt.scatter(x.data.numpy()[:, 0], x.data.numpy()[:, 1], c=pred_y, s=100, lw=0, cmap=&apos;RdYlGn&apos;) accuracy = sum(pred_y == target_y)/200. # 预测中有多少和真实值一样 plt.text(1.5, -4, &apos;Accuracy=%.2f&apos; % accuracy, fontdict=&#123;&apos;size&apos;: 20, &apos;color&apos;: &apos;red&apos;&#125;) plt.pause(0.1)plt.ioff() # 停止画图plt.show()]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F06%2F02%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%97%AE%E9%A2%98%E7%AD%94%E7%96%91%2FAdaboost%2F</url>
    <content type="text"><![CDATA[Adaboost 想问下机器学习实战这里是不是写错了，应该是大于1的是吗？ 你看那个参数threshIneq，它取值lt或gt。lt表示小于阈值的为-1，gt表示大于阈值的为-1 绘制roc曲线的代码逻辑都能看的懂，该函数接收的是训练样本被分类的概率值，将这些概率值按由小到大的顺序依次输出，如果是正例，则曲线向下移动，反之向左移动。老师能透彻的解释下，为什么这样绘制就是roc曲线吗？roc不是在不同的阈值下真阳率和假阳率的对应关系吗？ 先要了解正阳率和假阳率的定义。正阳率是预测的正类中确实是正类占所有真实正类的概率，假阳率是预测的正类中确实是负类占所有真实负类的概率。曲线横坐标是假阳率，纵坐标是真阳率。注意，书中的代码跟一般的的做法不一样，一般的做法是将概率从大到小排列，曲线从左下角开始画，原理是一样的。 老师您好，请问在做回归预测的时候，多项式构造选取多少个属性合适?随机森林选取的属性 重要性小于多少的可以摒弃，一般取前几? 这没有固定的答案，看具体问题能有几个较好的特征，剔除不必要特征。可以看看特征选择算法。随机森林每棵树可以随机选择一部分特征进行训练。 kMeans 如何理解kmeans++算法在解决标准kmeans算法执行时初始质心选择的的作用？该算法的第3步该如何理解，1.先从数据库随机挑个随机点当“种子点”2.对于每个点，都计算其和最近的一个“种子点”的距离D(x)并保存在一个数组里，然后把这些距离加起来得到Sum(D(x))。3.然后，再取一个随机值，用权重的方式来取计算下一个“种子点”。这个算法的实现是，先取一个能落在Sum(D(x))中的随机值Random，然后用Random -= D(x)，直到其&lt;=0，此时的点就是下一个“种子点”。4.重复2和3直到k个聚类中心被选出来5.利用这k个初始的聚类中心运行标准的k-means算法 kmeans选择初始化质心有不同的方法，可以选择批次距离尽可能远的K个点，也可以选用层次聚类算法BIRCH和ROCK或者Canopy。具体的还要看具体文献或出处。 SVM 请问SVM中的惩罚因子C的作用是什么，如何调节误分类点的个数和距离二者的关系，为什么当C无穷大时，软间隔就等价于硬间隔 SVM中允许有分类错误的点时引入了参数C，C表示尽可能选择宽边界和尽可能不要犯错两者之间的权衡。C越大表示希望得到更少的分类错误，即不惜选择窄边界也要尽可能把更多点正确分类；C越小表示希望得到更宽的边界，即不惜增加错误点个数也要选择更宽的分类边界。这种处理问题的思路有点类似于正则化。 当C无穷大时候，可以想象选择极窄的边界让所有的点都分类正确，也就等价于硬间隔了。也可以从另一方面来看，硬间隔满足 0≤αn，软间隔满足 0≤αn≤C，当C无穷大时，0≤αn≤C等价于0≤αn。 想问一下关于alpha的选择问题，首先，条件a两个alpha必须要在间隔边界之外，b这两个alpha还没有进行过区间化处理或者不在边界上。我的问题是，1)区间话处理是什么意思？2)程序中alpha(i)的选择是在边界之外，但alpha(j)的选择确实非i的任意一个，这能保证alpha(j)满足两个条件吗？其次，随着程序的运行alpha(i)也会选到已经更新的alpha(j),这是不是和未区间化的条件向矛盾 书中的做法是先找到违背条件的alpha1，然后alpha2取其误差与alpha1相差较大的那个，这样减少迭代次数。使用最优化解出alpha2后，根据其与alpha1的关系求得其值。 针对逻辑回归的这个推导看不明白，您能给讲下吗？数学哪方面知识是讲导数参与运算的，您能大体说下或有相关资料吗 逻辑回归的偏导数计算是比较简单的，用到了指数偏导计算。高数书里有基本的偏导数计算方法。 这边有个项目需要用GBDT去做训练，如果训练样本数大概是60万左右，正负样本比例是1:5左右 很不均衡，如果只是通过下采样使之达到均衡的话，觉得训练样本量20万，有点少，这种情况，一般按照什么方式去处理比较好？ 可以试试上采样构造正样本，或者即使正负样本比例不同，采用不同的类别权重，在损失函数中使用。 SVM中参数C的调教:C和松弛向量的乘积加入到了目标函数中，如果要使目标函数有最小值，就是要C和松弛向量的乘积越小，那是不是可以看成:C越大，松弛向量越小，那1-ξ就越大，间距就越大，容错就越大。为啥不能看成这样啊？我看答案是C小点儿越好。。。。 C是权衡犯错率和宽间隔的，C越大表示宁愿间隔小也要分类正确，对错误的容忍度小。小的C值争取获得更宽的边界。 请问有没有用svm做多分类的代码可以分享给我的 可以使用OVO方式，没有手写代码，可以使用libSVM库 请问这个最小值应该在边界上达到是为什么？ 举个简单的例子，y=(x-1)^2，x在1处取得最小值。但是如果x的取值范围是[2,3]，那么只能在x的取值边界上得到最小值。 svm里 1 优化目标 maxmin 为什么要换成minmax 是为了引入基变换？方便计算？2 xi的位置是即可以是原始的特征 也可以是转换后由基函数表示吗 ？将不可分映射到高维可分？3 合约页损失函数是岭回归的思想吗 1.我们习惯解决最小化的优化问题，便于使用优化方法求解； \2. 不太明白意思，xi可以是原始特征，但引入kernel后，可以认为是映射到高维，得到非线性分类面； \3. 你说的应该是合页损失函数，它与岭回归不同，岭回归是平方误差函数加上了L2正则化项目，用于回归问题而不是分类问题。网上搜一下二者的区别很容易查到。 老师，这一步是你的博客上LinearSVM的代码，没看懂，感觉代码的逻辑说不通，并没有提取出每一个测试样本正例的分数 这篇文章来源于之前看cs231n写的笔记。它的LinearSVM使用的Hinge Loss，使用梯度下降算法计算的，比较简单。这里还是以理解传统的SVM为主。 统计学习超平面是什么？ 这里的超平面可以通俗理解为分类问题中的分类面，例如二维平面中的分类线，三维空间的分类面，对应到n维，就叫超平面。 老师，SVM中找到不满足KKT条件公式如图所示。但在实际代码中实现如下。这是为什么。 if ((self.y_train[i] Ei &lt; -self.toler) and (self.alpha[i] &lt; self.C) or (self.y_train[i] Ei &gt; self.toler) and (self.alpha[i] &gt; 0)): 你把代码中Ei=y-y_train代入，移位一下就会发现与理论是一样的了。我在之前SVM直播答疑的视频里讲过，你可以去看看。《机器学习实战》书训练营直播间 - 网易云课堂 老师，为什么李航的统计学128页在SVM中提出SMO算法在每个子问题中选择两个变量优化，其中至少一个变量是违反KKT条件的。 选择两个变量的原因是所有的alpha满足下面图片所示的这个条件，为了保证等式成立，必须同时优化两个alpha。违反KKT条件是因为先找到违背KKT条件的点，让其满足条件。若所有的点都满足KKT条件，则优化结束了。 老师，为什么因为先找到违背KKT条件的点，让其满足条件。这一步不理解 优化的目的是让所有点都满足KKT条件，因为满足KKT条件了，就得到最优化了。使用SMO是把整个优化问题切分成一个个小的优化问题，每次对两个alpha进行优化，让其满足KKT条件。 那找到违背KKT条件的点是不是要把这个点排除掉吗 更新alpha的值，让它满足KKT 老师，我想问一下机器学习实战第八章的内容，图中高斯核函数中的距离，指的是x轴的距离吗? 同时书中给的代码，感觉是x轴距离的平方，不知道我理解的对不对。 这里就是高斯核，x表示所有坐标轴，不是单指x轴。以书中代码为准，平方。 老师您好，机器学习实战svm这章KernelTrans这个函数里A表示什么?kTup[1]又表示的是什么，不是很明白，望解答一下 这里的A就是每个训练样本，kTup是一个元组，kTup[0]表示核函数类型，kTup[1]表示高斯函数方差，sigma。 老师，请问使用svm做回归时，怎样评价模型的好坏 一般的回归模型评价指标可以是均方误差（MSE）、均方根误差（RMSE）、平均绝对误差（MAE）、校正决定系数（Ajusted R-Squre）等。 老师你好，我想问一下关于b阈值的理解，有效相等的话b1=b2，除此之外无效取值有什么意义嘛 第一种情况是该样本点是支持向量时，可以直接计算得到 b，若不是支持向量，b取两个b1和b2的均值。 决策树 决策树有两个优点是对中间值缺失不敏感，可以处理不相关的特征数据，如何理解 第一个优点是对缺失值不敏感，因为决策树不是基于距离度量，大部分时候可以在数据有缺失的时候使用。如果涉及到距离度量，缺失数据就变得比较重要。第二个优点是可以处理不相关特征，树形结构并不要求特征之间具有较高相关性。 请问在决策树中找出最好的数据集划分方式，可不可以理解为以每一列为特征值计算熵，然后找出最小的呢？ 对每个特征计算条件熵，可以理解为条件熵越小，信息增益越大，就以该特征进行划分。 能把这个注解函数的各个参数详细的说下嘛 这里使用的是python里的annotate，网上资料很多，可以自己搜一下https://blog.csdn.net/leaf_zizi/article/details/82886755 一般决策树用于连续值划分的用例多不多，西瓜书上决策树对连续值处理的理论我没太看明白 你说的应该是CART算法，应用蛮多的，Random Forest、GBDT都会用到决策树。CART部分建议好好看看，西瓜书理论对初学者不太友好，网上搜一搜简洁教程。 老师、请问这里为什么要分两段（featVec[：axis]和featVec[axis+1:]）添加featVec的信息？ 因为要把featVec[axis]这个特征删去。第三章决策树里面，它是使用一个特征之后就把该特征删去的，就像我们根据条件判断一样，这个条件使用过了就不会再用了。 k-近邻算法 k近临算法的思路，是不是待观测值与训练数据之间求距离，然后寻找距离最短的点，认为与最短距离的点位一类。 那我想不明白的，图片中第一个矩形框的内容为什么这样写？第2框是对距离排序，而labels并没有排序，那怎么确定第三个框中labels是最短距离所对应的label？#k邻近 sortedDistIndicies存储的就是距离最小对应的label中类的下标。建议每个变量用print函数打印出来看看，加深理解。 首先，第一个框目的是求一个点和所有点的距离差值，这里的写法是把维度匹配上，做一个向量减法。换句话说就是把待测点的坐标复制出很多份（数量由训练数据大小决定），然后做向量减法。第二个框sort的是index不是里面的值，就是根据值sort了label。 ax.scatter(returnMat[:,0],returnMat[:,1],15.0np.array(classLabelVector), 15.0np.array(classLabelVector)) 请问下，scatter这个函数第三第四个参数为啥都乘以15，而且为啥需要两个相同的参数？ 第三个参数是s表示大小，第四个参数c表示颜色。之所以用label是让每个类别呈现不同的大小和颜色。15.0是变量，你可以调整为其他值看看效果。 knn算法为什要对数据进行normalization预处理，normalization方法有哪几种？ 一般的机器学习算法都会对输入进行归一化，其主要目的是将各个特征归一化到相似尺度，提高训练精度。如果是梯度下降算法的话还会提高训练速度。常用的归一化有线性归一化，标准差标准化，非线性归一化等。 kdtree可以实现k邻近的搜索吗？看李航老师的书在讲knn的时候叙述了kdtree，只实现了最邻近。 当然可以！kdtree只是使用了特殊的存储结构，可以实现最近邻，也可以实现k近邻。而且李航的书中也说了。附图： 能请教一下kdtree实现k邻近的思路吗，是需要遍历所有的节点吗，想编程实现来着，但是对于在另一半子树中存在比当前子树中多个点更近的点时，想不出来怎么解决，网上也没看到资料 既然能找到最邻近点，一定能找到k邻近点。例如k=3，设置一个包含3个元素的数组，在往上寻找的时候总是把最邻近的3个点保存下来。最后统计最多所占的类别就好。 但是搜索完根节点的左子树，假设找到了三个点，怎么确定右子树中没有比这三个点更近的点，是需要在走一遍右子树吗，就是这没想通。在李航老师树的42页那个特征空间划分的图中，如果我想查找(6,1)并且k=3，走完左子树后查到三个最近的点，然后右子树还有一个(8，1)，比左子树中部分点更近，这种应该怎么办？ 这还是需要在继续便利，直到右子树也遍历完，像你说的这种情况，会让右子树中的小值替换当前3个元素的数组中的大值，直到找出整个树中最小的前3个值。 KNN算法中的K是怎样取值的？越大越好？k它有什么意义吗? K一般没有固定的取值，根据具体问题具体分析。一般可以使用交叉验证选择最佳的K值。 实战书的k近邻算法第二章第四小节说，”k近邻算法的另一个缺陷是它无法给出任何数据的基础信息结构，因此我们也无法知晓平均实例样本和典型实例样本具有什么特征。下一章我们将使用概率测量方法处理分类问题，该算法可以解决这个问题”。疑问1:没理解这个缺陷是什么缺陷，样本数据每个纬度值都有了，为什么说无法知晓具有什么特征和基础信息结构？疑问二:为什么说概率测量方法可以解决？ 1、knn是基于实例的学习，训练的时候使用的是样本所有的输入值进行距离计算，例如图片识别中一张图片所有的像素点。整个过程并没有提取样本本身的固有特征。训练过程保持了所有的训练样本。2、决策树使用信息增益寻找最佳划分特征，信息增益是通过训练样本中的概率测量方法得到的。 knn运行加载学会数据出错，是什么原因呢 看下你的“datingTestSet.txt”数据集，类别标签不是int型，你可以是使用“datingTestSet2.txt”数据集，label是int型的，或者修改file2matrix函数。这些都在训练营的github上：https://github.com/RedstoneWill/MachineLearningInAction-Camp 老师，K近邻算法中如何理解？voteIlabel = labels[sortedDistIndicies[i]] ？为什么不能写成voteIlabel = labels[sortedDistIndicies==i] ？（当i 为0时 取sortedDistIndicies为0的那项） sortedDistIndicies存储的是从大到小排列，距离最近的label下标，即位置。然后找到前k个点，计这k个点属于哪一个类别，统计最多的那一个类别就是预测类别。建议把每行语句打印出来看看，这样理解得比较透彻。 朴素贝叶斯 如何用极大似然估计法推出概率估计公式（4.8）和（4.9）。也是统计学习方法朴素贝叶斯法中的课后习题。 https://blog.csdn.net/xiaoxiao_wen/article/details/54097917 最近我在做文本分类的题目，按理说使用word2vec进行模型建立后，效果应该比用countvectorizer好的，但是我使用word2vec反而效果差了一些，不太懂为什么会这样？ 可能跟样本集、算法、模型都有关系，没有说某个模型一定好。 想问一下图片上是怎么由1式得到的2式啊（记号处） 这是条件期望的表达式，可以看成是取每个类别的概率乘以当前类别的期望风险，最后所有类别的总和。 这个的解答，能直接算p(y=1!2,s)和p(y=-1|2,s)的概率，哪个大就代表是哪个类别吗 是的，朴素贝叶斯公式中，由于分母是全概率都是一样的，所以一般比较分子就行了。 针对4.5.2的公式和例子有两个问题1.例子中的文档和词条，谁是w谁是c?我理解c是词条，w是文档分类2.在4.5.3的classifyNB的计算中并没有除p(w) \1. c 意为 class，c 是类别，w 是特征词条。 \2. 之所以不除以 p(w) 是因为计算所有c别可能性的时候，p(w) 都是相同的，比较大小的时候只看分子就行了。 请问老师，在书上朴素贝叶斯分类器过滤垃圾邮件中，图中这几行代码该怎么理解？trainMatrix不是文本向量吗？+＝操作是咋回事？ p1Num 和 p0Num 中的 += 操作是向量相加，统计的是训练样本两个类别每个单词出现的次数，p1Denom 和 p0Denom 中的 += 操作是数字相加，统计的是训练样本两个类别各自总的单词数。 老师，我想问下《机器学习实战》71页程序清单4-7这里，为什么要条件概率大于-6.0的单词加入到列表里，-6.0意味着什么 条件概率加了log，可能会出现负值。这里为什么是-6.0可能是个阈值把。这4.7节不是我们需要完成的任务。原始网站数据因为是外网，所以爬不到，就不用看了。 其他疑问 想问一下逻辑回归求参数为什么不是直接求导 而是要梯度下降呢 像SVM这种不都是直接求导结合拉格朗日就可以求参数吗 首先并不是所有函数都可以直接令导数为零求得极值的，有时可以求出导数在每个点的值, 但是直接解方程解不出来。对计算机而言，更加适合用循环迭代的方法来求极值，即梯度下降。SVM的解是二次规划问题，对于二次规划问题，有经典的最速下降法，牛顿法等，并不是简单的直接求导。 训练数据集经常出现训练数据集里面没有的属性，这种属性训练的有什么用吗？这么利用，比如下图，希望老师解答的详细一点，之前没有接触过这种数据训练 我看的是训练集里有一些特征，但是在测试集中却并未出现。尝试使用特征工程找出不同特征之间的相互关系。或者简单的方法忽略这些特征，训练的模型在测试集上测试看看效果如何。 请问训练营的10.1的阅读材料是台大机器学习基石的PLA, 训练营大纲是从knn开始的，1. 请问国庆节后训练营从哪个算法开始呢？2. 我们平时自己学习的时候按照实战的顺序学习吗？3. 那您博客的基石与技法的笔记要不要看呢？毕竟他们的内容不一样 4. 是否老师根据自己的需要安排算法的讲解顺序，并不是完全参考书本，然后讲解的算法内容与台大的笔记相结合学习？ 我自己比较纠结学习顺序，谢谢老师解答\ 你好，1. 训练营的课程大纲是按照《机器学习实战》这本书为基础的，国庆假期的任务是我给大家安排的选修作业，并不在我们的任务要求之内，有兴趣的同学可以做一做。国庆后从决策树开始，具体见知识圈。2. 按照《机器学习实战》顺序。3. 我的个人博客、微信公众号有不错的资源和文章，大家可以作为参考资料看看。4. 目前这一期的训练营我们还是按照《机器学习实战》这本书的章节顺序来学习的，跟着我们大纲的顺序学习就好了。后期如果有调整会告知大家。 关于PLA算法的实现，我想问一下这里的x1和x2是随便设的嘛，为什么y1和y2要这样算呢 这里是画出当前w对应的分类线，x1和x2选取合适的值就好。y1和y2是根据分类线表达式推导的，见下图： 有没有什么好的，理论教材，比如最优化，凸优化这些？ Bubeck的《Convex Optimization: Algorithms and Complexity》。最好根据自己实际情况找到最适合自己的就行。 能推荐对极大似然估计和softmax解释得通俗易懂的博客？ 没有专门的推荐，网上资料很多，CSDN和博客园的文章都不错。Softmax的有一篇可以看看：https://mp.weixin.qq.com/s/XBK7T1P7z3rm3o-3BDNeOA 例如lstm的变体GRU，每个参数在编程中如何体现的，以及W如何设置呢？能写个样例吗？要是能debug到细节更好了！ 原理上与一般神经网络类似，使用梯度下降更新参数。现在多是直接调用深度学习框架来做，自己手写LSTM没啥必要。学有余力可以直接看看深度学习框架中LSTM的实现源码。 我安装scikit-learn的时候出现这个问题怎么解决呢 换个镜像源试试。建议使用Anaconda自带的conda，直接输入：conda install scikit-learn即可。 能详细介绍一下lstm的代码实现，以及每个参数的含义吗？最好是能跑起来的程序！现在网上搜的程序规模太大！不适合从浅入深的学习！ 这个问题太大了，现在基本都是使用tensorflow或pytorch等库来实现LSTM。如果是入门的话，建议看看莫烦的教学视频，这里面讲到了LSTM：https://morvanzhou.github.io/tutorials/machine-learning/torch/ 机器学习工具包sklearn，有没有比较好的教程，以及还有其他的比较好的工具包吗 最好的教程就是官方文档。ApachenCN翻译了中文版，可以参考：http://sklearn.apachecn.org/ 机器学习最好的就是sklearn了，其它的深度学习如TensorFlow，PyTorch，Keras等 老师，请问跟着你学完这门课程，还需要学习哪些东西，才能找到一分机器学习的工作呢？ 这是一个很大又很实际的问题。首先学习这门课只是基础，帮助大家在理解一定机器学习理论的基础上使用python手写各个基本机器学习算法的实现过程。但掌握这些还远远不够。建议从三个方面入手：1. 补充机器学习理论基础知识，这点在找工作笔试、面试的时候是很重要的，例如SVM、AdaBoost、GBDT等等要吃得透一点。2. 提高代码编程能力，掌握对机器学习库的使用，例如scikit-learn、深度学习的tensorflow等。3. 有机会的话做一些项目积累经验，或者了解构建一个完整的机器学习项目的整个流程。 我这边在训练一个卷积网络的模型，但是我的数据中正样本很多，负样本很少，我觉得这样的数据训练出来的模型可能会有问题，我想请问一下数据集中正负样本的比例多大时训练出来的模型比较好？ 一般是正负样本近似相等的时候比较好。但实际中出现正负样本不均匀的情况，可以使用重采样和欠采样来尽量让其数量接近。 我把一个训练好的模型用C++进行调用的时候，我发现单次调用的时候用时100ms左右，但是做100次循环求平均用时5ms左右！请问您知道是什么原因导致的吗？ 第一次参数传到模型时耗时比较多。 老师您好，我今年27了，在一家国企的设计单位做轨道交通线路走向设计工作。我打算现在转行机器学习，是否来得及？因为年龄比较大了。 你好，任何时候想要转行机器学习都不晚。况且你也才27岁，还算年轻。建议原先的工作先干着，平时多学点机器学习，为找份机器学习工作做准备。我不知道你的基础如何，一般半年时间可以入门了。 关于PLA算法，我做机器学习基石作业1的第15题的时候，迭代次数一直都是21，网上的答案是45，不知道自己究竟是哪里出错了 PLA得到的分类超平面不是固定的，每次都可能产出新的结果，迭代次数也与初始选择的参数有关。你把最终的分类线在数据集上画出来看一下，正确的话就没有问题。 老师您好。本来想提一些关于实战书籍里的问题，结果发现琢磨着也很快看懂了。想来我的目的是来学习，并希望找一份相应的工作，请老师指点一下。我是一个走机器学习的转行人员，具备一年的python自学基础，目前毕业3年，软件测试干了2年，也算是IT行业吧。1,在这个深入一点的IT行业，机器学习里面，实际上机器学习会有怎样的使用。(应该不仅仅是kaggle或者天池020刷题一样吧？)2,机器学习涉及到大量数据的处理，我想问一下这些数据也是我自己来获取吗？3,对于面试，我应该准备到多少？(不会面多年经验的，所以就从初等看吧。)4,对于这种面试的情况下，我们应该面试的工资在什么范围？ 你好。1.实际工程项目与打比赛区别还是很大的。比较仅仅考虑的是模型的准确率而不惜使用非常复杂、臃肿的模型。但是再工程应用上，除了考虑性能之外，还要注重速度、资源消耗、成本等各个方面。实际上机器学习有很大应用，比如推荐系统、图像识别等。2.数据不用自己获取，网络上有大量可供下载的数据集。公司里的话，也会有专门的人做数据收集、清洗等工作的。但是机器学习工程师也多少做过数据收集这些事情。3.面试的话多多准备一些机器学习典型问题的知识点，比如SVM、集成学习AdaBoost等，还有你的项目经验。4.这个得根据工作地点、公司、具体什么工作等来确定。可以根据当地IT均资来设个心理价位。 LSTM中的cell state表示什么意思？ cell state一般是保存模型当前及历史状态。有点像是传输带，它直直地流过整个链，受到轻微的非线性相互作用影响。因此信息可以轻松地沿它流动而不发生改变。可以看下这篇文章：Understanding LSTM Networks — colah’s blog 老师你好，学习算法需不需要每一步自己去证明它呢? 其实，对大部分人来说，机器学习算法的每一步详细的数学证明是不需要的。但是我们要感性地理解它的意思和推导方式。就像SVM中涉及的理论推导很多，拉格朗日那块内容每一步推导要大致知道思路和方法，但详细的数学证明可能就不需要了。]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F06%2F02%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2FPytorch%E8%8E%AB%E7%83%A6%2Fpytorch%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[https://morvanzhou.github.io/tutorials/machine-learning/torch/2-01-torch-numpy/ 基于0.1.11的版本 用 Numpy 还是 Torch我们对 Numpy 还是爱不释手的, 因为我们太习惯 numpy 的形式了. 不过 torch 看出来我们的喜爱, 他把 torch 做的和 numpy 能很好的兼容. 比如这样就能自由地转换 numpy array 和 torch tensor 了: 1234567891011import torchimport numpy as npnp_data = np.arange(6).reshape((2, 3))torch_data = torch.from_numpy(np_data)tensor2array = torch_data.numpy()print( '\nnumpy array:', np_data, # [[0 1 2], [3 4 5]] '\ntorch tensor:', torch_data, # 0 1 2 \n 3 4 5 [torch.LongTensor of size 2x3] '\ntensor to array:', tensor2array, # [[0 1 2], [3 4 5]]) Torch 中的数学运算其实 torch 中 tensor 的运算和 numpy array 的如出一辙, 我们就以对比的形式来看. 如果想了解 torch 中其它更多有用的运算符, API就是你要去的地方. 12345678910111213141516171819202122# abs 绝对值计算data = [-1, -2, 1, 2]tensor = torch.FloatTensor(data) # 转换成32位浮点 tensorprint( &apos;\nabs&apos;, &apos;\nnumpy: &apos;, np.abs(data), # [1 2 1 2] &apos;\ntorch: &apos;, torch.abs(tensor) # [1 2 1 2])# sin 三角函数 sinprint( &apos;\nsin&apos;, &apos;\nnumpy: &apos;, np.sin(data), # [-0.84147098 -0.90929743 0.84147098 0.90929743] &apos;\ntorch: &apos;, torch.sin(tensor) # [-0.8415 -0.9093 0.8415 0.9093])# mean 均值print( &apos;\nmean&apos;, &apos;\nnumpy: &apos;, np.mean(data), # 0.0 &apos;\ntorch: &apos;, torch.mean(tensor) # 0.0) 除了简单的计算, 矩阵运算才是神经网络中最重要的部分. 所以我们展示下矩阵的乘法. 注意一下包含了一个 numpy 中可行, 但是 torch 中不可行的方式. 1234567891011121314151617# matrix multiplication 矩阵点乘data = [[1,2], [3,4]]tensor = torch.FloatTensor(data) # 转换成32位浮点 tensor# correct methodprint( &apos;\nmatrix multiplication (matmul)&apos;, &apos;\nnumpy: &apos;, np.matmul(data, data), # [[7, 10], [15, 22]] &apos;\ntorch: &apos;, torch.mm(tensor, tensor) # [[7, 10], [15, 22]])# !!!! 下面是错误的方法 !!!!data = np.array(data)print( &apos;\nmatrix multiplication (dot)&apos;, &apos;\nnumpy: &apos;, data.dot(data), # [[7, 10], [15, 22]] 在numpy 中可行 &apos;\ntorch: &apos;, tensor.dot(tensor) # torch 会转换成 [1,2,3,4].dot([1,2,3,4) = 30.0) 新版本中(&gt;=0.3.0), 关于 tensor.dot() 有了新的改变, 它只能针对于一维的数组. 所以上面的有所改变. 1234tensor.dot(tensor) # torch 会转换成 [1,2,3,4].dot([1,2,3,4) = 30.0# 变为torch.dot(tensor.dot(tensor) 变量 (Variable)什么是 Variable在 Torch 中的 Variable 就是一个存放会变化的值的地理位置. 里面的值会不停的变化. 就像一个裝鸡蛋的篮子, 鸡蛋数会不停变动. 那谁是里面的鸡蛋呢, 自然就是 Torch 的 Tensor 咯. 如果用一个 Variable 进行计算, 那返回的也是一个同类型的 Variable. 我们定义一个 Variable: 12345678910111213141516171819202122import torchfrom torch.autograd import Variable # torch 中 Variable 模块# 先生鸡蛋tensor = torch.FloatTensor([[1,2],[3,4]])# 把鸡蛋放到篮子里, requires_grad是参不参与误差反向传播, 要不要计算梯度variable = Variable(tensor, requires_grad=True)print(tensor)""" 1 2 3 4[torch.FloatTensor of size 2x2]"""print(variable)"""Variable containing: 1 2 3 4[torch.FloatTensor of size 2x2]""" Variable 计算, 梯度我们再对比一下 tensor 的计算和 variable 的计算. 1234t_out = torch.mean(tensor*tensor) # x^2v_out = torch.mean(variable*variable) # x^2print(t_out)print(v_out) # 7.5 到目前为止, 我们看不出什么不同, 但是时刻记住, Variable 计算时, 它在背景幕布后面一步步默默地搭建着一个庞大的系统, 叫做计算图, computational graph. 这个图是用来干嘛的? 原来是将所有的计算步骤 (节点) 都连接起来, 最后进行误差反向传递的时候, 一次性将所有 variable 里面的修改幅度 (梯度) 都计算出来, 而 tensor 就没有这个能力啦. v_out = torch.mean(variable*variable) 就是在计算图中添加的一个计算步骤, 计算误差反向传递的时候有他一份功劳, 我们就来举个例子: 1234567891011v_out.backward() # 模拟 v_out 的误差反向传递# 下面两步看不懂没关系, 只要知道 Variable 是计算图的一部分, 可以用来传递误差就好.# v_out = 1/4 * sum(variable*variable) 这是计算图中的 v_out 计算步骤# 针对于 v_out 的梯度就是, d(v_out)/d(variable) = 1/4*2*variable = variable/2print(variable.grad) # 初始 Variable 的梯度&apos;&apos;&apos; 0.5000 1.0000 1.5000 2.0000&apos;&apos;&apos; 获取 Variable 里面的数据直接print(variable)只会输出 Variable 形式的数据, 在很多时候是用不了的(比如想要用 plt 画图), 所以我们要转换一下, 将它变成 tensor 形式. 1234567891011121314151617181920print(variable) # Variable 形式&quot;&quot;&quot;Variable containing: 1 2 3 4[torch.FloatTensor of size 2x2]&quot;&quot;&quot;print(variable.data) # tensor 形式&quot;&quot;&quot; 1 2 3 4[torch.FloatTensor of size 2x2]&quot;&quot;&quot;print(variable.data.numpy()) # numpy 形式&quot;&quot;&quot;[[ 1. 2.] [ 3. 4.]]&quot;&quot;&quot;]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F06%2F01%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2FPytorch%E8%AE%AD%E7%BB%83%E8%90%A5%2FD1%20pytorch%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[笔记见 幕布 pytorch和其他框架的区别 pytorch：接口简洁，面向对象设计中最优雅的一个。底层c tensorflow：接口复杂，且为静态图机制。底层c Mxnet：小众框架 Keras：高度封装，最容易上手，但不能自定义函数。底层是python，速度慢 Caffe：缺少灵活性。 智慧课堂项目 面向全国学生，识别学生的姿态，举手/睡觉/看书 困难 图片背景复杂，衣着、动作、体态都不一样 数据采集不均衡，比如举手的人很少，而看书/听课的人很多 有些动作差异不大，难以量化。比如举手和托腮 方案 骨架提取OpenPose。把背景复杂度降到最低 备选方案有目标检测、视频行为检测。很多具体项目使用单一方案很难达到预期效果，必须使用某些算法的组合，比如人体行为识别的项目，使用openpose提取骨架图+图像分类 打卡要求打卡要求：在训练和测试时自动求导的区别？如何调用CUDA？程序中如何使用多进程？ 打卡内容：文字或图片拍照提交，文字要求最少50字，图片要求最少3张 打卡截止时间：6/3 1、在训练和测试时自动求导的区别？12345678910import torchfrom torch.autograd import Variable x = Variable(torch.randn(5,5))z = Variable(torch.randn(5,5), requires_grad=True)b = x + zb.requires_grad"""True""" 训练时，需要开启自动求导，这样backward就可以自动计算所有的导数，更新权重。 而测试时，因为不需要更新权重，所以不用保存梯度，而pytorch默认是保存的，这就会导致测试时消耗无谓的资源。这个时候需要通过显式的设置requires_grad=False，这样测试和验证阶段就不会保存梯度了。 2、如何调用CUDA？1234567891011121314151617181920212223# 一种方法是定义时就在CUDAx = torch.cuda.FloatTensor(1)"""x.get_device() == 0此时会默认使用GPU 0也就是第一张显卡来进行操作"""# 也可以指定用哪一个显卡with torch.cuda.device(1): a = torch.cuda.FloatTensor(1)"""a.get_device() == 1""" # 另一种方法是从CPU复制到GPU，直接通过.cuda()ten1 = torch.FloatTensor(2)if torch.cuda.is_available(): ten1 = ten1.cuda()"""6.1101e+244.5659e-41[torch.cuda.FloatTensor of size 2 (GPU 0)]""" 3、程序中如何使用多进程？通过torch.multiprocessing使用多进程，扩展了python的multiprocessing，可以通过multiprocessing.Queue移动所有tensor的数据到共享内存中。 这种方式可以异步训练模型，参数可以一直共享，也可以定期同步。 pytorch安装官网选择版本 https://pytorch.org/get-started/locally/ 1pip3 install torch torchvision 目前稳定版是1.1版 安装后，import torch报错 12345678&gt;&gt;&gt; import torchTraceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; File &quot;/Users/david/anaconda3/lib/python3.6/site-packages/torch/__init__.py&quot;, line 79, in &lt;module&gt; from torch._C import *ImportError: dlopen(/Users/david/anaconda3/lib/python3.6/site-packages/torch/_C.cpython-36m-darwin.so, 9): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib Referenced from: /Users/david/anaconda3/lib/python3.6/site-packages/torch/lib/libshm.dylib Reason: image not found 解决： 1brew install libomp]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F21%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2Ftensorflow%2F%E5%9F%BA%E7%A1%80-%E8%8E%AB%E7%83%A6%E6%95%99%E7%A8%8B%2F5-2%20CNN%2F</url>
    <content type="text"><![CDATA[https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/5-03-A-CNN/ 流行的 CNN 结构比较流行的一种搭建结构是这样, 从下到上的顺序, 首先是输入的图片(image), 经过一层卷积层 (convolution), 然后在用池化(pooling)方式处理卷积的信息, 这里使用的是 max pooling 的方式. 然后在经过一次同样的处理, 把得到的第二次处理的信息传入两层全连接的神经层 (fully connected),这也是一般的两层神经网络层,最后在接上一个分类器(classifier)进行分类预测 我们在代码中实现一个基于MNIST数据集的例子 定义卷积层的 weight bias1234import tensorflow as tfpython from tensorflow.examples.tutorials.mnist import input_datamnist=input_data.read_data_sets(&apos;MNIST_data&apos;,one_hot=true) 定义Weight变量，输入shape，返回变量的参数。其中我们使用tf.truncted_normal产生随机变量来进行初始化: 123def weight_variable(shape): inital=tf.truncted_normal(shape,stddev=0.1) return tf.Variable(initial) 同样的定义biase变量，输入shape ,返回变量的一些参数。其中我们使用tf.constant常量函数来进行初始化:]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F19%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E6%A0%91%2FB%E6%A0%91%2F</url>
    <content type="text"><![CDATA[B-树1、为什么需要B-树之前讲的都是二叉树结构，其查找的时间复杂度$O(log_2 N)$ 与树的深度有关，那么降低深度就能提高查找效率。 在大规模的数据存储中，树节点存储的元素数量是有限的，这样导致二叉查找树结构由于树的深度过大而造成磁盘I/O读写过于频繁，进而导致查询效率低下。 假设树的高度是4，查找的值是10，流程如下： 二叉查找树的结构： 第1次磁盘IO： 第2次磁盘IO： 第3次磁盘IO： 第4次磁盘IO： 所以最坏情况下，磁盘的IO次数等于索引树的高度。为了减少IO次数，要把”瘦高”的树变得”矮胖”，这就是B-树的特征之一。 2、概念一个m阶的B树具有如下几个特征： 根结点至少有两个子女。 每个中间节点都包含k-1个元素和k个孩子，其中 m/2 &lt;= k &lt;= m 每一个叶子节点都包含k-1个元素，其中 m/2 &lt;= k &lt;= m 所有的叶子结点都位于同一层。 每个节点中的元素从小到大排列，节点当中k-1个元素正好是k个孩子包含的元素的值域分划。 以一个3阶B-树为例，看看具体结构 重点看(2, 6)节点，该节点有两个元素2，6，又有三个孩子。其中16，符合规则。 3、插入和删除节点3.1 插入B-树插入新节点的过程比较复杂，而且分成很多情况。举一个最典型的例子，假如我们要插入的值是4。 1）自顶向下查找4的节点位置，发现4应当插入到节点元素3，5之间。 2）节点3，5已经是两元素节点，无法再增加。父亲节点 2， 6 也是两元素节点，也无法再增加。根节点9是单元素节点，可以升级为两元素节点。于是拆分节点3，5与节点2，6，让根节点9升级为两元素节点4，9。节点6独立为根节点的第二个孩子。 可以看出，为了插入一个元素，B-树的很多节点都发生了连锁改变。但正因为如此，B-树能够始终保持平衡。 3.2 删除假如删除节点11 1）自顶向下查找元素11的节点位置。 2）删除11后，节点12只有一个孩子，不符合B树规范。因此找出12,13,15三个节点的中位数13，取代节点12，而节点12自身下移成为第一个孩子。（这个过程称为左旋） 参考资料漫画算法：什么是B树 https://blog.csdn.net/qq_35644234/article/details/66969238 https://blog.csdn.net/dog250/article/details/81151687 https://www.cnblogs.com/zhenbianshu/p/8185345.html?utm_source=debugrun&amp;utm_medium=referral]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F18%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E6%A0%91%2FB%2B%E6%A0%91%2F</url>
    <content type="text"><![CDATA[B+树1、概念 一个m阶的B+树具有如下几个特征： 有k个子树的中间节点包含有k个元素（B树中是k-1个元素），每个元素不保存数据，只用来索引，所有数据都保存在叶子节点。 即[卫星数据]。卫星数据指的是索引元素所指向的数据记录，比如数据库中的某一行。在Ｂ-树中，无论中间节点还是叶子节点都带有卫星数据。 而在B+树中，只有叶子节点带有卫星数据，其余中间节点仅仅是索引，没有任何数据关联。 所有的叶子结点中包含了全部元素的信息，及指向含这些元素记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接。 所有的中间节点元素都同时存在于子节点，在子节点元素中是最大（或最小）元素。 能够推导出，根节点的最大元素，也就等同于整个B+树的最大元素。以后无论插入删除多少元素，始终要保持最大元素在根节点中。 2、B+树的特点2.1 为什么MySQL的索引用B+树 B-树每个节点都有data，如果data很大会增大节点大小，可能会增加磁盘IO次数。B+树的所有data都在叶子节点，磁盘IO次数就少。 B+树所有的Data域在叶子节点，一般来说都会进行一个优化，就是将所有的叶子节点用指针串起来。这样遍历叶子节点就能获得全部数据，这样就能进行区间访问了。 (数据库索引采用B+树的主要原因是 B树在提高了磁盘IO性能的同时并没有解决元素遍历的效率低下的问题。正是为了解决这个问题，B+树应运而生。B+树只要遍历叶子节点就可以实现整棵树的遍历。而且在数据库中基于范围的查询是非常频繁的，而B树不支持这样的操作（或者说效率太低）) 比如要查询范围3-11的元素 B-树的范围查找过程 1）自顶向下，查找到范围的下限（3）： 2）中序遍历到元素6： 3）中序遍历到元素8： 4）中序遍历到元素9： 5）中序遍历到元素11，遍历结束： B+树的范围查找过程 1）自顶向下，查找到范围的下限（3）： 2）通过链表指针，遍历到元素6, 8： 3）通过链表指针，遍历到元素9, 11，遍历结束： 2.2 为什么MongoDB的索引用B-树它并不是传统的关系性数据库，而是以Json格式作为存储的nosql，目的就是高性能，高可用，易扩展。而由于 B+树内节点不存储数据，所有 data 存储在叶节点导致查询时间复杂度固定为 log n。而B-树查询时间复杂度不固定，与 key 在树中的位置有关，最好为O(1)。 尽可能少的磁盘 IO 是提高性能的有效手段。MongoDB 是聚合型数据库，而 B-树恰好 key 和 data 域聚合在一起。找到key就可以直接访问data。 HBase的索引——LSM树 LSM树 VS B+树 2.3 B+树的优点 单一节点存储更多的元素（这样该节点下分支变多了，树变矮胖了），使得查询的IO次数更少。 所有查询都要查找到叶子节点，查询性能稳定。 所有叶子节点形成有序链表，便于范围查询。 3、从B+树看MySQL索引设计 比如想要找出位于北京的所有90后用户列表，怎么设计索引。 先看一下sql会是怎么写的（比如地域，北京对应的是1） 1select * from xxx where localId=1 and age&lt;=28 如果给地域加个索引，再给年龄加个索引。如果单独对两个字段做索引，那么针对这个sql语句，其实只能用到地域这一个索引，年龄是不会走索引的。看了上面B+树的原理，应该很好理解这里的原因。 正确的索引设计应该是做联合索引： KEY(localId,age) 。这样如果只需要对地域做查询，可以用到这个索引，两个同时使用也可以用这个索引。但是如果只想对年龄做索引，这个索引是没有办法使用的（因为年龄在不同地域不是连续的）。 联合索引B+树是怎么存储的呢，其实也会做排序，先根据localId进行排序，再根据age进行排序存储。所以我们想直接用age进行索引的话就没法用这个联合索引了。但是想要找到北京的90后，就很容易可以找到。为什么呢，首先可以定位到localId为1的索引部分，接下来找到1岁开始，一直到28岁都拿出来。因为是个链表结构，所以这块的内容可以顺序索引出来而不需要再去重新走索引了。 如果想找出北京和天津的90后呢？显然有点麻烦，根据B+树的结构，找到北京，然后再去列出年龄范围数据；再去找到天津，再去列出年龄范围数据。所以还是需要找两遍，而没有办法一次就找出来两组数据不是连续的。 举下例子，比如数据叶子节点链表是这样的： （北京，1岁）、（北京，2岁）、（北京，3岁）、（上海，1岁），（上海，3岁）、（天津，2岁），（天津，3岁）。 索引找到北京，找对应年龄范围，而没有办法再通过链表找天津的年龄范围数据了。当地域城市的过滤条件多的话，其实效率就并不高了。 参考资料漫画算法：什么是B+树 从 MongoDB 及 Mysql 谈B-/B+树 B+Tree在数据库索引上拥有独特优势的原因 MySQL · 引擎特性 · B+树并发控制机制的前世今生) 透过B树、B+树来聊聊Mysql索引) 《MySQL技术内幕：InnoDB存储引擎》 浅析MySQL InnoDB中的B+树索引]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F18%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E6%A0%91%2F%E7%BA%A2%E9%BB%91%E6%A0%91%2F</url>
    <content type="text"><![CDATA[红黑树1、为什么需要红黑树1.1 BST的缺点BST有个比较大的缺陷，会影响查询性能。 依次插入5个节点，{7,6,5,4,3} 缺陷：多次插入新节点导致的不平衡。 1.2 AVL的缺点是一种严格按照定义来实现的平衡二叉查找树，所以它查找的效率非常稳定，为O(log n),由于其严格按照左右子树高度差不大于1的规则，插入和删除操作中需要大量的旋转操作来保持AVL树的平衡，因此ALV树适用于大量查询，少量插入和删除的场景中。 假设有这样一种场景：大量查询，大量插入和删除，现在使用AVL树就不太合适了，因为ALV树大量的插入和删除会非常耗时间。红黑树只要求部分地达到平衡要求，降低了对旋转的要求，从而提高了性能。 2、概念红黑树是一种近似平衡的二叉查找树，它能够确保任何一个节点的左右子树的高度差较低的两倍。具体来说，红黑树是满足如下条件的二叉查找树（binary search tree）： 每个节点要么是红色，要么是黑色。 根节点必须是黑色 每个叶子节点都是黑色的空节点(null) 红色节点不能连续（也即是，红色节点的孩子和父亲都不能是红色）。 对于每个节点，从该点至叶子节点的任何路径，都含有相同个数的黑色节点。 下图就是一个典型的红黑树 3、红黑树的调整在树的结构发生改变时（插入或者删除操作），往往会破坏上述条件4或条件5，在什么情况下会破坏平衡？ 插入值为14的节点，由于父节点15是黑节点，没有破坏结构 插入值为21的节点 违背了规则4，需要通过调整使得查找树重新满足红黑树的条件。 3.1 变色为了符合红黑树的规则，可以改变节点颜色 22红变黑，但不符合规则5 25黑变红，但又不符合规则4 27红变黑 3.2 旋转3.2.1 左旋（Rotate Left）逆时针旋转两个节点，使父节点被自己的右孩子取代，而自己成为自己的左孩子。 3.2.2 右旋（RotateRight）顺时针旋转两个节点。 4、TreeMap的实现IBM DevelopWorks 上一篇文章讲解非常好，供参考。 TreeMap 和 TreeSet 是 Java Collection Framework 的两个重要成员，其中 TreeMap 是 Map 接口的常用实现类，而 TreeSet 是 Set 接口的常用实现类。虽然 HashMap 和 HashSet 实现的接口规范不同，但 TreeSet 底层是通过 TreeMap 来实现的，因此二者的实现方式完全一样。而 TreeMap 的实现就是红黑树算法。 对于 TreeMap 而言，由于它底层采用一棵“红黑树”来保存集合中的 Entry，这意味这 TreeMap 添加元素、取出元素的性能都比 HashMap 低：当 TreeMap 添加元素时，需要通过循环找到新增 Entry 的插入位置，因此比较耗性能；当从 TreeMap 中取出元素时，需要通过循环才能找到合适的 Entry，也比较耗性能。 但 TreeMap、TreeSet 比 HashMap、HashSet 的优势在于：TreeMap 中的所有 Entry 总是按 key 根据指定排序规则保持有序状态，TreeSet 中所有元素总是根据指定排序规则保持有序状态。 参考资料漫画算法：什么是红黑树 算法原理系列：红黑树：分析红黑树的诞生过程]]></content>
  </entry>
  <entry>
    <title><![CDATA[SCIP - 第三章 模块化、对象和状态]]></title>
    <url>%2F2019%2F05%2F18%2F%E8%AF%BB%E4%B9%A6%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9E%84%E9%80%A0%E5%92%8C%E8%A7%A3%E9%87%8A%2FCH3%2F</url>
    <content type="text"><![CDATA[第三章 模块化、对象和状态前两章回顾 如何组合基本过程和基本数据 如何构造各种复合对象(组合过程/数据) 抽象在控制和处理程序复杂性中的重要作用 简单问题1 12&gt; cons、car和cdr的含义&gt; &gt; 简单问题2 1234&gt; ;写出以下函数的等价lambda表达式&gt; (define (add a b)&gt; (+ a b))&gt; 但对于程序设计而言，上面这三种手段还不够用，有效设计大型系统，还需要一些组织系统的原则，这体现在下面两方面： 只有高效算法，不足以构造出良好的大型系统 系统的功能分解，结构组织和管理与算法一样重要(或更甚之) 在大型系统的复杂性问题上，仅学会抽象的思维还不够，还需要一些能帮我们构造模块化的大型系统的策略。在这一章，会学习两种组织策略。 基于对象的策略 真实系统中的对象随着时间的进展不断变化，模拟它们的系统对象也吸引相应地变化 基于流处理的策略 关注流过系统的信息流 3.1 赋值和局部状态3.1.1 局部状态变量 例子：银行取钱 用withdraw表示该行为，入参amount表示取钱的数量，若账户有足够的钱，返回余额；否则返回Insufficient funds。 为了实现withdraw，我们用一个变量balance表示余额，withdraw检查balance是否够amount，如果是， balance -= amount 123456789101112131415(define balance 100)(define (withdraw amount) (if (&gt;= balance amount) (begin (set! balance (- balance amount)) balance) "Insufficient funds"))-----------------&gt; (withdraw 50)50&gt; (withdraw 50)0&gt; (withdraw 50)"Insufficient funds" 其中，set的语法是 1(set! &lt;name&gt; &lt;new-value&gt; ) begin描述对表达式的求值 1(begin &lt;exp1&gt;&lt;exp2&gt;...&lt;expk&gt;) 按顺序求值，并返回最后一个表达式的值。 可以看出，这里的balance是全局变量，在哪里都能读取或修改这个值。如果将其作为局部变量，就只能通过withdraw来访问balance，这样才能更准确的模拟balance这个概念。 于是将其转为局部变量。 1234567(define new-withdraw (let ((balance 100)) (lambda (amount) (if (&gt;= balance amount) (begin (set! balance (- balance amount)) balance) "Insufficient funds")))) 其中，let创建一个包含局部变量的环境，并设初始值。 make-withdraw创建一种”提款处理器”，它的形参balance描述了有关账户的初始值。(把balance当形参传入) 123456789101112131415(define (make-withdraw balance) (lambda (amount) (if (&gt;= balance amount) (begin (set! balance (- balance amount)) balance) "Insufficient funds")))(define W1 (make-withdraw 100))(define W2 (make-withdraw 100))-----------------&gt; (W1 50)50&gt; (W2 60)40 可以看出，W1和W2是完全独立的对象，每个都有自己的局部状态变量balance。 再创建一个存钱的对象make-account 123456789101112131415161718(define (make-account balance) (define (withdraw amount) (if (&gt;= balance amount) (begin (set! balance (- balance amount)) balance) "Insufficient funds")) (define (deposit amount) (set! balance (+ balance amount)) balance) (define (dispatch m) (cond ((eq? m 'withdraw) withdraw) ((eq? m 'deposit) deposit) (else (error "Unknown request -- MAKE-ACCOUNT" m)))) dispatch) 对make-account的每次调用都会设置好一个带有balance的环境，在这个环境中，定义了能访问balance的过程deposit和withdraw，另外还有一个过程dispatch，它以一个字符串作为输入，返回这两个局部过程之一。 该过程可以这样使用 123456789101112131415161718192021&gt; (define acc (make-account 100)); 对acc的每次调用将返回局部定义的过程&gt; (acc 'withdraw)#&lt;procedure:withdraw&gt;; 这个过程随后被应用于给定的amount&gt; ((acc 'withdraw) 50)50&gt; ((acc 'withdraw) 60)"Insufficient funds"&gt; ((acc 'deposit) 40)90&gt; ((acc 'withdraw) 60)30; 这样将产生另一个完全独立的对象，维护自己的局部balance&gt; (define acc2 (make-account 1000)) 3.1.2 引进赋值带来的利益 例子：考虑设计一个过程rand，每次被调用就返回一个随机整数 “随机”是指，对rand的反复调用将产生出一系列的数，这一序列具有均匀分布的统计性质。 假设现在有一个过程rand-update，如果给定一个数x1，执行 12x2 = (rand-update x1)x3 = (rand-update x2) 得到序列x1, x2, x3…将具有我们希望的性质 可以将rand时限为一个带局部状态变量x的过程，初始化为某个固定值random-init，对rand的每次调用算出当前x的rand-update值，返回作为随机数，并将其更新为x的新值 12345(define random-init 7)(define (rand-update x) (let ((a 27) (b 26) (m 127)) (modulo (+ (* a x) b) m))) 12345(define rand (let ((x random-init)) (lambda () (set! x (rand-update x)) x))) 当然，你也可以在需要随机数的时候直接调用rand-update，生成同样的随机数序列。但缺点是，这就需要程序中任何使用随机数的地方都必须显式的记住x的值，在生成下一个时，将x的值传给rand-update作为参数。 考虑用随机数实现蒙特卡洛法 monte-carlo 蒙特卡洛模拟：从总体抽取大量随机样本，并通过这些随机样本估计这一随机事件的概率，将这个概率作为问题的解。 比如，$6/\pi^2$是随机选取的两个整数之间没有公共因子（最大公因子（greatest common divisor，GCD）是1）的概率。 则可以通过每次随机选择两个证书并检查它们的GCD是否为1来近似的获得这个概率，π的近似求值。（Cesaro定理） 理解黎曼猜想（二）两个自然数互质的概率是多少？ 那么，这个过程的核心就是蒙特卡洛模拟（monte-carlo），它以做某个实验的次数，以及这个实现本身作为参数。 123456789101112131415161718192021(define (estimate-pi trials) (sqrt (/ 6 (monte-carlo trials cesaro-test)))); Cesaro实验(define (cesaro-test) (= (gcd (rand) (rand)) 1)); 做某个实验的次数，以及这个实现本身作为参数(define (monte-carlo trials experiment) (define (iter trials-remaining trials-passed) (cond ((= trials-remaining 0) (/ trials-passed trials)) ((experiment) (iter (- trials-remaining 1) (+ trials-passed 1))) (else (iter (- trials-remaining 1) trials-passed)))) (iter trials 0))---------------------&gt; (estimate-pi 50)2.449489742783178 现在试一下不用rand，直接用rand-update，如果不使用赋值去模拟局部状态，则 123456789101112131415(define (random-gcd-test trials initial-x) (define (iter trials-remaining trials-passed x) (let ((x1 (rand-update x))) (let ((x2 (rand-update x1))) (cond ((= trials-remaining 0) (/ trials-passed trials)) ((= (gcd x1 x2) 1) (iter (- trials-remaining 1) (+ trials-passed 1) x2)) (else (iter (- trials-remaining 1) trials-passed x2)))))) (iter trials 0 initial-x)) 很明显看到区别，在上面的方法中，蒙特卡洛可以抽象出一个公共方法，不限制experiment的具体形式，而下面的方法中，由于没有随机数生成器的局部状态，random-gcd-test必须显式的操作随机数x1和x2。 赋值和局部变量的好处： 从复杂计算的角度来看，其他部分都像是随着时间不断变化，而它们自己隐藏起随时间变化的内部状态（比如银行账户和随机数生成器）。因此在进行系统抽象的时候，用局部变量去模拟系统的状态，用对这些变量的赋值去模拟状态的变化。 3.1.3 引进赋值的代价只要不使用赋值，以同样参数对同意过程的两次求值一定能产生同样的效果，就像在计算数学函数。不用任何赋值的程序设计成为函数式编程。 赋值如何让事情复杂化了？ 来看make-withdraw的一个简化版本，其中不再关注是否有足够余额的问题： 1234567891011; The Costs of Introducing Assignment(define (make-simplified-withdraw balance) (lambda (amount) (set! balance (- balance amount)) balance))(define W (make-simplified-withdraw 25))&gt; (W 20)5&gt; (W 20)-15 如果没有set 12345678910(define (make-decrementer balance) (lambda (amount) (- balance amount)))(define D (make-decrementer 25))&gt; (D 20)5&gt; (D 20)5 用代换模型来解释make-decrementer如何工作。 1234567((make-decrementer 25) 20); 25代替balance((lambda (amount) (- 25 amount)) 20); 应用运算符，20代替lambda中的amount(- 25 20) 类似的，来看make-simplified-withdraw如何工作 1234567((make-simplified-withdraw 25) 20); 25代替balance((lambda (amount) (set! balance (- 25 amount)) 25) 20)); 20代替lambda中的amount，先将balance设为5，再返回25(set! balance (- 25 20)) 25 可见这里如果用代换模型，则这个过程首先将balance设为5，再返回25，显示是有问题的。造成这种问题的根源是： 在代换模型中，语言的符号（如balance）就是值的名字，而一旦引进了set和变量的值可以变化的想法，一个变量就不再是一个简单的名字了。 同一和变化 从这里暴露出的问题，远远不是打破了一个简单的计算模型，其意义要深远得多。一旦将变化引入了我们的计算模型，许多以前非常简单明了的概念现在都变得有问题了。 首先考虑两个物体实际上“同一”的概念。如果一个语言在表达式里支持“同一的东西可以相互替换”的概念，这样替换不会改变表达式的值，这个语言就称为是具有引用透明性。 在我们的计算机语言包含了赋值操作之后，也就打破了引用透明性，产生了副作用。 修改变量的值 IO 操作，如写数据到磁盘 UI 操作，如修改了一个按钮的可操作状态 引用透明性和等式推理 命令式编程的缺陷 除了引入变量和赋值导致计算模型的复杂性之外，还容易出现一些不会在函数式编程中出现的错误，比如： 1.2.1节的迭代求阶乘 12345678(define (factorial n) (define (iter product counter) (if (&gt; counter n) product ; 注意这里 (iter (* counter product) (+ counter 1)))) (iter 1 1)) 如果用命令式编程，显式的通过赋值去更新变量product和counter的值 1234567891011(define (factorial n) (let ((product 1) (counter 1)) (define (iter) (if (&gt; counter n) product ; 这两行的陷阱 (begin (set! product (* counter product)) (set! counter (+ counter 1)) (iter)))) (iter))) 上述写法是正确的，但是如果颠倒其中两行的顺序 12;(set! counter (+ counter 1));(set! product (* counter product)) 结果就不对了。一般来说，带有赋值操作的程序强迫人们去考虑赋值的相对顺序，以保证每个语句所用的是被修改变量的正确版本。 接下来要解决的问题就是，要给涉及赋值的表达式提供一种区别于之前的计算模型。 3.2 求值的环境模型之前说代换模型不再适用于赋值表达式。由于赋值操作的存在，此时的变量必须以某种方式指定了一个“位置”，相应的值可以存储在那里。在我们的新求值模型里，这种位置将维持在称为 环境 的结构中。 一些概念： 1）环境：框架的一个序列 2）框架：每个框架包含着一些 约束 的表格 3）约束：变量名和值相关联。（在一个框架里，任何变量至多只能有一个约束） 4）指针：每个框架还包含着一个指针，指向这一框架的外围环境。 5）变量：一个变量相对于某个特定环境的值， ​ 一个简单的环境结构 1、2、3是三个框架， ABCD都是环境指针，其中CD指向同一个环境。 环境对于求值过程是至关重要的，因为它确定了表达式求值的上下文。假设始终有一个全局环境，只包含一个框架（没有外围环境），这个环境包含所有关于基本过程的符号的值。 3.2.1 求值规则在求值的环境模型中，过程只能通过一种方式创建，那就是通过求值一个lambda表达式。例如 12(define (square x) (* x x)) 过程定义的语法形式，不过是作为隐含lambda表达式的语法糖，等价于 12(define square (lambda (x) (* x x)) 过程对象的环境部分是一个指向全局环境的指针，因此产生这个过程的lambda表达式是在全局环境中求值的。 这里增加了一个新约束：将过程对象约束给符号square。一般来说，define就是把一个新的约束加入到框架中。 假设现在对表达式(square 5)求值，结果是创建了一个新环境——E1。 1）E1从一个框架开始，这个框架包含5赋值给x的约束。 2）E1引出的指针表示这个框架的外围环境是全局环境 过程应用的环境模型： 将一个过程对象应用于一组实际参数，将构造出一个新框架，其中将过程的形式参数约束到调用时的实际参数，而后在构造起的这一新环境的上下文中求值过程体。这个新框架的外围环境就是作为被应用的那个过程对象的一部分的环境。 相对于一个给定环境求值一个 lambda表达式，将会创建起一个过程对象，这个过程对象是一个序对，由该 lambda表达式 的正文和一个指向环境的指针组成，这一指针指向的就是创建这个过程对象时的环境。 关于set!的行为方式 在某个环境里求值 赋值表达式 时，要求我们首先在环境中确定有关变量的约束位置，而后再修改这个约束，使之表示这个新值。 这也就是说，首先需要找到包含这个变量的约束的第一个框架，而后修改这一框架。如果该变量在环境中没有约束，将报告一个错误。 3.2.2 简单过程的应用12345678(define (square x) (* x x))(define (sum-of-squares x y) (+ (square x) (square y)))(define (f a) (sum-of-squares (+ a 1) (* a 2))) 求值 (f 5)： 创建一个新环境 E1，参数 a 被约束到 5。在 E1 中求值 (sum-of-square (+ a 1) (* a2)) 求值组合式，首先求值子表达式。第一个子表达式 sum-of-square 以一个过程对象为值。（在 E1 的框架中未寻找到约束，而后进入有关的外围环境，并找到约束） 创建环境 E2，现在需要把过程对象 sum-of-square 应用于实参 6 和 10。 3.3 用变动的数据做模拟3.3.1 变动的表结构针对序对的基本改变函数是set-car!和set-cdr!。set-car!要求两个参数，其中第一个参数必须是一个序对。然后修改这个序对，将它的car指针替换为指向set-car!第二个参数的指针。 假设x是((a b) c d)，y是(e f)，执行 1(set-car! x y) 修改x约束的那个表，将它的car用y的值取代。 共享和相等 当不同的数据对象共享某些序对时，会产生一些问题。例如，考虑下面的结构： 12(define x (list 'a 'b))(define z1 (cons x x)) 这里的z1是一个序对，car和cdr都指向同一个序对x。 再看另一个结构 12(define z2 (cons (list 'a 'b) (list 'a 'b))) 两个表(a, b)的各个序对互不相同，虽然其中的符号是共享的。 作为表考虑，z1和z2表示同一个表((a b) a b)。一般而言，如果我们只用cons、car、cdr，z1和z2看不出差别。然而，如果允许修改表结构，共享的情况就会体现出差别。考虑下面的过程： 123(define (set-to-wow! x) (set-car! (car x) 'wow) x) 将set-to-wow!应用于z1和z2，将产生不同的结果。对于z1，修改car也就同时修改了cdr。而对于z2，只修改了car。 1234567891011&gt; z1((a b) a b)&gt; (set-to-wow! z1)((wow b) wow b)&gt; z2((a b) a b)&gt; (set-to-wow! z2)((wow b) a b) 检测表结构是否共享的一种方式是使用谓词eq?。谓词(eq? x y)检查x和y作为指针是否相等。这样，对于z1和z2，(eq? (car z1) (cdr z1)) 为true (eq? (car z2) (cdr z2)) 为false。 3.3.2 队列的表示利用set-car! 和 set-cdr!，我们可以用序对构造出一些单靠cons、car、cdr无法构造的数据结构。这一节展示如何构造队列。 一个队列是一个序对，数据只能从一端插入（队列的尾），另一端删除（队列的首）。下面显示一个初始是空的队列，之后插入a和b，再删除a，再插入c和d，再删除b。 队列可以看做是由下面一组操作定义的结构： 一个构造函数(make-queue) ，返回一个空队列。 两个选择函数 1(empty-queue? ⟨queue⟩) 检查队列是否为空 1(front-queue ⟨queue⟩) 返回队首的对象，如果空就报错。 两个改变函数 1(insert-queue! ⟨queue⟩ ⟨item⟩) 插入队尾，返回修改后的队列。 1(delete-queue! ⟨queue⟩) 删除队首对象，返回修改后的队列。 队列可以表示成一个常规的表，用car、cdr来完成上述操作。但是效率很低，为了插入一个数据项，需要扫描整个表，有O(n)的时间复杂度。那么简单修改一下表的表示方式，就可以只需O(1)的时间复杂度。方法就是加入一个指向队列尾的指针，这里就有两个指针front-ptr 和rear-ptr。 123456789(define (front-ptr queue) (car queue))(define (rear-ptr queue) (cdr queue))(define (set-front-ptr! queue item) (set-car! queue item))(define (set-rear-ptr! queue item) (set-cdr! queue item)) 现在来实现队列的其他操作 12345678910(define (empty-queue? queue) (null? (front-ptr queue))) (define (make-queue) (cons '() '()))(define (front-queue queue) (if (empty-queue? queue) (error "FRONT called with an empty queue" queue) (car (front-ptr queue)))) 如果要插入一个对象，按照下图的方式， 1）创建一个新队列，car是需要插入的项，cdr是空表。 2）若队列原来是空，就让front-ptr 和rear-ptr共同指向新序对。否则就修改rear-ptr，而指向新序对。 12345678910(define (insert-queue! queue item) (let ((new-pair (cons item '()))) (cond ((empty-queue? queue) (set-front-ptr! queue new-pair) (set-rear-ptr! queue new-pair) queue) (else (set-cdr! (rear-ptr queue) new-pair) (set-rear-ptr! queue new-pair) queue)))) 要删除对象，就修改front-ptr，指向第二个数据项。 12345678(define (delete-queue! queue) (cond ((empty-queue? queue) (error "DELETE! called with an empty queue" queue)) (else (set-front-ptr! queue (cdr (front-ptr queue))) queue))) 3.3.3 表格的表示首先考虑一维表格的问题，每个记录实现为key和value的序对。这些记录连接起来就构成一个序对的表。为了向表格里面插入记录时有可以修改的位置，将这种表格构造为一种带有表头单元的表。表开头有一个“哑”记录——存放一个特殊符号，*table*。 为了从表格中提取信息，定义lookup过程，以key为参数，返回value。lookup基于assoc定义。assoc返回key的那条记录。lookup检查assoc返回的记录是否为假，而后返回value。 1234567891011(define (lookup key table) (let ((record (assoc key (cdr table)))) (if record (cdr record) false)))(define (assoc key records) (cond ((null? records) false) ((equal? key (caar records)) (car records)) (else (assoc key (cdr records))))) 如果是插入操作， 1）首先用assoc检查表格中是否有这个key，如果没有，就cons这个key和value，构造出一个新记录。接着插入到表的最前面，位于哑记录之后。 2）如果表格中存在这个记录，就用该记录的cdr设置为这个新值。 12345678(define (insert! key value table) (let ((record (assoc key (cdr table)))) (if record (set-cdr! record value) (set-cdr! table (cons (cons key value) (cdr table))))) 'ok) 在构造一个新表时，只需创建起一个包含符号*table*的表。 12(define (make-table) (list '*table*)) 二维表格 二维表格的每个值用两个key索引。 二维表的查询。先用第一个key确定对应的子表格，而后用第二个key确定value。 1234567(define (lookup key-1 key-2 table) (let ((subtable (assoc key-1 (cdr table)))) (if subtable (let ((record (assoc key-2 (cdr subtable)))) (if record (cdr record) false)) false))) 二维表的插入。首先用assoc查看key-1是否有一个子表格，没有就构造，其中只包含一个记录(key-2, value)。若有，就将新值插入该子表格。 12345678910111213141516(define (insert! key-1 key-2 value table) (let ((subtable (assoc key-1 (cdr table)))) (if subtable (let ((record (assoc key-2 (cdr subtable)))) (if record (set-cdr! record value) (set-cdr! subtable (cons (cons key-2 value) (cdr subtable))))) (set-cdr! table (cons (list key-1 (cons key-2 value)) (cdr table))))) 'ok) 创建局部表格 上述的lookup和insert!都以整个表格为参数，这也使我们可以将它们用到包含多个表格的程序中。处理多个表格的方式是为每个表格提供一对独立的lookup和insert！过程。 实现的方案是，用过程表示表格，将表格表示为一个以局部状态的方式维持一个内部表格的对象。 123456789101112131415161718192021222324252627282930313233343536(define (make-table) (let ((local-table (list '*table*))) (define (lookup key-1 key-2) (let ((subtable (assoc key-1 (cdr local-table)))) (if subtable (let ((record (assoc key-2 (cdr subtable)))) (if record (cdr record) false)) false))) (define (insert! key-1 key-2 value) (let ((subtable (assoc key-1 (cdr local-table)))) (if subtable (let ((record (assoc key-2 (cdr subtable)))) (if record (set-cdr! record value) (set-cdr! subtable (cons (cons key-2 value) (cdr subtable))))) (set-cdr! local-table (cons (list key-1 (cons key-2 value)) (cdr local-table))))) 'ok) (define (dispatch m) (cond ((eq? m 'lookup-proc) lookup) ((eq? m 'insert-proc!) insert!) (else (error "Unknown operation: TABLE" m)))) dispatch)) 利用make-table，我们可以实现get和put操作 123(define operation-table (make-table))(define get (operation-table 'lookup-proc))(define put (operation-table 'insert-proc!)) 这两个操作都访问同一个局部表格，这一表格被封装在由对make-table的调用创建起的对象里面。 3.3.4 数字电路的模拟器 连接基本组件来构造更复杂的功能。比如下面的半加器电路，包括一个或门，两个与门和一个非门。 当A=1或B=1之一是1时，S=1，当A=1且B=1时，C=1。 半加器 半加器是实现两个一位二进制数加法运算的器件。它具有两个输入端(被加数A和加数B)及输出端Y。 [1] A和B是相加的两个数，S是半加和数，C是进位数。 所谓半加就是不考虑进位的加法，它的真值表如下 (见表)： 现在要构造一个程序来模拟数字逻辑电路。最基本元素是make-wire，用于构造连线。比如可以构造出 123456(define a (make-wire))(define b (make-wire))(define c (make-wire))(define d (make-wire))(define e (make-wire))(define s (make-wire)) 如果需要把一个功能连到一组连线上，就调用一个构造这类功能的过程，参数就是连线。 1234567891011(or-gate a b d)ok(and-gate a b c)ok(inverter c e)ok(and-gate d e s)ok 再拼接成半加器。 1234567(define (half-adder a b s c) (let ((d (make-wire)) (e (make-wire))) (or-gate a b d) (and-gate a b c) (inverter c e) (and-gate d e s) 'ok)) 在这个基础上，可以再构建全加器。 全加器 全加器能进行加数、被加数和低位来的进位信号相加，并根据求和结果给出该位的进位信号。 当多位数相加时，半加器可用于最低位求和，并给出进位数。第二位的相加有两个待加数和，还有一个来自前面低位送来的进位数。这三个数相加，得出本位和数（全加和数）和进位数。这种就是“全加“，下表为全加器的逻辑状态表。 12345678(define (full-adder a b c-in sum c-out) (let ((c1 (make-wire)) (c2 (make-wire)) (s (make-wire))) (half-adder b c-in s c1) (half-adder a s sum c2) (or-gate c1 c2 c-out) 'ok)) 从模拟器的角度，各种功能块构成了基础，将功能块连接起来就是这里的组合方法，而将特定的连接模式定义为过程就是这里的抽象方法。 基本功能块基本功能块使得在一根连线上的信号变化能够影响其他连线上的信号。我们添加如下操作： 1(get-signal ⟨wire⟩) 返回连线上信号的当前值。 1(set-signal! ⟨wire⟩ ⟨new value⟩) 将连线上信号修改为新的值。 1(add-action! ⟨wire⟩ ⟨procedure of no arguments⟩) 改变信号值就需要执行该过程。这种过程是一些媒介，能够将相应连线上值的变化传递到其他的连线。 利用这些过程，可以定义基本的数字逻辑功能了。为了把输入通过一个反门连接到输出，用add-action! 为输入关联一个过程，当输入路线的值改变时，执行这一过程。 先看非门，在一个inverter-delay后将输出线路设置为这个新值。 12345678910111213141516(define (inverter input output) (define (invert-input) (let ((new-value (logical-not (get-signal input)))) (after-delay inverter-delay (lambda () (set-signal! output new-value))))) (add-action! input invert-input) 'ok)(define (logical-not s) (cond ((= s 0) 1) ((= s 1) 0) (else (error "Invalid signal" s)))) 下面是与门的定义。 12345678910111213(define (and-gate a1 a2 output) (define (and-action-procedure) (let ((new-value (logical-and (get-signal a1) (get-signal a2)))) (after-delay and-gate-delay (lambda () (set-signal! output new-value))))) (add-action! a1 and-action-procedure) (add-action! a2 and-action-procedure) 'ok) 线路的表示一条线路是一个具有两个局部状态变量的计算对象：一个是信号值signal-value（初值为0），一个是一组过程action-procedures，在信号值改变时，这些过程需要运行。类似于在3.1.1中处理银行账户的做法实现： 1234567891011121314151617181920212223242526(define (make-wire) (let ((signal-value 0) (action-procedures '())) (define (set-my-signal! new-value) (if (not (= signal-value new-value)) (begin (set! signal-value new-value) (call-each action-procedures)) 'done)) (define (accept-action-procedure! proc) (set! action-procedures (cons proc action-procedures)) (proc)) (define (dispatch m) (cond ((eq? m 'get-signal) signal-value) ((eq? m 'set-signal!) set-my-signal!) ((eq? m 'add-action!) accept-action-procedure!) (else (error "Unknown operation: WIRE" m)))) dispatch)) set-my-signal!检查新的信号值是否实际改变了线路上的信号，如果是，就用call-each运行每个动作过程。 12345(define (call-each procedures) (if (null? procedures) 'done (begin ((car procedures)) (call-each (cdr procedures))))) 一旦设置好dispatch过程，就可以提供以下访问线路中局部操作的过程： 123456(define (get-signal wire) (wire 'get-signal))(define (set-signal! wire new-value) ((wire 'set-signal!) new-value))(define (add-action! wire action-procedure) ((wire 'add-action!) action-procedure)) 待处理表agenda最后是after-delay的实现。要维护一个称为待处理表的数据结构，包含需要完成的事项清单。定义如下操作： (make-agenda) 返回一个新的空agenda. (empty-agenda? ⟨agenda⟩) . (first-agenda-item ⟨agenda⟩) 返回agenda第一个item. (remove-first-agenda-item! ⟨agenda⟩) . (add-to-agenda! ⟨time⟩ ⟨action⟩ ⟨agenda⟩) (current-time ⟨agenda⟩) 返回当时的模拟时间。 用the-agenda表示特定的待处理表。以下过程向the-agenda中插入一个新item 12345(define (after-delay delay action) (add-to-agenda! (+ delay (current-time the-agenda)) action the-agenda)) agenda中的模拟过程用propagate实现，它操作the-agenda，顺序执行agenda的每个过程。 12345678(define (propagate) (if (empty-agenda? the-agenda) 'done (let ((first-item (first-agenda-item the-agenda))) (first-item) (remove-first-agenda-item! the-agenda) (propagate)))) 一个简单的实例模拟下面过程将一个监测器放到一个线路上，用于显示模拟器的活动。这一过程会告诉相应线路，只要它的值改变了，就打印出新的值，同时打印当前时间和线路名称。 12345678910(define (probe name wire) (add-action! wire (lambda () (newline) (display name) (display " ") (display (current-time the-agenda)) (display " New-value = ") (display (get-signal wire))))) 我们从初始化待处理表和描述各种功能块的延时开始 1234(define the-agenda (make-agenda))(define inverter-delay 2)(define and-gate-delay 3)(define or-gate-delay 5) 现在定义4条线路，在其中两条线路上安装监测器 12345678910(define input-1 (make-wire))(define input-2 (make-wire))(define sum (make-wire))(define carry (make-wire))(probe 'sum sum)sum 0 New-value = 0(probe 'carry carry)carry 0 New-value = 0 然后将这些线路连接到一个半加器电路上 123456789(half-adder input-1 input-2 sum carry)ok(set-signal! input-1 1)done(propagate)sum 8 New-value = 1done 在时间8, sum上的信号变为1。现在到了模拟开始之后的8个时间单位。在这一点上，我们可 以将input-2上的信号设置为1，并让有关的值向前传播： 1234567(set-signal! input-2 1)done(propagate)carry 11 New-value = 1sum 16 New-value = 0done 在时间11处，carry变成1，16处sum变成0。 agenda的实现这种待处理表由一些时间段组成，每个时间段是由一个数值（表示时间）和一个队列(见练习3.32)组成的序对，在这个队列里，保存着那些已经安排好的，应该在这一时间段运行的过程。 1234(define (make-time-segment time queue) (cons time queue))(define (segment-time s) (car s))(define (segment-queue s) (cdr s)) 我们将用3.3.2节描述的队列操作完成在时间段队列上的操作。待处理表本身就是时间段的一个一维表格。与3.3.3节所示的表格的不同之处，就在于这 些时间段应该按照时间递增的顺序排列。此外，我们还需在待处理表的头部保存一个当前时 间（即，此前最后被处理的那个动作的时间）。一个新构造出的待处理表里没有时间段，其当前时间是0。 1234567891011121314151617(define (make-agenda) (list 0))(define (current-time agenda) (car agenda))(define (set-current-time! agenda time) (set-car! agenda time))(define (segments agenda) (cdr agenda))(define (set-segments! agenda segments) (set-cdr! agenda segments))(define (first-segment agenda) (car (segments agenda)))(define (rest-segments agenda) (cdr (segments agenda))) 若一个agenda没有时间段，那它就是空的 12(define (empty-agenda? agenda) (null? (segments agenda))) 为了将一个动作加入待处理表， 1）我们首先要检査这个待处理表是否为空。如果真是这样，那么就创建一个新的时间段，并将这个时间段装入待处理表里。 2）否则我们就扫描整个的待处理表，检査其中各个时间段的时间。如果发现某个时间段具有合适的时间，那么就把这个动作加入与之关联的队列里。如果碰到了某个比需要预约的时间更晚的时间，那么就将一个新的时间段插入待处理表，插入这个位置之前。如果到达了待处理表的末尾，我们就必须在最后加上一个新的时间段。 12345678910111213141516171819202122232425262728293031323334(define (add-to-agenda! time action agenda) (define (belongs-before? segments) (or (null? segments) (&lt; time (segment-time (car segments))))) (define (make-new-time-segment time action) (let ((q (make-queue))) (insert-queue! q action) (make-time-segment time q))) (define (add-to-segments! segments) (if (= (segment-time (car segments)) time) (insert-queue! (segment-queue (car segments)) action) (let ((rest (cdr segments))) (if (belongs-before? rest) (set-cdr! segments (cons (make-new-time-segment time action) (cdr segments))) (add-to-segments! rest))))) (let ((segments (segments agenda))) (if (belongs-before? segments) (set-segments! agenda (cons (make-new-time-segment time action) segments)) (add-to-segments! segments)))) 从待处理表中删除第一项的过程，应该删去第一个时间段的队列前端的那一项。如果删 除使这个时间段变空了，我们就将这个时间段也从时间段的表里删去: 12345678(define (remove-first-agenda-item! agenda) (let ((q (segment-queue (first-segment agenda)))) (delete-queue! q) (if (empty-queue? q) (set-segments! agenda (rest-segments agenda))))) 找出待处理表中里第一项，也就是找出其第一个时间段队列里的第一项。无论何时提取 这个项时，都需要更新待处理表的当前时间 1234567891011(define (first-agenda-item agenda) (if (empty-agenda? agenda) (error "Agenda is empty: FIRST-AGENDA-ITEM") (let ((first-seg (first-segment agenda))) (set-current-time! agenda (segment-time first-seg)) (front-queue (segment-queue first-seg))))) 3.3.5 约束的传播在传统上，计算机程序总被组织成一种单向的计算，它们对一些事先给定的参数执行某些操作，产生出所需要的输出。但在另一方面，我们也经常需要模拟一些由各种量之间的关系描述的系统。例如，某个机械结构的数学模型里可能包含着这样的一些信息：在一个金属杆的偏转量d与作用于这个杆的力F、杆的长度L、截面面积A和弹性模数之间的关系可以由下面方程描述 dAE=FL这种关系并不是单向的，给定了其中任意的4个量，我们就可以利用它计算出第5个量。然而，要将这种方程翻译到传统的程序设计语言，就会迫使我们选出一个量，要求基于另外的4个量去计算出它。这样，一个用于计算面积A的过程将不能用于计算偏转量。虽然对于A和d的计算都出自这同一个方程。 在这一节里，我们要描绘一种语言的设计，这种语言将使我们可以基于各种关系进行工作。这一语言里的基本元素就是基本约束，它们描述了在不同量之间的某种特定关系。例如，(adder a b c)描述的是量a、b和c之间必须有关系$a+b=c$，(multiplier x y z)描述的是约束关系$xy=z$，而(constant 3.14 x)表示x的值永远都是3.14。 我们的语言里还提供了一些方法，使它们可以用于组合各种基本约束，以便去描述更复杂的关系。在这里，我们将通过构造约束网络的方式组合起各种约束，在这种约束网络里，约束通过连接器连接起来。连接器是一种对象，它们可以“保存”一个值，使之能参与一个或者多个约束。例如，我们知道在华氏温度和摄氏温度之间的关系是： 9C=5(F-32)这样的约束就可以看做是一个网络。通过基本加法约束、乘法约束和常量约束组成。在这个图里，我们看到左边的乘法块有三个引线，分别标记为 m1 、 m2 和p。该乘法约束的这些引线以如下方式连接到网络的其他部分：引线 m1连到连接器 C ，这个连接器将保存摄氏温度。引线 m2 接在连接器 w ，该连接器还连接着一个保存常量 9 的约束块。引线 p 被这一乘法块约束到 m1和 m2 的乘积，它还连接到另一个乘法块的引线p。另一乘法块的 m2连接到常量 5 ，它的 m1 连接到另一加法块的一条引线上。 约束系统的使用首先调用构造函数make-connector，创建起两个连接器C和F 1234(define C (make-connector))(define F (make-connector))(celsius-fahrenheit-converter C F)ok 12345678910111213(define (celsius-fahrenheit-converter c f) (let ((u (make-connector)) (v (make-connector)) (w (make-connector)) (x (make-connector)) (y (make-connector))) (multiplier c w u) (multiplier v x u) (adder v y f) (constant 9 w) (constant 5 x) (constant 32 y) 'ok)) 约束系统的实现类似于数字电路模拟器。虽然约束系统里的基本对象在某些方面更复杂一些,但整个系统却更为简单,因为这里完全不需要关心待处理表和时间延迟等等问题。 连接器的基本操作包括 (has-value? ⟨connector⟩) . (get-value ⟨connector⟩) . (set-value! ⟨connector⟩ ⟨new-value⟩ ⟨informant⟩) (forget-value! ⟨connector⟩ ⟨retractor⟩) 要求连接器忘记改值。 (connect ⟨connector⟩ ⟨new-constraint⟩) 通过连接器参与一个新约束。 通过inform-about-value与各个相关约束通信，这一过程告知给定的约束，该连接器有了新值。而inform-about-no-value告知该连接器丧失了原有的值。 adder在被求和连接器a1和a2和连接器sum之间构造出一个加法约束。 12345678910111213141516171819202122232425262728293031323334353637383940; adder将一个新的加法约束连接到指定连接器。me就代表那个加法约束。(define (adder a1 a2 sum) ; 当加法约束得到了通知，知道自己的一个连接器有了新值后，process-new-value就会被调用。 (define (process-new-value) (cond ((and (has-value? a1) (has-value? a2)) (set-value! sum (+ (get-value a1) (get-value a2)) me)) ((and (has-value? a1) (has-value? sum)) (set-value! a2 (- (get-value sum) (get-value a1)) me)) ((and (has-value? a2) (has-value? sum)) (set-value! a1 (- (get-value sum) (get-value a2)) me)))) ; 如果加法对象被告知自己的一个连接器丢失了值，就要求所有连接器丢掉值，然后再执行process-new-value。 (define (process-forget-value) (forget-value! sum me) (forget-value! a1 me) (forget-value! a2 me) (process-new-value)) (define (me request) (cond ((eq? request 'I-have-a-value) (process-new-value)) ((eq? request 'I-lost-my-value) (process-forget-value)) (else (error "Unknown request: ADDER" request)))) (connect a1 me) (connect a2 me) (connect sum me) me) adder将一个新的加法约束连接到指定连接器。me就代表那个加法约束。 当加法约束得到了通知，知道自己的一个连接器有了新值后，process-new-value就会被调用。 如果加法对象被告知自己的一个连接器丢失了值，就要求所有连接器丢掉值，然后再执行process-new-value。 乘法对象类似于加法对象。 123456789101112131415161718192021222324252627282930313233343536373839404142434445(define (multiplier m1 m2 product) (define (process-new-value) (cond ((or (and (has-value? m1) (= (get-value m1) 0)) (and (has-value? m2) (= (get-value m2) 0))) (set-value! product 0 me)) ((and (has-value? m1) (has-value? m2)) (set-value! product (* (get-value m1) (get-value m2)) me)) ((and (has-value? product) (has-value? m1)) (set-value! m2 (/ (get-value product) (get-value m1)) me)) ((and (has-value? product) (has-value? m2)) (set-value! m1 (/ (get-value product) (get-value m2)) me)))) (define (process-forget-value) (forget-value! product me) (forget-value! m1 me) (forget-value! m2 me) (process-new-value)) (define (me request) (cond ((eq? request 'I-have-a-value) (process-new-value)) ((eq? request 'I-lost-my-value) (process-forget-value)) (else (error "Unknown request: MULTIPLIER" request)))) (connect m1 me) (connect m2 me) (connect product me) me) 最后，监视器在指定连接器被设置或取消值的时候打印出一个消息： 123456789101112131415161718(define (probe name connector) (define (print-probe value) (newline) (display "Probe: ") (display name) (display " = ") (display value)) (define (process-new-value) (print-probe (get-value connector))) (define (process-forget-value) (print-probe "?")) (define (me request) (cond ((eq? request 'I-have-a-value) (process-new-value)) ((eq? request 'I-lost-my-value) (process-forget-value)) (else (error "Unknown request: PROBE" request)))) (connect connector me) me) 连接器的表示连接器用带有局部状态变量value，informant和constraint的过程对象表示，value中保存这个连接器的当前值，informant是设置连接器值的对象，constraint是这一连接器所涉及的所有约束的表。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657(define (make-connector) (let ((value false) (informant false) (constraints '())) ; 当出现了设置一个连接器的要求时，该连接器的局部过程set-my-value就会被调用。 (define (set-my-value newval setter) ; 如果这一连接器当时并没有值，那么它就设置自己的值，并在informant里记录下要求设置当前值的那个约束 (cond ((not (has-value? me)) (set! value newval) (set! informant setter) ; 而后这一连接器将通知它所参与的所有约束，除了刚刚要求设置值的那个约束之外 (for-each-except setter inform-about-value constraints)) ((not (= value newval)) (error "Contradiction" (list value newval))) (else 'ignored))) ; 当连接器被要求忘记自己的值时，它就会去运行局部过程forget-my-value。 (define (forget-my-value retractor) ; 首先检査这一要求是否来自原先设置值的同一个对象 (if (eq? retractor informant) (begin (set! informant false) ; 如果情况确实如此，连接器就通知它所参与的所有约束，告知它们自己的值已经没有了。 (for-each-except retractor inform-about-no-value constraints)) 'ignored)) ; connect向约束表里加入一个新约束（如果它以前不在表里)。如果这个连接器已经有值，它就会将这一事实通知这个新约束。 (define (connect new-constraint) (if (not (memq new-constraint constraints)) (set! constraints (cons new-constraint constraints))) (if (has-value? me) (inform-about-value new-constraint)) 'done) ; 连接器过程me完成对于内部过程服务的分派工作，它同时也作为这个连接器对象的代表 (define (me request) (cond ((eq? request 'has-value?) (if informant true false)) ((eq? request 'value) value) ((eq? request 'set-value!) set-my-value) ((eq? request 'forget) forget-my-value) ((eq? request 'connect) connect) (else (error "Unknown operation: CONNECTOR" request)))) me)) 当出现了设置一个连接器的要求时，该连接器的局部过程set-my-value就会被调用。如果这一连接器当时并没有值，那么它就设置自己的值，并在informant里记录下要求设置当前值的那个约束。而后这一连接器将通知它所参与的所有约束，除了刚刚要求设置值的那个约束之外。这一工作通过下面的迭代过程完成，它将一个指定过程应用于一个表中的所有对象，除了一个给定的例外： 1234567891011; 将一个指定过程应用于一个表中的所有对象(define (for-each-except exception procedure list) (define (loop items) (cond ((null? items) 'done) ((eq? (car items) exception) (loop (cdr items))) (else (procedure (car items)) (loop (cdr items))))) (loop list)) 下面几个过程为分派提供了一个语法界面： 1234567891011121314(define (has-value? connector) (connector 'has-value?))(define (get-value connector) (connector 'value))(define (set-value! connector new-value informant) ((connector 'set-value!) new-value informant))(define (forget-value! connector retractor) ((connector 'forget) retractor))(define (connect connector new-constraint) ((connector 'connect) new-constraint)) 3.4 并发：时间是一个本质问题在并发的情况下，由赋值引入的复杂性问题将变得更加严重。 3.4.1 并发系统中时间的性质假设由Peter和Paul进行的取款被实现为两个独立的进程，共享同一个变量balance 并发程序的正确行为 1）对并发地一种可能限制方式是：规定能修改共享状态变量的两个操作都不允许同时发生。（低效） 2）另一种不严厉的方式是：保证并发系统产生的结果与各个进程按照某种方式顺序运行产生出的结果完全一样。 2.1）它并没有要求各个进程实际上顺序地运行，而只是要求它们产生的结果与 假设 它们顺序运行所产生的结果相同。 2.2）一个并发程序完全可能产生多于一个 “正确的” 结果，因为我们只要求其结果与按照 某种 方式顺序化的结果相同。 对于2.2），比如Peter和Paul的共享账户有100，Peter存入40，同时Paul取出账户中钱的一半。则可能产生两种余额，70或90。 3.4.2 控制并发的机制在设计并发系统时，设法做出一些一般性的机制，使我们可能限制并行进程之间的交错情况，以保证程序具有正确的行为方式。人们已经为此目的而开发了许多不同的机制，我们讨论其中的一种：串行化组（serializer） 对共享变量的串行访问 串行化：使进程可以并发地执行，但是其中也有一些过程不能并发地执行。 Scheme的串行化 通过串行化组实现这种限制。构造的方式是调用make-serializer，这一过程的实现将在后面给出。对一个给定串行化组的所有调用返回的串行化过程都属于同一个集合。 12345678910111213141516171819(define (make-account balance) (define (withdraw amount) (if (&gt;= balance amount) (begin (set! balance (- balance amount)) balance) "Insufficient funds")) (define (deposit amount) (set! balance (+ balance amount)) balance) ; 关键改动 (let ((protected (make-serializer))) (define (dispatch m) (cond ((eq? m 'withdraw) (protected withdraw)) ((eq? m 'deposit) (protected deposit)) ((eq? m 'balance) deposit) (else (error "Unknown request -- MAKE-ACCOUNT" m)))) 使用多重共享资源的复杂性 如果只存在一个共享资源（如银行账户），串行化的使用问题相对简单。如果存在多项共享资源，并发程序的设计就可能变得非常难以把握了。 比如现在可以交换两个账户的余额。假设Peter和Paul都能访问账户a1, a2, a3。Peter要交换a1和a3，同时Paul要交换a1和a2，虽然对单个账户做了串行化，但交换操作还是可能产生不正确的结果。 串行化的实现 使用互斥元（mutex）的同步机制来实现串行化。mutex是一种对象，提供两种操作 1）获取（acquired） 2）释放（released） 一旦一个mutex被获取，对它的任何操作都必须等released之后。 在make-serializer的实现中，关联一个mutex。给定一个过程p，串行化组先返回一个mutex，再运行p，然后释放mutex，这样就能保证这个串行化组产生的所有过程中，一次只能运行一个p。 123456789(define (make-serializer) (let ((mutex (make-mutex)) (lambda (p) (define (serialized-p . args) (mutex 'acquired) (let ((val (apply p args))) (mutex 'release) val)) serialized-p)))) mutex是一个Boolean，在mutex的构造函数中，初始化为false。 在acquired时，先判断是否为false，若是，设置为true然后使用；否则在循环中等待，直到检测为false。 在acquired操作时，有一个test-and-set!方法，用于检查mutex并返回结果，其中若检查结果为false，在返回false之前还要设置为true。 这里的关键是，test-and-set!必须以原子操作的方式执行。比如说，一旦某进程检查了一个mutex发现是false，就必须在其他进程检查这个mutex之前完成为true的设置，否则mutex的机制就失效了。目前多CPU的电脑中提供了专门指令，直接在硬件中支持原子操作。 死锁 比如Peter要交换a2和a1，则进入了保护a2的串行化进程；Paul同时要交换a1和a2，则进入了保护a1的串行化进程。于是双方都无法继续了。 避免死锁的一种方式，就是首先给每个账户确定一个唯一的标识编号，再重写serialized-exchange，使每个进程总是首先去保护较低编号的账户。 并发性、时间和通信 我们已经看到，在并发系统的程序设计中，为什么需要去控制不同进程访问共享变量的事件发生的顺序，也看到了如何通过审慎地使用串行化去完成这方面的控制。但是并发性的基本问题比这些更深刻，因为，从一种更基本的观点看，“共享状态”究竟意味着什么，这件事常常并不清楚。 像test-and-set!这样的机制，都要求进程能在任意时刻去检查一个全局性的共享标志。在实现新型高速处理器时，由于在那里需要采用各种优化技术，例如流水线和缓存，因此就不可能在每个时刻都保持存储器内容的一致性，此时完成上述的检查将很有问题，也必然非常低效。正因为这样，在当前的多处理器系统里，串行化方式正在被并发控制的各种新技术取代，比如屏障同步 3.5 流流是另一种模拟现实物理世界的设计策略，其核心思想就是用数学概念上的函数来表示一现实物体的改变，比如对象X，可以用X(t)来表示，如果我们想集中关心的是一个个时刻的x，那么就可以将它看作一个变化的量。如果关注的是这些值的整个时间史，那么就不需要强调其中的变化——这一函数本身是没有改变的。 在第二章我们学习了序列和表的概念（P84）。从抽象的观点看，流也是一个序列，但如果把流表示为表，必能完全揭示流处理的威力。这里要引入一种叫“延时求值”的技术。 3.5.1 流作为延时的表我们之前建立了一些对序列操作的抽象机制，比如map、filter、accumulate等。但如果我们将序列表示为表，表达可以更优雅，但效率很低。 例如，要计算一个区间内的素数之和。 12345678910111213141516171819202122232425262728293031(define (accumulate op initial sequence) (if (null? sequence) initial (op (car sequence) (accumulate op initial (cdr sequence)))))(define (prime? n) (define (iter n next) (cond ((= n next) #t) ((= 0 (remainder n next)) #f) (else (iter n (+ next 1))))) (iter n 2))(define (enumerate-interval low high) (if (&gt; low high) '() (cons low (enumerate-interval (+ low 1) high))))(define (filter predicate sequence) (cond ((null? sequence) '()) ((predicate (car sequence)) (cons (car sequence) (filter predicate (cdr sequence)))) (else (filter predicate (cdr sequence)))))(define (sum-primes a b) (define (iter count accum) (cond ((&gt; count b) accum) ((prime? count) (iter (+ count 1) (+ count accum)) (else (iter (+ count 1) accum)))) (iter a 0))) 如果用序列操作 1234(define (sum-primes a b) (accumulate + 0 (filter prime? (enumerate-interval a b)))) 在执行计算时，第一个程序只需要维护正在累积的和。第二个程序只有等enumate-interval构造完成这一区间所有整数的表之后，过滤器才能开始工作。这就需要大量的中间存储，增加计算开销。 流是一种非常巧妙的想法，使我们既可以利用各种序列操作，又不会带来将序列作为表操作的性能代价。 从表面上看，流也是表，但对他们进行操作的过程的名字不同。有构造函数cons-stream，以及两个选择函数stream-car和stream-cdr，满足如下约束 12(stream-car (cons-stream x y))=x(stream-cdr (cons-stream x y))=y 我们用和第二章各种表操作（如list-ref，map和for-each等）类似的方式来操作流。 12345678910111213141516171819(define (stream-ref s n) (if (= n 0) (stream-car s) (stream-ref (stream-cdr s) (- n 1))))(define (stream-map proc s) (if (stream-null? s) the-empty-stream (cons-stream (proc (stream-car s)) (stream-map proc (stream-cdr s)))))(define (stream-for-each proc s) (if (stream-null? s) 'done (begin (proc (stream-car s)) (stream-for-each proc (stream-cdr s))))) 核心的诉求是，对于流的cdr的求值要等到真正通过过程stream-cdr去访问它的时候再做，而不是在构造stream-cdr的时候做。 流的实现将基于一种称为delay的特殊形式，对于(delay &lt;exp&gt;)的求值不是对表达式求值，而是返回一个称为 延时对象 的对象。这个对象可以看做是对未来某个时间要对表达式求值的一个允诺。 和delay一起的还有一个force的过程，它以一个延时对象为参数，执行相应的求值工作，也就是说，force就用来迫使delay完成所允诺的求值。下面用这两个概念来构造流。 cons-stream的定义 1(cons-stream ⟨a⟩ ⟨b⟩) 等价于 1(cons ⟨a⟩ (delay ⟨b⟩)) 可见，定义的时候，b还没有放到cons的cdr中。再看对cons的取值， 12345(define (stream-car stream) (car stream))(define (stream-cdr stream) (force (cdr stream))) 流实现的行为方式 我们再来看之前的过滤出素数的例子 12345(stream-car (stream-cdr (stream-filter prime? (stream-enumerate-interval 10000 1000000)))) 计算开始于对参数10000 1000000调用stream-enumerate-interval。它的实现是 1234567(define (stream-enumerate-interval low high) (if (&gt; low high) the-empty-stream (cons-stream low (stream-enumerate-interval (+ low 1) high)))) 这样，由stream-enumerate-interval返回的结果就是通过cons-stream形成的： 12345(cons 10000 (delay (stream-enumerate-interval 10001 1000000))) 也就是说，当stream-enumerate-interval返回一个流的时候，car是10000，而cdr是一个允诺，表示当需要的时候，才在这个区间中枚举更多的内容。 再来看这个流程构建后的过滤 123456789101112(define (stream-filter pred stream) (cond ((stream-null? stream) the-empty-stream) ((pred (stream-car stream)) (cons-stream (stream-car stream) (stream-filter pred (stream-cdr stream)))) (else (stream-filter pred (stream-cdr stream))))) 首先检查stream-car，因为这个数不是素数（10000），再进一步检查stream-cdr，这个时候对stream-cdr的调用会迫使系统对延时的stream-enumerate-interval求值，这一次就返回 12345(cons 10001 (delay (stream-enumerate-interval 10002 1000000))) 如此进行，直到找到第一个素数10007，此时stream-filter根据其定义返回 123(cons-stream (stream-car stream) (stream-filter pred (stream-cdr stream))) 相当于 12345678(cons 10007 (delay (stream-filter prime? (cons 10008 (delay (stream-enumerate-interval 10009 1000000)))))) 这样在stream-cdr中，又迫使延时的stream-filter求值，转而再去迫使stream-enumerate-interval求值，直到再找到下一个素数…… 一般而言，可以将延时求值看做是一种“由需要驱动”的设计，其中流处理的每个阶段都仅仅活动到足够满足下一阶段的需要。 delay和force的实现 delay必须包装起一个表达式，使其可以在以后根据需要求值。delay实际上也是一个lambda表达式的语法糖 123(delay ⟨exp⟩)=(lambda () ⟨exp⟩) 而force就是简单调用由delay产生的过程 12(define (force delayed-object) (delayed-object)) Java 8 - 通过lambda表达式进行惰性计算 lambda表达式的出现使得JDK8内部发生了很多有趣的变化, 其中就包括惰性计算的特性.这里以JDK标准库中的Logger为例, 1.8以前的log方法有如下签名: 12345678&gt; public void log(Level level, String msg) &#123;&gt; if (!isLoggable(level)) &#123;&gt; return;&gt; &#125;&gt; LogRecord lr = new LogRecord(level, msg);&gt; doLog(lr);&gt; &#125;&gt; &gt; 也就是说客户端程序调用log方法的时候, 无论最终是否触发log行为, msg始终是要被计算的. 若计算msg是非常耗时的行为, 那么无疑会造成不必要的开销. 下面是一个调用的例子: 12&gt; log(Level.WARNING, "Log msg: " + someExpensiveOperation());&gt; &gt; 在java 1.8版本出现之后, 该方法多了如下重载: 12345678&gt; public void log(Level level, Supplier&lt;String&gt; msgSupplier) &#123;&gt; if (!isLoggable(level)) &#123;&gt; return;&gt; &#125;&gt; LogRecord lr = new LogRecord(level, msgSupplier.get());&gt; doLog(lr);&gt; &#125;&gt; &gt; Supplier是一个FunctionalInterface, 也就是说现在的log方法可以接受一个无参的lambda表达式作为参数, 而计算的过程也被延迟到了supplier.get()的调用时. 改进后的调用例子: 123&gt; // 注意: 传入的lambda表达式并不会立即执行, 而是在log中判断isLoggable(level)成功后才会执行&gt; log(Level.WARNING, () -&gt; "Log msg: " + someExpensiveOperation());&gt; 这里还存在一个优化。有时需要多次对同一个delay求值，这就需要delay能保存上一次求出的值。于是可以将delay实现为一种特殊的记忆过程，它以一个无参过程为参数，返回该过程的记忆性版本。这种记忆性过程在第一次执行时将结果保存，下一次求值时再返回之前保存的 12345678(define (memo-proc proc) (let ((already-run? false) (result false)) (lambda () (if (not already-run?) (begin (set! result (proc)) (set! already-run? true) result) result)))) 此后再定义delay，使得(delay ⟨exp⟩)等价于 1(memo-proc (lambda () ⟨exp⟩)) 3.5.2 无穷流用流表示无穷长的序列。 比如以下的流可以表示所有正整数序列。这是一个无穷长的流。 1234567(define (integers-starting-from n) (cons-stream n (integers-starting-from (+ n 1))))(define integers (integers-starting-from 1))&gt; (stream-ref integers 100)101 用这个就可以表示，例如不能被7整除的整数的流 123456(define (divisible? x y) (= (remainder x y) 0))(define no-sevens (stream-filter (lambda (x) (not (divisible? x 7))) integers)) 之后在用访问这个流元素的方式找出 12&gt; (stream-ref no-sevens 100)117 也可以定义斐波那契数列的无穷流 1234567(define (fibgen a b) (cons-stream a (fibgen b (+ a b))))(define fibs (fibgen 0 1))------------------&gt; (stream-ref fibs 6)8 这样定义出的fibs是一个序对，其car是0，cdr是一个求值(fibgen 1 1)的允诺。求该表达式时，又将产生一个序对，car是，cdr是(fibgen 1 2)。 隐式的定义流 上面的integers和fibs是通过描述“生成”过程的方式定义的，这种过程是一个个的计算出流的元素。另一种就是隐式的求值。 例如，下面表达式将ones定义为1的一个无穷流。 1(define ones (cons-stream 1 ones)) 这种方式就像在定义一个递归过程：这里的ones是一个序对，car是1，cdr是求值ones的一个允诺。而对cdr的求值又得到了一个1和cdr的允诺。 add-streams操作产生出两个给定流的逐对元素之和 12(define (add-streams s1 s2) (stream-map + s1 s2)) 现在可以用另一种方式定义整数流integers 12345(define integers (cons-stream 1 (add-streams ones integers)))&gt; (stream-ref integers 10)11 首元素是1，其余是ones和integers之和。这样，integers的第二个元素就是1加上integers的第一个元素，也就是2。第三个元素就是1加上integers的第二个元素，也就是3。 12345; 原版integers定义(define (integers-starting-from n) (cons-stream n (integers-starting-from (+ n 1))))(define integers (integers-starting-from 1)) 同样的风格也可以定义出斐波那契数列 12345678(define fibs (cons-stream 0 (cons-stream 1 (add-streams (stream-cdr fibs) fibs))))&gt; (stream-ref fibs 11)89 这个定义是fib是一个从0和1开始的流，而这个流的其余部分都可以通过加起流fibs和移动了一个位置的fibs而得到 123 1 1 2 3 5 8 13 21 … = (stream-cdr fibs) 0 1 1 2 3 5 8 13 … = fibs0 1 1 2 3 5 8 13 21 34 … = fibs 3.5.5 函数式程序的模块化和对象的模块化正如在3.1.2中看到的，引进赋值的主要收益就是使我们可以增强系统的模块化，把一个大系统的状态中的某些部分封装，或者说“隐藏”到局部变量里。 流模型可以提供等价的模块化，同时又不必使用赋值。为了展示这方面的情况，我们可以重新实现前面在 3.1.2 节看过的π的蒙特卡罗估计，这次从流的观点出发来做。 这里的一个关键性的模块化问题，就是我们希望将一个随机数生成器的内部状态隐蔽起来，隔离在使用随机数的程序之外。从过程 rand-update 开始，它所提供的一系列值就是我们所需的随机数，用它作为一个随机数生成器： 12345(define rand (let ((x random-init)) (lambda () (set! x (rand-update x)) x))) 在这个流的描述中，看不到什么随机数生成器。在这里只有一个随机数的流，通过对rand-update的一系列顺序调用产生： 1234(define random-numbers (cons-stream random-init (stream-map rand-update random-numbers))) 用它构造出在random-numbers流中顺序的数对上的的Cesaro实验的输出流 1234567891011(define (map-successive-pairs f s) (cons-stream (f (stream-car s) (stream-car (stream-cdr s))) (map-successive-pairs f (stream-cdr (stream-cdr s)))))(define cesaro-stream (map-successive-pairs (lambda (r1 r2) (= (gcd r1 r2) 1)) random-numbers)) 现在将cesaro-stream扔进monte-carlo过程，该过程生成一个可能性估计的流。得到的结果就变换到一个估计π值的流。这一版本的程序不需要用参数去告诉它试多少次，只要查看更后面的值，就可以得到更好的π的估计。 12345678910111213141516171819202122(define (monte-carlo experiment-stream passed failed) (define (next passed failed) (cons-stream (/ passed (+ passed failed)) (monte-carlo (stream-cdr experiment-stream) passed failed))) (if (stream-car experiment-stream) (next (+ passed 1) failed) (next passed (+ failed 1))))(define pi (stream-map (lambda (p) (sqrt (/ 6 p))) (monte-carlo cesaro-stream 0 0)))------------------------&gt; (stream-ref pi 1000)3.24037034920393 而且，这一方法也非常模块化，这里构造了一个一般性的monte-carlo过程，它可以处理任何试验，而且这里没有赋值，也没有局部状态。 附：流执行的预先定义函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051;; stream-&gt;list(define stream-&gt;list (lambda (strm n) (if (or (stream-null? strm) (zero? n)) '() (cons (stream-car strm) (stream-&gt;list (stream-cdr strm) (sub1 n))))))(define-syntax delay (syntax-rules () ((_ exp) (lambda () exp))))(define (force delayed-object) (delayed-object))(define-syntax cons-stream (syntax-rules () ((_ a b) (cons a (delay b)))))(define (stream-car stream) (car stream)) (define (stream-cdr stream) (force (cdr stream)))(define (stream-null? stream) (null? stream))(define the-empty-stream '())(define (stream-ref s n) (if (= n 0) (stream-car s) (stream-ref (stream-cdr s) (- n 1))))(define (stream-map proc . argstreams) (if (stream-null? (car argstreams)) the-empty-stream (cons-stream (apply proc (map stream-car argstreams)) (apply stream-map (cons proc (map stream-cdr argstreams))))))(define (stream-filter pred stream) (cond ((stream-null? stream) the-empty-stream) ((pred (stream-car stream)) (cons-stream (stream-car stream) (stream-filter pred (stream-cdr stream)))) (else (stream-filter pred (stream-cdr stream))))) 参考资料SICP 第三章总结 SICP note 第三章习题索引 第三章英文版 SICP Python 描述 中文版]]></content>
      <categories>
        <category>读书</category>
      </categories>
      <tags>
        <tag>SICP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL技巧总结（不断更新中）]]></title>
    <url>%2F2019%2F05%2F08%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FMySQL%2Fmysql%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[ON DUPLICATE KEY UPDATE123456789val merge_sql = "INSERT INTO GPS_WARN_INFO(GPS_ID,CREATE_TIME,UPDATE_TIME,IS_TIME_INITIALIZED,HAS_TRACK,HAS_SPEED,HAS_BEEN_TO_BLACK_LIST_AREA)" + " values(?,?,?,?,?,?,?) " + "ON DUPLICATE KEY UPDATE " + "UPDATE_TIME=values(UPDATE_TIME), " + "IS_TIME_INITIALIZED=case WHEN GPS_WARN_INFO.IS_TIME_INITIALIZED=1 || values(IS_TIME_INITIALIZED) = 1 then 1 else 0 end, " + "HAS_TRACK=case WHEN GPS_WARN_INFO.HAS_TRACK=1 || values(HAS_TRACK)=1 then 1 else 0 end, " + "HAS_SPEED=case WHEN GPS_WARN_INFO.HAS_SPEED=1 || values(HAS_SPEED)=1 then 1 else 0 end, " + "HAS_BEEN_TO_BLACK_LIST_AREA=case when GPS_WARN_INFO.HAS_BEEN_TO_BLACK_LIST_AREA=''||GPS_WARN_INFO.HAS_BEEN_TO_BLACK_LIST_AREA is null then values(HAS_BEEN_TO_BLACK_LIST_AREA)" + " else case when values(HAS_BEEN_TO_BLACK_LIST_AREA) =''|| values(HAS_BEEN_TO_BLACK_LIST_AREA) is null THEN GPS_WARN_INFO.HAS_BEEN_TO_BLACK_LIST_AREA else case when instr(HAS_BEEN_TO_BLACK_LIST_AREA,values(HAS_BEEN_TO_BLACK_LIST_AREA)) &gt;0 then GPS_WARN_INFO.HAS_BEEN_TO_BLACK_LIST_AREA else CONCAT_WS('|',GPS_WARN_INFO.HAS_BEEN_TO_BLACK_LIST_AREA,values(HAS_BEEN_TO_BLACK_LIST_AREA)) end end end" 12345val merge_sql = "INSERT INTO GPS_WARN_INFO(GPS_ID,CREATE_TIME,UPDATE_TIME,IS_TIME_INITIALIZED,HAS_TRACK,HAS_SPEED,HAS_BEEN_TO_BLACK_LIST_AREA)" + " values(?,?,?,?,?,?,?) " + "ON DUPLICATE KEY UPDATE " + "UPDATE_TIME=values(UPDATE_TIME), " + "IS_TIME_INITIALIZED=GPS_WARN_INFO.IS_TIME_INITIALIZED"; 删除主键1alter table gps_analysis_wx_cluster drop primary key ; 增加主键、自增并放到第一个字段1ALTER TABLE gps_analysis_wx_cluster ADD `id` BIGINT(20) NOT NULL PRIMARY KEY AUTO_INCREMENT FIRST; 增加字段1alter table gps_analysis_wx_cluster add column `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT &apos;主键&apos;; 删除索引12ALTER TABLE gps_analysis_wx_cluster DROP INDEX data_date_index]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F04%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2Ftensorflow%2F%E5%9F%BA%E7%A1%80-%E8%8E%AB%E7%83%A6%E6%95%99%E7%A8%8B%2F5-1%20%E5%88%86%E7%B1%BB%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/5-01-classifier/ MNIST 数据首先准备数据（MNIST库） 12from tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets(&apos;MNIST_data&apos;, one_hot=True) 搭建网络1xs = tf.placeholder(tf.float32, [None, 784]) # 28x28 每张图片都表示一个数字，所以我们的输出是数字0到9，共10类。 1ys = tf.placeholder(tf.float32, [None, 10]) 调用add_layer函数搭建一个最简单的训练网络结构，只有输入层和输出层。 1prediction = add_layer(xs, 784, 10, activation_function=tf.nn.softmax) 其中输入数据是784个特征，输出数据是10个特征，激励采用softmax函数，网络结构图是这样子的 Cross entropy lossloss函数（即最优化目标函数）选用交叉熵函数。交叉熵用来衡量预测值和真实值的相似程度，如果完全相同，它们的交叉熵等于零。 12cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction),reduction_indices=[1])) # loss train方法（最优化算法）采用梯度下降法。 12345train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)sess = tf.Session()# tf.initialize_all_variables() 这种写法马上就要被废弃# 替换成下面的写法:sess.run(tf.global_variables_initializer()) 训练现在开始train，每次只取100张图片，免得数据太多训练太慢。 12batch_xs, batch_ys = mnist.train.next_batch(100)sess.run(train_step, feed_dict=&#123;xs: batch_xs, ys: batch_ys&#125;) 每训练50次输出一下预测精度 123if i % 50 == 0: print(compute_accuracy( mnist.test.images, mnist.test.labels)) 输出结果如下： Dropout解决过拟合建立 dropout 层本次内容需要使用一下 sklearn 数据库当中的数据, 没有安装 sklearn 的同学可以参考一下这个教程 安装一下. 然后 import 以下模块. 123456789import tensorflow as tffrom sklearn.datasets import load_digitsfrom sklearn.cross_validation import train_test_splitfrom sklearn.preprocessing import LabelBinarizerkeep_prob = tf.placeholder(tf.float32)......Wx_plus_b = tf.nn.dropout(Wx_plus_b, keep_prob) 这里的keep_prob是保留概率，即我们要保留的结果所占比例，它作为一个placeholder，在run时传入， 当keep_prob=1的时候，相当于100%保留，也就是dropout没有起作用。 下面我们分析一下程序结构，首先准备数据， 12345digits = load_digits()X = digits.datay = digits.targety = LabelBinarizer().fit_transform(y)X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3) 其中X_train是训练数据, X_test是测试数据。 然后添加隐含层和输出层 123# add output layerl1 = add_layer(xs, 64, 50, &apos;l1&apos;, activation_function=tf.nn.tanh)prediction = add_layer(l1, 50, 10, &apos;l2&apos;, activation_function=tf.nn.softmax) loss函数（即最优化目标函数）选用交叉熵函数。交叉熵用来衡量预测值和真实值的相似程度，如果完全相同，交叉熵就等于零。 12cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction), reduction_indices=[1])) # loss train方法（最优化算法）采用梯度下降法。 1train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy) 训练最后开始train，总共训练500次。 12sess.run(train_step, feed_dict=&#123;xs: X_train, ys: y_train, keep_prob: 0.5&#125;)#sess.run(train_step, feed_dict=&#123;xs: X_train, ys: y_train, keep_prob: 1&#125;) 可视化结果训练中keep_prob=1时，就可以暴露出overfitting问题。keep_prob=0.5时，dropout就发挥了作用。 我们可以两种参数分别运行程序，对比一下结果。 当keep_prob=1时，模型对训练数据的适应性优于测试数据，存在overfitting，输出如下： 红线是 train 的误差, 蓝线是 test 的误差. https://github.com/MorvanZhou/Tensorflow-Tutorial 这里有简化版代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677"""Know more, visit my Python tutorial page: https://morvanzhou.github.io/tutorials/My Youtube Channel: https://www.youtube.com/user/MorvanZhouDependencies:tensorflow: 1.1.0matplotlibnumpy"""import tensorflow as tfimport numpy as npimport matplotlib.pyplot as plttf.set_random_seed(1)np.random.seed(1)# Hyper parametersN_SAMPLES = 20N_HIDDEN = 300LR = 0.01# training datax = np.linspace(-1, 1, N_SAMPLES)[:, np.newaxis]y = x + 0.3*np.random.randn(N_SAMPLES)[:, np.newaxis]# test datatest_x = x.copy()test_y = test_x + 0.3*np.random.randn(N_SAMPLES)[:, np.newaxis]# show dataplt.scatter(x, y, c='magenta', s=50, alpha=0.5, label='train')plt.scatter(test_x, test_y, c='cyan', s=50, alpha=0.5, label='test')plt.legend(loc='upper left')plt.ylim((-2.5, 2.5))plt.show()# tf placeholderstf_x = tf.placeholder(tf.float32, [None, 1])tf_y = tf.placeholder(tf.float32, [None, 1])tf_is_training = tf.placeholder(tf.bool, None) # to control dropout when training and testing# overfitting neto1 = tf.layers.dense(tf_x, N_HIDDEN, tf.nn.relu)o2 = tf.layers.dense(o1, N_HIDDEN, tf.nn.relu)o_out = tf.layers.dense(o2, 1)o_loss = tf.losses.mean_squared_error(tf_y, o_out)o_train = tf.train.AdamOptimizer(LR).minimize(o_loss)# dropout netd1 = tf.layers.dense(tf_x, N_HIDDEN, tf.nn.relu)d1 = tf.layers.dropout(d1, rate=0.5, training=tf_is_training) # drop out 50% of inputsd2 = tf.layers.dense(d1, N_HIDDEN, tf.nn.relu)d2 = tf.layers.dropout(d2, rate=0.5, training=tf_is_training) # drop out 50% of inputsd_out = tf.layers.dense(d2, 1)d_loss = tf.losses.mean_squared_error(tf_y, d_out)d_train = tf.train.AdamOptimizer(LR).minimize(d_loss)sess = tf.Session()sess.run(tf.global_variables_initializer())plt.ion() # something about plottingfor t in range(500): sess.run([o_train, d_train], &#123;tf_x: x, tf_y: y, tf_is_training: True&#125;) # train, set is_training=True if t % 10 == 0: # plotting plt.cla() o_loss_, d_loss_, o_out_, d_out_ = sess.run( [o_loss, d_loss, o_out, d_out], &#123;tf_x: test_x, tf_y: test_y, tf_is_training: False&#125; # test, set is_training=False ) plt.scatter(x, y, c='magenta', s=50, alpha=0.3, label='train'); plt.scatter(test_x, test_y, c='cyan', s=50, alpha=0.3, label='test') plt.plot(test_x, o_out_, 'r-', lw=3, label='overfitting'); plt.plot(test_x, d_out_, 'b--', lw=3, label='dropout(50%)') plt.text(0, -1.2, 'overfitting loss=%.4f' % o_loss_, fontdict=&#123;'size': 20, 'color': 'red'&#125;); plt.text(0, -1.5, 'dropout loss=%.4f' % d_loss_, fontdict=&#123;'size': 20, 'color': 'blue'&#125;) plt.legend(loc='upper left'); plt.ylim((-2.5, 2.5)); plt.pause(0.1)plt.ioff()plt.show()]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F05%2F03%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2Ftensorflow%2F%E5%9F%BA%E7%A1%80-%E8%8E%AB%E7%83%A6%E6%95%99%E7%A8%8B%2F4-2%20TensorBoard%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/4-2-tensorboard2/ 上一篇讲到了 如何可视化TesorBorad整个神经网络结构的过程。 其实tensorboard还可以可视化训练过程( biase变化过程) , 这节重点讲一下可视化训练过程的图标是如何做的 。请看下图, 这是如何做到的呢？ 在histograms里面我们还可以看到更多的layers的变化: 这里还有一个events , 在这次练习中我们会把 整个训练过程中的误差值（loss）在event里面显示出来, 甚至你可以显示更多你想要显示的东西. 制作输入源由于这节我们观察训练过程中神经网络的变化, 所以首先要添一些模拟数据. Python 的 numpy 工具包可以帮助我们制造一些模拟数据. 所以我们先导入这个工具包: 12import tensorflow as tfimport numpy as np 然后借助 np 中的 np.linespace() 产生随机的数字, 同时为了模拟更加真实我们会添加一些噪声, 这些噪声是通过 np.random.normal() 随机产生的. 1234## make up some datax_data= np.linspace(-1, 1, 300, dtype=np.float32)[:,np.newaxis]noise= np.random.normal(0, 0.05, x_data.shape).astype(np.float32)y_data= np.square(x_data) -0.5+ noise 在 layer 中为 Weights, biases 设置变化图表 ¶通过上图的观察我们发现每个 layer 后面有有一个数字: layer1 和layer2 于是我们在 add_layer() 方法中添加一个参数 n_layer,用来标识层数, 并且用变量 layer_name 代表其每层的名名称, 代码如下: 123456789def add_layer( inputs , in_size, out_size, n_layer, activation_function=None): ## add one more layer and return the output of this layer layer_name=&apos;layer%s&apos;%n_layer ## define a new var ## and so on …… 接下来,我们为层中的Weights设置变化图, tensorflow中提供了tf.histogram_summary()方法,用来绘制图片, 第一个参数是图表的名称, 第二个参数是图表要记录的变量 123456789101112def add_layer(inputs , in_size, out_size,n_layer, activation_function=None): ## add one more layer and return the output of this layer layer_name='layer%s'%n_layer with tf.name_scope('layer'): with tf.name_scope('weights'): Weights= tf.Variable(tf.random_normal([in_size, out_size]),name='W') # tf.histogram_summary(layer_name+'/weights',Weights) # tensorflow 0.12 以下版的 tf.summary.histogram(layer_name + '/weights', Weights) # tensorflow &gt;= 0.12 ##and so no …… 同样的方法我们对biases进行绘制图标: 1234with tf.name_scope(&apos;biases&apos;): biases = tf.Variable(tf.zeros([1,out_size])+0.1, name=&apos;b&apos;) # tf.histogram_summary(layer_name+&apos;/biase&apos;,biases) # tensorflow 0.12 以下版的 tf.summary.histogram(layer_name + &apos;/biases&apos;, biases) # Tensorflow &gt;= 0.12 至于activation_function 可以不绘制. 我们对output 使用同样的方法: 12# tf.histogram_summary(layer_name+&apos;/outputs&apos;,outputs) # tensorflow 0.12 以下版本tf.summary.histogram(layer_name + &apos;/outputs&apos;, outputs) # Tensorflow &gt;= 0.12 最终经过我们的修改 , addlayer()方法成为如下的样子: 1234567891011121314151617181920212223242526272829def add_layer(inputs , in_size, out_size,n_layer, activation_function=None): ## add one more layer and return the output of this layer layer_name='layer%s'%n_layer with tf.name_scope(layer_name): with tf.name_scope('weights'): Weights= tf.Variable(tf.random_normal([in_size, out_size]),name='W') # tf.histogram_summary(layer_name+'/weights',Weights) tf.summary.histogram(layer_name + '/weights', Weights) # tensorflow &gt;= 0.12 with tf.name_scope('biases'): biases = tf.Variable(tf.zeros([1,out_size])+0.1, name='b') # tf.histogram_summary(layer_name+'/biase',biases) tf.summary.histogram(layer_name + '/biases', biases) # Tensorflow &gt;= 0.12 with tf.name_scope('Wx_plus_b'): Wx_plus_b = tf.add(tf.matmul(inputs,Weights), biases) if activation_function is None: outputs=Wx_plus_b else: outputs= activation_function(Wx_plus_b) # tf.histogram_summary(layer_name+'/outputs',outputs) tf.summary.histogram(layer_name + '/outputs', outputs) # Tensorflow &gt;= 0.12 return outputs 修改之后的名称会显示在每个tensorboard中每个图表的上方显示, 如下图所示: 由于我们对addlayer 添加了一个参数, 所以修改之前调用addlayer()函数的地方. 对此处进行修改: 1234# add hidden layerl1= add_layer(xs, 1, 10 , activation_function=tf.nn.relu)# add output layerprediction= add_layer(l1, 10, 1, activation_function=None) 添加n_layer参数后, 修改成为 : 1234# add hidden layerl1= add_layer(xs, 1, 10, n_layer=1, activation_function=tf.nn.relu)# add output layerprediction= add_layer(l1, 10, 1, n_layer=2, activation_function=None) 设置loss的变化图Loss 的变化图和之前设置的方法略有不同. loss是在tesnorBorad 的event下面的, 这是由于我们使用的是tf.scalar_summary() 方法. 观看loss的变化比较重要. 当你的loss呈下降的趋势,说明你的神经网络训练是有效果的. 修改后的代码片段如下： 12345with tf.name_scope(&apos;loss&apos;): loss= tf.reduce_mean(tf.reduce_sum( tf.square(ys- prediction), reduction_indices=[1])) # tf.scalar_summary(&apos;loss&apos;,loss) # tensorflow &lt; 0.12 tf.summary.scalar(&apos;loss&apos;, loss) # tensorflow &gt;= 0.12 给所有训练图合并接下来， 开始合并打包。 tf.merge_all_summaries() 方法会对我们所有的 summaries合并到一起. 因此在原有代码片段中添加： 12345678910sess= tf.Session()# merged= tf.merge_all_summaries() # tensorflow &lt; 0.12merged = tf.summary.merge_all() # tensorflow &gt;= 0.12# writer = tf.train.SummaryWriter(&apos;logs/&apos;, sess.graph) # tensorflow &lt; 0.12writer = tf.summary.FileWriter(&quot;logs/&quot;, sess.graph) # tensorflow &gt;=0.12# sess.run(tf.initialize_all_variables()) # tf.initialize_all_variables() # tf 马上就要废弃这种写法sess.run(tf.global_variables_initializer()) # 替换成这样就好 训练数据假定给出了x_data,y_data并且训练1000次. 12for i in range(1000): sess.run(train_step, feed_dict=&#123;xs:x_data, ys:y_data&#125;) 以上这些仅仅可以记录很绘制出训练的图表， 但是不会记录训练的数据。 为了较为直观显示训练过程中每个参数的变化，我们每隔上50次就记录一次结果 , 同时我们也应注意, merged 也是需要run 才能发挥作用的,所以在for循环中写下： 123if i%50 == 0: rs = sess.run(merged,feed_dict=&#123;xs:x_data,ys:y_data&#125;) writer.add_summary(rs, i) 最后修改后的片段如下： 12345for i in range(1000): sess.run(train_step, feed_dict=&#123;xs:x_data, ys:y_data&#125;) if i%50 == 0: rs = sess.run(merged,feed_dict=&#123;xs:x_data,ys:y_data&#125;) writer.add_summary(rs, i) 在 tensorboard 中查看效果程序运行完毕之后, 会产生logs目录 , 使用命令 tensorboard --logdir logs 注意: 本节内容会用到浏览器, 而且与 tensorboard 兼容的浏览器是 “Google Chrome”. 使用其他的浏览器不保证所有内容都能正常显示. 同时注意, 如果使用 http://0.0.0.0:6006 或者 tensorboard 中显示的网址打不开的朋友们, 请使用 http://localhost:6006, 大多数朋友都是这个问题.]]></content>
  </entry>
  <entry>
    <title><![CDATA[【转】Gitbook使用]]></title>
    <url>%2F2019%2F04%2F19%2F%E5%B7%A5%E5%85%B7%E5%92%8C%E7%8E%AF%E5%A2%83%2Fgitbook%2F</url>
    <content type="text"><![CDATA[https://blog.csdn.net/iron_ye/article/details/83254911 123npm install -g gitbook-cligitbook init 插件book.json新建一个 book.json 文件，可以配置网站信息、在 plugins 和 pluginsConfig 字段添加插件等。 插件命名方式为： gitbook-plugin-X: 插件； gitbook-theme-X: 主题。 可以在 npmjs 或 GitBook 插件 直接搜索插件或者主题。 book.json 内容大概如下： 1234567891011&#123; &quot;gitbook&quot;: &quot;3.2.3&quot;, &quot;title&quot;: &quot;GPS运营管理系统 - 使用手册&quot;, &quot;author&quot;: &quot;yin.xu01&quot;, &quot;language&quot;: &quot;zh-hans&quot;, &quot;plugins&quot;: [ &quot;-lunr&quot;, &quot;-search&quot;, &quot;search-pro&quot; ]&#125; 插件官方v2版本禁用了插件，但是仍可以从github上下载并安装 Search Pro支持中文搜索, 需要将默认的search插件去掉, 注意: 如果标题中有包含的关键字, 标题的样式会有所变化插件地址 123456&quot;plugins&quot;: [ &quot;-lunr&quot;, &quot;-search&quot;, &quot;-fontsettings&quot;, &quot;search-pro&quot; ]]]></content>
      <categories>
        <category>工具和环境</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F04%2F10%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2Ftensorflow%2F%E5%9F%BA%E7%A1%80-%E8%8E%AB%E7%83%A6%E6%95%99%E7%A8%8B%2F3-4%20%E5%8A%A0%E9%80%9F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%2F</url>
    <content type="text"><![CDATA[https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/3-4-A-speed-up-learning/ 包括以下几种模式: Stochastic Gradient Descent (SGD) Momentum AdaGrad RMSProp Adam Stochastic Gradient Descent (SGD) ¶)实证明, SGD 并不是最快速的训练方法, 红色的线是 SGD, 但它到达学习目标的时间是在这些方法中最长的一种. 我们还有很多其他的途径来加速训练. Momentum 更新方法 ¶大多数其他途径是在更新神经网络参数那一步上动动手脚. 传统的参数 W 的更新是把原始的 W 累加上一个负的学习率(learning rate) 乘以校正值 (dx). 这种方法可能会让学习过程曲折无比, 看起来像 喝醉的人回家时, 摇摇晃晃走了很多弯路. 所以我们把这个人从平地上放到了一个斜坡上, 只要他往下坡的方向走一点点, 由于向下的惯性, 他不自觉地就一直往下走, 走的弯路也变少了. 这就是 Momentum 参数更新. 另外一种加速方法叫AdaGrad. AdaGrad 更新方法 ¶ 这种方法是在学习率上面动手脚, 使得每一个参数更新都会有自己与众不同的学习率, 他的作用和 momentum 类似, 不过不是给喝醉酒的人安排另一个下坡, 而是给他一双不好走路的鞋子, 使得他一摇晃着走路就脚疼, 鞋子成为了走弯路的阻力, 逼着他往前直着走. 他的数学形式是这样的. 接下来又有什么方法呢? 如果把下坡和不好走路的鞋子合并起来, 是不是更好呢? 没错, 这样我们就有了 RMSProp 更新方法. RMSProp 更新方法 ¶ 有了 momentum 的惯性原则 , 加上 adagrad 的对错误方向的阻力, 我们就能合并成这样. 让 RMSProp同时具备他们两种方法的优势. 不过细心的同学们肯定看出来了, 似乎在 RMSProp 中少了些什么. 原来是我们还没把 Momentum合并完全, RMSProp 还缺少了 momentum 中的 这一部分. 所以, 我们在 Adam 方法中补上了这种想法. Adam 更新方法 ¶ 计算m 时有 momentum 下坡的属性, 计算 v 时有 adagrad 阻力的属性, 然后再更新参数时 把 m 和 V 都考虑进去. 实验证明, 大多数时候, 使用 adam 都能又快又好的达到目标, 迅速收敛. 所以说, 在加速神经网络训练的时候, 一个下坡, 一双破鞋子, 功不可没.]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F04%2F10%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2Ftensorflow%2F%E5%9F%BA%E7%A1%80-%E8%8E%AB%E7%83%A6%E6%95%99%E7%A8%8B%2F3-3%20%E5%8F%AF%E8%A7%86%E5%8C%96%2F</url>
    <content type="text"><![CDATA[https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/3-3-visualize-result/ matplotlib 可视化构建图形，用散点图描述真实数据之间的关系。 （注意：plt.ion()用于连续显示。） 123456# plot the real datafig = plt.figure()ax = fig.add_subplot(1,1,1)ax.scatter(x_data, y_data)plt.ion()#本次运行请注释，全局运行不要注释plt.show() 散点图的结果为： 接下来，我们来显示预测数据。 每隔50次训练刷新一次图形，用红色、宽度为5的线来显示我们的预测数据和输入之间的关系，并暂停0.1s。 12345678910111213for i in range(1000): # training sess.run(train_step, feed_dict=&#123;xs: x_data, ys: y_data&#125;) if i % 50 == 0: # to visualize the result and improvement try: ax.lines.remove(lines[0]) except Exception: pass prediction_value = sess.run(prediction, feed_dict=&#123;xs: x_data&#125;) # plot the prediction lines = ax.plot(x_data, prediction_value, &apos;r-&apos;, lw=5) plt.pause(0.1)]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F04%2F10%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2Ftensorflow%2F%E5%9F%BA%E7%A1%80-%E8%8E%AB%E7%83%A6%E6%95%99%E7%A8%8B%2F3-2%20%E5%BB%BA%E9%80%A0%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/3-2-create-NN/ add_layer 功能首先，我们导入本次所需的模块。 12import tensorflow as tfimport numpy as np 构造添加一个神经层的函数。（在上次课程中有详细介绍） 123456789def add_layer(inputs, in_size, out_size, activation_function=None): Weights = tf.Variable(tf.random_normal([in_size, out_size])) biases = tf.Variable(tf.zeros([1, out_size]) + 0.1) Wx_plus_b = tf.matmul(inputs, Weights) + biases if activation_function is None: outputs = Wx_plus_b else: outputs = activation_function(Wx_plus_b) return outputs 导入数据构建所需的数据。 这里的x_data和y_data并不是严格的一元二次函数的关系，因为我们多加了一个noise,这样看起来会更像真实情况。 123x_data = np.linspace(-1,1,300, dtype=np.float32)[:, np.newaxis]noise = np.random.normal(0, 0.05, x_data.shape).astype(np.float32)y_data = np.square(x_data) - 0.5 + noise 利用占位符定义我们所需的神经网络的输入。 tf.placeholder()就是代表占位符，这里的None代表无论输入有多少都可以，因为输入只有一个特征，所以这里是1。 12xs = tf.placeholder(tf.float32, [None, 1])ys = tf.placeholder(tf.float32, [None, 1]) 接下来，我们就可以开始定义神经层了。 通常神经层都包括输入层、隐藏层和输出层。这里的输入层只有一个属性， 所以我们就只有一个输入；隐藏层我们可以自己假设，这里我们假设隐藏层有10个神经元； 输出层和输入层的结构是一样的，所以我们的输出层也是只有一层。 所以，我们构建的是——输入层1个、隐藏层10个、输出层1个的神经网络。 搭建网络下面，我们开始定义隐藏层,利用之前的add_layer()函数，这里使用 Tensorflow 自带的激励函数tf.nn.relu。 1l1 = add_layer(xs, 1, 10, activation_function=tf.nn.relu) 接着，定义输出层。此时的输入就是隐藏层的输出——l1，输入有10层（隐藏层的输出层），输出有1层。 1prediction = add_layer(l1, 10, 1, activation_function=None) 计算预测值prediction和真实值的误差，对二者差的平方求和再取平均。 12loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction), reduction_indices=[1])) 接下来，是很关键的一步，如何让机器学习提升它的准确率。tf.train.GradientDescentOptimizer()中的值通常都小于1，这里取的是0.1，代表以0.1的学习率来最小化误差loss。 1train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) 使用变量时，都要对它进行初始化，这是必不可少的。 12# init = tf.initialize_all_variables() # tf 马上就要废弃这种写法init = tf.global_variables_initializer() # 替换成这样就好 定义Session，并用 Session 来执行 init 初始化步骤。 （注意：在tensorflow中，只有session.run()才会执行我们定义的运算。） 12sess = tf.Session()sess.run(init) 训练下面，让机器开始学习。 比如这里，我们让机器学习1000次。机器学习的内容是train_step, 用 Session 来 run每一次 training 的数据，逐步提升神经网络的预测准确性。 (注意：当运算要用到placeholder时，就需要feed_dict这个字典来指定输入。) 123for i in range(1000): # training sess.run(train_step, feed_dict=&#123;xs: x_data, ys: y_data&#125;) 每50步我们输出一下机器学习的误差。 123if i % 50 == 0: # to see the step improvement print(sess.run(loss, feed_dict=&#123;xs: x_data, ys: y_data&#125;)) 在电脑上运行本次代码的结果为： 通过上图可以看出，误差在逐渐减小，这说明机器学习是有积极的效果的。]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F04%2F10%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2Ftensorflow%2F%E5%9F%BA%E7%A1%80-%E8%8E%AB%E7%83%A6%E6%95%99%E7%A8%8B%2F3-1%20%E6%B7%BB%E5%8A%A0%E5%B1%82%2F</url>
    <content type="text"><![CDATA[https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/3-1-add-layer/ 定义 add_layer()在 Tensorflow 里定义一个添加层的函数可以很容易的添加神经层,为之后的添加省下不少时间. 神经层里常见的参数通常有weights、biases和激励函数。 首先，我们需要导入tensorflow模块。 1import tensorflow as tf 然后定义添加神经层的函数def add_layer(),它有四个参数：输入值、输入的大小、输出的大小和激励函数，我们设定默认的激励函数是None。 1def add_layer(inputs, in_size, out_size, activation_function=None): 接下来，我们开始定义weights和biases。 因为在生成初始参数时，随机变量(normal distribution)会比全部为0要好很多，所以我们这里的weights为一个in_size行, out_size列的随机变量矩阵。 1Weights = tf.Variable(tf.random_normal([in_size, out_size])) 在机器学习中，biases的推荐值不为0，所以我们这里是在0向量的基础上又加了0.1。 1biases = tf.Variable(tf.zeros([1, out_size]) + 0.1) 下面，我们定义Wx_plus_b, 即神经网络未激活的值。其中，tf.matmul()是矩阵的乘法。 1Wx_plus_b = tf.matmul(inputs, Weights) + biases 当activation_function——激励函数为None时，输出就是当前的预测值——Wx_plus_b，不为None时，就把Wx_plus_b传到activation_function()函数中得到输出。 1234if activation_function is None: outputs = Wx_plus_b else: outputs = activation_function(Wx_plus_b) 最后，返回输出，添加一个神经层的函数——def add_layer()就定义好了。 1return outputs]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F04%2F10%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2Ftensorflow%2F%E5%9F%BA%E7%A1%80-%E8%8E%AB%E7%83%A6%E6%95%99%E7%A8%8B%2F2-5%20Placeholder%E4%BC%A0%E5%85%A5%E5%80%BC%2F</url>
    <content type="text"><![CDATA[https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/2-5-placeholde/ 简单运用这一次我们会讲到 Tensorflow 中的 placeholder , placeholder 是 Tensorflow 中的占位符，暂时储存变量. Tensorflow 如果想要从外部传入data, 那就需要用到 tf.placeholder(), 然后以这种形式传输数据 sess.run(***, feed_dict={input: **}). 示例： 12345678import tensorflow as tf#在 Tensorflow 中需要定义 placeholder 的 type ，一般为 float32 形式input1 = tf.placeholder(tf.float32)input2 = tf.placeholder(tf.float32)# mul = multiply 是将input1和input2 做乘法运算，并输出为 output ouput = tf.multiply(input1, input2) 接下来, 传值的工作交给了 sess.run() , 需要传入的值放在了feed_dict={} 并一一对应每一个 input. placeholder 与 feed_dict={} 是绑定在一起出现的。 123with tf.Session() as sess: print(sess.run(ouput, feed_dict=&#123;input1: [7.], input2: [2.]&#125;))# [ 14.]]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F04%2F09%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2FNLP%2F%E7%AE%80%E6%98%93%E5%AE%9E%E4%BE%8B%E9%9B%86%2F</url>
    <content type="text"><![CDATA[https://github.com/caijie12138/CS224n-2019 https://github.com/graykode/nlp-tutorial]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F04%2F09%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2FNLP%2Fseq2seq%E5%AE%9E%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[https://zhuanlan.zhihu.com/p/59476019 实现的是一个输出反义词的Seq2Seq 参考链接：graykode/nlp-tutorial 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980import numpy as npimport tensorflow as tfchar_sum = [c for c in &apos;SEPabcdefghijklmnopqrstuvwxyz&apos;]#定义所有可能出现的char S、E、P是用来填充和界定边界的dic_char = &#123;c:i for i,c in enumerate(char_sum)&#125;#定义字符对应数字的dicdata = [[&apos;man&apos;,&apos;woman&apos;],[&apos;small&apos;,&apos;large&apos;],[&apos;in&apos;,&apos;out&apos;],[&apos;young&apos;,&apos;old&apos;],[&apos;black&apos;,&apos;white&apos;],[&apos;far&apos;,&apos;near&apos;],[&apos;short&apos;,&apos;long&apos;],[&apos;king&apos;,&apos;queen&apos;],[&apos;up&apos;,&apos;down&apos;]]#定义一堆反义词 seq2seq的目的就是输出反义词#定义一下超参数n_step=5#最长的词长度不超过5num_class=len(char_sum)#相当于char的种类数n_hidden = 128#每个RNN的隐藏单元个数def get_batch(data): input_batch,output_batch,target_batch = [],[],[] for seq in data:#做一个padding的操作 for i in range(2):#因为只有原词和反义词 所以用的2 seq[i] = seq[i]+&apos;P&apos;*(n_step-len(seq[i]))#把长度不足5的单词 padding到5 input_ = [dic_char[n] for n in seq[0]]#[&apos;m&apos;,&apos;a&apos;,&apos;n&apos;,&apos;P&apos;,&apos;P&apos;]-&gt;[&apos;15&apos;,&apos;3&apos;,&apos;16&apos;,&apos;2&apos;,&apos;2&apos;] output_ = [dic_char[n] for n in (&apos;S&apos; + seq[1])]#seq2seq要求target前有一位 target_ = [dic_char[n] for n in (seq[1] + &apos;E&apos;)]#[25, 17, 15, 3, 16, 1] #用one-hot表示各个字符 input_batch.append(np.eye(num_class)[input_])#从一个对角矩阵中找ont-hot向量 output_batch.append(np.eye(num_class)[output_])# target_batch.append(target_) #print(input_batch[0].shape)#（5，29）5个char 每个char由一个29维的one-hot表示 return input_batch,output_batch,target_batch#定义modelencoder_input = tf.placeholder(tf.float32,[None,None,num_class])#(batch_size,max_len,num_class)decoder_input = tf.placeholder(tf.float32,[None,None,num_class])#(batch_size,max_len+1,num_class) max_len+1 because&apos;S&apos;target = tf.placeholder(tf.int32,[None,None])#(batch_size,max_len+1)max_len+1 because&apos;E&apos;#encoderwith tf.variable_scope(&apos;encoder&apos;): encoder = tf.contrib.rnn.BasicRNNCell(n_hidden)#根据每个RNN的隐藏单元个数 创建RNN cell encoder = tf.contrib.rnn.DropoutWrapper(encoder,output_keep_prob=0.5)#dropout 随机失活 _,encoder_output = tf.nn.dynamic_rnn(encoder,encoder_input,dtype=tf.float32)#decoderwith tf.variable_scope(&apos;decoder&apos;): decoder = tf.contrib.rnn.BasicRNNCell(n_hidden)#根据每个RNN的隐藏单元个数 创建RNN cell decoder = tf.contrib.rnn.DropoutWrapper(decoder,output_keep_prob=0.5)#dropout 随机失活 decoder_output,_ = tf.nn.dynamic_rnn(decoder,decoder_input,initial_state=encoder_output,dtype=tf.float32)#还需要过一个全连接 把得到的值映射到29个类别model = tf.layers.dense(decoder_output,num_class,activation=None)#计算loss 以及优化cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model,labels=target))optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)#定义测试函数 测试的是一个单词def test(word): seq_data = [word,&apos;P&apos;*len(word)] input_batch,output_batch,_ = get_batch([seq_data]) predict = tf.argmax(model,2) # model : [batch_size, max_len+1, n_class] res = sess.run(predict,feed_dict=&#123;encoder_input:input_batch,decoder_input:output_batch&#125;) decoded = [char_sum[i] for i in res[0]] end = decoded.index(&apos;E&apos;) translated = &apos;&apos;.join(decoded[:end]) return translatedsess = tf.Session()#初始化sess.run(tf.global_variables_initializer())#准备batchinput_batch,output_batch,target_batch = get_batch(data)for i in range(5000):#跑5000轮 _,cost_ = sess.run([optimizer,cost], feed_dict=&#123;encoder_input:input_batch,decoder_input:output_batch,target:target_batch&#125;) if (i+1)%1000 == 0: print(&apos;Epoch %04d:&apos; % (i+1)) print(&apos;cost:%.6f&apos; % cost_)print(&apos;test&apos;)print(&apos;man -&gt;&apos;, test(&apos;man&apos;))print(&apos;mans -&gt;&apos;, test(&apos;mans&apos;))print(&apos;king -&gt;&apos;, test(&apos;king&apos;))print(&apos;black -&gt;&apos;, test(&apos;black&apos;))print(&apos;up -&gt;&apos;, test(&apos;up&apos;)) 在易学上直接能跑 1/home/ubuntu/MyFiles/dl_codes/nlp-tutorial.ipynb]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F04%2F09%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F%E6%98%93%E5%AD%A6%E6%99%BA%E8%83%BD%2F%E9%85%8D%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[首页 https://www.easyaiforum.cn/instanceList 个人存储将代码与数据准备好。如果比较大，建议开机前，先上传。 https://bbs.easyaiforum.cn/thread-1385-1-1.html 上传开机前，设置个人存储空间：https://bbs.easyaiforum.cn/thread-1384-1-1.html 存储位置 /home/ubuntu/MyFiles/autoupload{上传时间} 下载(1) ubuntu，使用FileZilla。传输文件可租用纯CPU的Ubuntu主机，更省钱。具体请参考教程：https://bbs.easyaiforum.cn/thread-915-1-1.html 1ssh ubuntu@60.205.219.151 -p 12560 1sftp -P 12560 ubuntu@60.205.219.151 个人数据、自行安装包或库，长期保存 个人数据与pip安装的程序包，永久保存。但需满足以下条件： (1) 这些文件，放在/home/ubuntu/MyFiles(Ubuntu)、或M:(Windows)。 (2) 使用pip/pip3 install package（不要用sudo pip/pip3 install package） (3) 你的个人云存储，没有被删除。若个人云存储已被删除，所有的个人数据、设置与安装包，均会丢失。 每次开机，系统都是新的。个人云存储的所有文件，都会挂载在/home/ubuntu/MyFiles(Ubuntu)、或M:(Windows)。 个人云存储，只有两种情况会被删除：(1) 用户主动删除；(2) 连续欠费3天。 可使用sudo apt/apt-get install安装程序，密码为注册邮箱。但是，apt/apt-get安装的程序，关机后丢失 (下次开机，还需使用，要重新安装)。 租用主机。你可新建主机，也可继续使用以前配好的主机(在“我的主机”中，想继续用的主机，点击“开机”即可)。 远程租用操作步骤：https://bbs.easyaiforum.cn/thread-398-1-1.html开发环境详细说明：https://bbs.easyaiforum.cn/forum.php?mod=viewthread&amp;tid=1005&amp;fromuid=5 安装包https://bbs.easyaiforum.cn/thread-1371-1-1.html 主要有pip与apt两种。(1) pip方式，请使用 pip install或pip3 install。不要加sudo。这种方式安装的包，会永久保存。也就是这次安装、用完关机，下次再开机，这些包，都还在。(2) apt方式，一般需要sudo apt……。这种方式，用完关机，就丢失了。下次开机，还得再装一次。 注意：pip或pip3，也可以用sudo pip/pip3 install。但是，这种方式安装的包，用完关机，就会丢失。注意：不要更新pip。不必理会pip更新的提示。 ​ ubuntu若采用conda_DL，也请先激活conda环境，再用conda install或pip install进行安装。 \2. 说明(1) 指定版本的安装：pip install tensorflow-gpu==1.11.0，这条命令会安装1.11.0的GPU版本tensorflow。其他包，采用类似方式。 (2) 指定pip源： 1pip install numpy -i https://pypi.tuna.tsinghua.edu.cn 这条命令，会在安装numpy时，采用清华源。换成国内源，而不是pythonhosted，会快很多。当然，我们平台的大多数镜像，默认源都已更换成清华源。 (3) 慎用 pip install -r requirements.txt。很多开源实现，为了方便大家，将依赖包放入requirements，一条命令安装所有包。但是，这种方式，很容易出现冲突。不建议大家这么做。那么，怎么做，看下条。 (5) 理解版本的冲突：假设我pip install tensorflow，这条命令会安装一个CPU版本的tensorflow。然后，我开一个带GPU的虚拟机，并采用 DL全家桶这样的环境。然后，我运行一个tensorflow代码。猜一猜，我能用得上GPU吗？很遗憾，用不上。原因：python找tensorflow环境时，会首先到MyFiles .local找，然后找到了一个CPU版tensorflow。然后，python不会再找。而GPU版tensorflow，以DL全家桶为例，是在系统目录中。解决方法：卸载自己安装的tensorflow pip uninstall tensorflow。然后就OK了。 (6) 卸载已安装的包：pip unistall xxx；apt remove xxx。 pre-trained modelUbuntu主机请到/home/ubuntu/PublicData/models 充值 充值指南，请参考&lt;https://bbs.easyaiforum.cn/forum.php?mod=viewthread&amp;tid=1031&amp;fromuid=5&gt; 【注意】充值后，可分多次使用，长期有效，不可退款。 在平台有长期使用需求的，可以考虑“单次充值，每满100送10”这样的推广大使优惠。易学币长期有效。充值时填入推广码，即可享受优惠：sUQ4oAIfNkVESu5M。【注意: 一定要填推广码，才能享受优惠，并不是充值100就有优惠】 关于Jupyter Notebook(1) 用于学习，Jupyter Notebook完全胜任。我们也内置了大量Jupyter的案例。(2) 正在使用Jupyter Notebook，当用pip3安装pkg后，需要重启kernel，Jupyter Notebook中“import pkg”才会正常。(3) Jupyter Notebook不适合作为大型开发工具。因此，上传大量文件或大size文件，长时间运行程序，运行规模较大的程序，慎用Jupyter Notebook。(4) Jupyter Notebook中不要同开启多个程序的运行，很容易出现卡死的问题。(5) Jupyter的程序只要申请资源，Jupyter就会去抢占系统资源。所以用Jupyter后，再做别的操作，可能都会很慢。甚至系统资源全部用掉后，它还会去申请，然后导致自身的报错退出。所以，喜欢Jupyter的亲们，租用主机时，内存、核数，适当地多选一点 ，对稳定性和使用体验大有益处。 关于进程被杀死或运行出错程序出错的原因有很多。除了编码bug之外，最常见的一类错误是系统资源的不足。（1）内存不足：简单比方，你的程序在运行中最大会占用12G内存，你的主机只有10G。那么，当你的程序请求12G内存时，系统就会把这个进程kill。（2）硬盘空间不足：每个人的数据空间都是有限的。如果你的硬盘空间满了，你的程序就会写不进数据。这时，程序也会出错。另外，提示一点，许多用户反映，Jupyter跑程序，时间一长就会卡死。内存爆了，是最常见的原因。 Ubuntu系统查看CPU、GPU、内存、硬盘的使用情况，请参考：https://bbs.easyaiforum.cn/thread-1052-1-1.html 关于Github下载代码见教程：https://bbs.easyaiforum.cn/thread-1244-1-1.html 显卡独占性说明 ​ 显卡是独占的，也就是，租期内用户拥有一块完整的物理显卡 (1080Ti或1050Ti)。用户若不相信，可跑多个程序，将显卡1000%的能力用起来。但是，用户的程序，如何将这个显卡100%地用起来，就需要用户去研究，平台不负责对具体某个程序的调优。 问题1、tensorflow1.12.0的环境，不能用pip，无法安装dlib，会报错 1CMake must be installed to build the following extensions: dlib 然后又无法安装cmake]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F04%2F09%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F00%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%81%E8%A5%BF%E7%93%9C%E4%B9%A6%E8%AE%B2%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[统计学习方法 https://github.com/SmirkCao/Lihang 说明：https://zhuanlan.zhihu.com/p/58324861 /Users/david/david/git/ml_book/Lihang 西瓜书 https://github.com/datawhalechina/pumpkin-book 在线阅读：https://datawhalechina.github.io/pumpkin-book/#/]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F04%2F09%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2Ftensorflow%2F%E7%89%88%E6%9C%AC%E5%8D%87%E7%BA%A7%2F</url>
    <content type="text"><![CDATA[104/09/2019 10:44:17 ERROR The minimum supported Tensorflow is version 1.12 but you have version 1.11 installed. Please upgrade Tensorflow. 1pip install --upgrade tensorflow]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F04%2F08%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2Fdeepface%2F</url>
    <content type="text"><![CDATA[https://www.cnblogs.com/zackstang/p/9011753.html https://www.zhihu.com/question/264019472?sort=created 1. 获取**deepfakes工具包 1git clone https://github.com/deepfakes/faceswap.git 2、安装包 12345pip install pynvx# 安装cv2pip install opencv-pythonpip install dlibpip install face_recognition 在易学上安装 123pip3 install psutilpip3 install nvidia-ml-pypip3 install dlib 安装dlib报错 12345RuntimeError: ******************************************************************* CMake must be installed to build the following extensions: dlib ******************************************************************* Command &quot;/usr/bin/python3 -u -c &quot;import setuptools, tokenize;__file__=&apos;/tmp/pip-build-k2zw6rng/dlib/setup.py&apos;;exec(compile(getattr(tokenize, &apos;open&apos;, open)(__file__).read().replace(&apos;\r\n&apos;, &apos;\n&apos;), __file__, &apos;exec&apos;))&quot; install --record /tmp/pip-n4xvap5e-record/install-record.txt --single-version-externally-managed --compile --user --prefix=&quot; failed with error code 1 in /tmp/pip-build-k2zw6rng/dlib/ 3. 面部抓取** 在收集完样本后，使用 1python faceswap.py extract -i /home/ubuntu/MyFiles/deepface/pp -o /home/ubuntu/MyFiles/deepface/pp_out 命令对样本图片进行面部抓取。再手工删除不合适的照片 4、解析视频为图片，或直接找一些图片，再进行面部抓取 1python faceswap.py extract -i /Users/david/david/code/deepface/kobe -o /Users/david/david/code/deepface/kobe_out 5、开始训练 1python faceswap.py train -A /home/ubuntu/MyFiles/deepface/pp_out/ -B /home/ubuntu/MyFiles/deepface/kobe_out -m models/ 12345===============================================04/09/2019 12:49:35 INFO - Starting -04/09/2019 12:49:35 INFO - Press &apos;ENTER&apos; to save and quit -04/09/2019 12:49:35 INFO - Press &apos;S&apos; to save model weights immediately -04/09/2019 12:49:35 INFO =============================================== 6、转换 1python faceswap.py convert –i /Users/david/david/code/deepface/pp_out/ -o /Users/david/david/code/deepface/pp_out_new -m models/ 报错123456789101112ModuleNotFoundError: No module named &apos;numpy.core._multiarray_umath&apos;ImportError: numpy.core.multiarray failed to importThe above exception was the direct cause of the following exception:Traceback (most recent call last): File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 960, in _find_and_loadSystemError: &lt;class &apos;_frozen_importlib._ModuleLockManager&apos;&gt; returned a result with an error setImportError: numpy.core._multiarray_umath failed to importImportError: numpy.core.umath failed to import2019-04-09 11:00:44.565633: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptrAbort trap: 6 解决 12pip install --upgrade --force-reinstall numpy==1.14.5pip install --upgrade --force-reinstall pandas==0.22.0 2、 12345678904/09/2019 12:49:42 ERROR Number of images is lower than batch-size (Note that too few images may lead to bad training). # images: 39, batch-size: 64Traceback (most recent call last): File &quot;/Users/david/david/git/dl/faceswap/lib/multithreading.py&quot;, line 261, in _runner target(*args, **kwargs) File &quot;/Users/david/david/git/dl/faceswap/lib/training_data.py&quot;, line 89, in load_batches self.validate_samples(images) File &quot;/Users/david/david/git/dl/faceswap/lib/training_data.py&quot;, line 122, in validate_samples assert length &gt;= self.batchsize, msgAssertionError: Number of images is lower than batch-size (Note that too few images may lead to bad training). # images: 39, batch-size: 64]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F04%2F07%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2Ftensorflow%2F%E7%AD%94%E7%96%91%2F</url>
    <content type="text"><![CDATA[以下Variable的定义有什么不同1234567891011w1 = tf.Variable(tf.random_uniform([1], -1.0, 1.0), name=&quot;weight&quot;)# 输出&lt;tf.Variable &apos;weight_3:0&apos; shape=(1,) dtype=float32_ref&gt;w2 = tf.Variable(tf.zeros([1]), name=&quot;weight&quot;)# 输出&lt;tf.Variable &apos;weight_4:0&apos; shape=(1,) dtype=float32_ref&gt;# 上面两种是一样的，都是一维的张量w3 = tf.Variable(0.0, name=&quot;weight&quot;)# 这个是0维的张量# w1[0] = w3]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F04%2F07%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2Ftensorflow%2F%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%8B%9F%E5%90%88%2F</url>
    <content type="text"><![CDATA[https://blog.csdn.net/luoganttcc/article/details/56481549 /Users/david/david/code/deep_learning/tensorflow/Least_Square_sample.ipynb 12345678910111213141516171819202122232425262728293031323334353637383940414243#程序 不是我写的，注释是我做的，转载请注明“lg土木设计” #最小二乘法拟合，用y=ax+b a=weight b=biasesfrom __future__ import print_functionimport tensorflow as tfimport numpy as np # create data 生成100个0-1之间的随机数 #np.random.rand(100) 1*100的矩阵#np.random.rand(3,3) 3*3的矩阵，其每个元素为0-1的随机数x_data = np.random.rand(100).astype(np.float32)y_data = x_data*0.2 + 0.5 ### create tensorflow structure start ###对权进行赋值 在-1到1之间随机数#uniform([1]为1*1的矩阵，即一个数Weights = tf.Variable(tf.random_uniform([1], -1.0, 1.0), name="weight")#偏差为零，zeros([1]为一个1*1的零矩阵，即初始偏差为零biases = tf.Variable(tf.zeros([1]))#权值与x相乘并加偏差y = Weights*x_data + biases #方差，(y-y_data)平方，求和，取均值loss = tf.reduce_mean(tf.square(y-y_data))#定义梯度下降法优化函数，优化，步长为0.5optimizer = tf.train.GradientDescentOptimizer(0.2) train = optimizer.minimize(loss) init = tf.initialize_all_variables()### create tensorflow structure end ### sess = tf.Session()sess.run(init) # Very important for step in range(300): sess.run(train) if step % 20 == 0: print(step, sess.run(Weights), sess.run(biases))]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F04%2F07%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2Ftensorflow%2Ftensorflow%E8%8A%82%E7%82%B9%2F</url>
    <content type="text"><![CDATA[https://www.cnblogs.com/Vulpers/p/7809276.html 一、 constant（常量） constant是TensorFlow的常量节点，通过constant方法创建，其是计算图（Computational Graph）中的起始节点，是传入数据。 创建方式 1cons = tf.constant(value=[1,2],dtype=tf.float32,shape=(1,2),name=&apos;testconst&apos;, verify_shape=False) 参数说明 value：初始值，必填，必须是一个张量（1或[1,2,3]或[[1,2,3],[2,2,3]]或……） dtype：数据类型，选填，默认为value的数据类型，传入参数为tensorflow下的枚举值（float32，float64…….） shape：数据形状，选填，默认为value的shape，设置时不得比value小，可以比value阶数、维度更高，超过部分按value提供最后一个数字填充，示例代码如下 1234567import tensorflow as tfsess = tf.InteractiveSession()cons1 = tf.constant([1, 2, 3], shape=[2, 3])print(sess.run(cons1))# [[1 2 3]# [3 3 3]] name：常量名，选填，默认值不重复，根据创建顺序为（Const，Const_1，Const_2…….） verify_shape:是否验证value的shape和指定shape相符，若设为True则进行验证，不相符时会抛出异常 二、placeholder（占位符） placeholder是TensorFlow的占位符节点，由placeholder方法创建，其也是一种常量，但是由用户在调用run方法时传递的，也可以将placeholder理解为一种形参。即其不像constant那样直接可以使用，需要用户传递常数值。 创建方式 1X = tf.placeholder(dtype=tf.float32, shape=[144, 10], name=&apos;X&apos;) 参数说明 dtype：数据类型，必填，默认为value的数据类型，传入参数为tensorflow下的枚举值（float32，float64…….） shape：数据形状，选填，不填则随传入数据的形状自行变动，可以在多次调用中传入不同形状的数据 name：常量名，选填，默认值不重复，根据创建顺序为（Placeholder，Placeholder_1，Placeholder_2…….） 示例代码 123456789101112131415161718import tensorflow as tfimport numpy.random as random#占位符shape不设时会按传入参数自行匹配node1 = tf.placeholder(tf.float32) # , shape=[4, 5])#也可以写成# node1 = tf.placeholder("float")node2 = tf.placeholder(tf.float32) # , shape=[4, 5])op = tf.multiply(node1, node2)session = tf.Session()const1 = tf.constant(random.rand(4, 5))const2 = tf.constant(random.rand(4, 5))#可以传入初始化后的常量print(session.run(op, &#123;node1: session.run(const1), node2: session.run(const2)&#125;))#也可以直接传入张量，其实同初始化后的常量一致print(session.run(op, &#123;node1: random.rand(2, 3), node2: random.rand(2, 3)&#125;)) 三、Variable（变量） Vatiable是tensorflow的变量节点，通过Variable（注：V大写）方法创建，并且需要传递初始值。在使用前需要通过tensorflow的初始化方法进行初始化。 12345678910W = tf.Variable(initial_value=tf.zeros([9, 5]), # 初始值，必填，张量或可以转换为张量的Python对象。初始值必须有指定一个形状，除非`validate_shape`设置为False。trainable=True, # 如果`True`，则默认值也将变量添加到图形中集合`GraphKeys.TRAINABLE_VARIABLES`。这个集合用作“Optimizer”类使用的默认变量列表collections=None, # 图表集合键的列表。新的变量被添加到这些集合。默认为`[GraphKeys.GLOBAL_VARIABLES]`。validate_shape=True, # 如果`False`，允许变量用初始化未知形状的值。如果“True”，默认的形状`initial_value`必须是已知的。caching_device=None, # 可选设备字符串，描述变量的位置应该被缓存以供阅读。默认为变量的设备。如果不是“None”，则缓存在另一个设备上。典型的用途是缓存在使用变量的Ops所在的设备上进行重复数据删除复制`Switch`和其他条件语句。name='W', # 变量的可选名称。默认为“Variable”并获取自动去重（Variable_1,Variable_2....）。variable_def=None, # `VariableDef`协议缓冲区。如果不是“无”，则重新创建变量对象及其内容，引用变量的节点在图中，必须已经存在。图形没有改变。`variable_def`和其他参数是互斥的。dtype=tf.float32, # 如果设置，initial_value将被转换为给定的类型。如果`None'，数据类型将被保存（如果`initial_value`是一个张量），或者“convert_to_tensor”来决定。expected_shape=None, # 张量的Shape。如果设置，initial_value需要符合这个形状。import_scope=None) # 可选的字符串。名称范围添加到`Variable.`仅在从协议缓冲区初始化时使用。 创建类似于 1W = tf.Variable(tf.zeros([3, 10]), dtype=tf.float64, name=&apos;W&apos;) 例如： 123# 0.0就是初始值w = tf.Variable(0.0, name=&quot;weight&quot;)b = tf.Variable(0.0, name=&quot;bias&quot;)]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F04%2F07%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2Ftensorflow%2F%E4%BC%98%E5%8C%96%E5%99%A8Optimizier%2F</url>
    <content type="text"><![CDATA[https://blog.csdn.net/xierhacker/article/details/53174558 二.常用的optimizer类Ⅰ.class tf.train.Optimizer优化器（optimizers）类的基类。这个类定义了在训练模型的时候添加一个操作的API。你基本上不会直接使用这个类，但是你会用到他的子类比如GradientDescentOptimizer, AdagradOptimizer, MomentumOptimizer.等等这些。 后面讲的时候会详细讲一下GradientDescentOptimizer 这个类的一些函数，然后其他的类只会讲构造函数，因为类中剩下的函数都是大同小异的 Ⅱ.class tf.train.GradientDescentOptimizer这个类是实现梯度下降算法的优化器。(结合理论可以看到，这个构造函数需要的一个学习率就行了) init(learning_rate, use_locking=False,name=’GradientDescent’) 123learning_rate: A Tensor or a floating point value. 要使用的学习率 use_locking: 要是True的话，就对于更新操作（update operations.）使用锁 name: 名字，可选，默认是”GradientDescent”. compute_gradients(loss,var_list=None,gate_gradients=GATE_OP,aggregation_method=None,colocate_gradients_with_ops=False,grad_loss=None) 作用：对于在变量列表（var_list）中的变量计算对于损失函数的梯度,这个函数返回一个（梯度，变量）对的列表，其中梯度就是相对应变量的梯度了。这是minimize()函数的第一个部分，参数：loss: 待减小的值var_list: 默认是在GraphKey.TRAINABLE_VARIABLES.gate_gradients: How to gate the computation of gradients. Can be GATE_NONE, GATE_OP, or GATE_GRAPH.aggregation_method: Specifies the method used to combine gradient terms. Valid values are defined in the class AggregationMethod.colocate_gradients_with_ops: If True, try colocating gradients with the corresponding op. grad_loss: Optional. A Tensor holding the gradient computed for loss. apply_gradients(grads_and_vars,global_step=None,name=None) 作用：把梯度“应用”（Apply）到变量上面去。其实就是按照梯度下降的方式加到上面去。这是minimize（）函数的第二个步骤。 返回一个应用的操作。参数:grads_and_vars: compute_gradients()函数返回的(gradient, variable)对的列表global_step: Optional Variable to increment by one after the variables have been updated.name: 可选，名字 get_name() minimize(loss,global_step=None,var_list=None,gate_gradients=GATE_OP,aggregation_method=None,colocate_gradients_with_ops=False,name=None,grad_loss=None) 作用：非常常用的一个函数通过更新var_list来减小loss，这个函数就是前面compute_gradients() 和apply_gradients().的结合 Ⅲ.class tf.train.AdadeltaOptimizer实现了 Adadelta算法的优化器，可以算是下面的Adagrad算法改进版本 构造函数：tf.train.AdadeltaOptimizer.init(learning_rate=0.001, rho=0.95, epsilon=1e-08, use_locking=False, name=’Adadelta’) 作用：构造一个使用Adadelta算法的优化器参数：learning_rate: tensor或者浮点数，学习率rho: tensor或者浮点数. The decay rate.epsilon: A Tensor or a floating point value. A constant epsilon used to better conditioning the grad update.use_locking: If True use locks for update operations.name: 【可选】这个操作的名字，默认是”Adadelta” IV.class tf.train.AdagradOptimizerOptimizer that implements the Adagrad algorithm. See this paper.tf.train.AdagradOptimizer.init(learning_rate, initial_accumulator_value=0.1, use_locking=False, name=’Adagrad’) Construct a new Adagrad optimizer.Args: learning_rate: A Tensor or a floating point value. The learning rate.initial_accumulator_value: A floating point value. Starting value for the accumulators, must be positive.use_locking: If True use locks for update operations.name: Optional name prefix for the operations created when applying gradients. Defaults to “Adagrad”. Raises: ValueError: If the initial_accumulator_value is invalid. The Optimizer base class provides methods to compute gradients for a loss and apply gradients to variables. A collection of subclasses implement classic optimization algorithms such as GradientDescent and Adagrad. You never instantiate the Optimizer class itself, but instead instantiate one of the subclasses. Ⅴ.class tf.train.MomentumOptimizerOptimizer that implements the Momentum algorithm. tf.train.MomentumOptimizer.init(learning_rate, momentum, use_locking=False, name=’Momentum’, use_nesterov=False) Construct a new Momentum optimizer. Args: learning_rate: A Tensor or a floating point value. The learning rate.momentum: A Tensor or a floating point value. The momentum.use_locking: If True use locks for update operations.name: Optional name prefix for the operations created when applying gradients. Defaults to “Momentum”. Ⅵ.class tf.train.AdamOptimizer实现了Adam算法的优化器构造函数：tf.train.AdamOptimizer.init(learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False, name=’Adam’) Construct a new Adam optimizer. Initialization: m_0 &lt;- 0 (Initialize initial 1st moment vector)v_0 &lt;- 0 (Initialize initial 2nd moment vector)t &lt;- 0 (Initialize timestep)The update rule for variable with gradient g uses an optimization described at the end of section2 of the paper: t &lt;- t + 1lr_t &lt;- learning_rate * sqrt(1 - beta2^t) / (1 - beta1^t) mt &lt;- beta1 * m{t-1} + (1 - beta1) gv_t &lt;- beta2 v_{t-1} + (1 - beta2) g gvariable &lt;- variable - lr_t * m_t / (sqrt(v_t) + epsilon)The default value of 1e-8 for epsilon might not be a good default in general. For example, when training an Inception network on ImageNet a current good choice is 1.0 or 0.1. Note that in dense implement of this algorithm, m_t, v_t and variable will update even if g is zero, but in sparse implement, m_t, v_t and variable will not update in iterations g is zero. Args: learning_rate: A Tensor or a floating point value. The learning rate.beta1: A float value or a constant float tensor. The exponential decay rate for the 1st moment estimates.beta2: A float value or a constant float tensor. The exponential decay rate for the 2nd moment estimates.epsilon: A small constant for numerical stability.use_locking: If True use locks for update operations.name: Optional name for the operations created when applying gradients. Defaults to “Adam”. 三.例子I.线性回归要是有不知道线性回归的理论知识的，请到http://blog.csdn.net/xierhacker/article/details/53257748http://blog.csdn.net/xierhacker/article/details/53261008熟悉的直接跳过。直接上代码: /Users/david/david/code/deep_learning/tensorflow/Optimizer_sample1.ipynb 123456789101112131415161718192021222324252627282930313233343536import tensorflow as tfimport numpy as npimport matplotlib.pyplot as plt# Prepare train datatrain_X = np.linspace(-1, 1, 100)train_Y = 2 * train_X + np.random.randn(*train_X.shape) * 0.33 + 10# Define the modelX = tf.placeholder("float")Y = tf.placeholder("float")w = tf.Variable(0.0, name="weight")b = tf.Variable(0.0, name="bias")loss = tf.square(Y - X*w - b)train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss)# Create session to runwith tf.Session() as sess: sess.run(tf.initialize_all_variables()) epoch = 1 # 让X和Y成对出现 for i in range(10): for (x, y) in zip(train_X, train_Y): _, w_value, b_value = sess.run([train_op, w, b],feed_dict=&#123;X: x,Y: y&#125;) print("Epoch: &#123;&#125;, w: &#123;&#125;, b: &#123;&#125;".format(epoch, w_value, b_value)) epoch += 1#drawplt.plot(train_X,train_Y,"+")plt.plot(train_X,train_X.dot(w_value)+b_value)# 若替换成w = tf.Variable(tf.zeros([1]), name="weight")会报错# ValueError: shapes (100,) and (1,) not aligned: 100 (dim 0) != 1 (dim 0)# 改成plt.plot(train_X,train_X.dot(w_value[0])+b_value)即可plt.show() 结果]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F04%2F07%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2Ftensorflow%2Ftensorflow%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[random_uniform1tf.random_uniform((6, 6), minval=low,maxval=high,dtype=tf.float32))) 返回6*6的矩阵，产生于low和high之间，产生的值是均匀分布的。 zeroshttps://blog.csdn.net/yexudengzhidao/article/details/80981494 12345tf.zeros( shape, dtype=tf.float32, name=None) 1. 一维数组里放一个值1234567import tensorflow as tfres = tf.random_uniform((4, 4), -1, 1)res2 = tf.zeros([1])with tf.Session() as sess: print(sess.run(res2))#结果为：[0.] 2. 一维数组里放两个值1234567import tensorflow as tfres = tf.random_uniform((4, 4), -1, 1)res2 = tf.zeros([2])with tf.Session() as sess: print(sess.run(res2))#结果为：[0. 0.] 3. 二维数组12345678import tensorflow as tfres = tf.random_uniform((4, 4), -1, 1)res2 = tf.zeros([2, 4])with tf.Session() as sess: print(sess.run(res2))#结果为：[[0. 0. 0. 0.] [0. 0. 0. 0.]] reduce_meanhttps://blog.csdn.net/dcrmg/article/details/79797826 计算张量tensor沿着指定的数轴（tensor的某一维度）上的的平均值，主要用作降维或者计算tensor（图像）的平均值。 12345reduce_mean(input_tensor, axis=None, keep_dims=False, name=None, reduction_indices=None) 第一个参数input_tensor： 输入的待降维的tensor;第二个参数axis： 指定的轴，如果不指定，则计算所有元素的均值;第三个参数keep_dims：是否降维度，设置为True，输出的结果保持输入tensor的形状，设置为False，输出结果会降低维度;第四个参数name： 操作的名称; 第五个参数 reduction_indices：在以前版本中用来指定轴，已弃用; matmul矩阵乘法 1234567import tensorflow as tf# create two matrixesmatrix1 = tf.constant([[3,3]])matrix2 = tf.constant([[2], [2]])product = tf.matmul(matrix1,matrix2) assigntf.assign(A, new_number): 这个函数的功能主要是把A的值变为new_number 12345678import tensorflow as tf; A = tf.Variable(tf.constant(0.0), dtype=tf.float32)with tf.Session() as sess: sess.run(tf.initialize_all_variables()) print sess.run(A) sess.run(tf.assign(A, 10)) print sess.run(A)]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F04%2F07%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2Ftensorflow2.0%2F%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[TensorFlow 2.0 preview 的安装方法如下https://blog.csdn.net/aliceyangxi1987/article/details/86478283 12345678910111213# 创建一个名为 python36 的 Python3.6 环境conda create -n python36tensor2 python=3.6# 进入环境source activate python36tensor2# 使用 pip 安装 TensorFlow 2.0 Preview：pip install tf-nightly-2.0-preview# 安装 GPU 版本pip install tf-nightly-gpu-2.0-preview--------------------- 作者：Alice熹爱学习 来源：CSDN 原文：https://blog.csdn.net/aliceyangxi1987/article/details/86478283 版权声明：本文为博主原创文章，转载请附上博文链接！ 3. 在 Github 上有一个 TensorFlow 2.0 的教程 repo：]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F04%2F07%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2Ftensorflow2.0%2F%E6%95%99%E7%A8%8B1%2F</url>
    <content type="text"><![CDATA[知乎专栏https://zhuanlan.zhihu.com/p/59507137 git https://github.com/czy36mengfei/tensorflow2_tutorials_chinese TensorFlow2.0教程-Keras 快速入门]]></content>
  </entry>
  <entry>
    <title><![CDATA[Swagger升级到自定义版本]]></title>
    <url>%2F2019%2F04%2F04%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FJAVA%2FSwagger%E5%8D%87%E7%BA%A7%E5%88%B0%E6%8C%AF%E6%B0%91%E7%89%88%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[1、本地安装jar包 1mvn install:install-file -DgroupId=com.mljr.swagger -DartifactId=swagger-mljr-ui -Dversion=1.9.4 -Dpackaging=jar -Dfile=swagger-mljr-ui-1.9.4.jar 2、修改POM 123456789101112131415&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.mljr.swagger&lt;/groupId&gt; &lt;artifactId&gt;swagger-mljr-ui&lt;/artifactId&gt; &lt;version&gt;1.9.4&lt;/version&gt; &lt;/dependency&gt; 3、实现WebMvcConfigurer 123456789101112131415161718192021@Configuration@ComponentScan(basePackages = &#123; "com.mljr.carthage.controller" &#125;, useDefaultFilters = false, includeFilters = &#123;@ComponentScan.Filter(type = FilterType.ANNOTATION, value = &#123; Controller.class &#125;) &#125;)public class WebMvcConfig implements WebMvcConfigurer &#123; /** * 此方法是用来添加静态资源映射的 * 发现如果继承了WebMvcConfigurationSupport，则在yml中配置的相关内容会失效。 需要重新指定静态资源 */ @Override public void addResourceHandlers(ResourceHandlerRegistry registry) &#123; registry.addResourceHandler("/**").addResourceLocations( "classpath:/static/"); registry.addResourceHandler("swagger-ui.html").addResourceLocations( "classpath:/META-INF/resources/"); registry.addResourceHandler("/webjars/**").addResourceLocations( "classpath:/META-INF/resources/webjars/"); registry.addResourceHandler("doc.html").addResourceLocations( "classpath:/META-INF/resources/"); &#125;&#125; 4、访问用 1http://localhost:5135/doc.html 原生的swagger-ui也可以访问 1http://localhost:5135/swagger-ui.html#/]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>自用代码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git开发上线流程]]></title>
    <url>%2F2019%2F04%2F02%2F%E5%B7%A5%E5%85%B7%E5%92%8C%E7%8E%AF%E5%A2%83%2Fgit%E5%BC%80%E5%8F%91%E4%B8%8A%E7%BA%BF%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[总体流程图 git开发工具SourceTree 定义远程仓库包括主仓库origin，和个人fork的仓库 git分支命名 分支 命名 说明 主分支 master 主分支，所有提供给用户使用的正式版本，都在这个主分支上发布 开发分支 develop 开发分支，永远是功能最新最全的分支 功能分支 feature-* 新功能分支，某个功能点正在开发阶段 修复分支 bugfix-* 修复线上代码的 bug 其中，功能分支的命名规范是： feature-20190402-M029461-3414 20190402是创建feature分支的日期 M029461是开发者的工号 3414是JIRA上需求的编号 修复分支的命名规范是 bugfix-20190402-M029461-3414 leader不定期清理历史创建的feature和bugfix分支。 具体开发流程1、每次迭代开始阶段123graph LRA[leader从master合develop] --&gt; B[开发基于develop创建feature]B --&gt; C[开发推送分支] 1）leader从master分支merge到develop分支 2）根据JIRA上的需求，每个开发者从develop分支创建feature或bugfix分支 3）并分别推送到个人仓库和远程仓库 2、某个需求开发阶段1234567graph TBA[pull develop到本地分支] --&gt; B[公共仓库中, develop到feature的merge]B --&gt; C[提交本地仓库]C --&gt; D[推送个人仓库]D--&gt;E[在gitlab上merge到远程仓库]E--&gt;F[分支测试]F--&gt;G[merge到develop] 1）每次新的commit之前，先拉取develop分支的最新代码，并解决冲突。 2）在公共仓库中，提交develop到feature的merge_request。保证公共仓库的feature分支也是最新的。 3）点击”提交”，提交到本地仓库。 4）点击”推送”，推送到个人仓库的feature分支。 5）在gitlab上，提交merge request，merge到远程仓库的对应分支。 6）在需求开发完成后，若有测试介入，通知测试拉取feature分支进行测试。 7）若测试无误，创建从feature分支到develop分支的merge request 3、某个迭代完成阶段1）测试基于develop分支做测试，也基于develop分支发布测试环境。 2）leader收到测试结果，将develop分支合并到master分支，再做一次回归测试。 3）若测试无误，发布stg环境，观察启动是否有问题，日志记录是否有问题等，若无误，走上线流程，上生产环境。]]></content>
      <categories>
        <category>工具和环境</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F03%2F23%2Fconfigs%2Fsorts%2F</url>
    <content type="text"><![CDATA[{"":["elastic search","myblogs","nlp","hadoop-spark","pic","地理信息挖掘","工具和环境","总结与思考","数学","机器学习","深度学习","特征工程","算法与数据结构","编程语言学习","计算广告","轨迹专著","错题集","风控"],"__positions":{"nlp":3,"算法与数据结构/布隆过滤器":13003,"pic":5,"深度学习":11,"算法与数据结构/图算法/图算法领域10大经典算法.md":13002003,"特征工程":12,"hadoop-spark":4,"算法与数据结构/异常检测":13004,"算法与数据结构/图算法/数据结构-图讲解":13002004,"计算广告":15,"总结与思考":8,"数学":9,"错题集":17,"算法与数据结构":13,"算法与数据结构/leecode":13001,"算法与数据结构/图算法/bfs和dfs.md":13002002,"算法与数据结构/算法导论":13005,"风控":18,"地理信息挖掘":6,"轨迹专著":16,"编程语言学习":14,"工具和环境":7,"算法与数据结构/图算法":13002,"机器学习":10,"elastic search":1,"算法与数据结构/图算法/数据结构-图讲解.md":13002001,"myblogs":2},"算法与数据结构":["leecode","图算法","布隆过滤器","异常检测","算法导论"],"__raw_positions":{"nlp":3,"算法与数据结构/布隆过滤器":3,"pic":5,"深度学习":11,"算法与数据结构/图算法/图算法领域10大经典算法.md":3,"特征工程":12,"hadoop-spark":4,"算法与数据结构/异常检测":4,"编程语言学习":14,"计算广告":15,"总结与思考":8,"数学":9,"错题集":17,"算法与数据结构/算法导论":5,"算法与数据结构/leecode":1,"算法与数据结构/图算法/bfs和dfs.md":2,"算法与数据结构":13,"风控":18,"地理信息挖掘":6,"轨迹专著":16,"算法与数据结构/图算法/数据结构-图讲解":4,"工具和环境":7,"算法与数据结构/图算法":2,"机器学习":10,"elastic search":1,"算法与数据结构/图算法/数据结构-图讲解.md":1,"myblogs":2},"算法与数据结构/图算法":["数据结构-图讲解.md","bfs和dfs.md","图算法领域10大经典算法.md","数据结构-图讲解"]}]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F03%2F23%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E5%9B%BE%E7%AE%97%E6%B3%95%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%9B%BE%E8%AE%B2%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[数据结构-图基本理论[TOC] 1、概念1.1 图图（Graph）是由顶点的集合和顶点之间边的集合组成，通常表示为： G（V，E）其中，G表示一个图，V是图G中顶点的集合，E是图G中边的集合。在图中的数据元素，我们称之为顶点（Vertex），顶点集合有穷非空。在图中，任意两个顶点之间都可能有关系，顶点之间的逻辑关系用边来表示，边集可以是空的。 1.2 有向图、无向图图按照边的有无方向分为无向图和有向图。无向图由顶点和边组成，有向图由顶点和弧构成。弧有弧尾和弧头之分，带箭头一端为弧头。 若边有权重，称为带权图。 图按照边或弧的多少分稀疏图和稠密图。如果图中的任意两个顶点之间都存在边叫做完全图，有向的叫有向完全图。若无重复的边或顶点到自身的边则叫简单图。 权（Weight）：与图的边或弧相关的数。 网（Network）：带权的图。 路径的长度：一条路径上边或弧的数量。 连通图：图中任意两个顶点都是连通的。 1.3 入度和出度图中顶点之间有邻接点、依附的概念。无向图顶点的边数叫做度。有向图顶点分为入度和出度。 入度就是有多少条边指向这个点，出度就是从这个点出发有多少条边。 2、图的存储2.1 邻接矩阵图的邻接矩阵的表示方式需要两个数组来表示图的信息，一个一维数组表示每个数据元素的信息，一个二维数组（邻接矩阵）表示图中的边或者弧的信息。 如果图有n个顶点，那么邻接矩阵就是一个n*n的方阵，方阵中每个元素的值的计算公式如下： 邻接矩阵表示图的具体示例如下图所示： 首先给个无向图的实例： 下面是一个有向图的实例： 我们可以从这两个邻接矩阵得出一些结论： 无向图的邻接矩阵都是沿对角线对称的 要知道无向图中某个顶点的度，其实就是这个顶点vi在邻接矩阵中第i行或（第i列）的元素之和； 对于有向图，要知道某个顶点的出度，其实就是这个顶点vi在邻接矩阵中第i行的元素之和，如果要知道某个顶点的入度，那就是第i列的元素之和。 需要注意的是，当边上有权值的时候，称之为网图，则邻接矩阵中的元素不再仅是0和1了，邻接矩阵M中的元素定义为 无穷大表示一个计算机允许的、大于所有边上权值的值，也就是一个不可能的极限值。 2.2 邻接表对于顶点数很多但是边数很少的图来说，用邻接矩阵显得略为“奢侈”，因为矩阵元素为1的很少，即其中的有用信息很少，但却占了很大的空间。 邻接表是图的一种链式存储结构。主要是应对于邻接矩阵在顶点多边少的时候，浪费空间的问题。它的方法就是声明两个结构。如下图所示： 1234567891011121314typedef char Vertextype;//表结点结构struct ArcNode &#123; int adjvex; //某条边指向的那个顶点的位置（一般是数组的下标）。 ArcNode * nextarc; //指向下一个表结点 int weight; //这个只有网图才需要使用。普通的图可以直接忽略&#125;;//头结点struct Vnode&#123; Vertextype data; //这个是记录每个顶点的信息（现在一般都不需要怎么使用） ArcNode * firstarc; //指向第一条依附在该顶点边的信息（表结点）&#125;; 无向图的示例 下面再给出一个有向图的例子： 优点：对于，稀疏图，邻接表比邻接矩阵更节约空间。 缺点：不容易判断两个顶点是有关系（边），顶点的出度容易，但是求入度需要遍历整个邻接表。改进：十字链表 3、图的遍历3.1 广度优先遍历BFS广度优先搜索在进一步遍历图中顶点之前，先访问当前顶点的所有邻接结点。 https://visualgo.net/zh/dfsbfs 1234567891011121314151617181920# 图的广度优先遍历# 1.利用队列实现# 2.从源节点开始依次按照宽度进队列，然后弹出# 3.每弹出一个节点，就把该节点所有没有进过队列的邻接点放入队列# 4.直到队列变空from queue import Queuedef bfs(node): if node is None: return queue = Queue() nodeSet = set() queue.put(node) nodeSet.add(node) while not queue.empty(): cur = queue.get() # 弹出元素 print(cur.value) # 打印元素值 for next in cur.nexts: # 遍历元素的邻接节点 if next not in nodeSet: # 若邻接节点没有入过队，加入队列并登记 nodeSet.add(next) queue.put(next) 3.2 深度优先遍历DFS 主要思想：类似于二叉树的先序遍历 深度优先遍历（Depth-First-Search，简称DFS）的主要思想就是：首先以一个未被访问过的顶点作为起始顶点，沿当前顶点的边走到未访问过的顶点；当没有未访问过的顶点时，则回到上一个顶点，继续试探访问别的顶点，直到所有的顶点都被访问。 沿着某条路径遍历直到末端，然后回溯，再沿着另一条进行同样的遍历，直到所有的顶点都被访问过为止。 递归解法DFS在搜索过程中访问某个顶点后，需要递归地访问此顶点的所有未访问过的相邻顶点。 https://visualgo.net/zh/dfsbfs 12345DFS(u)for each neighbor v of u if v is unvisited, DFS(v) else if v is visited, forward/cross edge 12345678910111213141516171819#递归DFS def depth_first_search(self,root=None): order=[] def dfs(node): self.visited[node] = True order.append(node) for n in self.node_neighbors[node]: if not n in self.visited: dfs(n) if root: dfs(root) #对于不连通的结点（即dfs（root）完仍是没有visit过的单独处理，再做一次dfs for node in self.nodes(): if not node in self.visited: dfs(node) self.visited = &#123;&#125; print order 非递归解法 图的邻接矩阵存储方式 使用一个一维数组存储所有的顶点，对应的下标的元素为1（代表已经被访问），0（代表没有被访问） 先访问 v1，0进栈，0处置为1 继续访问 v2，1进栈，1处置为1 继续访问v4（依据邻接矩阵），3入栈，3处置为1 继续访问 v8，7入栈，7处置为1 继续访问 v5，4入栈，4处置为1 继续访问，发现没有还没访问的结点了，那么好，退栈（也就是回退）开始，回退到 v1处，也就是0的时候，发现了没有被访问的结点，那么继续访问之。 继续访问 v3，2进栈，2处置为1，继续访问v6，5进栈，5处置为1，继续访问v7，6进栈，6处置为1 12345678910111213141516171819202122# 图的深度优先遍历# 1.利用栈实现# 2.从源节点开始把节点按照深度放入栈，然后弹出# 3.每弹出一个点，把该节点下一个没有进过栈的邻接点放入栈# 4.直到栈变空def dfs(node): if node is None: return nodeSet = set() stack = [] print(node.value) nodeSet.add(node) stack.append(node) while len(stack) &gt; 0: cur = stack.pop() # 弹出最近入栈的节点 for next in cur.nexts: # 遍历该节点的邻接节点 if next not in nodeSet: # 如果邻接节点不重复 stack.append(cur) # 把节点压入 stack.append(next) # 把邻接节点压入 set.add(next) # 登记节点 print(next.value) # 打印节点值 break # 退出，保持深度优先 参考数据结构在线演示 数据结构（七）图 数据结构—-图的详细介绍 数据结构之图（一）图的存储结构 数据结构-图-知识点总结 Python实现图的DFS（递归和非递归）和BFS]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F03%2F23%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2FLeeCode%2FWeek3-1%2F</url>
    <content type="text"><![CDATA[有向无环图（Directed Acyclic Graph, DAG） 如果一个有向图的任意顶点都无法通过一些有向边回到自身，那么称这个有向图为有向无环图(DAG)。 常常被用来表示事件之间的驱动依赖关系，管理任务之间的调度。 拓扑排序 拓扑排序是将有向无环图G的所有顶点排成一个线性序列，使得对图G中的任意两个顶点u、v， 如果存在边u-&gt;v，那么在序列中u一定在v前面。这个序列又被称为拓扑序列。 也可理解为对某点v而言，只有当v的所有源点均出现了，v才能出现。 下图给出的顶点排序不是拓扑排序，因为顶点D的邻接点E比其先出现： 其实这道题的本质即给定一个图，让我们判断该图是否是拓扑有序。 这里我们需要用到一个图里入度的概念，在初始的图中，入度为0的点，即是课程中最基础的课程（需要先修，比如数据结构、C语言基础），在找到图中所有入度为0的点以后，将它们依次放入一个队列中，每次循环从队列头提取一个点，然后将这个点放入图中查询，查出哪些点被这个点所指向，并依次将这些点的入度减1，直观上的看的话，即是一个删除一个入度为0的点的操作，每次减1时，检测其他节点的入度，若出现新的入度为0的点，将其加入队列，循环往复，直到队列为空为止。 1、找入度为0的点。 参考 1、【图论】有向无环图的拓扑排序]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F03%2F21%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2FPyOD%2F</url>
    <content type="text"><![CDATA[https://zhuanlan.zhihu.com/p/58313521]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F03%2F18%2FElastic%20Search%2F%E5%9C%B0%E7%90%86%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[https://blog.csdn.net/ctwy291314/article/details/82686954]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F03%2F17%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E5%9B%BE%E7%AE%97%E6%B3%95%2FBFS%E5%92%8CDFS%2F</url>
    <content type="text"><![CDATA[广度优先BFS1234567891011121314151617181920# 图的广度优先遍历# 1.利用队列实现# 2.从源节点开始依次按照宽度进队列，然后弹出# 3.每弹出一个节点，就把该节点所有没有进过队列的邻接点放入队列# 4.直到队列变空from queue import Queuedef bfs(node): if node is None: return queue = Queue() nodeSet = set() queue.put(node) nodeSet.add(node) while not queue.empty(): cur = queue.get() # 弹出元素 print(cur.value) # 打印元素值 for next in cur.nexts: # 遍历元素的邻接节点 if next not in nodeSet: # 若邻接节点没有入过队，加入队列并登记 nodeSet.add(next) queue.put(next) 深度优先DFS12345678910111213141516171819202122# 图的深度优先遍历# 1.利用栈实现# 2.从源节点开始把节点按照深度放入栈，然后弹出# 3.每弹出一个点，把该节点下一个没有进过栈的邻接点放入栈# 4.直到栈变空def dfs(node): if node is None: return nodeSet = set() stack = [] print(node.value) nodeSet.add(node) stack.append(node) while len(stack) &gt; 0: cur = stack.pop() # 弹出最近入栈的节点 for next in cur.nexts: # 遍历该节点的邻接节点 if next not in nodeSet: # 如果邻接节点不重复 stack.append(cur) # 把节点压入 stack.append(next) # 把邻接节点压入 set.add(next) # 登记节点 print(next.value) # 打印节点值 break # 退出，保持深度优先]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F03%2F17%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E5%9B%BE%E7%AE%97%E6%B3%95%2F%E5%9B%BE%E7%AE%97%E6%B3%95%E9%A2%86%E5%9F%9F10%E5%A4%A7%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[第一章、基本遍历 一、深度优先搜索二、广度优先搜索 此图遍历中最基本的俩种算法，BFS，DFS，入选本图算法十大算法，自是无可争议。因为，这俩种搜索算法，应用实为广泛而重要。 关于此BFS、DFS算法，更多，请参考：经典算法研究系列：四、教你通透彻底理解：BFS和DFS优先搜索算法http://blog.csdn.net/v_JULY_v/archive/2011/01/01/6111353.aspx 三、A*搜索算法DFS和BFS在展开子结点时均属于盲目型搜索，也就是说，它不会选择哪个结点在下一次搜索中更优而去跳转到该结点进行下一步的搜索。在运气不好的情形中，均需要试探完整个解集空间, 显然，只能适用于问题规模不大的搜索问题中。 A算法，作为启发式算法中很重要的一种，被广泛应用在最优路径求解和一些策略设计的问题中。而A算法最为核心的部分，就在于它的一个估值函数的设计上： f(n)=g(n)+h(n)其中f(n)是每个可能试探点的估值，它有两部分组成：一部分，为g(n)，它表示从起始搜索点到当前点的代价（通常用某结点在搜索树中的深度来表示）。一部分，即h(n)，它表示启发式搜索中最为重要的一部分，即当前结点到目标结点的估值。 更多，请参考：经典算法研究系列：一、A*搜索算法http://blog.csdn.net/v_JULY_v/archive/2010/12/23/6093380.aspx 附：Flood FillLeeMaRS、wtzyb4446：图形学中Flood Fill是满水法填充，是用来填充区域的。就好比在一个地方一直到水，水会往四周满延开，直到高地阻挡。 Flood Fill就是从一个点开始往四周寻找相同的点填充，直到有不同的点为止。我们用的Flood Fill和这个差不多原理,就是BFS的一种形式. 假设在(i,j)滴好大一滴红墨水,然后水开始漫开,向它的上下左右染色,也就是(i-1,j),(i+1,j),(i,j-1),(i,j+1)这四个点.然后在分别再从这四个点开始向周围染色…直到碰到某种边界为止. 把这个转化为BFS的思想,就是队列中初始元素是(i,j),然后把(i,j)扩展状态,得到(i-1,j),(i+1,j),(i,j-1),(i,j+1)这四个状态,加入队列.把(i,j)出列,继续扩展下一个结点…如此 第二章、最短路径算法四、DijkstraDijkstra 算法，又叫迪科斯彻算法（Dijkstra），算法解决的是有向图中单个源点到其他顶点的最短路径问题。 此Dijkstra 算法已在本BLOG内俩篇文章中，有所具体阐述，请参见：I、经典算法研究系列：二、Dijkstra 算法初探http://blog.csdn.net/v_JULY_v/archive/2010/12/24/6096981.aspxII、经典算法研究系列：二之续、彻底理解Dijkstra算法http://blog.csdn.net/v_JULY_v/archive/2011/02/13/6182419.aspx 五、Bellman-FordBellman-Ford：求单源最短路，可以判断有无负权回路（若有，则不存在最短路），时效性较好，时间复杂度O（VE）。 附：SPFA：是Bellman-Ford的队列优化，时效性相对好，时间复杂度O（kE）。（k&lt;&lt;V）。 六、Floyd-WarshallFloyd-Warshall：求多源、无负权边的最短路。用矩阵记录图。时效性较差，时间复杂度O(V^3)。此算法是解决任意两点间的最短路径的一种算法，可以正确处理有向图或负权的最短路径问题。 更多，请参考：几个最短路径算法比较：http://blog.csdn.net/v_JULY_v/archive/2011/02/12/6181485.aspx 附：Kneser图Kneser图是与图的分数染色有关的算法。给定正整数a，b，a≥2b，Kneser图Ka:b是以如下方式定义的一个图：其顶点是从给定的a个元素的集合中选出的b个元素构成的子集，两顶点间有边当且仅当这两个顶点为不交的集合。 第三章、最小生成树 七、Prim八、Kruskal 此最小(权值)生成树的俩种算法，日后，会在本BLOG内 具体而深入阐述。 第四章、图匹配九、匈牙利算法 匈牙利算法是众多用于解决线性任务分配问题的算法之一，是用来解决二分图最大匹配问题的经典算法，可以在多项式时间内解决问题，由匈牙利数学家Jack Edmonds于1965年提出。这个算法，比较生疏，下面，稍微阐述下： I、匈牙利算法应用问题的描述：设G=(V,E)是一个无向图。如顶点集V可分区为两个互不相交的子集V1,V2之并，并且图中每条边依附的两个顶点都分属于这两个不同的子集。则称图G为二分图。二分图也可记为G=(V1,V2,E)。 给定一个二分图G，在G的一个子图M中，M的边集{E}中的任意两条边都不依附于同一个顶点，则称M是一个匹配。 选择这样的子集中边数最大的子集称为图的最大匹配问题(maximal matching problem) 如果一个匹配中，图中的每个顶点都和图中某条边相关联，则称此匹配为完全匹配，也称作完备，完美匹配。 II、算法描述求最大匹配的一种显而易见的算法是：先找出全部匹配，然后保留匹配数最多的。但是这个算法的时间复杂度为边数的指数级函数。因此，需要寻求一种更加高效的算法。 下面介绍用增广路求最大匹配的方法(称作匈牙利算法，匈牙利数学家Edmonds于1965年提出)。增广路的定义(也称增广轨或交错轨)：若P是图G中一条连通两个未匹配顶点的路径，并且属于M的边和不属于M的边(即已匹配和待匹配的边)在P上交替出现，则称P为相对于M的一条增广路径。 由增广路的定义可以推出下述三个结论：1－P的路径长度必定为奇数，第一条边和最后一条边都不属于M。2－将M和P进行异或操作(去同存异)可以得到一个更大的匹配M’。3－M为G的最大匹配当且仅当不存在M的增广路径。 算法轮廓：(1)置M为空(2)找出一条增广路径P，通过异或操作获得更大的匹配M’代替M(3)重复(2)操作直到找不出增广路径为止 III、时间复杂度与空间复杂度时间复杂度邻接矩阵：最坏为O(n^3) 邻接表：O(mn) 空间复杂度：邻接矩阵：O(n^2) 邻接表：O(m+n) 附：Edmonds’s matching 第五章、强连通分支算法与网络流十、Ford-Fulkerson最大流量算法(Ford-Fulkerson Algorithm)，也叫做贝尔曼-福特算法，被用于作为一个距离向量路由协议例如RIP, BGP, ISO IDRP, NOVELL IPX的算法。 附：Edmonds-Karp、Dinic、Push-relabel、maximum flow 强连通分支算法：Kosaraju、 Gabow、 Tarjan。此类算法，日后阐述。完。]]></content>
  </entry>
  <entry>
    <title><![CDATA[【转】Graphviz使用方法]]></title>
    <url>%2F2019%2F03%2F13%2F%E5%B7%A5%E5%85%B7%E5%92%8C%E7%8E%AF%E5%A2%83%2FGraphviz%2F</url>
    <content type="text"><![CDATA[http://www.graphviz.org/download/ 1brew install graphviz 教程 https://program-think.blogspot.com/2016/02/opensource-review-graphviz.html https://www.jianshu.com/p/6d9bbbbf38b1 12//定义a节点为长方形, 节点显示的文本为&quot;Hello world&quot;样式为填充, 填充颜色为#ABACBAa[shape=box,label=&quot;Hello world&quot;,style=filled,fillcolor=&quot;#ABACBA&quot;];]]></content>
      <categories>
        <category>工具和环境</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F03%2F09%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E9%80%92%E5%BD%92%E8%AE%B2%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[递归讲解[TOC] 递归概念 递归就是方法里调用自身。 在使用递增归策略时，必须有一个明确的递归结束条件，称为递归出口。 递归一般用于解决三类问题： (1)数据的定义是按递归定义的。（Fibonacci函数，n的阶乘） (2)问题解法按递归实现。（回溯） (3)数据的结构形式是按递归定义的。（二叉树的遍历，图的搜索） 123456int FibonacciRecursive(int n)&#123; if( n &lt; 2) return n; return (FibonacciRecursive(n-1)+FibonacciRecursive(n-2));&#125; 尾递归顾名思义，尾递归就是从最后开始计算, 每递归一次就算出相应的结果, 也就是说, 函数调用出现在调用者函数的尾部, 因为是尾部, 所以根本没有必要去保存任何局部变量. 直接让被调用的函数返回时越过调用者, 返回到调用者的调用者去。 尾递归是极其重要的，不用尾递归，函数的堆栈耗用难以估量，需要保存很多中间函数的堆栈。比如f(n, sum) = f(n-1) + value(n) + sum; 会保存n个函数调用堆栈，而使用尾递归f(n, sum) = f(n-1, sum+value(n)); 这样则只保留后一个函数堆栈即可，之前的可优化删去。 123456int FibonacciTailRecursive(int n,int ret1,int ret2)&#123; if(n==0) return ret1; return FibonacciTailRecursive(n-1,ret2,ret1+ret2);&#125; 可以看出，尾递归就是把当前的运算结果（或路径）放在参数里传给下层函数，深层函数所面对的不是越来越简单的问题，而是越来越复杂的问题，因为参数里带有前面若干步的运算路径。]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F03%2F06%2F%E6%80%BB%E7%BB%93%E4%B8%8E%E6%80%9D%E8%80%83%2F%E9%A1%B9%E7%9B%AE%E5%A4%8D%E7%9B%98%E7%A4%BA%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[故障过程: ​ 2月28日晚完成上线 ​ 3月1日凌晨宋楠发现存储到数据表的快照协议号出现重复,并且存在一定的规律性.表现为前100000数据无重复,从100001开始到102000持续出现重复,且重复值范围就在100001到102000之间.定位具体原因可能为1.数据库层面查询重复 2.分页组件bug ​ 3月1日上午与倩倩沟通影响范围.与资付产品沟通后认为范围影响不大,确定可以接受后几日补偿推送数据或当日重新推送 ​ 3月1日晚上复现问题并解决.紧急上线问题修复.补偿推送当日数据 ​ 3月4,5日与DBA占杰共同确定问题发生具体原因 ​ 问题影响: ​ 推送数据重复导致部分有效数据未推送到资付,资付的部分资金方无法获取最新的还款计划数据 ​ 判断依据: ​ 数据可后续补偿推送 ​ 资金方对贷后还款计划关注度不高 ​ 本次上线需求: ​ 优化贷后还款计划推送模块,提高效率 ​ 问题原因: ​ mysql使用distinct+limit时,mysql不会扫描所有记录,而是找到满足条件后立即停止,然后将结果返回.因此需要强制显示指定order by的顺序.告诉mysql根据某子段生成排序后生成临时表，返回满足偏移量结果集 ​ 改进计划: ​ 使用distinct + limit的组合需增加强制排序 ​ 如有有相对复杂且不常用的sql上线前需优先咨询DBA,并按照DBA的建议进行优化 ​ 遇到数据量大的操作评估是否需要从线上拉去部分脱敏数据到测试库中进行充分测试 ​ 附出现问题的sql ​ 有问题: ​ select DISTINCT p.agreement_no ​ from repayment_plan p ​ where p.modified_time &gt;= ‘开始时间’ ​ and p.repayment_method != 4 ​ and p.agreement_type != 2 ​ 修复后: ​ select DISTINCT p.agreement_no ​ from repayment_plan p ​ where p.modified_time &gt;= ‘开始时间’ ​ and p.repayment_method != 4 ​ and p.agreement_type != 2 ​ ​ order by p.agreement_no asc]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F03%2F05%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2Ftensorflow%2FTransformer%E5%86%99%E6%98%A5%E8%81%94%2F</url>
    <content type="text"><![CDATA[https://mp.weixin.qq.com/s/N6sSQ3EG_NPFaOGwkC0i6w 数据集数据集来自一位名为冯重朴_梨味斋散叶的博主的新浪博客，总共包含超过70万副对联。 github地址（脚本已无法使用，可以用我们上传的couplets，google drive需科学翻墙）： https://github.com/wb14123/couplet-dataset google drive： https://drive.google.com/file/d/13cJWWp_ST2Xqt76pEr5ZWa6LVJwGYC4-/view?usp=sharing 我下载后，放在/Users/david/david/code/deep_learning/duilian/couplet.tar.gz 整个数据集解压过后总共56.9兆，其中训练数据集56.4兆，测试数据集400多KB。另外，数据是以TXT格式储存的。全文不含标点15,153,840字，如果每天看100条的话，20多年都看不完。而文摘菌采用Tesla K80，2个小时就能来一个Epoch。 数据预处理首先进行数据预处理，因为拿到的数据比较干净，所以这任务主要就是创建词表。值得一提的是，数据集的压缩文件中存在着一个创建好的词表。你也可以用数据集给出的词表。因为对联任务并不难，所以我们直接使用字粒度来作为输入，也就是像“我们”这样的词，我们是把其当做两个单元进行输入，而不是一个。 关键代码如下： 123456789101112131415161718192021def make_vocab(fpaths, out): '''Constructs vocabulary. Args: fpaths: A string. Input file path. out: A string. Output file name. ''' char2cnt = Counter() for path in fpaths: for line in open(path, 'r'): line = line.strip() if not line: # detect the empty line continue chars = line.split() char2cnt.update(chars) with open(out, 'w') as fout: fout.write("&#123;&#125;\t1000000000\n&#123;&#125;\t1000000000\n&#123;&#125;\t1000000000\n&#123;&#125;\t1000000000\n".format("&lt;PAD&gt;", "&lt;UNK&gt;", "&lt;S&gt;", "&lt;/S&gt;")) # special tokens for word, cnt in char2cnt.most_common(len(char2cnt)): fout.write(u"&#123;&#125;\t&#123;&#125;\n".format(word, cnt)) print("%d chars written!" % (len(char2cnt))) 建立完词表之后，所有单个汉字加上一些特殊字符，总共9126个字符。其实我们看看频率为1的字符就会发现，好像都不认识。一般来说，在序列到序列任务里面，我们会在词表里面忽略频率小不常用的字词。但是因为这里我们词表并不大，所以就全部采用了。 之后，我们对训练数据的字长进行统计，来决定训练时句子的最大长度。统计后发现，对联数据中单句的长度大部分集中在5-10字之间，最长大概30字左右，所以训练的最大长度文摘菌选的也是30。这个长度并不长，所以之后整个 Transformer 的大小都不是很大。 模型编写与训练这里我们首先搭建好模型，之后通过数据将模型中的参数训练成我们所希望的，之后存储起来。这里便是一个epoch存储一个模型。而测试的时候，就可以直接读入参数获得一个训练好的模型来使用。 此外，我们也可以将保存的模型分享给别人，让其他人也可以直接利用我们的训练结果。 模型和训练的代码太多，这里就全不放出来了，感兴趣的读者可以去github上下载完整代码。 github地址： https://github.com/andy-yangz/coupletsseq2seqtransformer 于是整个模型的训练过程主要分为四个步骤： 第一步，先下载对联数据 couplets, 然后解压到 data 里去 第二步，调整超参数 第三步，运行 prepro.py预处理数据，产生词表，也可以用提供的词表 第四步，运行 train.py 训练模型 评估训练完之后，保存模型，评估脚本会自动读入最新模型来进行评估。 评估的主要步骤包括： 建立模型 读入训练好的参数 读入测试数据 将数据喂入模型 编码解码获得结果 最主要代码应该是解码这一块： 12345### Autoregressive inferencepreds = np.zeros((1, hp.maxlen), np.int32)for j in range(hp.maxlen): _preds = sess.run(g.preds, &#123;g.x: x, g.y: preds&#125;) preds[:, j] = _preds[:, j] 最开始喂入x，还有一个空的preds，然后获得一个预测的字，将这个字放在preds相应的位置，之后循环，不断将preds填满。最后就能获得完整的预测结果。 这里每次预测的字，取得都是预测概率最大的那个字，这叫做 greedy decoding（贪婪搜寻解码），因为每次都是选最大的。这样虽然快，但是也会出现陷入局部最优解的情况。更好些解码方法，可以使用Beam search（束搜索）。 这是我们训练了6个epoch，也就是模型将数据看了六遍的结果。这次我们总共保存了7个模型，从Epoch1到Epoch7模型，其实要说那个好只能是见仁见智了，比如说我就觉得6个Epoch会好些。但是可以确认的是随着Epoch增加模型就越能学到对联数据里的模式。 其他模型关于AI写对联，其实之前已经有了基于深度学习seq2seq的模型，同样其用到的也是TensorFlow和那位冯重朴_梨味斋散叶的博主提供的数据集。 目前，代码已经开源，你可以在下面的github中找到： https://github.com/wb14123/seq2seq-couplet]]></content>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot用法记录]]></title>
    <url>%2F2019%2F03%2F01%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FJAVA%2FSpring%20Boot%2F</url>
    <content type="text"><![CDATA[启动时用DEBUG模式1/Users/david/david/gitlab_mljr/carthage-greenwich/carthage-web/src/main/resources/logback.groovy 1root(DEBUG, [&quot;APP_INFO&quot;, &quot;APP_ERROR&quot;, &quot;CONSOLE&quot;]) 项目打包123456789101112131415&lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;!--这里写上main方法所在类的路径--&gt; &lt;configuration&gt; &lt;mainClass&gt;com.mljr.carthage.web.SpringBootFlyingApsarasMain&lt;/mainClass&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;repackage&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; Spring Boot 推荐目录结构（1）代码层的结构 根目录：com.springboot 1.工程启动类(ApplicationServer.java)置于com.springboot.build包下 2.实体类(domain)置于com.springboot.domain 3.数据访问层(Dao)置于com.springboot.repository 4.数据服务层(Service)置于com,springboot.service,数据服务的实现接口(serviceImpl)至于com.springboot.service.impl 5.前端控制器(Controller)置于com.springboot.controller 6.工具类(utils)置于com.springboot.utils 7.常量接口类(constant)置于com.springboot.constant 8.配置信息类(config)置于com.springboot.config 9.数据传输类(vo)置于com.springboot.vo （2）资源文件的结构 根目录:src/main/resources 1.配置文件(.properties/.json等)置于config文件夹下 2.国际化(i18n))置于i18n文件夹下 3.spring.xml置于META-INF/spring文件夹下 4.页面以及js/css/image等置于static文件夹下的各自文件下 实体类，也可以包括调用第三方接口后，解析返回json的类 queryForList1234567List&lt;Map&lt;String, Object&gt;&gt; rows = jdbcTemplate.queryForList(query); for(Map&lt;String, Object&gt; row : rows)&#123; String id = row.get(&quot;id&quot;).toString(); String name = (String)row.get(&quot;name&quot;); String salary = row.get(&quot;salary&quot;).toString(); System.out.println(id + &quot; &quot; + name + &quot; &quot; + salary ); &#125; 缓存https://blog.csdn.net/qq_38974634/article/details/80650810 现在用redis缓存 mybatisparameterTypehttps://www.jianshu.com/p/db20f3224038 错题集如果不加BeanPropertyRowMapper会报错org.springframework.jdbc.IncorrectResultSetColumnCountException: Incorrect column count: expected 1, actual 4 12345// 这种不行jdbcTemplate.queryForObject(sql, TopMetricInfoVO.class) // 这种可以 不能自动注入12The bean &apos;dashBoardServiceImpl&apos; could not be injected as a &apos;com.mljr.carthage.service.Impl.DashBoardServiceImpl&apos; because it is a JDK dynamic proxy that implements: com.mljr.carthage.service.DashboardService mybatis1不允许有匹配 &quot;[xX][mM][lL]&quot; 的处理指令目标。 在mapper的xml中不能有代码注释 123456org.apache.ibatis.exceptions.PersistenceException: ### Error querying database. Cause: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: Unable to load authentication plugin &apos;caching_sha2_password&apos;.### The error may exist in mapper/mybatis/mysql/carthage/gps/AnalysisMapper.xml### The error may involve com.mljr.carthage.dao.mapper.gps.AnalysisMapper.selectDistributionCount### The error occurred while executing a query### Cause: com.zaxxer.hikari.pool.HikariPool$PoolInitializationException: Failed to initialize pool: Unable to load authentication plugin &apos;caching_sha2_password&apos;. 本地没设密码 1ALTER USER &apos;root&apos;@&apos;localhost&apos; IDENTIFIED WITH mysql_native_password BY &apos;123456&apos;; 1Unknown system variable &apos;query_cache_size&apos; 服务器版本低，本地版本是8.0.15 jdbcTemplate返回string的list1List&lt;String&gt; defaultAnomalies = jdbcTemplate.queryForList(sql, String.class); Swagger技巧配置了@ApiModelProperty的allowableValues属性但不显示https://blog.csdn.net/dyc87112/article/details/83241038 我们只需要通过,分割来定义可选值，或者用range函数定义范围等方式就能正确显示了，比如： public class Filter { 1234@ApiModelProperty(allowableValues = &quot;range[1,5]&quot;)Integer order@ApiModelProperty(allowableValues = &quot;111, 222&quot;)String code; swagger命名风格变量用@ApiModelProperty]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F02%2F24%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2FLeeCode%2F%E5%A5%87%E5%81%B6%E8%B7%B3%2F</url>
    <content type="text"><![CDATA[第奇数次跳，可以跳到后面的所有比当前数字大的，其中最小的那个数字的位置上； 第偶数次跳，可以跳到后面的所有比当前数字小的，其中最大的那个数字的位置上； 如果跳到最后的一个位置，就相当于成功了。问有多少个位置可以成功。 暴力解法： 12345678for (int i = 0; i &lt; A.length; i++) while ( j != A.length - 1) if ( jump % 2 == 1) &#123; for (int k = j + 1; k &lt; A.length; k++) &#123; if (A[j] &lt;= A[k] &amp;&amp; A[k] &lt; tmp) else &#123; &#125; 从后往前遍历，如果某个点能够跳到的点已经被证实可以跳到终点，那么这个点肯定能跳到终点。 但是，某个点能不能跳到终点要分情况看的，要看这个点在奇数跳次数和偶数跳次数是不是分别能达到终点。 比如，从A跳到B，当【从A出发且第一步是奇数步，最后能到达终点】，当且仅当【从B出发，且B的第一步是偶数步能到达终点】。 所以就要知道每个点出发的时候，奇数步或偶数步能不能到达终点。 再假设odd_jump[i]表示【从i点进行奇数跳时下一步的位置】，even_jump[i]表示【从i点进行偶数跳时下一步的位置】，若无法跳跃，取值为-1。这个数组的计算先放一边，假设已经算好了。 再领odd_position[i]表示【从i点开始奇数跳能不能到达终点】，even_position[i]表示【从i点开始偶数跳能不能到达终点】。 123456789101112131415161718odd_jump = findJump(A)even_jump = findJump([-e for e in A])N = len(A)odd_position = [False] * Neven_position = [False] * Nresult = 0for i in range(N)[::-1]: if i == N - 1: odd_position[i] = even_position[i] = True else: odd_position[i] = even_position[odd_jump[i]] if odd_jump != [-1] then False even_position[i] = odd_position[even_jump[i]] if even_jump != [-1] then False if odd_position[i]: result += 1 接下来是findJump的实现。 在一个有序数组中，对于奇数跳，对于某一个数，要找到比它第一个比它大的数的最早出现的位置。 对于偶数跳，要找到第一个比它小的数的最早出现位置。 这个用二分法实现。]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F02%2F16%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2FLeeCode%2FWeek1-3%E4%B8%8D%E5%90%ABAAA%E6%88%96BBB%E7%9A%84%E5%AD%97%E7%AC%A6%E4%B8%B2%2F</url>
    <content type="text"><![CDATA[方法一：贪心思路 直观感觉，我们应该先选择当前所剩最多的待写字母写入字符串中。举一个例子，如果 A = 6, B = 2，那么我们期望写出 &#39;aabaabaa&#39;。进一步说，设当前所剩最多的待写字母为 x，只有前两个已经写下的字母都是 x 的时候，下一个写入字符串中的字母才不应该选择它。 算法 我们定义 A, B：待写的 &#39;a&#39; 与 &#39;b&#39; 的数量。 设当前还需要写入字符串的 &#39;a&#39; 与 &#39;b&#39; 中较多的那一个为 x，如果我们已经连续写了两个 x 了，下一次我们应该写另一个字母。否则，我们应该继续写 x。 123456789101112131415161718192021222324252627class Solution &#123; public String strWithout3a3b(int A, int B) &#123; StringBuilder ans = new StringBuilder(); while (A &gt; 0 || B &gt; 0) &#123; boolean writeA = false; int L = ans.length(); if (L &gt;= 2 &amp;&amp; ans.charAt(L-1) == ans.charAt(L-2)) &#123; if (ans.charAt(L-1) == 'b') writeA = true; &#125; else &#123; if (A &gt;= B) writeA = true; &#125; if (writeA) &#123; A--; ans.append('a'); &#125; else &#123; B--; ans.append('b'); &#125; &#125; return ans.toString(); &#125;&#125; 复杂度分析 时间复杂度：O(A+B)O(A+B)。 空间复杂度：O(A+B)O(A+B)。 若A&gt;=B， 若i&gt;2且i的前三位是aaa，则b s.append(x) 若A&lt;0或B&lt;0，则报错]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F02%2F13%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2FLeeCode%2FWeek1-2%E6%97%A0%E9%87%8D%E5%A4%8D%E5%AD%97%E7%AC%A6%E7%9A%84%E6%9C%80%E9%95%BF%E5%AD%90%E4%B8%B2%2F</url>
    <content type="text"><![CDATA[解决方案 方法一：暴力法思路 逐个检查所有的子字符串，看它是否不含有重复的字符。 算法 假设我们有一个函数 boolean allUnique(String substring) ，如果子字符串中的字符都是唯一的，它会返回true，否则会返回false。 我们可以遍历给定字符串 s 的所有可能的子字符串并调用函数 allUnique。 如果事实证明返回值为true，那么我们将会更新无重复字符子串的最大长度的答案。 现在让我们填补缺少的部分： 为了枚举给定字符串的所有子字符串，我们需要枚举它们开始和结束的索引。假设开始和结束的索引分别为$ i$ 和 $j$。那么我们有 $0 \leq i \lt j \leq n $（这里的结束索引 $j$是按惯例排除的）。因此，使用 $i$从0到 $n - 1$ 以及$ j$ 从$ i+1$到 $n$ 这两个嵌套的循环，我们可以枚举出 s 的所有子字符串。 要检查一个字符串是否有重复字符，我们可以使用集合。我们遍历字符串中的所有字符，并将它们逐个放入 set中。在放置一个字符之前，我们检查该集合是否已经包含它。如果包含，我们会返回 false。循环结束后，我们返回 true。 1234567891011121314151617181920public class Solution &#123; public int lengthOfLongestSubstring(String s) &#123; int n = s.length(); int ans = 0; for (int i = 0; i &lt; n; i++) for (int j = i + 1; j &lt;= n; j++) if (allUnique(s, i, j)) ans = Math.max(ans, j - i); return ans; &#125; public boolean allUnique(String s, int start, int end) &#123; Set&lt;Character&gt; set = new HashSet&lt;&gt;(); for (int i = start; i &lt; end; i++) &#123; Character ch = s.charAt(i); if (set.contains(ch)) return false; set.add(ch); &#125; return true; &#125;&#125; 复杂度分析 时间复杂度：$O(n^3)$ 。 要验证索引范围在 $[i, j)​$ 内的字符是否都是唯一的，我们需要检查该范围中的所有字符。 因此，它将花费 $O(j - i)​$ 的时间。 对于给定的 i，对于所有 $j \in [i+1, n]$ 所耗费的时间总和为： $\sum_{i+1}^{n}O(j - i)$ 因此，执行所有步骤耗去的时间总和为： O\left(\sum{i = 0}^{n - 1}\left(\sum{j = i + 1}^{n}(j - i)\right)\right) = O\left(\sum_{i = 0}^{n - 1}\frac{(1 + n - i)(n - i)}{2}\right) = O(n^3) 空间复杂度：$O(min(n, m))$，我们需要 $O(k)$ 的空间来检查子字符串中是否有重复字符，其中 $k$ 表示 Set 的大小。而 Set 的大小取决于字符串 $n$ 的大小以及字符集/字母 $m$ 的大小。 方法二：滑动窗口算法 暴力法非常简单。但它太慢了。那么我们该如何优化它呢？ 在暴力法中，我们会反复检查一个子字符串是否含有有重复的字符，但这是没有必要的。如果从索引 $i$ 到 $j - 1$ 之间的子字符串 $s{ij}$ 已经被检查为没有重复字符。我们只需要检查 $s[j]$ 对应的字符是否已经存在于子字符串 $s{ij}$ 中。 要检查一个字符是否已经在子字符串中，我们可以检查整个子字符串，这将产生一个复杂度为 $O(n^2)$的算法，但我们可以做得更好。 通过使用 HashSet 作为滑动窗口，我们可以用 $O(1)$ 的时间来完成对字符是否在当前的子字符串中的检查。 滑动窗口是数组/字符串问题中常用的抽象概念。 窗口通常是在数组/字符串中由开始和结束索引定义的一系列元素的集合，即 $[i, j)$（左闭，右开）。而滑动窗口是可以将两个边界向某一方向“滑动”的窗口。例如，我们将 $[i, j)$ 向右滑动 1个元素，则它将变为 $[i+1, j+1)$（左闭，右开）。 回到我们的问题，我们使用 HashSet 将字符存储在当前窗口$ [i, j)$（最初 $j = i$）中。 然后我们向右侧滑动索引$ j$，如果它不在 HashSet 中，我们会继续滑动 $j$。直到 $s[j]$已经存在于 HashSet 中。此时，我们找到的没有重复字符的最长子字符串将会以索引 $i$ 开头。如果我们对所有的 $i​$ 这样做，就可以得到答案。 123456789101112131415161718public class Solution &#123; public int lengthOfLongestSubstring(String s) &#123; int n = s.length(); Set&lt;Character&gt; set = new HashSet&lt;&gt;(); int ans = 0, i = 0, j = 0; while (i &lt; n &amp;&amp; j &lt; n) &#123; // try to extend the range [i, j] if (!set.contains(s.charAt(j)))&#123; set.add(s.charAt(j++)); ans = Math.max(ans, j - i); &#125; else &#123; set.remove(s.charAt(i++)); &#125; &#125; return ans; &#125;&#125; 复杂度分析 时间复杂度：$O(2n) = O(n)$，在最糟糕的情况下，每个字符将被 $i$ 和 $j$访问两次。 空间复杂度：$O(min(m, n))$，与之前的方法相同。滑动窗口法需要 $O(k)$ 的空间，其中 $k$ 表示 Set 的大小。而Set的大小取决于字符串 $n$ 的大小以及字符集/字母 $m$ 的大小。 方法三：优化的滑动窗口上述的方法最多需要执行 2n 个步骤。事实上，它可以被进一步优化为仅需要 n 个步骤。我们可以定义字符到索引的映射，而不是使用集合来判断一个字符是否存在。 当我们找到重复的字符时，我们可以立即跳过该窗口。 也就是说，如果 $s[j]$ 在 $[i, j)$ 范围内有与 $j’$ 重复的字符，我们不需要逐渐增加 $i$ 。 我们可以直接跳过 $[i，j’]$ 范围内的所有元素，并将 $i$ 变为 $j’ + 1$。 123456789101112131415public class Solution &#123; public int lengthOfLongestSubstring(String s) &#123; int n = s.length(), ans = 0; Map&lt;Character, Integer&gt; map = new HashMap&lt;&gt;(); // current index of character // try to extend the range [i, j] for (int j = 0, i = 0; j &lt; n; j++) &#123; if (map.containsKey(s.charAt(j))) &#123; i = Math.max(map.get(s.charAt(j)), i); &#125; ans = Math.max(ans, j - i + 1); map.put(s.charAt(j), j + 1); &#125; return ans; &#125;&#125; Java（假设字符集为 ASCII 128） 以前的我们都没有对字符串 s 所使用的字符集进行假设。 当我们知道该字符集比较小的时侯，我们可以用一个整数数组作为直接访问表来替换 Map。 常用的表如下所示： int [26] 用于字母 ‘a’ - ‘z’或 ‘A’ - ‘Z’ int [128] 用于ASCII码 int [256] 用于扩展ASCII码 12345678910111213public class Solution &#123; public int lengthOfLongestSubstring(String s) &#123; int n = s.length(), ans = 0; int[] index = new int[128]; // current index of character // try to extend the range [i, j] for (int j = 0, i = 0; j &lt; n; j++) &#123; i = Math.max(index[s.charAt(j)], i); ans = Math.max(ans, j - i + 1); index[s.charAt(j)] = j + 1; &#125; return ans; &#125;&#125; 复杂度分析 时间复杂度：$O(n)$，索引 $j$ 将会迭代 $n$ 次。 空间复杂度（HashMap）：$O(min(m, n))$，与之前的方法相同。 空间复杂度（Table）：$O(m)$，$m$ 是字符集的大小。 自己做最终代码 12345678910111213141516class Solution: def lengthOfLongestSubstring(self, s): """ :type s: str :rtype: int """ arr = [] maxLen = 0 for i in s: if i in arr: index = arr.index(i) arr = arr[index+1:] arr.append(i) if len(arr) &gt; maxLen: maxLen = len(arr) return maxLen 1、”dvdf”失败 原因：做法是遇到一个重复的字符就清空数组，这种情况下就有问题了。 解决方案：遇到重复时，删掉重复的前面的所有字符 2、”aabaab!bb”失败 原因：代码写错了， 1index = s.index(i) -&gt; index = arr.index(i)]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F02%2F08%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2FLeeCode%2FWeek1-1%E4%B8%A4%E6%95%B0%E4%B9%8B%E5%92%8C%2F</url>
    <content type="text"><![CDATA[方法一：暴力法暴力法很简单。遍历每个元素 $x$，并查找是否存在一个值与 target - x相等的目标元素。 12345678910public int[] twoSum(int[] nums, int target) &#123; for (int i = 0; i &lt; nums.length; i++) &#123; for (int j = i + 1; j &lt; nums.length; j++) &#123; if (nums[j] == target - nums[i]) &#123; return new int[] &#123; i, j &#125;; &#125; &#125; &#125; throw new IllegalArgumentException("No two sum solution");&#125; 复杂度分析： 时间复杂度：$O(n^2)​$， 对于每个元素，我们试图通过遍历数组的其余部分来寻找它所对应的目标元素，这将耗费 $O(n)​$ 的时间。因此时间复杂度为 $O(n^2)​$。 空间复杂度：$O(1)​$。 方法二：两遍哈希表为了对运行时间复杂度进行优化，我们需要一种更有效的方法来检查数组中是否存在目标元素。如果存在，我们需要找出它的索引。保持数组中的每个元素与其索引相互对应的最好方法是什么？哈希表。 通过以空间换取速度的方式，我们可以将查找时间从 O(n) 降低到 O(1)。哈希表正是为此目的而构建的，它支持以 近似 恒定的时间进行快速查找。我用“近似”来描述，是因为一旦出现冲突，查找用时可能会退化到 O(n)。但只要你仔细地挑选哈希函数，在哈希表中进行查找的用时应当被摊销为 O(1)。 一个简单的实现使用了两次迭代。在第一次迭代中，我们将每个元素的值和它的索引添加到表中。然后，在第二次迭代中，我们将检查每个元素所对应的目标元素（target - nums[i]）是否存在于表中。注意，该目标元素不能是 nums[i]本身！ 12345678910111213public int[] twoSum(int[] nums, int target) &#123; Map&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;(); for (int i = 0; i &lt; nums.length; i++) &#123; map.put(nums[i], i); &#125; for (int i = 0; i &lt; nums.length; i++) &#123; int complement = target - nums[i]; if (map.containsKey(complement) &amp;&amp; map.get(complement) != i) &#123; return new int[] &#123; i, map.get(complement) &#125;; &#125; &#125; throw new IllegalArgumentException("No two sum solution");&#125; 复杂度分析： 时间复杂度：O(n)， 我们把包含有 n 个元素的列表遍历两次。由于哈希表将查找时间缩短到 O(1) ，所以时间复杂度为 O(n)。 空间复杂度：O(n)， 所需的额外空间取决于哈希表中存储的元素数量，该表中存储了 n个元素。 方法三：一遍哈希表事实证明，我们可以一次完成。在进行迭代并将元素插入到表中的同时，我们还会回过头来检查表中是否已经存在当前元素所对应的目标元素。如果它存在，那我们已经找到了对应解，并立即将其返回。 1234567891011public int[] twoSum(int[] nums, int target) &#123; Map&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;(); for (int i = 0; i &lt; nums.length; i++) &#123; int complement = target - nums[i]; if (map.containsKey(complement)) &#123; return new int[] &#123; map.get(complement), i &#125;; &#125; map.put(nums[i], i); &#125; throw new IllegalArgumentException("No two sum solution");&#125; 复杂度分析： 时间复杂度：O(n)， 我们只遍历了包含有 n个元素的列表一次。在表中进行的每次查找只花费 O(1) 的时间。 空间复杂度：O(n)， 所需的额外空间取决于哈希表中存储的元素数量，该表最多需要存储 n个元素。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Python出错总结]]></title>
    <url>%2F2019%2F02%2F08%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FPYTHON%2F%E9%97%AE%E9%A2%98%E9%9B%86%2F</url>
    <content type="text"><![CDATA[inconsistent use of tabs and spaces in indentationhttps://blog.csdn.net/godot06/article/details/80974884 这个错误目前笔者在Subline3遇到的都是看似空格实则没有空格引起的:： 解决方法：就是打开subline的空格制表显示就可以清楚的显示出自己是否真的空格了。 Subline3显示空格制表符的设置方法： ①.Preferences -&gt; Setting ②在Preferences-&gt;Settings-User中添加以下代码：（如上图） 1"draw_white_space": "all", ‘ascii’ codec can’t decode byte 0xe2 in position 5: ordinal not in range(128)产生原因： 有些字符加了u，有些没有加 解决 1234import sys reload(sys) sys.setdefaultencoding(&apos;utf8&apos;)]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>错题集</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F02%2F08%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2Ftensorflow%2F%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMAhttps://blog.csdn.net/u013061183/article/details/78792181 1如果只用cpu，直接pip install tensorflow，官方文档写了已经支持python3.6了。2.如果要用gpu，输入pip install tensorflow-gpu，即可。此外gpu还要安装cuda和cudnn，需要注意版本，运行import tensorflow会有提示。3.pip操作安装了的tensorflow会有提示：Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2，原因好像是编译问题，cpu没有得到充分的利用，如果从官网下载源码不会有这种问题，如果用gpu可以忽略，使用： 12import osos.environ[&apos;TF_CPP_MIN_LOG_LEVEL&apos;] = &apos;2&apos; #忽略烦人的警告]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F02%2F07%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FNLP%2FTransformer%E5%AF%B9%E8%81%94%2F</url>
    <content type="text"><![CDATA[https://mp.weixin.qq.com/s/N6sSQ3EG_NPFaOGwkC0i6w 为了写春联，我用Transformer训练了一个“对穿肠”这里我们用6层Transformer单元，里面的Self-Attention的头数为8，隐单元数512个。模型采用 tensorflow 框架，GPU 则用深度学习标配的 Tesla K80。 github地址（脚本已无法使用，可以用我们上传的couplets，google drive需科学翻墙）： https://github.com/wb14123/couplet-dataset google drive： https://drive.google.com/file/d/13cJWWp_ST2Xqt76pEr5ZWa6LVJwGYC4-/view?usp=sharing 整个数据集解压过后总共56.9兆，其中训练数据集56.4兆，测试数据集400多KB。另外，数据是以TXT格式储存的。全文不含标点15,153,840字，如果每天看100条的话，20多年都看不完。而文摘菌采用Tesla K80，2个小时就能来一个Epoch。 数据预处理 首先进行数据预处理，因为拿到的数据比较干净，所以这任务主要就是创建词表。值得一提的是，数据集的压缩文件中存在着一个创建好的词表。你也可以用数据集给出的词表。因为对联任务并不难，所以我们直接使用字粒度来作为输入，也就是像“我们”这样的词，我们是把其当做两个单元进行输入，而不是一个。 关键代码如下： 1234567891011121314151617181920def make_vocab(fpaths, out): '''Constructs vocabulary. Args: fpaths: Astring. Input file path. fname: Astring. Output file name. ''' char2cnt = Counter() for path in fpaths: for line inopen(path, 'r'): line = line.strip() if not line: *# detect the empty line* continue chars = line.split() char2cnt.update(chars) withopen(out, 'w') as fout: fout.write("&#123;&#125;\t1000000000\n&#123;&#125;\t1000000000\n&#123;&#125;\t1000000000\n&#123;&#125;\t1000000000\n".format("&lt;PAD&gt;", "&lt;UNK&gt;", "&lt;S&gt;", "&lt;/S&gt;")) # special tokens for word, cnt in char2cnt.most_common(len(char2cnt)): fout.write(u"&#123;&#125;\t&#123;&#125;\n".format(word, cnt)) print("%d chars written!"% (len(char2cnt)))]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F02%2F07%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FNLP%2FBert%2F</url>
    <content type="text"><![CDATA[https://mp.weixin.qq.com/s/WDq8tUpfiKHNC6y_8pgHoA Transformer由论文《Attention is All You Need》提出，现在是谷歌云TPU推荐的参考模型。论文相关的Tensorflow的代码可以从GitHub获取，其作为Tensor2Tensor包的一部分。哈佛的NLP团队也实现了一个基于PyTorch的版本，并注释该论文。 Attention is All You Need： https://arxiv.org/abs/1706.03762 它是由编码组件、解码组件和它们之间的连接组成。 编码组件部分由一堆编码器（encoder）构成（论文中是将6个编码器叠在一起——数字6没有什么神奇之处，你也可以尝试其他数字）。解码组件部分也是由相同数量（与编码器对应）的解码器（decoder）组成的。 所有的编码器在结构上都是相同的，但它们没有共享参数。每个解码器都可以分解成两个子层。 从编码器输入的句子首先会经过一个自注意力（self-attention）层，这层帮助编码器在对每个单词编码时关注输入句子的其他单词。我们将在稍后的文章中更深入地研究自注意力。 自注意力层的输出会传递到前馈（feed-forward）神经网络中。每个位置的单词对应的前馈神经网络都完全一样（译注：另一种解读就是一层窗口为一个单词的一维卷积神经网络）。 解码器中也有编码器的自注意力（self-attention）层和前馈（feed-forward）层。除此之外，这两个层之间还有一个注意力层，用来关注输入句子的相关部分（和seq2seq模型的注意力作用相似）。 词嵌入过程只发生在最底层的编码器中。所有的编码器都有一个相同的特点，即它们接收一个向量列表，列表中的每个向量大小为512维。在底层（最开始）编码器中它就是词向量，但是在其他编码器中，它就是下一层编码器的输出（也是一个向量列表）。向量列表大小是我们可以设置的超参数——一般是我们训练集中最长句子的长度。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Typora技巧]]></title>
    <url>%2F2019%2F01%2F22%2F%E5%B7%A5%E5%85%B7%E5%92%8C%E7%8E%AF%E5%A2%83%2Ftypora%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[图片缩放通过如下方式插入图片 12345&lt;img src=&quot;/Users/david/david/00projects/00markdown/pic/image-20190118153222511.png&quot; style=&quot;zoom:70&quot; /&gt;或者&lt;img src=“url” style=“width:200px height:300px” /&gt;]]></content>
      <categories>
        <category>工具和环境</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F01%2F18%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA%2FUntitled%2F</url>
    <content type="text"><![CDATA[刷完一道题其实是一件很难的事情，因为普通解法很容易，但是最优解真得去耐着性子研究好久，去查资料，去做优化，这个过程很漫长。 刷题圈流行一句话，“刷200道题之后，再无庸手”，说的就是用必须找到最优解的心态去刷200道题之后，进步真的非常大。 https://www.csdn.net/article/2015-10-23/2826015 1、排序和查找 2、栈和队列 3、链表 4、树 5、动态规划 6、图]]></content>
  </entry>
  <entry>
    <title><![CDATA[Azkaban技巧]]></title>
    <url>%2F2018%2F12%2F18%2F%E5%B7%A5%E5%85%B7%E5%92%8C%E7%8E%AF%E5%A2%83%2FAzkaban%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[指定服务器节点执行 useExecutor]]></content>
      <categories>
        <category>工具和环境</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python画图]]></title>
    <url>%2F2018%2F11%2F18%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FPYTHON%2Fpython%E7%94%BB%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[散点图12345678910111213141516171819202122232425262728fig = plt.figure()ax1 = fig.add_subplot(111)#设置X轴标签plt.xlabel('Guage (mm/5min)')#设置Y轴标签plt.ylabel('R(Z) (mm/5min)')plt.title('(c)')#设置XY轴的范围plt.xlim(xmax=20,xmin=0)plt.ylim(ymax=20,ymin=0)# 在散点图上画对角线Y = linspace(0, 30, 13)# 横轴数据。X = linspace(0, 30, 13)ax1.plot(X,Y,c='black')#画散点图# cValue = ['r','y','g','b','r','y','g','b','r']# c是颜色，linewidths是点的大小ax1.scatter(data_real_np, ydz_np , c='black', linewidths=1, s=1)# plt.plot(data_real_np,ydz_np,'ro')#设置图标# plt.legend('x1')#显示所画的图plt.show()]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MarginNote使用技巧]]></title>
    <url>%2F2018%2F11%2F16%2F%E5%B7%A5%E5%85%B7%E5%92%8C%E7%8E%AF%E5%A2%83%2FMarginNote%2F</url>
    <content type="text"><![CDATA[https://sspai.com/post/47317 高亮使用规则 高亮仅用在关键词上，而勾选句子则使用用下划线； 高亮只用黄色，下划线只用红色和蓝色； 红色下划线用于重要内容，例如定义、总结性的句子； 蓝色下划线用于自己存在疑问的内容。]]></content>
      <categories>
        <category>工具和环境</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F11%2F15%2Fhadoop-spark%2Fazkaban%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[Azkaban 指定在executor上执行任务 executor 对应关系： 2 10.8.49.177 5 10.8.49.188 6 10.8.49.178 7 10.8.49.196 useExecutor 6]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F11%2F11%2F%E6%80%BB%E7%BB%93%E4%B8%8E%E6%80%9D%E8%80%83%2F%E4%B8%89%E6%9D%BF%E6%96%A7%E5%A4%8D%E7%9B%98%2F</url>
    <content type="text"><![CDATA[前序 分组（41人分6组）、团建结束后制定团队目标（最后发现这个目标并没有实际意义） 七星队-队长姚良 翼王之箭-队长咸海泓 双11-队长许小常 平头哥-队长关迪 无限可能-队长徐杨 勇闯天涯-队长王煜青 理论部分一、管理者的定位1.1 案例讨论小明从业务骨干晋升管理层，组员有三个角色： 1）小钱，同为业务骨干，性格张扬，时不时质疑小明；—— “野马” 2）小高，老黄牛，小明为小高制定发展计划，小高有些不满； —— “黄牛” 3）小陈，与小明之前关系不错，业务能力弱，感觉与小明关系发生了一点微妙的变化。 —— “下游” 分别对每位组员思考： 1）为什么态度发生了变化 2）他的心理活动是什么 3）管理者要采取哪些行动？ 采取哪些行动，比如对于小钱： 1）通过私下吃饭，沟通等，找共同点，建立最基本的信任 2）稳住情绪，给一定的空间让他发展 3）客观考核KPI 4）小明加强自己的能力，树立威信 5）模糊管理者的界限，平等沟通 6）向更高层管理者请求帮助 6）实在不行，可以happy go，有意识培养新人 比如对于小高： 1）认可对团队以前的贡献，在一些事情上提供帮助 2）让他了解团队目标，尊重他的意见，一起制定计划。 具体开展工作时： 一步一步来 该强时强，该示弱时示弱 1.2 管理者的日常事务1）深度上，难点攻关，架构设计 2）项目上，项目管理，进度管理 3）人员上，人员管理，带领团员成长 4）思考团队在公司中的发展定位 总结：身体力行，管控团队，达成结果。 1.3 理解管理者管理者充当5个角色： 1、团队战略的规划者 团队的战略和目标要清晰 2、团队梦想的缔造者 会画大饼，sell梦想 告诉组员团队的价值所在 3、团队目标的引领者 4、下属成长的辅导者 “超越伯乐”，当导师，帮助团员成长。 5、企业文化的传播者 团建 统一价值观、愿景、文化 管理者的境界： 站在现在看未来 —— 树立目标，带领团队“活下去” 站在未来看现在 —— 带领团队“冲出去” 1.4 管理能力与领导能力的不同管理者冰山模型 水面上：方法、技巧、管理、招人、解雇 —— “术”，招式，可以短时间练成 水面下：格局、感召力、胸怀、担当、生产力 —— “道”，内功，需要历练和打磨 1.5 我们公司是怎样的企业公司使命：以金融服务创造美好生活 价值观：拥抱变化 愿景：持续发展100年。服务最多的消费金融用户。成为年轻人改变生活的第一站。 二、Hire &amp; File一直在强调：招聘是管理者的事 2.1 选育留选：确立选人标准 育：培养，融入团队，让员工增值，关注头部20% 留：留住好员工，帮助有问题的员工。 2.2 HR面试和管理者面试的不同HR关注： 表达能力；抗压能力；应聘动机；态度；价值观； 有否决权 管理者关注： 专业技能；成长空间；与团队是否匹配 有否则和决定权 2.3 面试官理念1、为今天、为明天 了解专业经验和发展潜力。 仓促招人好不好：仓促招的人可能是团队未来的瓶颈。 2、接收和自己不一样的人 光环效应 大同大异 3、寻找比自己更优秀的人 找候选人比你优秀的方面 不然自己会当救火队长，是团队能力的天花板。 2.4 招聘对的人员工冰山： 水面上：专业、人脉、经历、学历、年龄 —— 容易在面试中了解 水面下：风格、性格、人格倾向、内驱力 如何在面试中了解水面下能力： 1、情景模拟法评测 多问开放式问题，比如： 封闭式：喜欢旅游吗？ 开放式：去过哪儿旅游？ 通过开放式问题，可以评估逻辑思考能力，临场应变能力，沟通表达能力。 有三种提问方式： 1）引导式：能解决XX技术中的问题吗？ 2）理论式：怎么解决问题？ 3）行为式：讲一下在工作中遇到的一个问题，你是怎么解决的？ 2、STAR行为面试法 举例： 是否遇到过违反制度的人？怎么做的？ 尝试过什么步骤，工作或计划？如何避免？如何让结果令人满意？ 2.5 面试流程1、开场：问候，拉近距离 2、背景回顾 3、行为提问 4、其他信息确认 5、结束前sell梦想 6、评估 参考https://wenku.baidu.com/view/a3a104bd5ff7ba0d4a7302768e9951e79b8969d4.html 注意面试微表情传达的信息 2.6 离职管理阿里的人才矩阵： 明星：个人能力强，业绩突出，对公司目标和价值观认同度高的员工 瘦狗：个人能力弱（核心标志是业绩萎靡），对目标和价值观认同度低的员工 野狗：个人能力强（核心标志是业绩突出），但对公司目标和价值观认同度极低的员工 小白兔：个人能力弱，业绩长期萎靡，但目标和价值观认同度极高，工作态度极好的员工 牛：普通员工，团队中的大多数人。 2.7 解聘时的面谈内容被动式： 1）决定和原因 2）具体流程 3）听员工的想法 4）实质性的帮助 5）法务支持 6）关注问题的解答 主动式： 1）告知原因和想法 2）了解员工的考虑和设想 3）告知对员工的评价 4）代表公司谈自己的看法 5）违约责任 6）交接 三、团队建设3.1 高效团队的特质员工和管理者组成共同体，合理利用每个成员的知识、技能 1、树立共同目标 2、协同效率机制：分工、合作、监督 3、凝聚团队文化 4、拥有高知识&amp;技能的员工 参考http://www.doc88.com/p-5817406373165.html公司团队协作效率机制 3.2 团队成熟四阶段各自为政-&gt;组织成型-&gt;齐心协力-&gt;业绩高效 也可以参考塔克曼团队发展阶段模型 各自为政阶段：是因为缺乏信任和归属感，不了解目标。解决方法是，管理者需要与员工沟通、了解、交心，展现个人影响力和专业能力。 如何树立管理者的威信：1）解决业务难题；2）解决遗留管理问题。 组织成型阶段： 1）建制度；2）给帮助；3）树立典范员工；4）解决人际关系，工作冲突 齐心协力阶段： 1）慢退出具体工作。简单的事情员工自己想办法，难题再找管理者。 2）当催化剂，知道每个人的优缺点。 3）定期回顾总结。 4）建设团队文化。 5）关注个人需求，针对性辅导和激励。 业绩高效阶段： 1）管理者闲下来，着眼于完成并超越业绩目标。 2）培养人，合理给员工授权。 3）从更高境界看团队。 3.3 团队建设的四个核心点1、定目标 sell梦想，以身作则，借力使力（老板等），个人与团队梦想结合。 2、建制度 用流程来简化复杂问题。将历史案例总结为标准流程。 如何建制度？ 1）开会讨论，解决冲突后定下来机制 2）把流程、方式梳理出来，形成基准线。 热炉法则：警示所有人、只要触犯立刻反馈、所有人公平。”心慈刀快“ 3、树文化 阿里的”闻味道“ 4、养人才 从工作能力和工作态度划分四个象限 激励 给予机会&amp;评估 辅导&amp;观察 辅导 四、绩效管理4.1 绩效管理流程PDCAPlan：制定绩效计划 关键绩效指标符合PE-SMART原则： Positively Phrased：正面词语 Ecologically Sound：符合三赢 Specific：清楚明确 Measurable：可以度量 Achivevable：可以达成 Rewarding：值得满足 Time-frame set：时间期限 Do：绩效沟通与指导 Check：绩效考核与反馈 Action：绩效诊断与提高 4.2 目标达成五部曲1、目标设定 2、绩效指导 两种询问方式：辅导式、教练式（苏格拉底教学法） 管理者如何激励：马斯洛需求、赫兹伯格的双因素激励理论 3、绩效评估 4、面谈反馈 1）交流、定计划 2）营造融洽氛围 3）员工自评 4）上级评价 5）讨论近期问题 6）确认下阶段 7）需要支持的资源 8）总结（达成一致） 5、提升改进 共创部分 队长的职责： 1、时间管理。什么时间该做什么 2、团队分工 3、把控节奏 什么时候该发散，什么时候该收敛。 4、定目标 5、发挥组员特长 7、会借调场外资源 8、帮助最弱的组员。 如何打绩效 定可量化，有加分减分项的标准 自评 总结 最大化每个人的贡献度，再用贡献度衡量别人。]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F11%2F07%2F%E6%80%BB%E7%BB%93%E4%B8%8E%E6%80%9D%E8%80%83%2F%E4%B8%89%E6%9D%BF%E6%96%A7%2F</url>
    <content type="text"><![CDATA[每个管理层的三板斧基层管理者 1、定目标：希望三天之后拿到的结果是什么。形成统一认识。项目计划书需要哪些模块，把目标定下来。 2、追过程：需要知道是如何分工的。追踪每个细节和过程，前面越清晰，越知道问题在哪里。 3、拿结果：每个阶段要拿出明确结果，否则就是不达标。 用六点工作制：记录今天最重要且最紧要的事情，按先后顺序做。下班后对这些事情做一个总结，再计划明天的六点。 中层管理者： 1、Hire and Fire：会招人。如果人都招不到，说明对外部的链接不够。会开除人，真要开除，要问开除之前帮过他哪三次？若没有帮过，说明管理者不称职。 2、Team Building：凝聚力，感情，认同。 3、Get result：既要过程，也要结果。一个价值多少钱的团队，就可以拿到多少的业绩。 高层管理者： 1、揪头发：远离舒适区。剩下不知道在讲什么。开眼界跟揪头发的关系？ 2、照镜子：让别人评价你，你也客观评价别人。 3、闻味道：感性判断这个人对团队的影响是好是坏。第一真实，第二担当，第三能够视人为人。 培训前会问1、为什么做这个事情？ 目的是什么 2、凭什么可以做？ 握有的资源，市场行情，团队的能力。 3、事情是什么，讲的清楚吗？ 痛点，对手，实施计划，最终产品 4、竞争分析是什么？ 5、最能打动所有团队和客户的东西是什么？最核心的价值点是什么？ 6、事情达成统一了吗？是否有团队成员没想清楚？ 三板斧的戒定慧戒律1、不挑战主持人 2、不用手机 3、作息管控。 定心一定：课程流程五轮项目PK 不能太大，越大越容易被抓住漏洞反复问。 智慧一慧：因为信任，所以简单。信任团队成员。换位思考，有话直说。 二慧：彼此为土壤。借鉴其他组想法。 三慧：借势修人，借假修真。把三天当一年用，小组有多久没有一起为一个事情经受这么多拷问和坚持。 项目操练一 项目雏形要考虑什么拷问关键问题从市场分析、运营模式、优劣势分析、企业终局介绍项目运营方案。 系统最关键的要素是什么，对关键要素的打造在哪里 （核心功能是什么，怎么做） 为什么会由你们来做？ 如果什么都不做，只做一件事情，要干什么？ 整体方案不能太空，想做的事情不要多。项目的核心客户是那个人群？ 定位是什么定位不仅仅是年龄层。 地理位置分类，年龄分类都是人群分类的误区。 不够精细和准确吧应该，分析市场人群 PPT逻辑混乱。后面如何保障，如何盈利没有讲清楚。我们很难完全站在竞争对手角度。因为不知道别人做了多少步 做的事情可以有一个样本，要么是成功的，要么是失败的，在这个基础上再走三步，这也是很好的方式。 或许这个案例不是我们行业的，学别人是最快的进步方法，山寨是学起来最快的方式，而不是重新构架一个系统。 所有的要素都在人、货、场三个要素。首先是人，然后货在哪里，场如何连起来。 可以告诉大家，这个项目中有10个问题没有想清楚，需要找别人一起想。 如果把所有问题都解决了，这就有问题了，怎么可能解决所有问题？ 第一天不需要拿很完整的方案，只要告诉我们为什么，把做的事情讲清楚。后面肯定有更完整的方案。 老师提了很多问题后，收集信息进行提炼，解决核心问题，不要一一应对。 计划书包括哪些从市场分析、运营模式、优劣势分析、企业终局介绍项目运营方案。 人员规模，组织架构、财务预算、营销推广。 PPT怎么做PPT逻辑混乱。后面如何保障，如何盈利没有讲清楚。我们很难完全站在竞争对手角度。 不要为了分析而分析，市场的分析要有结论。 第一天，可以告诉大家，这个项目中有10个问题没有想清楚，需要找别人一起想。 如果把所有问题都解决了，这就有问题了，怎么可能解决所有问题？ 怎么搭项目的架构即使第一天问题没有解决，也要走到第二步，把架构搭完。把部分问题留着。 公司人齐了之后，也就是小组定型之后，第一件事就是组织架构分工。 组长要强势，在企业发展初期，没有强势领导就会出现混乱的局面。组织要给每个人做清洗的分工。 每个人是否有老板思维，要揪头发。 把一些不重要的砍掉，制作一件事情。要丢东西和留东西。 轻资产模式还是重资产模式？养很多服务人员，还是做一个优步？很多假想的问题都是针对重资产模式。 一个修改后比较完善的例子。 系统化思考商业问题的工具做任何一个项目，从四个方面入手都可以考虑：消费者运营、商家运营、平台运营、营销活动。 第二个是九宫格，把任何事情分为9个领域，8个方面。下面三个是基础，中间两个是抓手，上面3个是子项目。 项目立项的修炼不破不立，产品可能不变，但运营模式等敢于推翻后重新思考。 1、技术手段 扫二维码 why？ 目的和背景： 现状分析： 痛点： 可行性论证 转化 what 线下touch到用户 提前推荐录车 2：45分钟 我怎么通过一个城市我们已经贷出去的人的轨迹，找到这个城市的二手车使用偏好 需要 WHY要达到这个目的，为什么我们要做这件事—痛点 凭什么我们可以做什么事—可行性论证 目的是什么现状分析和行业痛点WHAT我们要做一个什么样的事情： 运营模式我们做的是这样的一个运营模式： 1、找到线下市场的若干场景，touch到人，发传单，或其他方式宣传， 2、让他们扫码着陆到一个公众号。拿到手机信息，公众号推送一些文章，比较好的二手车等，养护帖子。 一旦用户需要买车，输入买车意向，然后分发。 3、销售主动联系用户。而不是被动 1）明确看车意向，引导用户 2）约到店时间 用户到店前做好： 3）车辆信息录入（车审环节做的事） 4、用户到店，人车分离审核。 触达潜在人群（能让我们可以触达到潜在人群） 维系与人群互动的公众号 增加销售主动联系流程 车辆录入前置（保证用户到店后，人车审核分离） 中长期愿景优质一手单 增强对车商议价能力 — 主动带来客户有议价能力，依赖关系倒转 提高美利在车商侧曝光度 成本预估我们已经了成本粗略的核算，包括有没有盈利模型 1、确认潜在用户规模 分析 2、每个环节估计转化率、目标转化率 每个转化率，怎么来的，为什么能作为参考 1）3%，车联盟带看业务 线上推广后， 用户主动呼入 - 成单的转化率 我们与3%的不同是 用户填写后销售联系的数量 - 成单 3% 场景相似 区别： 业态不同、潜在用户群不同 途径不相似 2） 每个环节的估计转化率和目标转化率每个环节的成本预估如何实施— 如何保证可持续盈利 一步一步实现的步骤步骤如何实施 test 如何可持续发展todo哪些地方还没有论证好或者想好的 哪些地方是风险点 PPT至少预留2个小时做 思维导图后，要及时转成PPT内容 主动与队友沟通，而不是被动等待录入。 谁沟通了谁没有沟通心理没底。 拼PPT而不是只一个人做 发散之后没有收 目的与现状 实现规模扩大 受限原因 车商与我们合作意外会导致 线下接触和线上手段。 围绕这个目的，我们需要分几个步骤来实现这个事情 什么样的场景是线下用户比较集中的。 2、希望线下转换到线上场景。 3、销售沟通 是否真实，是否有细节要求，把意愿固话，约定时间看车。 前置完成车辆审核。 这些是销售准备工作 激活-&gt;销售线索-&gt;销售与用户沟通，完成车辆前置准备和审核。 整个是业务流程。 总转化率 3单/5000 10%和20%的转化率 传单的转化率：2%就很高。 姚 1、转化率为什么是10% 顺权 2、跟驾校合作的利益成本关系 3、目标人群，我们主要不是年轻人，而驾校主要是年轻人 红波 1、如果到了车商不想贷款怎么办 kevin 1、有没有算账，增加营收需要的成本、投入有多大。 算进去地推人员的成本 2、成单转化率多少？ 万分之六，5000个转化3个。 3、找到拐点，转化多少才能赚钱 4、线下有不确定性，地推怎么接触到转化的人 5、瓜子优信都在做为什么还亏钱 6、最大的假设，这些人是不是买二手车的客群。 有车辆信息的维护。实际上把扫车的过程贯穿到业务场景中。 7、演讲缺乏最关键信息。怎么挣钱，挣多少钱，成本和利润，要把账算清楚。不然业务规模扩大了，成本花了多少。金融公司通过带看来盈利，是伪命题。先考虑主业，人车分离是副业。 沉默 1、不想带来什么好处，先想有什么坑。用一个解决方案解决一个业务场景。 2、做过这个，都在赔钱。只有58通过卖信息挣钱。 3、500线索，选1500次。这个成本足够把一个市场所有车摸一遍。 4、不要“我希望”。。。 5、进步有限 6、能不能一个月有点效果？这样还能花点钱 时专 1、5000的基数太小。如果长沙每个月能做2、300单，1/1000。带看成交是百十。 2、建议：每种可能性放到基数上考虑细节。 3、PPT，让每个组讲看完就可以落地。投入多少，收入多少。投入怎么算，支出怎么算。把结论一点一点往回拆。 4、怎么让评委进入你的逻辑。 问题是基数小，转化率低，哪一个有变大的可能和方法。 获客成本能否再降低，比如联系在驾校的学生做推广。 盘车量太多：不是所有车都录入，推一些热门车，把流行的车型做精。 我们公司的热门车型，我们公司的人群分布 三四线城市买车人群聚集还是分散（找几个城市看居住地址，工作地址） 传单成本：联系到在驾校的学生 只做扩大潜在用户 1、有没有这么多的潜在用户 2、为什么竞对也触达不到这些潜在用户 内容不必要多，把点讲清楚 1、安装GPS目的，不是防止被盗。花GPS是一个比较高的成本， 是获得用户贷后数据， 2、目前是赛格，久劲还没正式接入。 3、3%占比怎么来的 4、渠道卡单？？半个小时 5、验证耗时？接口故障和卡单导致耗时 6、容易选错设备商，系统操作提示？去室外重启设备，可以防止填错？ 不是根本的解决方法。错误率和卡单率降低。 沉默 1、跑题 2、痛点和目的达成。解决这个问题有可能还能帮我们解决其他问题。 1）要解决的问题 2）问题的原因 3）什么解决方式 4）能达成的结果 kevin 1、做和没做的不同 2、辛巴做了十几版交互，把GPS放在哪儿。提高的空间，提高GPS在车金融业务环节上的优化。 看其他人放在什么环节。很影响线上时效。赛格断电导致所有订单审核延长2小时以上。 六爷 1、不是卡单，而是填错了会超时。有一套优化销售填写的方式 车抵贷 车商数量不变的前提下拿到更多销售线索 差异化覆盖 提高转化率 拿到更多的一手单 跨过销售直接找车商 不行。被刷过征信， 通过B touch C， 1、用服务和管理体系，为了拿更多单子 2、好中坏车商，针对车商，尾部变腰部，腰部变头部。找到点去经营车商。除了销售拜访能否有别的方式。 在单子里面，做成我们的单能给车商什么好处？ 维系小车商关系。小车商更容易对我们的返点感兴趣，而中车商要找其他方面的好处。 小车商容易被大平台忽略 -&gt; 拿到一手单 现有的数据可以支撑我们发现更多合适的小车商吗？ 让销售能够主动维系车商关系。 找短链条的业务。 1、收益的量化 2、成本核算的具体数值 3、新老业务的对比表 老的，用户自己选择，办单时长 4、效率增加带来单量的增加，估算 — 直接实现规模增长 5、按照我们模式的评估报告。 若有系统，做结果对接。 没有系统，用我们的系统。 评估报告 ： 纸质版 -&gt; 系统化， 检测数据系统化等。 开发成本 测算 1、城市二手车市场日均成交量 2、4个百分比 3、一个城市检测结构数量。假设跟其中N家合作，合作一个月有没有成本 4、日均节省开支可以忽略 2018年11月10日一通率 kevin 1、通过演练看到事情和团队的蜕变 2、复盘这三天的方案 3、做优化有一个北极星指标，要持续观测。 要拆解，有哪些子指标会影响核心指标。 沉默 1、整个逻辑的合理性、正当性是第一位的 2、讲闭环 3、手段和目标之间的关联性不够强 GPS没有信号，单就过不了。 kevin 1、没有明确目标 2、方案解决不了问题 3、可以考虑常规渠道出问题时有备用渠道能查到GPS信号。 客户诉求是，签约提车。卡单后熔断会让客户一直等，没有解决问题。 沉默 1、舒适区、焦虑区、恐慌区 2、系统；系统性：基于一系列的系统能够解决问题，达到目的。 锦鲤卡 kevin 1、哪些环节是想当然的 2、上线之后的维护，客户纠纷的处理 考虑哪些有隐形成本 3、要招多少销售来做这件事。 4、做这个方案就是大家拆解成本，把账算清楚。然后得出一个悲观收入，乐观收入。 陈默 1、解决一个问题，有相应策略，会遇到多少坑，每个怎么解决。 美利通 经过的尝试 1、绕过车商走不通 2、做服务走不通 kevin 1、通病，在解决一个问题的同时尽可能多的把好处罗列出来。尽可能聚焦一个目标。 2、风险，面临检测机构造假。除非跟权威机构直联。 3、现实中，秒拒过了还没有开始选车。销售实际中介入的很早。有人去车商看车，就打电话给销售。 4、美利做自己认证的成本有多少 5、录入车信息时间段，真正要减少审批时间。而风控中车和人需要一起审。如果分开审核，成本是多少。 6、审批时间是7-30分钟， 7、把审批人员的工作安排到可并行的程度。 沉默 1、方法的呈现形式最好 2、风险点可能是致命点。坦诚的说了风险点，去深入了解真实的风险点。 石头 1、这个case在场景上有问题，在什么场景下才会转化过来。 2、太多判断条件，离大家背景知识比较远，这个前提下容易走歪。 平头哥 kevin 1、不是做的分享，而是一个可行的，可落地的项目。 2、数据分析，财务分析 3、硬实力。是否能通过用户画像知道用户是否缺钱。软技能。发现自己的不足。 4、需要展示一个项目的提案，每个环节成本多少，收益多少。风险在哪儿，当每个风险发生时有什么影响。 被拒掉的人，每个环节拒掉多少，哪些被拒掉得人是可以再贷的。 陈默 1、直说增长点，缺少每个增长点如何盈利。 2、内容服务于目的，形式服务于内容。 3、团队领导被质疑了。 姚老师 陈默 1、没有达到闭环。闭环是：有一个目标，解决这个问题有什么方案，什么方法，怎么通过方案达到具体目标。 2、面对问题，不是找相似解决方案套进去就能解决，而是真正找到什么问题背后的困难，再针对具体的困难再找解决方案。]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F10%2F31%2Fhadoop-spark%2Fspark%2Fpresto%E7%9A%84%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E4%B8%8EHQL%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[为什么需要presto由于基于 Hive 的 HQL 是把 SQL 转换成 MapReduce 任务来完成的，通常因为作业提交和作业调度需要大量的开销，所以 HiveQL 查询的延迟极高。 presto为什么快 与 Hive 的批处理不同，Presto 主要用于低频的，随机的，交互式查询。 语法与 ANSI SQL 极为相似，但略有不同。 Presto 的实现和 Hive 有着本质的不同。Hive 是把一个 query 转化成多个 stage 的 MapReduce 的任务，然后一个接一个执行。执行的中间结果通过对磁盘的读写来同步。Presto 没有使用 MapReduce，它是通过一个定制的 query 和执行引擎来完成的 Presto 所有的查询处理是 in memory 的，这也是它性能很高的一个主要原因。所以在日常使用中，如果有大量的 join 偶尔会发生内存不足的报错，一个常见的解决方法是生成中间表的方式来减少 join 的次数。 Presto 支持多种存储系统和文件结构。存储系统的支持包括 HDFS，HBase，Scribe 等。文件结构的支持包括 Jason，Avro，Parquet，RCFile，ORCFile 等。Presto 对非结构化的数据查询效率最低，对 RCFile 或 ORCFile 的查询性能最高： Presto实现原理和美团的使用实践https://tech.meituan.com/presto.html 与HQL语法的区别https://blog.csdn.net/cdyjy_litao/article/details/80693635 https://www.cnblogs.com/cssdongl/p/8394000.html https://blog.csdn.net/u012965373/article/details/83352896 使用注意1、This data is only top 500. 2、 1234select * from dwh_dm_bgrk.dm_bgrk_car_welcome_call_details where data_date=&apos;20181031&apos; order by data_date Query failed (#20181102_013813_00480_4yewd) in bigdata: Query exceeded local user memory limit of 1.60GB 3、一次只能执行一个SQL，也不能选中某条执行 日期计算1date_diff(&apos;day&apos;, date(last_date), date(&apos;2019-03-05&apos;))]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F10%2F28%2F%E7%AE%97%E6%B3%95%E8%83%8C%E5%90%8E%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86%2F%E7%A8%8B%E5%BA%8F%E5%91%98%E6%95%B0%E5%AD%A6%E4%B9%8B%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1%2FUntitled%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F10%2F24%2F%E6%80%BB%E7%BB%93%E4%B8%8E%E6%80%9D%E8%80%83%2F%E6%BC%94%E8%AE%B2%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[先设一个强大的敌人]]></content>
  </entry>
  <entry>
    <title><![CDATA[Python多进程写入同一文件]]></title>
    <url>%2F2018%2F10%2F24%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FPYTHON%2FPython%E5%A4%9A%E8%BF%9B%E7%A8%8B%E5%86%99%E5%85%A5%E5%90%8C%E4%B8%80%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[https://blog.csdn.net/Q_AN1314/article/details/51923022 最近用python的正则表达式处理了一些文本数据，需要把结果写到文件里面，但是由于文件比较大，所以运行起来花费的时间很长。但是打开任务管理器发现CPU只占用了25%，上网找了一下原因发现是由于一个叫GIL的存在，使得Python在同一时间只能运行一个线程，所以只占用了一个CPU，由于我的电脑是4核的，所以CPU利用率就是25%了。 既然多线程没有什么用处，那就可以使用多进程来处理，毕竟多进程是可以不受GIL影响的。Python提供了一个multiprocessing的多进程库，但是多进程也有一些问题，比如，如果进程都需要写入同一个文件，那么就会出现多个进程争用资源的问题，如果不解决，那就会使文件的内容顺序杂乱。这就需要涉及到锁了，但是加锁一般会造成程序的执行速度下降，而且如果进程在多处需要向文件输出，也不好把这些代码整个都锁起来，如果都锁起来，那跟单进程还有什么区别。有一个解决办法就是把向文件的输出都整合到一块去，在这一块集中加个锁，这样问题就不大了。不过还有一种更加优雅的解决方式：使用multiprocessing库的回调函数功能。 具体思路跟把文件输出集中在一起也差不多，就是把进程需要写入文件的内容作为返回值返回给惠和的回调函数，使用回调函数向文件中写入内容。这样做在windows下面还有一个好处，在windows环境下，python的多进程没有像linux环境下的多进程一样，linux环境下的multiprocessing库是基于fork函数，父进程fork了一个子进程之后会把自己的资源，比如文件句柄都传递给子进程。但是在windows环境下没有fork函数，所以如果你在父进程里打开了一个文件，在子进程中写入，会出现ValueError: I/O operation on closed file这样的错误，而且在windows环境下最好加入if name == ‘main‘这样的判断，以避免一些可能出现的RuntimeError或者死锁。 下面是代码： 123456789101112131415161718192021222324from multiprocessing import Poolimport timedef mycallback(x): with open(&apos;123.txt&apos;, &apos;a+&apos;) as f: f.writelines(str(x))def sayHi(num): return numif __name__ == &apos;__main__&apos;: e1 = time.time() pool = Pool() for i in range(10): pool.apply_async(sayHi, (i,), callback=mycallback) pool.close() pool.join() e2 = time.time() print float(e2 - e1)]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python http请求和响应]]></title>
    <url>%2F2018%2F10%2F22%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FPYTHON%2Fhttp%E8%AF%B7%E6%B1%82%E5%92%8C%E5%93%8D%E5%BA%94%2F</url>
    <content type="text"><![CDATA[python2https://www.cnblogs.com/poerli/p/6429673.html python发送post和get请求 get请求： 使用get方式时，请求数据直接放在url中。方法一、import urllibimport urllib2 url = “http://192.168.81.16/cgi-bin/python_test/test.py?ServiceCode=aaaa“ req = urllib2.Request(url)print req res_data = urllib2.urlopen(req)res = res_data.read()print res 方法二、import httplib url = “http://192.168.81.16/cgi-bin/python_test/test.py?ServiceCode=aaaa“ conn = httplib.HTTPConnection(“192.168.81.16”)conn.request(method=”GET”,url=url) response = conn.getresponse()res= response.read()print res post请求： 使用post方式时，数据放在data或者body中，不能放在url中，放在url中将被忽略。 1234567891011121314import urllibimport urllib2test_data = &#123;&apos;ServiceCode&apos;:&apos;aaaa&apos;,&apos;b&apos;:&apos;bbbbb&apos;&#125;test_data_urlencode = urllib.urlencode(test_data)requrl = &quot;http://192.168.81.16/cgi-bin/python_test/test.py&quot;req = urllib2.Request(url = requrl,data =test_data_urlencode)print reqres_data = urllib2.urlopen(req)res = res_data.read()print res 如果需要post的是json 12345678910111213141516171819import urllibimport urllib2import jsontest_data = &#123; &apos;callLetter&apos;:&apos;1064854207232&apos;, &apos;flag&apos;:&apos;false&apos;, &apos;sign&apos;:&apos;335BB919C5476417E424FF6F0BC5AD6F&apos;&#125;requrl = &quot;http://218.17.3.228:8008/mljrserver/vehicle/queryYXGpsInfo&quot;headers = &#123;&apos;Content-Type&apos;: &apos;application/json&apos;&#125;req = urllib2.Request(url = requrl,headers=headers,data =json.dumps(test_data))print reqres_data = urllib2.urlopen(req)res = res_data.read()print res 带cookie访问1234567891011121314151617181920212223242526272829url = &quot;http:///otherLogin&quot;payload = &quot;&#123;\&quot;appCode\&quot;:\&quot;gp&#125;&quot;headers = &#123; &apos;appcode&apos;: &quot;gps&quot;, &apos;content-type&apos;: &quot;application/json;charset=UTF-8&quot;, &apos;cache-control&apos;: &quot;no-cache&quot;, &apos;postman-token&apos;: &quot;32774965-91b4-a8b7-34ed-02c9a5810c5e&quot; &#125;# 用sessionsessions = requests.session()# req = requests.request(&quot;POST&quot;, url, data=payload, headers=headers)sessions.post(url, headers = headers, data = payload)url2 = &quot;http:///AnomalyMetricInfo&quot;headers = &#123; &apos;accept&apos;: &quot;application/json, text/plain, */*&quot;, &apos;dnt&apos;: &quot;1&quot;, &apos;user-agent&apos;: &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.108 Safari/537.36&quot;, &apos;token&apos;: &quot;2edd923582334485bf57b9c5ab747d5a&quot;, &apos;appcode&apos;: &quot;gps&quot;, &apos;cache-control&apos;: &quot;no-cache&quot;, &apos;postman-token&apos;: &quot;57f1b022-55cb-8c30-f49d-2d16d61fea1b&quot; &#125;response = sessions.get(url2, headers = headers)print(response.text)]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F10%2F21%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2Ftensorflow%2F%E5%9F%BA%E7%A1%80-%E8%8E%AB%E7%83%A6%E6%95%99%E7%A8%8B%2F2-4%20Variable%20%E5%8F%98%E9%87%8F%2F</url>
    <content type="text"><![CDATA[https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/2-4-variable/ 简单运用这节课我们学习如何在 Tensorflow 中使用 Variable . 在 Tensorflow 中，定义了某字符串是变量，它才是变量，这一点是与 Python 所不同的。 定义语法： state = tf.Variable() 123456789101112import tensorflow as tfstate = tf.Variable(0, name=&apos;counter&apos;)# 定义常量 oneone = tf.constant(1)# 定义加法步骤 (注: 此步并没有直接计算)new_value = tf.add(state, one)# 将 State 更新成 new_valueupdate = tf.assign(state, new_value) 如果你在 Tensorflow 中设定了变量，那么初始化变量是最重要的！！所以定义了变量以后, 一定要定义 init = tf.initialize_all_variables() . 到这里变量还是没有被激活，需要再在 sess 里, sess.run(init) , 激活 init 这一步. 12345678910# 如果定义 Variable, 就一定要 initialize# init = tf.initialize_all_variables() # tf 马上就要废弃这种写法init = tf.global_variables_initializer() # 替换成这样就好 # 使用 Sessionwith tf.Session() as sess: sess.run(init) for _ in range(3): sess.run(update) print(sess.run(state)) 注意：直接 print(state) 不起作用！！ 一定要把 sess 的指针指向 state 再进行 print 才能得到想要的结果！ 以上就是我们今天所学的 Variable 打开模式，欢迎继续学习下一章 ———— Tensorflow 中的 Placeholder。]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F10%2F21%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2Ftensorflow%2F%E5%9F%BA%E7%A1%80-%E8%8E%AB%E7%83%A6%E6%95%99%E7%A8%8B%2F2-3%20session%E4%BC%9A%E8%AF%9D%E6%8E%A7%E5%88%B6%2F</url>
    <content type="text"><![CDATA[https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/2-3-session/ 简单运用欢迎回来！这一次我们会讲到 Tensorflow 中的 Session, Session 是 Tensorflow 为了控制,和输出文件的执行的语句. 运行 session.run() 可以获得你要得知的运算结果, 或者是你所要运算的部分. 首先，我们这次需要加载 Tensorflow ，然后建立两个 matrix ,输出两个 matrix 矩阵相乘的结果。 12345678import tensorflow as tf# create two matrixesmatrix1 = tf.constant([[3,3]])matrix2 = tf.constant([[2], [2]])product = tf.matmul(matrix1,matrix2) 因为 product 不是直接计算的步骤, 所以我们会要使用 Session 来激活 product 并得到计算结果. 有两种形式使用会话控制 Session 。 123456789101112# method 1sess = tf.Session()result = sess.run(product)print(result)sess.close()# [[12]]# method 2with tf.Session() as sess: result2 = sess.run(product) print(result2)# [[12]] 以上就是我们今天所学的两种 Session 打开模式，欢迎继续学习下一章 ———— Tensorflow 中的 Variable。]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F10%2F21%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2Ftensorflow%2F%E5%9F%BA%E7%A1%80-%E8%8E%AB%E7%83%A6%E6%95%99%E7%A8%8B%2F2-1%20%E5%9F%BA%E7%A1%80%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/2-2-example2/ 数据流图结构https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/2-1-structure/ 采用数据流图(Data flow graph)计算。得创建一个数据流流图, 然后再将我们的数据（数据以张量(tensor)的形式存在）放在数据流图中计算. 节点Node：数学操作 线edges：节点间相互联系的多维数据数组, 即张量（tensor)。 训练模型时tensor会不断的从数据流图中的一个节点flow到另一节点, 这就是TensorFlow名字的由来. 张量的意义 张量有多种. 零阶张量为 纯量或标量 (scalar) 也就是一个数值. 比如 [1] 一阶张量为 向量 (vector), 比如 一维的 [1, 2, 3]，一行三列 二阶张量为 矩阵 (matrix), 比如 二维的 [[1, 2, 3],[4, 5, 6],[7, 8, 9]] 以此类推, 还有 三阶 三维的 … 创建数据首先, 我们这次需要加载 tensorflow 和 numpy 两个模块, 并且使用 numpy 来创建我们的数据. 123456import tensorflow as tfimport numpy as np# create datax_data = np.random.rand(100).astype(np.float32)y_data = x_data*0.1 + 0.3 接着, 我们用 tf.Variable 来创建描述 y 的参数. 我们可以把 y_data = x_data*0.1 + 0.3 想象成 y=Weights * x + biases, 然后神经网络也就是学着把 Weights 变成 0.1, biases 变成 0.3. 搭建模型1234Weights = tf.Variable(tf.random_uniform([1], -1.0, 1.0))biases = tf.Variable(tf.zeros([1]))y = Weights*x_data + biases 计算误差接着就是计算 y 和 y_data 的误差: 最小二乘误差 1loss = tf.reduce_mean(tf.square(y-y_data)) 传播误差反向传递误差的工作就教给optimizer了, 我们使用的误差传递方法是梯度下降法: Gradient Descent 让后我们使用 optimizer 来进行参数的更新. 12optimizer = tf.train.GradientDescentOptimizer(0.5)train = optimizer.minimize(loss) 训练到目前为止, 我们只是建立了神经网络的结构, 还没有使用这个结构. 在使用这个结构之前, 我们必须先初始化所有之前定义的Variable, 所以这一步是很重要的! 12# init = tf.initialize_all_variables() # tf 马上就要废弃这种写法init = tf.global_variables_initializer() # 替换成这样就好 接着,我们再创建会话 Session. 我们会在下一节中详细讲解 Session. 我们用 Session 来执行 init 初始化步骤. 并且, 用 Session 来 run 每一次 training 的数据. 逐步提升神经网络的预测准确性. 1234567sess = tf.Session()sess.run(init) # Very importantfor step in range(201): sess.run(train) if step % 20 == 0: print(step, sess.run(Weights), sess.run(biases))]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F10%2F21%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2Ftensorflow%2F%E5%9F%BA%E7%A1%80-%E8%8E%AB%E7%83%A6%E6%95%99%E7%A8%8B%2FRNN%E4%BE%8B%E5%AD%90%2F</url>
    <content type="text"><![CDATA[tensorflow的例子合集 https://github.com/MorvanZhou/Tensorflow-Tutorial]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F10%2F21%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2FKeras%2F1%E3%80%81%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[12345# 如果你是 python 2+ 版本, 复制下面pip install keras# 如果你是 python 3+ 版本, 复制下面pip3 install keras BackendBackend也就是Keras基于什么东西来做运算。Keras 可以基于两个Backend，一个是 Theano，一个是 Tensorflow。如果我们选择Theano作为Keras的Backend， 那么Keras就用 Theano 在底层搭建你需要的神经网络；同样，如果选择 Tensorflow 的话呢，Keras 就使用 Tensorflow 在底层搭建神经网络。 查看当前backend1import keras 显示Using TensorFlow backend. 但是因为目前没有安装tensorflow，因此提示找不到tensorflow。 修改backend1vim ~/.keras/keras.json 这样修改后，import 的时候会出现错误信息。 解决的方法有几种: 可以在其他文本编辑器内编辑好这段文本，然后整体拷贝到这个文件里。 还可以在terminal中直接输入临时环境变量执行 1234# python2+输入:KERAS_BACKEND=tensorflow python -c &quot;from keras import backend&quot;# python3+输入:KERAS_BACKEND=tensorflow python3 -c &quot;from keras import backend&quot; 最好的解决方法，还是在python代码中import keras前加入一个环境变量修改的语句： 12import osos.environ[&apos;KERAS_BACKEND&apos;]=&apos;theano&apos;]]></content>
  </entry>
  <entry>
    <title><![CDATA[【转】轨迹伴随挖掘]]></title>
    <url>%2F2018%2F10%2F05%2F%E5%9C%B0%E7%90%86%E4%BF%A1%E6%81%AF%E6%8C%96%E6%8E%98%2F%E8%BD%A8%E8%BF%B9%E4%BC%B4%E9%9A%8F%E6%8C%96%E6%8E%98%2F</url>
    <content type="text"><![CDATA[https://blog.csdn.net/sparkexpert/article/details/80844306 摘自ICDM2013年论文Mining Following Relationships in Movement Data的表述： 挑战一：伴随的时间滞后性不固定且经常变化； 挑战二：伴随者的轨迹不一定与前者完全一致； 挑战三：伴随关系可能发生在较短的时间范围内； 在上面的论文中，提出一种LSA的伴随分析算法，其原理如下面两图所示： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253function [interval,j_min_set] = find_following(seqA, seqB, d_max, l_max)%% FIND_FOLLOWING Finds following intervals that seqB is following seqA% INTERVAL = FIND_FOLLOWING(SEQA,SEQB,D_MAX,L_MAX)% SEQA and SEQB are d X n trajectories, where d is the dimension% of corrdinates and n is the trajectory length.% D_MAX is the distance threshold.% L_MAX is the time threshold.% The result is in INTERVAL, where each row is one following interval.%% [INTERVAL J_MIN_SET] = FIND_FOLLOWING(SEQA,SEQB,D_MAX,L_MAX) also% returns time lag set J_MIN_SET% % Euclidean distance is used. n = length(seqA);match = zeros(1,n);valid = zeros(1,n);j_min_set = zeros(1,n);dist_min_set = zeros(1,n);for i=1:n dist_min = 1e6; j_min = -1; for j=max(1, i-l_max):min(n, i+l_max) dist = norm(seqB(:,i) - seqA(:,j),2); % Euclidean distance if (dist &lt; dist_min) j_min = j; dist_min = dist; end; end; dist_min_set(i) = dist_min; if dist_min &lt; d_max valid(i) = 1; if (j_min &lt; i) dist_min2 = 1e6; k_min = -1; for k=max(1, j_min-l_max):min(n, j_min+l_max) dist2 = norm(seqB(:,k) - seqA(:,j_min),2); % Euclidean distance if dist2 &lt; dist_min2 k_min = k; dist_min2 = dist2; end end if k_min &gt; j_min match(i) = 1; else match(i) = 0; end else match(i) = -1; end; j_min_set(i) = j_min - i; end;end;]]></content>
      <categories>
        <category>地理信息处理和挖掘</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F09%2F24%2F%E6%80%BB%E7%BB%93%E4%B8%8E%E6%80%9D%E8%80%83%2F%E5%BE%AE%E6%9D%83%E5%8A%9B%E4%B8%8B%E7%9A%84%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[项目经理必备的素质— 大局观。从企业角度思考问题 — 对目标执着。目标不清晰时会本能的明确目标，分解和传递目标给团队，始终围绕目标展开工作。 — 换位思考。 — 应变能力。遇到突然事件，迅速评估各种状况。 — 平衡能力。识别各个利益方的动机和分歧点，并能帮助他们达成共识。 — 仆人式领导。放低姿态，采用服务他人的态度和方式。 技术转型管理时的几个难点关注点：做事 -&gt; 做人 角色：演员 -&gt; 导演 知识范畴：技术 -&gt; 技术+业务 思考方式：点（专注） -&gt; 面（大局观） 项目是否具备投资价值的三个维度1、项目是值得做的 衡量收益 2、有能力交付 3、有能力实现收益 项目成果能否转化？ 项目有三个关键利益方： 1、代表项目投资人的项目发起方 2、代表项目交付能力的项目实施方 3、代表项目收益实现的项目成果使用方。 项目业务可行性分析报告包括内容 1、内容提要 2、项目产生的原因 3、备选业务方案和建议 4、预期项目收益 5、可能的项目负收益 6、项目收益实现的时间 7、实现项目收益的成本 8、项目的投入产出分析 9、项目收益实现的主要风险 10、项目的关键成功要素 — 可以更好帮助企业高层管理团队明确项目需要高层支持什么。例如，如果希望在汇报时说服高层从资源或关注度上给予一些倾斜，就必须讲明白项目的关键成功要素与项目在稀缺资源的获取方面和高层领导的关注度之间的密切关系。 项目业务价值论证的生命周期要持续论证，在整个项目过程中，每隔一段时间论证一次。 常用的投资评估指标：ROI、净现值NPV、净收益、整个生命周期成本、投资回收期 避免哥伦布式管理“走的时候不知道去哪儿，到了的时候不知道自己在哪儿，回来之后不知道自己去过哪儿” 项目产品描述成为项目目标和范围的依据 包括： 名称、目的 项目产品的组成部分 各个组成部分的来源 所需的开发技能 客户的质量期望 验收标准 项目的质量容许偏差 验证方法 验证职责 里程碑的作用Milestone帮助行人判断自己走完路程的百分比。项目里程碑用来衡量项目工作完成的百分比。 认清风险项目中会泛泛而谈风险，如资源不足、时间不够、人员不配合、供应商不能按时交付等。这些风险所有项目都存在，这就是成立项目和任命项目经理的原因。成立项目团队的目的就是能够在很有有限的时间和资源内组织一群原本不配合的人去完成一个有挑战性的目标。 要把风险描述清楚，得从企业风险偏好讲起。不同企业对风险的认知和接受程度是不同的。 1、风险偏好定义：组织对承受风险的独特态度决定了它认为可承受风险的程度。 把企业氛围风险追逐者、厌恶者、中立者。 厌恶者：银行。有非常严格的体系，通过复杂的流程和监控进行管理，避免承受任何风险。 追逐者：互联网企业。赔钱赚吆喝，画大饼，拉风投，筹到钱后高薪组建团队，大量做营销。 2、风险描述三段论1）风险原因：风险驱动点。什么导致了风险的出现。 2）风险事件：描述一旦发生了，表现形式是什么。时间、地点、哪个环节、概率多大。 3）风险结果：对谁造成什么影响，程度有多严重。 风险原因的注意点： 1）不仅要描述风险根源，还要说明导致这个的责任人是谁。防止不能引起相关的关注。 2）有个原则，这个风险的原因必须要在企业内部找。 风险影响： 明确这个风险会影响哪个目标，是项目的产品工期、成果转化，还是收益实现。 如果是交付工期、交付质量、交付成本的影响，容易引起项目建设团队的重视。 如果是成果转化的影响，会引起项目使用方的关注 如果把影响和项目收益关联，引起企业领导关注。 如果与企业战略落地关联，会引起高层领导关注和帮助。 注意： 有项目经理把风险影响描述为“项目工期延迟、人员不配合、项目整体资源不足、客户对项目产品不满意或预算超支”。这些就是对项目经理的考核指标，如果这些状况发生，那就是项目经理自己的工作没做好。 问题出在，没有把风险影响进一步分解，分解到对更具体的事件的影响上。]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F09%2F24%2F%E6%80%BB%E7%BB%93%E4%B8%8E%E6%80%9D%E8%80%83%2F%E5%A6%82%E4%BD%95%E8%90%BD%E5%9C%B0%E9%9C%80%E6%B1%82%2F</url>
    <content type="text"><![CDATA[落地需求 确认需求：用自己的话描述一遍需求 分析需求：除了功能需求，是否有潜在需求，比如还需要UI等内容。 分析资源 需求的范围： 功能性需求 非功能性需求 DMA data model action]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F09%2F16%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FNLP%2F%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[词向量 自然语言理解的问题要转化为机器学习的问题，第一步肯定是要找一种方法把这些符号数学化。 NLP 中最直观的词表示方法是 One-hot Representation，这种方法把每个词表示为一个很长的向量。这个向量的维度是词表大小，其中绝大多数元素为 0，只有一个维度的值为 1，这个维度就代表了当前的词。 这么简洁的表示方法配合上最大熵、SVM、CRF 等等算法已经很好地完成了 NLP 领域的各种主流任务。 one-hot的缺点是：“词汇鸿沟”现象：任意两个词之间都是孤立的。光从这两个向量中看不出两个词是否有关系。 Deep Learning的词向量不是用one-hot，而是Distributed Representation。这个就是词向量 可以认为，该模型最大的贡献就是解决了词汇鸿沟现象，可以让相关的词在向量空间上更接近。 Distributed representation 最早是 Hinton 在 1986 年的论文《Learning distributed representations of concepts》中提出的。 Distributed representation 用来表示词，通常被称为“Word Representation”或“Word Embedding”，中文俗称“词向量”。周志华叫他词嵌入 词向量的训练获得要介绍词向量是怎么训练得到的，就不得不提到语言模型。到目前为止我了解到的所有训练方法都是在训练语言模型的同时，顺便得到词向量的。 词向量的训练最经典的有 3 个工作，C&amp;W 2008、M&amp;H 2008、Mikolov 2010。 Bengio 的经典之作用神经网络训练语言模型的思想最早由百度 IDL 的徐伟于 2000 提出。其论文《Can Artificial Neural Networks Learn Language Models?》提出一种用神经网络构建二元语言模型（即 $P(wt|w{t-1})$ ）的方法。文中的基本思路与后续的语言模型的差别已经不大了。 训练语言模型的最经典之作，要数 Bengio 等人在 2001 年发表在 NIPS 上的文章《A Neural Probabilistic Language Model》。当然现在看的话，肯定是要看他在 2003 年投到 JMLR 上的同名论文了。 Bengio 用了一个三层的神经网络来构建语言模型，同样也是 n-gram 模型。 图中最下方的 wt−n+1,…,wt−2,wt−1wt−n+1,…,wt−2,wt−1 就是前 n−1n−1 个词。现在需要根据这已知的 n−1n−1 个词预测下一个词 wtwt。C(w)C(w) 表示词 ww 所对应的词向量，整个模型中使用的是一套唯一的词向量，存在矩阵 CC（一个 |V|×m|V|×m 的矩阵）中。其中 |V||V| 表示词表的大小（语料中的总词数），mm 表示词向量的维度。ww 到 C(w)C(w) 的转化就是从矩阵中取出一行。 网络的第一层（输入层）是将 C(wt−n+1),…,C(wt−2),C(wt−1)C(wt−n+1),…,C(wt−2),C(wt−1) 这 n−1n−1 个向量首尾相接拼起来，形成一个 (n−1)m(n−1)m 维的向量，下面记为 xx。 网络的第二层（隐藏层）就如同普通的神经网络，直接使用 d+Hxd+Hx 计算得到。dd 是一个偏置项。在此之后，使用 tanhtanh 作为激活函数。 网络的第三层（输出层）一共有 |V||V| 个节点，每个节点 yiyi 表示 下一个词为 ii 的未归一化 log 概率。最后使用 softmax 激活函数将输出值 yy 归一化成概率。 C&amp;W 的 SENNARonan Collobert 和 Jason Weston 在 2008 年的 ICML 上发表的《A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning》里面首次介绍了他们提出的词向量的计算方法。 如果现在要看的话，应该去看他们在 2011 年投到 JMLR 上的论文《Natural Language Processing (Almost) from Scratch》。 实际上 C&amp;W 这篇论文主要目的并不是在于生成一份好的词向量，甚至不想训练语言模型，而是要用这份词向量去完成 NLP 里面的各种任务，比如词性标注、命名实体识别、短语识别、语义角色标注等等。 由于目的的不同，C&amp;W 的词向量训练方法在我看来也是最特别的。他们没有去近似地求 P(wt|w1,w2,…,wt−1)P(wt|w1,w2,…,wt−1)，而是直接去尝试近似 P(w1,w2,…,wt)P(w1,w2,…,wt)。在实际操作中，他们并没有去求一个字符串的概率，而是求窗口连续 n 个词的打分 f(wt−n+1,…,wt−1,wt)f(wt−n+1,…,wt−1,wt)。打分 ff 越高的说明这句话越是正常的话；打分低的说明这句话不是太合理；如果是随机把几个词堆积在一起，那肯定是负分（差评）。打分只有相对高低之分，并没有概率的特性。 有了这个对 ff的假设，C&amp;W 就直接使用 pair-wise 的方法训练词向量。具体的来说，就是最小化下面的目标函数。 \sum\limits_{x\in \mathfrak{X}} { \sum\limits_{w\in \mathfrak{D}} {\max \{0 , 1-f(x)+f(x^{(w)})\} } }x(w)x(w) 是将短语 xx 的最中间的那个词，替换成 ww。在大多数情况下，在一个正常短语的基础上随便找个词替换掉中间的词，最后得到的短语肯定不是正确的短语，所以这样构造的负样本是非常可用的（多数情况下确实是负样本，极少数情况下把正常短语当作负样本也不影响大局）。同时，由于负样本仅仅是修改了正样本中的一个词，也不会让分类面距离负样本太远而影响分类效果。再回顾这个式子，xx 是正样本，x(w)x(w) 是负样本，f(x)f(x) 是对正样本的打分，f(x(w))是对负样本的打分。最后希望正样本的打分要比负样本的打分至少高 1 分。 f函数的结构与Bengio的网络结构基本一致，同样是把n个词对应的词向量串成一个长向量，同样是经过一层网络（乘一个矩阵）得到隐藏层。不同在于C&amp;W的输出层只有一个节点，表示得分。当然有这种简化还是因为 C&amp;W 并不想做一个真正的语言模型，只是借用语言模型的思想辅助他完成 NLP 的其它任务。 因为动机不同，他公布的词向量与其他相比的主要区别是： 1、只有小写单词。也就是说他把大写开头的单词和小写单词当作同一个词处理。其它的词向量都是把他们当作不同的词处理的。 2.他公布的词向量并不直接是上述公式的优化结果，而是在此基础上进一步跑了词性标注、命名实体识别等等一系列任务的 Multi-Task Learning 之后，二次优化得到的。也可以理解为是半监督学习得到的，而非其他方法中纯无监督学习得到的。 不过好在 Turian 在 2010 年对 C&amp;W 和 M&amp;H 向量做对比时，重新训练了一份词向量放到了网上，那份就没上面的两个“问题”（确切的说应该是差别），也可以用的更放心。 M&amp;H 的 HLBLAndriy Mnih 和 Geoffrey Hinton 在 2007 年和 2008 年各发表了一篇关于训练语言模型和词向量的文章。2007 年发表在 ICML 上的《Three new graphical models for statistical language modelling》表明了 Hinton 将 Deep Learning 战场扩展到 NLP 领域的决心。2008 年发表在 NIPS 上的《A scalable hierarchical distributed language model》则提出了一种层级的思想替换了 Bengio 2003 方法中最后隐藏层到输出层最花时间的矩阵乘法，在保证效果的基础上，同时也提升了速度。 Mikolov 的 RNNLMmikolov就是word2vec的发明人。前文说到，Bengio 2003 论文里提了一句，可以使用一些方法降低参数个数，比如用循环神经网络。Mikolov 就抓住了这个坑，从此与循环神经网络结下了不解之缘。 他最早用循环神经网络做语言模型是在 INTERSPEECH 2010 上发表的《Recurrent neural network based language model》里。 三篇关于word2vec的论文 1、Efficient Estimation of Word Representation in Vector Space, 2013 传统的NNLM模型包含四层，即输入层、映射层、隐含层和输出层，计算复杂度很大程度上依赖于映射层到隐含层之间的计算，而且需要指定上下文的长度。RNNLM模型被提出用来改进NNLM模型，去掉了映射层，只有输入层、隐含层和输出层，计算复杂度来源于上一层的隐含层到下一层隐含层之间的计算。 传统的NNLM模型包含四层，即输入层、映射层、隐含层和输出层，计算复杂度很大程度上依赖于映射层到隐含层之间的计算，而且需要指定上下文的长度。RNNLM模型被提出用来改进NNLM模型，去掉了映射层，只有输入层、隐含层和输出层，计算复杂度来源于上一层的隐含层到下一层隐含层之间的计算。 2、Distributed Representations of Sentences and Documents, 2014 句向量以及段落向量如何表示： 句向量：利用one-hot的表示方法作为网络的输入，乘以词矩阵W，然后将得到的每个向量通过平均或者拼接的方法得到整个句子的表示，最后根据任务要求做一分类，而这过程中得到的W就是词向量矩阵，基本上还是word2vec的思路。 段落向量：依旧是相同的方法，只是在这里加上了一个段落矩阵，用以表示每个段落，当这些词输入第i个段落时，通过段落id就可以从这个矩阵中得到相对应的段落表示方法。需要说明的是，在相同的段落中，段落的表示是相同的。文中这样表示的动机就是段落矩阵D可以作为一个memory记住在词的context中遗失的东西，相当于增加了一个额外的信息。这样经过训练之后，我们的就得到了段落表示D，当然这个段落就可以是一段或者一篇文章。 3、Enriching Word Vectors with Subword Information, 2016 问题：如何解决word2vec方法中罕见词效果不佳的问题，以及如何提升词形态丰富语言的性能？ 如果一个word出现次数较少那么学到的vector质量也不理想。针对这一问题作者提出使用subword信息来弥补这一问题，简单来说就是通过词缀的vector来表示词。比如unofficial是个低频词，其数据量不足以训练出高质量的vector，但是可以通过un+official这两个高频的词缀学习到不错的vector。 参考 https://blog.csdn.net/sinat_26917383/article/details/52577551 https://licstar.net/archives/328]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F09%2F16%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FNLP%2F%E7%89%9B%E6%B4%A5%E5%A4%A7%E5%AD%A6xDeepMind%20%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%2F%E7%AC%AC%E4%B8%80%E8%AF%BE%2F</url>
    <content type="text"><![CDATA[课程英文资料https://github.com/oxford-cs-deepnlp-2017/lectures 视频https://study.163.com/course/courseMain.htm?courseId=1004336028 概念对照discrete variables 离散变量]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F08%2F29%2Fhadoop-spark%2Fpython%E6%93%8D%E4%BD%9Chdfs%2F</url>
    <content type="text"><![CDATA[https://www.cnblogs.com/Jims2016/p/8047914.html 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970#!coding:utf-8import sys from hdfs.client import Client#设置utf-8模式reload(sys)sys.setdefaultencoding( "utf-8" )#关于python操作hdfs的API可以查看官网:#https://hdfscli.readthedocs.io/en/latest/api.html #读取hdfs文件内容,将每行存入数组返回def read_hdfs_file(client,filename): #with client.read('samples.csv', encoding='utf-8', delimiter='\n') as reader: # for line in reader: #pass lines = [] with client.read(filename, encoding='utf-8', delimiter='\n') as reader: for line in reader: #pass #print line.strip() lines.append(line.strip()) return lines #创建目录def mkdirs(client,hdfs_path) : client.makedirs(hdfs_path) #删除hdfs文件def delete_hdfs_file(client,hdfs_path): client.delete(hdfs_path) #上传文件到hdfsdef put_to_hdfs(client,local_path,hdfs_path): client.upload(hdfs_path, local_path,cleanup=True) #从hdfs获取文件到本地def get_from_hdfs(client,hdfs_path,local_path): download(hdfs_path, local_path, overwrite=False)#追加数据到hdfs文件 def append_to_hdfs(client,hdfs_path,data): client.write(hdfs_path, data,overwrite=False,append=True) #覆盖数据写到hdfs文件def write_to_hdfs(client,hdfs_path,data): client.write(hdfs_path, data,overwrite=True,append=False) #移动或者修改文件 def move_or_rename(client,hdfs_src_path, hdfs_dst_path): client.rename(hdfs_src_path, hdfs_dst_path) #返回目录下的文件def list(client,hdfs_path): return client.list(hdfs_path, status=False) #client = Client(url, root=None, proxy=None, timeout=None, session=None) #client = Client("http://hadoop:50070") #move_or_rename(client,'/input/2.csv', '/input/emp.csv')#read_hdfs_file(client,'/input/emp.csv')#put_to_hdfs(client,'/home/shutong/hdfs/1.csv','/input/')#append_to_hdfs(client,'/input/emp.csv','我爱你'+'\n')#write_to_hdfs(client,'/input/emp.csv','我爱你'+'\n')#read_hdfs_file(client,'/input/emp.csv')#move_or_rename(client,'/input/emp.csv', '/input/2.csv')#mkdirs(client,'/input/python')#print list(client,'/input/')#chown(client,'/input/1.csv', 'root') 12345# 若路径已存在，则删除再创建is_path_exist = client.status(hdfs_path=hdfs_path,strict=False)if is_path_exist != None: delete_hdfs_file(client, hdfs_path)mkdirs(client, hdfs_path)]]></content>
  </entry>
  <entry>
    <title><![CDATA[Excel技巧]]></title>
    <url>%2F2018%2F08%2F27%2F%E5%B7%A5%E5%85%B7%E5%92%8C%E7%8E%AF%E5%A2%83%2Fexcel%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[使用Excel求两列的交集https://jingyan.baidu.com/article/c1a3101ea6c0afde656debca.html 第一步，选择一个空的单元格。图中的A、B列是原始数据列，C列是选择的单元格列，又来存储结果。我们利用公式“=IF(COUNTIF(B:B,A2)&gt;0,A2,0)”来求交集，即查找“A2”在B列里面有吗？如果有，则把C列的单元格设置为A2的值，如果不是则设为0（0是可以选择的）。 第二步，扩展单元格。当我们在C2里面设置好公式之后，把鼠标放在C2单元格右下角，回出现一个“+”符号，然后按住鼠标下拉，会把同样的公式应用到C列。可以快速的求A、B列的交集。如果数据记录太多，用鼠标不方便，可以在出现“+”符号的地方双击左键回自动应用。 第三步，筛选。当我们做好上面的处理之后，C列会有0和非0两个值（如果交集结果有0，那就再选一个数字作为处理结果）。我们选择C1单元格，然后点击筛选。然后C1会变成筛选项。 第四步，设置筛选条件。我们点击C1的下拉栏，设置筛选条件。由于不好截图，只能文字描述下了。点击下拉栏，会出现条件选项，如果不符合你的要求，就选择自定义，如图所示。 如果检验B1在A列中是否存在： 1=IF(COUNTIF(A:A,B1)&gt;0,B1,0)]]></content>
      <categories>
        <category>工具和环境</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F08%2F24%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E7%BB%BC%E5%90%88%E6%80%A7%E8%AE%B2%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[图解机器学习 http://www.r2d3.us/%E5%9B%BE%E8%A7%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F08%2F23%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%AF%94%E8%B5%9B%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[微额借款用户人品预测大赛 http://www.dcjingsai.com/common/bbs/topicDetails.html?tid=348]]></content>
  </entry>
  <entry>
    <title><![CDATA[【转】GeoScala]]></title>
    <url>%2F2018%2F08%2F21%2F%E5%9C%B0%E7%90%86%E4%BF%A1%E6%81%AF%E6%8C%96%E6%8E%98%2Fscala%E5%B7%A5%E5%85%B7%E5%8C%85%2F</url>
    <content type="text"><![CDATA[https://zhuanlan.zhihu.com/p/40268852?utm_source=com.alibaba.android.rimet&amp;utm_medium=social&amp;utm_oi=27309297893376 GeoScala-GPS数据挖掘前处理的一站式解决方案]]></content>
      <categories>
        <category>地理信息处理和挖掘</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F08%2F16%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E5%A6%82%E4%BD%95%E5%9C%A8Kaggle%E9%A6%96%E6%88%98%E4%B8%AD%E8%BF%9B%E5%85%A5%E5%89%8D10%2F</url>
    <content type="text"><![CDATA[https://dnc1994.com/2016/04/rank-10-percent-in-first-kaggle-competition/ 如何在 Kaggle 首战中进入前 10% Posted on 2016-04-29 | In Data Science | 53 Comments Reading time ≈ 28Introduction本文采用署名 - 非商业性使用 - 禁止演绎 3.0 中国大陆许可协议进行许可。著作权由章凌豪所有。Kaggle 是目前最大的 Data Scientist 聚集地。很多公司会拿出自家的数据并提供奖金，在 Kaggle 上组织数据竞赛。我最近完成了第一次比赛，在 2125 个参赛队伍中排名第 98 位（~ 5%）。因为是第一次参赛，所以对这个成绩我已经很满意了。在 Kaggle 上一次比赛的结果除了排名以外，还会显示的就是 Prize Winner，10% 或是 25% 这三档。所以刚刚接触 Kaggle 的人很多都会以 25% 或是 10% 为目标。在本文中，我试图根据自己第一次比赛的经验和从其他 Kaggler 那里学到的知识，为刚刚听说 Kaggle 想要参赛的新手提供一些切实可行的冲刺 10% 的指导。本文的英文版见这里。Kaggler 绝大多数都是用 Python 和 R 这两门语言的。因为我主要使用 Python，所以本文提到的例子都会根据 Python 来。不过 R 的用户应该也能不费力地了解到工具背后的思想。首先简单介绍一些关于 Kaggle 比赛的知识：不同比赛有不同的任务，分类、回归、推荐、排序等。比赛开始后训练集和测试集就会开放下载。比赛通常持续 2 ~ 3 个月，每个队伍每天可以提交的次数有限，通常为 5 次。比赛结束前一周是一个 Deadline，在这之后不能再组队，也不能再新加入比赛。所以想要参加比赛请务必在这一 Deadline 之前有过至少一次有效的提交。一般情况下在提交后会立刻得到得分的反馈。不同比赛会采取不同的评分基准，可以在分数栏最上方看到使用的评分方法。反馈的分数是基于测试集的一部分计算的，剩下的另一部分会被用于计算最终的结果。所以最后排名会变动。LB 指的就是在 Leaderboard 得到的分数，由上，有 Public LB 和 Private LB 之分。自己做的 Cross Validation 得到的分数一般称为 CV 或是 Local CV。一般来说 CV 的结果比 LB 要可靠。新手可以从比赛的 Forum 和 Scripts 中找到许多有用的经验和洞见。不要吝啬提问，Kaggler 都很热情。那么就开始吧！P.S. 本文假设读者对 Machine Learning 的基本概念和常见模型已经有一定了解。 Enjoy Reading!General Approach在这一节中我会讲述一次 Kaggle 比赛的大致流程。Data Exploration在这一步要做的基本就是 EDA (Exploratory Data Analysis)，也就是对数据进行探索性的分析，从而为之后的处理和建模提供必要的结论。通常我们会用 pandas 来载入数据，并做一些简单的可视化来理解数据。Visualization通常来说 matplotlib 和 seaborn 提供的绘图功能就可以满足需求了。比较常用的图表有：查看目标变量的分布。当分布不平衡时，根据评分标准和具体模型的使用不同，可能会严重影响性能。对 Numerical Variable，可以用 Box Plot 来直观地查看它的分布。对于坐标类数据，可以用 Scatter Plot 来查看它们的分布趋势和是否有离群点的存在。对于分类问题，将数据根据 Label 的不同着不同的颜色绘制出来，这对 Feature 的构造很有帮助。绘制变量之间两两的分布和相关度图表。这里有一个在著名的 Iris 数据集上做了一系列可视化的例子，非常有启发性。Statistical Tests我们可以对数据进行一些统计上的测试来验证一些假设的显著性。虽然大部分情况下靠可视化就能得到比较明确的结论，但有一些定量结果总是更理想的。不过，在实际数据中经常会遇到非 i.i.d. 的分布。所以要注意测试类型的的选择和对显著性的解释。在某些比赛中，由于数据分布比较奇葩或是噪声过强，Public LB 的分数可能会跟 Local CV 的结果相去甚远。可以根据一些统计测试的结果来粗略地建立一个阈值，用来衡量一次分数的提高究竟是实质的提高还是由于数据的随机性导致的。Data Preprocessing大部分情况下，在构造 Feature 之前，我们需要对比赛提供的数据集进行一些处理。通常的步骤有：有时数据会分散在几个不同的文件中，需要 Join 起来。处理 Missing Data。处理 Outlier。必要时转换某些 Categorical Variable 的表示方式。有些 Float 变量可能是从未知的 Int 变量转换得到的，这个过程中发生精度损失会在数据中产生不必要的 Noise，即两个数值原本是相同的却在小数点后某一位开始有不同。这对 Model 可能会产生很负面的影响，需要设法去除或者减弱 Noise。这一部分的处理策略多半依赖于在前一步中探索数据集所得到的结论以及创建的可视化图表。在实践中，我建议使用 iPython Notebook 进行对数据的操作，并熟练掌握常用的 pandas 函数。这样做的好处是可以随时得到结果的反馈和进行修改，也方便跟其他人进行交流（在 Data Science 中 Reproducible Results 是很重要的)。下面给两个例子。Outlier这是经过 Scaling 的坐标数据。可以发现右上角存在一些离群点，去除以后分布比较正常。Dummy Variables对于 Categorical Variable，常用的做法就是 One-hot encoding。即对这一变量创建一组新的伪变量，对应其所有可能的取值。这些变量中只有这条数据对应的取值为 1，其他都为 0。如下，将原本有 7 种可能取值的 Weekdays 变量转换成 7 个 Dummy Variables。要注意，当变量可能取值的范围很大（比如一共有成百上千类）时，这种简单的方法就不太适用了。这时没有有一个普适的方法，但我会在下一小节描述其中一种。Feature Engineering有人总结 Kaggle 比赛是 “Feature 为主，调参和 Ensemble 为辅”，我觉得很有道理。Feature Engineering 能做到什么程度，取决于对数据领域的了解程度。比如在数据包含大量文本的比赛中，常用的 NLP 特征就是必须的。怎么构造有用的 Feature，是一个不断学习和提高的过程。一般来说，当一个变量从直觉上来说对所要完成的目标有帮助，就可以将其作为 Feature。至于它是否有效，最简单的方式就是通过图表来直观感受。比如：Feature Selection总的来说，我们应该生成尽量多的 Feature，相信 Model 能够挑出最有用的 Feature。但有时先做一遍 Feature Selection 也能带来一些好处：Feature 越少，训练越快。有些 Feature 之间可能存在线性关系，影响 Model 的性能。通过挑选出最重要的 Feature，可以将它们之间进行各种运算和操作的结果作为新的 Feature，可能带来意外的提高。Feature Selection 最实用的方法也就是看 Random Forest 训练完以后得到的 Feature Importance了。其他有一些更复杂的算法在理论上更加 Robust，但是缺乏实用高效的实现，比如这个。从原理上来讲，增加 Random Forest 中树的数量可以在一定程度上加强其对于 Noisy Data 的 Robustness。看 Feature Importance 对于某些数据经过脱敏处理的比赛尤其重要。这可以免得你浪费大把时间在琢磨一个不重要的变量的意义上。Feature Encoding这里用一个例子来说明在一些情况下 Raw Feature 可能需要经过一些转换才能起到比较好的效果。假设有一个 Categorical Variable 一共有几万个取值可能，那么创建 Dummy Variables 的方法就不可行了。这时一个比较好的方法是根据 Feature Importance 或是这些取值本身在数据中的出现频率，为最重要（比如说前 95% 的 Importance）那些取值（有很大可能只有几个或是十几个）创建 Dummy Variables，而所有其他取值都归到一个“其他”类里面。Model Selection准备好 Feature 以后，就可以开始选用一些常见的模型进行训练了。Kaggle 上最常用的模型基本都是基于树的模型：Gradient BoostingRandom ForestExtra Randomized Trees以下模型往往在性能上稍逊一筹，但是很适合作为 Ensemble 的 Base Model。这一点之后再详细解释。（当然，在跟图像有关的比赛中神经网络的重要性还是不能小觑的。）SVMLinear RegressionLogistic RegressionNeural Networks以上这些模型基本都可以通过 sklearn 来使用。当然，这里不能不提一下 Xgboost。Gradient Boosting 本身优秀的性能加上 Xgboost 高效的实现，使得它在 Kaggle 上广为使用。几乎每场比赛的获奖者都会用 Xgboost 作为最终 Model 的重要组成部分。在实战中，我们往往会以 Xgboost 为主来建立我们的模型并且验证 Feature 的有效性。顺带一提，在 Windows 上安装 Xgboost 很容易遇到问题，目前已知最简单、成功率最高的方案可以参考我在这篇帖子中的描述。Model Training在训练时，我们主要希望通过调整参数来得到一个性能不错的模型。一个模型往往有很多参数，但其中比较重要的一般不会太多。比如对 sklearn 的 RandomForestClassifier 来说，比较重要的就是随机森林中树的数量 n_estimators 以及在训练每棵树时最多选择的特征数量 max_features。所以我们需要对自己使用的模型有足够的了解，知道每个参数对性能的影响是怎样的。通常我们会通过一个叫做 Grid Search) 的过程来确定一组最佳的参数。其实这个过程说白了就是根据给定的参数候选对所有的组合进行暴力搜索。param_grid = {&#39;n_estimators&#39;: [300, 500], &#39;max_features&#39;: [10, 12, 14]}model = grid_search.GridSearchCV(estimator=rfr, param_grid=param_grid, n_jobs=1, cv=10, verbose=20, scoring=RMSE)model.fit(X_train, y_train)顺带一提，Random Forest 一般在 max_features 设为 Feature 数量的平方根附近得到最佳结果。这里要重点讲一下 Xgboost 的调参。通常认为对它性能影响较大的参数有：eta：每次迭代完成后更新权重时的步长。越小训练越慢。num_round：总共迭代的次数。subsample：训练每棵树时用来训练的数据占全部的比例。用于防止 Overfitting。colsample_bytree：训练每棵树时用来训练的特征的比例，类似 RandomForestClassifier 的 max_features。max_depth：每棵树的最大深度限制。与 Random Forest 不同，Gradient Boosting 如果不对深度加以限制，最终是会 Overfit 的。early_stopping_rounds：用于控制在 Out Of Sample 的验证集上连续多少个迭代的分数都没有提高后就提前终止训练。用于防止 Overfitting。一般的调参步骤是：将训练数据的一部分划出来作为验证集。先将 eta 设得比较高（比如 0.1），num_round 设为 300 ~ 500。用 Grid Search 对其他参数进行搜索逐步将 eta 降低，找到最佳值。以验证集为 watchlist，用找到的最佳参数组合重新在训练集上训练。注意观察算法的输出，看每次迭代后在验证集上分数的变化情况，从而得到最佳的 early_stopping_rounds。X_dtrain, X_deval, y_dtrain, y_deval = cross_validation.train_test_split(X_train, y_train, random_state=1026, test_size=0.3)dtrain = xgb.DMatrix(X_dtrain, y_dtrain)deval = xgb.DMatrix(X_deval, y_deval)watchlist = [(deval, &#39;eval&#39;)]params = { &#39;booster&#39;: &#39;gbtree&#39;, &#39;objective&#39;: &#39;reg:linear&#39;, &#39;subsample&#39;: 0.8, &#39;colsample_bytree&#39;: 0.85, &#39;eta&#39;: 0.05, &#39;max_depth&#39;: 7, &#39;seed&#39;: 2016, &#39;silent&#39;: 0, &#39;eval_metric&#39;: &#39;rmse&#39;}clf = xgb.train(params, dtrain, 500, watchlist, early_stopping_rounds=50)pred = clf.predict(xgb.DMatrix(df_test))最后要提一点，所有具有随机性的 Model 一般都会有一个 seed 或是 random_state 参数用于控制随机种子。得到一个好的 Model 后，在记录参数时务必也记录下这个值，从而能够在之后重现 Model。Cross ValidationCross Validation) 是非常重要的一个环节。它让你知道你的 Model 有没有 Overfit，是不是真的能够 Generalize 到测试集上。在很多比赛中 Public LB 都会因为这样那样的原因而不可靠。当你改进了 Feature 或是 Model 得到了一个更高的 CV 结果，提交之后得到的 LB 结果却变差了，一般认为这时应该相信 CV 的结果。当然，最理想的情况是多种不同的 CV 方法得到的结果和 LB 同时提高，但这样的比赛并不是太多。在数据的分布比较随机均衡的情况下，5-Fold CV 一般就足够了。如果不放心，可以提到 10-Fold。但是 Fold 越多训练也就会越慢，需要根据实际情况进行取舍。很多时候简单的 CV 得到的分数会不大靠谱，Kaggle 上也有很多关于如何做 CV 的讨论。比如这个。但总的来说，靠谱的 CV 方法是 Case By Case 的，需要在实际比赛中进行尝试和学习，这里就不再（也不能）叙述了。Ensemble GenerationEnsemble Learning 是指将多个不同的 Base Model 组合成一个 Ensemble Model 的方法。它可以同时降低最终模型的 Bias 和 Variance（证明可以参考这篇论文，我最近在研究类似的理论，可能之后会写新文章详述)，从而在提高分数的同时又降低 Overfitting 的风险。在现在的 Kaggle 比赛中要不用 Ensemble 就拿到奖金几乎是不可能的。常见的 Ensemble 方法有这么几种：Bagging：使用训练数据的不同随机子集来训练每个 Base Model，最后进行每个 Base Model 权重相同的 Vote。也即 Random Forest 的原理。Boosting：迭代地训练 Base Model，每次根据上一个迭代中预测错误的情况修改训练样本的权重。也即 Gradient Boosting 的原理。比 Bagging 效果好，但更容易 Overfit。Blending：用不相交的数据训练不同的 Base Model，将它们的输出取（加权）平均。实现简单，但对训练数据利用少了。Stacking：接下来会详细介绍。从理论上讲，Ensemble 要成功，有两个要素：Base Model 之间的相关性要尽可能的小。这就是为什么非 Tree-based Model 往往表现不是最好但还是要将它们包括在 Ensemble 里面的原因。Ensemble 的 Diversity 越大，最终 Model 的 Bias 就越低。Base Model 之间的性能表现不能差距太大。这其实是一个 Trade-off，在实际中很有可能表现相近的 Model 只有寥寥几个而且它们之间相关性还不低。但是实践告诉我们即使在这种情况下 Ensemble 还是能大幅提高成绩。Stacking相比 Blending，Stacking 能更好地利用训练数据。以 5-Fold Stacking 为例，它的基本原理如图所示：整个过程很像 Cross Validation。首先将训练数据分为 5 份，接下来一共 5 个迭代，每次迭代时，将 4 份数据作为 Training Set 对每个 Base Model 进行训练，然后在剩下一份 Hold-out Set 上进行预测。同时也要将其在测试数据上的预测保存下来。这样，每个 Base Model 在每次迭代时会对训练数据的其中 1 份做出预测，对测试数据的全部做出预测。5 个迭代都完成以后我们就获得了一个 #训练数据行数 x #Base Model 数量 的矩阵，这个矩阵接下来就作为第二层的 Model 的训练数据。当第二层的 Model 训练完以后，将之前保存的 Base Model 对测试数据的预测（因为每个 Base Model 被训练了 5 次，对测试数据的全体做了 5 次预测，所以对这 5 次求一个平均值，从而得到一个形状与第二层训练数据相同的矩阵）拿出来让它进行预测，就得到最后的输出。这里给出我的实现代码：class Ensemble(object): def __init__(self, n_folds, stacker, base_models): self.n_folds = n_folds self.stacker = stacker self.base_models = base_models def fit_predict(self, X, y, T): X = np.array(X) y = np.array(y) T = np.array(T) folds = list(KFold(len(y), n_folds=self.n_folds, shuffle=True, random_state=2016)) S_train = np.zeros((X.shape[0], len(self.base_models))) S_test = np.zeros((T.shape[0], len(self.base_models))) for i, clf in enumerate(self.base_models): S_test_i = np.zeros((T.shape[0], len(folds))) for j, (train_idx, test_idx) in enumerate(folds): X_train = X[train_idx] y_train = y[train_idx] X_holdout = X[test_idx] # y_holdout = y[test_idx] clf.fit(X_train, y_train) y_pred = clf.predict(X_holdout)[:] S_train[test_idx, i] = y_pred S_test_i[:, j] = clf.predict(T)[:] S_test[:, i] = S_test_i.mean(1) self.stacker.fit(S_train, y) y_pred = self.stacker.predict(S_test)[:] return y_pred获奖选手往往会使用比这复杂得多的 Ensemble，会出现三层、四层甚至五层，不同的层数之间有各种交互，还有将经过不同的 Preprocessing 和不同的 Feature Engineering 的数据用 Ensemble 组合起来的做法。但对于新手来说，稳稳当当地实现一个正确的 5-Fold Stacking 已经足够了。Pipeline可以看出 Kaggle 比赛的 Workflow 还是比较复杂的。尤其是 Model Selection 和 Ensemble。理想情况下，我们需要搭建一个高自动化的 Pipeline，它可以做到：模块化 Feature Transform，只需写很少的代码就能将新的 Feature 更新到训练集中。自动化 Grid Search，只要预先设定好使用的 Model 和参数的候选，就能自动搜索并记录最佳的 Model。自动化 Ensemble Generation，每个一段时间将现有最好的 K 个 Model 拿来做 Ensemble。对新手来说，第一点可能意义还不是太大，因为 Feature 的数量总是人脑管理的过来的；第三点问题也不大，因为往往就是在最后做几次 Ensemble。但是第二点还是很有意义的，手工记录每个 Model 的表现不仅浪费时间而且容易产生混乱。Crowdflower Search Results Relevance 的第一名获得者 Chenglong Chen 将他在比赛中使用的 Pipeline 公开了，非常具有参考和借鉴意义。只不过看懂他的代码并将其中的逻辑抽离出来搭建这样一个框架，还是比较困难的一件事。可能在参加过几次比赛以后专门抽时间出来做会比较好。Home Depot Search Relevance在这一节中我会具体分享我在 Home Depot Search Relevance 比赛中是怎么做的，以及比赛结束后从排名靠前的队伍那边学到的做法。首先简单介绍这个比赛。Task 是判断用户搜索的关键词和网站返回的结果之间的相关度有多高。相关度是由 3 个人类打分取平均得到的，每个人可能打 1 ~ 3 分，所以这是一个回归问题。数据中包含用户的搜索词，返回的产品的标题和介绍，以及产品相关的一些属性比如品牌、尺寸、颜色等。使用的评分基准是 RMSE。这个比赛非常像 Crowdflower Search Results Relevance 那场比赛。不过那边用的评分基准是 Quadratic Weighted Kappa，把 1 误判成 4 的惩罚会比把 1 判成 2 的惩罚大得多，所以在最后 Decode Prediction 的时候会更麻烦一点。除此以外那次比赛没有提供产品的属性。EDA由于加入比赛比较晚，当时已经有相当不错的 EDA 了。尤其是这个。从中我得到的启发有：同一个搜索词/产品都出现了多次，数据分布显然不 i.i.d.。文本之间的相似度很有用。产品中有相当大一部分缺失属性，要考虑这会不会使得从属性中得到的 Feature 反而难以利用。产品的 ID 对预测相关度很有帮助，但是考虑到训练集和测试集之间的重叠度并不太高，利用它会不会导致 Overfitting？Preprocessing这次比赛中我的 Preprocessing 和 Feature Engineering 的具体做法都可以在这里看到。我只简单总结一下和指出重要的点。利用 Forum 上的 Typo Dictionary 修正搜索词中的错误。统计属性的出现次数，将其中出现次数多又容易利用的记录下来。将训练集和测试集合并，并与产品描述和属性 Join 起来。这是考虑到后面有一系列操作，如果不合并的话就要重复写两次了。对所有文本能做 Stemming 和 Tokenizing，同时手工做了一部分格式统一化（比如涉及到数字和单位的）和同义词替换。FeatureAttribute Features是否包含某个特定的属性（品牌、尺寸、颜色、重量、内用/外用、是否有能源之星认证等）这个特定的属性是否匹配Meta Features各个文本域的长度是否包含属性域品牌（将所有的品牌做数值离散化）产品 ID简单匹配搜索词是否在产品标题、产品介绍或是产品属性中出现搜索词在产品标题、产品介绍或是产品属性中出现的数量和比例搜索词中的第 i 个词是否在产品标题、产品介绍或是产品属性中出现搜索词和产品标题、产品介绍以及产品属性之间的文本相似度BOW Cosine SimilairtyTF-IDF Cosine SimilarityJaccard SimilarityEdit DistanceWord2Vec Distance（由于效果不好，最后没有使用，但似乎是因为用的不对）Latent Semantic Indexing：通过将 BOW/TF-IDF Vectorization 得到的矩阵进行 SVD 分解，我们可以得到不同搜索词/产品组合的 Latent 标识。这个 Feature 使得 Model 能够在一定程度上对不同的组合做出区别，从而解决某些产品缺失某些 Feature 的问题。值得一提的是，上面打了 * 的 Feature 都是我在最后一批加上去的。问题是，使用这批 Feature 训练得到的 Model 反而比之前的要差，而且还差不少。我一开始是以为因为 Feature 的数量变多了所以一些参数需要重新调优，但在浪费了很多时间做 Grid Search 以后却发现还是没法超过之前的分数。这可能就是之前提到的 Feature 之间的相互作用导致的问题。当时我设想过一个看到过好几次的解决方案，就是将使用不同版本 Feature 的 Model 通过 Ensemble 组合起来。但最终因为时间关系没有实现。事实上排名靠前的队伍分享的解法里面基本都提到了将不同的 Preprocessing 和 Feature Engineering 做 Ensemble 是获胜的关键。Model我一开始用的是 RandomForestRegressor，后来在 Windows 上折腾 Xgboost 成功了就开始用 XGBRegressor。XGB 的优势非常明显，同样的数据它只需要不到一半的时间就能跑完，节约了很多时间。比赛中后期我基本上就是一边台式机上跑 Grid Search，一边在笔记本上继续研究 Feature。这次比赛数据分布很不独立，所以期间多次遇到改进的 Feature 或是 Grid Search 新得到的参数训练出来的模型反而 LB 分数下降了。由于被很多前辈教导过要相信自己的 CV，我的决定是将 5-Fold 提到 10-Fold，然后以 CV 为标准继续前进。Ensemble最终我的 Ensemble 的 Base Model 有以下四个：RandomForestRegressor``ExtraTreesRegressor``GradientBoostingRegressor``XGBRegressor第二层的 Model 还是用的 XGB。因为 Base Model 之间的相关都都太高了（最低的一对也有 0.9），我原本还想引入使用 gblinear的 XGBRegressor 以及 SVR，但前者的 RMSE 比其他几个 Model 高了 0.02（这在 LB 上有几百名的差距），而后者的训练实在太慢了。最后还是只用了这四个。值得一提的是，在开始做 Stacking 以后，我的 CV 和 LB 成绩的提高就是完全同步的了。在比赛最后两天，因为身心疲惫加上想不到还能有什么显著的改进，我做了一件事情：用 20 个不同的随机种子来生成 Ensemble，最后取 Weighted Average。这个其实算是一种变相的 Bagging。其意义在于按我实现 Stacking 的方式，我在训练 Base Model 时只用了 80% 的训练数据，而训练第二层的 Model 时用了 100% 的数据，这在一定程度上增大了 Overfitting 的风险。而每次更改随机种子可以确保每次用的是不同的 80%，这样在多次训练取平均以后就相当于逼近了使用 100% 数据的效果。这给我带来了大约 0.0004 的提高，也很难受说是真的有效还是随机性了。比赛结束后我发现我最好的单个 Model 在 Private LB 上的得分是 0.46378，而最终 Stacking 的得分是 0.45849。这是 174 名和 98 名的差距。也就是说，我单靠 Feature 和调参进到了 前 10%，而 Stacking 使我进入了前 5%。Lessons Learned比赛结束后一些队伍分享了他们的解法，从中我学到了一些我没有做或是做的不够好的地方：产品标题的组织方式是有 Pattern 的，比如一个产品是否带有某附件一定会用 With/Without XXX 的格式放在标题最后。使用外部数据，比如 WordNet，Reddit 评论数据集等来训练同义词和上位词（在一定程度上替代 Word2Vec）词典。基于字母而不是单词的 NLP Feature。这一点我让我十分费解，但请教以后发现非常有道理。举例说，排名第三的队伍在计算匹配度时，将搜索词和内容中相匹配的单词的长度也考虑进去了。这是因为他们发现越长的单词约具体，所以越容易被用户认为相关度高。此外他们还使用了逐字符的序列比较（difflib.SequenceMatcher），因为这个相似度能够衡量视觉上的相似度。像这样的 Feature 的确不是每个人都能想到的。标注单词的词性，找出中心词，计算基于中心词的各种匹配度和距离。这一点我想到了，但没有时间尝试。将产品标题/介绍中 TF-IDF 最高的一些 Trigram 拿出来，计算搜索词中出现在这些 Trigram 中的比例；反过来以搜索词为基底也做一遍。这相当于是从另一个角度抽取了一些 Latent 标识。一些新颖的距离尺度，比如 Word Movers Distance除了 SVD 以外还可以用上 NMF。最重要的 Feature 之间的 Pairwise Polynomial Interaction。针对数据不 i.i.d. 的问题，在 CV 时手动构造测试集与验证集之间产品 ID 不重叠和重叠的两种不同分割，并以与实际训练集/测试集的分割相同的比例来做 CV 以逼近 LB 的得分分布。至于 Ensemble 的方法，我暂时还没有办法学到什么，因为自己只有最简单的 Stacking 经验。SummaryTakeaways比较早的时候就开始做 Ensemble 是对的，这次比赛到倒数第三天我还在纠结 Feature。很有必要搭建一个 Pipeline，至少要能够自动训练并记录最佳参数。Feature 为王。我花在 Feature 上的时间还是太少。可能的话，多花点时间去手动查看原始数据中的 Pattern。Issues Raised我认为在这次比赛中遇到的一些问题是很有研究价值的：在数据分布并不 i.i.d. 甚至有 Dependency 时如何做靠谱的 CV。如何量化 Ensemble 中 Diversity vs. Accuracy 的 Trade-off。如何处理 Feature 之间互相影响导致性能反而下降。Beginner Tips给新手的一些建议：选择一个感兴趣的比赛。如果你对相关领域原本就有一些洞见那就更理想了。根据我描述的方法开始探索、理解数据并进行建模。通过 Forum 和 Scripts 学习其他人对数据的理解和构建 Feature 的方式。如果之前有过类似的比赛，可以去找当时获奖者的 Interview 和 Blog Post 作为参考，往往很有用。在得到一个比较不错的 LB 分数（比如已经接近前 10%）以后可以开始尝试做 Ensemble。如果觉得自己有希望拿到奖金，开始找人组队吧！到比赛结束为止要绷紧一口气不能断，尽量每天做一些新尝试。比赛结束后学习排名靠前的队伍的方法，思考自己这次比赛中的不足和发现的问题，可能的话再花点时间将学到的新东西用实验进行确认，为下一次比赛做准备。好好休息！ReferenceBeating Kaggle the Easy Way - Dong YingSolution for Prudential Life Insurance Assessment - NutastraySearch Results Relevance Winner’s Interview: 1st place, Chenglong ChenBuy me an Asahi Beer :)Donate# XGBoost # Kaggle # Machine Learning # Data Science # Python # Ensemble Learning # Stacking# Scikit-learn # Pandas # Seaborn # Matplotlib # Natural Language Processing如何备考 TOEFL/GREHow to Rank 10% in Your First Kaggle CompetitionTable of Contents Overview1. Introduction2. General Approach3. Home Depot Search Relevance4. Summary5. Reference © 2015 — 2018 Linghao Zhang Powered by Hexo | Theme — NexT.Muse v5.1.4]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F08%2F15%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[1、训练后，拟合训练集100%的正确率，但预测集会把所有的负样本预测为正]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F08%2F14%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FPYTHON%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5%2F%E7%89%B9%E5%BE%81%E8%BD%AConehot%2F</url>
    <content type="text"><![CDATA[某些特征是离散的字符串型，用sklearn转成onehot编码。 数据如 1239, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, &lt;=50K50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, &lt;=50K 123456789101112131415161718192021222324252627282930feature_names = [&quot;Age&quot;, &quot;Workclass&quot;, &quot;fnlwgt&quot;, &quot;Education&quot;, &quot;Education-Num&quot;, &quot;Marital Status&quot;,&quot;Occupation&quot;, &quot;Relationship&quot;, &quot;Race&quot;, &quot;Sex&quot;, &quot;Capital Gain&quot;, &quot;Capital Loss&quot;,&quot;Hours per week&quot;, &quot;Country&quot;]data = np.genfromtxt(&apos;/Users/david/david/code/00project/carthage/scripts/adult.data&apos;, delimiter=&apos;, &apos;, dtype=str)labels = data[:,14]le= sklearn.preprocessing.LabelEncoder()le.fit(labels)labels = le.transform(labels)class_names = le.classes_data = data[:,:-1]categorical_features = [1,3,5, 6,7,8,9,13]categorical_names = &#123;&#125;for feature in categorical_features: le = sklearn.preprocessing.LabelEncoder() le.fit(data[:, feature]) data[:, feature] = le.transform(data[:, feature]) categorical_names[feature] = le.classes_data = data.astype(float)encoder = sklearn.preprocessing.OneHotEncoder(categorical_features=categorical_features)np.random.seed(1)train, test, labels_train, labels_test = sklearn.model_selection.train_test_split(data, labels, train_size=0.80)encoder.fit(data)encoded_train = encoder.transform(train)import xgboostgbtree = xgboost.XGBClassifier(n_estimators=300, max_depth=5)gbtree.fit(encoded_train, labels_train)sklearn.metrics.accuracy_score(labels_test, gbtree.predict(encoder.transform(test)))]]></content>
  </entry>
  <entry>
    <title><![CDATA[Jupyter技巧]]></title>
    <url>%2F2018%2F08%2F13%2F%E5%B7%A5%E5%85%B7%E5%92%8C%E7%8E%AF%E5%A2%83%2FJupyter%2F</url>
    <content type="text"><![CDATA[安装anaconda后，开启jupyter 1/Users/david/anaconda3/bin/jupyter_mac.command ; exit; 访问地址是 1http://localhost:8888/?token=a4e05eb5fea790544205871d861cff0795998ebc9c78c972 Jupyter连接到colab https://research.google.com/colaboratory/local-runtimes.html 第 1 步：安装 Jupyter在本地机器上安装 Jupyter。 第 2 步：安装并启用 jupyter_http_over_ws jupyter 扩展程序（一次性）jupyter_http_over_ws 扩展程序由 Colaboratory 团队开发并在 GitHub 上提供。 12pip install --upgrade jupyter_http_over_ws&gt;=0.0.1a3 &amp;&amp; \ jupyter serverextension enable --py jupyter_http_over_ws 第 3 步：启动服务器并进行身份验证新的笔记本服务器按正常方式启动，但您需要设置一个标记来明确表明信任来自 Colaboratory 前端的 WebSocket 连接。 1234jupyter notebook \ --NotebookApp.allow_origin=&apos;https://colab.research.google.com&apos; \ --port=8888 \ --NotebookApp.port_retries=0 第 4 步：连接到本地运行时如果您已通过 --no-browser 标记启动 Jupyter 笔记本服务器，则可能需要先访问在控制台中输出的网址，然后再从 Colab 连接到该服务器。该网址设置了浏览器 Cookie，用于在浏览器和 Jupyter 笔记本服务器之间进行身份验证。 在 Colaboratory 中，点击“连接”按钮，然后选择“连接到本地运行时…”。在随即显示的对话框中输入上一步中的端口，然后点击“连接”按钮。完成此操作后，您应该就已经连接到本地运行时了。 画图在 notebook 中生成绘图有许多选项： matplotlib， 使用%matplotlib inline进行激活。 %matplotlib notebook提供了一些交互性，不过可能会有点慢，因为渲染由服务器端完成。 mpld3为matplotlib代码提供了另一个渲染器(使用d3)。非常漂亮，不过尚不完备还有待发展。 bokeh是构建交互性绘图的一个更好的选择。 plot.ly也可以生成漂亮的绘图，不过是付费服务。 常用魔术命令执行py1%run ./LinearRegression.ipynb 导入代码1%load test.py 注：也可以%load +链接 代入网络上的代码。 计时 %%time 将会给出cell的代码运行一次所花费的时间。 1234%%timeimport timefor _ in range(1000): time.sleep(0.01)# sleep for 0.01 seconds 执行shell123!ls!pip list | grep pandas cell输出多行12from IPython.core.interactiveshell import InteractiveShellInteractiveShell.ast_node_interactivity = &quot;all&quot; 安装自动补全插件接着让 jupyter notebook 实现自动代码补全，首先安装 nbextensions 12pip install jupyter_contrib_nbextensionsjupyter contrib nbextension install --user 然后安装 nbextensions_configurator 12pip install jupyter_nbextensions_configuratorjupyter nbextensions_configurator enable --user 如果提示缺少依赖，就使用pip安装对应依赖即可。 最后重启jupyter，在弹出的主页面里，能看到增加了一个Nbextensions标签页，在这个页面里，勾选Hinterland即启用了代码自动补全，如图所示： 用jupyter做代码笔记]]></content>
      <categories>
        <category>工具和环境</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F08%2F13%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%2FLIME%E6%A8%A1%E5%9E%8B%E8%A7%A3%E9%87%8A%E5%99%A8%2F</url>
    <content type="text"><![CDATA[LIME模型解释器LIME的git https://github.com/marcotcr/lime LIME：一种解释机器学习模型的方法 https://blog.csdn.net/happytofly/article/details/80122605#0-tsina-1-96833-397232819ff9a47a7b7e80a40613cfe1 https://blog.csdn.net/hero_fantao/article/details/51116165 https://wenku.baidu.com/view/cbd7ebb36aec0975f46527d3240c844769eaa0ed.html https://blog.csdn.net/real_myth/article/details/51136832 jupyter示例 http://marcotcr.github.io/lime/tutorials/Lime%20-%20basic%20usage%2C%20two%20class%20case.html https://blog.csdn.net/a358463121/article/details/52313585 论文翻译 Using lime() on xgboost objecthttps://github.com/thomasp85/lime/issues/1 Combining LIME with Gradient Boosted Modelshttps://github.com/marcotcr/lime/issues/51 lime+lightgbm123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import lightgbm as lgbimport pandas as pdimport numpy as np# load or create your datasetprint('Load data...')df_train = pd.read_csv('../binary_classification/binary.train', header=None, sep='\t')df_test = pd.read_csv('../binary_classification/binary.test', header=None, sep='\t')W_train = pd.read_csv('../binary_classification/binary.train.weight', header=None)[0]W_test = pd.read_csv('../binary_classification/binary.test.weight', header=None)[0]y_train = df_train[0]y_test = df_test[0]X_train = df_train.drop(0, axis=1)X_test = df_test.drop(0, axis=1)num_train, num_feature = X_train.shape# create dataset for lightgbm# if you want to re-use data, remember to set free_raw_data=Falselgb_train = lgb.Dataset(X_train, y_train, weight=W_train, free_raw_data=False)lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train, weight=W_test, free_raw_data=False)# specify your configurations as a dictparams = &#123; 'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'binary_logloss', 'num_leaves': 31, 'learning_rate': 0.05, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5, 'verbose': 0&#125;# generate a feature namefeature_name = ['feature_' + str(col) for col in range(num_feature)]print('Start training...')# feature_name and categorical_featuregbm = lgb.train(params, lgb_train, num_boost_round=10, valid_sets=lgb_train, # eval training data feature_name=feature_name, categorical_feature=[21])# check feature nameprint('Finish first 10 rounds...')print('7th feature name is:', repr(lgb_train.feature_name[6]))import limeimport lime.lime_tabularexplainer = lime.lime_tabular.LimeTabularExplainer(X_train.as_matrix(), y_train.as_matrix(), feature_names=feature_name, discretize_continuous=True, class_names=['c0', 'c1'])xtest = X_test.as_matrix()def predict_fn(x): preds = gbm.predict(x).reshape(-1, 1) p0 = 1 - preds return np.hstack((p0, preds)) i = 0exp = explainer.explain_instance(xtest[i], predict_fn, num_features=10)exp.show_in_notebook() lime + irishttp://marcotcr.github.io/lime/tutorials/Tutorial%20-%20continuous%20and%20categorical%20features.html 12345678910111213141516171819202122import sklearnimport sklearn.datasetsimport sklearn.ensembleimport numpy as npimport limeimport lime.lime_tabularfrom __future__ import print_functionnp.random.seed(1)iris = sklearn.datasets.load_iris()train, test, labels_train, labels_test = sklearn.model_selection.train_test_split(iris.data, iris.target, train_size=0.80)rf = sklearn.ensemble.RandomForestClassifier(n_estimators=500)rf.fit(train, labels_train)sklearn.metrics.accuracy_score(labels_test, rf.predict(test))explainer = lime.lime_tabular.LimeTabularExplainer(train, feature_names=iris.feature_names, class_names=iris.target_names, discretize_continuous=True)i = np.random.randint(0, test.shape[0])exp = explainer.explain_instance(test[i], rf.predict_proba, num_features=2, top_labels=1)exp.show_in_notebook(show_table=True, show_all=False) LIME数值型https://marcotcr.github.io/lime/tutorials/Tutorial%20-%20continuous%20and%20categorical%20features.html 1explainer = lime.lime_tabular.LimeTabularExplainer(train, feature_names=iris.feature_names, class_names=iris.target_names, discretize_continuous=True) LIME离散型和数值型兼有https://marcotcr.github.io/lime/tutorials/Tutorial%20-%20continuous%20and%20categorical%20features.html 1234567explainer = lime.lime_tabular.LimeTabularExplainer(X_train.as_matrix(), mode='classification', categorical_features=gbm.feature_name(), categorical_names=gbm.feature_name(), training_labels=y_train.as_matrix() , feature_names=gbm.feature_name(), kernel_width=3, class_names=['0', '1']) LIME参数1234feature_names 训练集中的特征列名categorical_features 标记为离散型的特征列名，特征值必须是intclass_names 默认是&apos;0&apos;、&apos;1&apos;discretize_continuous 如果为True，所有非离散特征 LIME代码解读12345678910111213入口是engine.py的explain_instance输入的参数中num_features: maximum number of features present in explanation def explain_instance(self, data_row, predict_fn, labels=(1,), top_labels=None, num_features=10, num_samples=5000, distance_metric=&apos;euclidean&apos;, model_regressor=None): 代码修改不保留两位小数/Users/david/anaconda3/lib/python3.6/site-packages/lime/lime_tabular.py 1values = self.convert_and_round(data_row) 这一行要注释 注释后的代码在 /Users/david/Downloads/lime-0.1.1.32.zip 1python setup.py install 本地使用，不安装包 1234/Users/david/david/code/00project/carthage/scripts/lime_tabular.py/Users/david/david/code/00project/carthage/scripts/lime_base.py/Users/david/david/code/00project/carthage/scripts/explanation.py/Users/david/david/code/00project/carthage/scripts/exceptions.py]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F08%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%2F%E7%89%B9%E5%BE%81%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[如何构造出对模型有帮助的特征？ 如何判断特征取值的不同角度对模型有没有作用？ 如何看特征之间相互有没有影响？ 如何看对模型最重要的特征？ 特征工程思维图!(/Users/david/david/00projects/00markdown/项目文档/pic/特征工程思维图.jpg) 风控模型的常规思路 特征选择工具WillKoehrsen/feature-selectorhttps://github.com/WillKoehrsen/feature-selector 看分布某个特征的正负样本数值分布比较均匀，不算好特征；若不均匀，可以使用 或者若越高/越低会影响逾期与否，也是好特征。 GBDT筛选扔进去gbdt看一波重要性分析，然后分析一下皮尔森系数，淘汰一下不重要的特征和相关程度高的特征 越细粒度的特征越重要特征组合就是将特征粒度变细。比如aid只有173维，age有3维，特征组合之后变成了173*3，这个时候粒度就变细了。粒度越细，越容易定位到某一个单独的用户。 类别特征与数值特征的组合用N1和N2表示数值特征，用C1和C2表示类别特征，利用pandas的groupby操作，可以创造出以下几种有意义的新特征：（其中，C2还可以是离散化了的N1） 12345678910median(N1)_by(C1) \\ 中位数mean(N1)_by(C1) \\ 算术平均数mode(N1)_by(C1) \\ 众数min(N1)_by(C1) \\ 最小值max(N1)_by(C1) \\ 最大值std(N1)_by(C1) \\ 标准差var(N1)_by(C1) \\ 方差freq(C2)_by(C1) \\ 频数freq(C1) \\这个不需要groupby也有意义 仅仅将已有的类别和数值特征进行以上的有效组合，就能够大量增加优秀的可用特征。 将这种方法和线性组合等基础特征工程方法结合（仅用于决策树），可以得到更多有意义的特征，如： 12N1 - median(N1)_by(C1)N1 - mean(N1)_by(C1) 用基因编程创造新特征Welcome to gplearn’s documentation! 基于genetic programming的symbolic regression，具体的原理和实现参见文档。目前，python环境下最好用的基因编程库为gplearn。基因编程的两大用法： 转换（transformation）：把已有的特征进行组合转换，组合的方式（一元、二元、多元算子）可以由用户自行定义，也可以使用库中自带的函数（如加减乘除、min、max、三角函数、指数、对数）。组合的目的，是创造出和目标y值最“相关”的新特征。这种相关程度可以用spearman或者pearson的相关系数进行测量。spearman多用于决策树（免疫单特征单调变换），pearson多用于线性回归等其他算法。 回归（regression）：原理同上，只不过直接用于回归而已。 用决策树创造新特征在决策树系列的算法中（单棵决策树、gbdt、随机森林），每一个样本都会被映射到决策树的一片叶子上。因此，我们可以把样本经过每一棵决策树映射后的index（自然数）或one-hot-vector（哑编码得到的稀疏矢量）作为一项新的特征，加入到模型中。 具体实现：apply()以及decision_path()方法，在scikit-learn和xgboost里都可以用。 决策树、基于决策树的ensemble spearman correlation coefficient 线性模型、SVM、神经网络 对数（log） pearson correlation coefficient 数据泄露（Data Leakage）在数据挖掘中，数据泄露（leakage）指的是：本来不应该出现在X里的、和目标y有关的数据，出现在了X中。如此一来，机器学习算法就会有好到不真实的表现。 Kaggle Wiki链接：Leakage | Kaggle 6.1. 数据泄露的种类以及影响分析 测试集数据被泄露到训练集：过拟合，模型在现实中的表现远不如test accuracy；测试集失去意义。 正确的预测（y）被泄露到测试集：严重过拟合，训练出的模型毫无用处，比赛组织者的极大失败 未来的信息被泄露到过去：时间序列相关，现实中模型将无法有效根据过去情况预测未来。 模型可以获得一些不该获得的信息，比如和目标变量有较大关系的变量、现实里接触不到的变量。例子：y是“病人是否患有癌症”，但是X包括了“病人是否接受肿瘤切除手术”。 反向工程，去匿名化，去除数据集中的随机打乱操作，社会工程学。这种行为是数据比赛明令禁止的，而且在现实中也涉嫌侵犯隐私。例子：反向工程“随机的”用户编码，得出用户的真名。 第三方信息。例子：已知坐标，利用geocoder类型的服务推出所在城市；在预测金融市场时加入第三方的政策新闻的特征。 6.2. 有效发现和利用数据泄露 数据泄露可以分为两大类： 由于自己的疏忽，在交叉验证、训练过程中，产生的数据泄露。这种情况属于失误，应当尽量避免。 在数据竞赛中，找到了理论上不能使用（但是也没有明令禁止）的额外数据，从而提升分数。 避免第一种数据泄露的方法，可以参考kaggle的各类比赛。假设有大量数据，我们可以把未处理的数据分为训练集和测试集，其中，测试集包括Public LB和Private LB两部分。 在模型的训练、选择和交叉验证时，我们只能接触训练集。 在对自己的模型非常自信时，可以偶尔在Public LB上验证。 只有模型即将被用于正式商业用途时，才能看模型在Private LB上的表现。 交叉验证误差、public LB误差、private LB误差：如果后者的误差值显著高于前者，那么需要考虑过拟合或第一类数据泄露。 第二类的数据泄露，属于旁门左道。本质上，这相当于在模型训练阶段，干了数据收集阶段的工作。搜集原始数据，或是自己提供数据举办竞赛（试图避免他人利用数据泄露）时，可以参考这种思路。 文件夹的创造时间。 看似乱码的字符串（如各类id）可能有统计分布的规律。 地理位置信息：如果提供了坐标，则可反向地理编码，得出相关地理信息。 这类数据可能会导致过拟合。 特征选择利器: Borutahttps://zhuanlan.zhihu.com/p/33730850 大规模特征构建实践总结https://zhuanlan.zhihu.com/p/37547126 利用 gplearn 进行特征工程https://zhuanlan.zhihu.com/p/39377036 米筐科技（RiceQuant）策略研究报告：特征选择方法探析—沪深300指数的集成特征选择和聚类分析https://zhuanlan.zhihu.com/p/21406355 探索性数据分析套路解读https://zhuanlan.zhihu.com/p/29364225 《探索性数据分析》翻译https://zhuanlan.zhihu.com/p/26928750 参考 https://zhuanlan.zhihu.com/p/38341881 https://zhuanlan.zhihu.com/p/26444240 最小冗余最大关联特征选择https://medium.com/ai%C2%B3-theory-practice-business/three-effective-feature-selection-strategies-e1f86f331fb1 将所有的想法整合起来就能得出我们的算法，即 mRMR 特征选择。算法背后的考虑是，同时最小化特征的冗余并最大化特征的关联。因此，我们需要计算冗余和关联的方程： 基于单特征因子的隐马尔科夫模型在商品期货上的应用https://mp.weixin.qq.com/s?src=3&amp;timestamp=1536718793&amp;ver=1&amp;signature=mXrKo-NTo9B7kWkxdnmtU8hyNP*zcC2eakm1gbJYZAFhWCAVxzu9gWE4uC7CS1gyevwV-VXNzQ3HrnRFDipJwdBMuaWGrXS7tRzVqk1BHcCL1H7tgymi8rDrNLUDG8XBLz2azHMykg4Ncczyo4XxPh*pmRpSXTFeUBYH56Pyvtw=]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F08%2F08%2Fhadoop-spark%2Fspark%2Fdataframe%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[Row转map12345val df = Seq((1, 2.0, &quot;a&quot;)).toDF(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;) val row = df.firstrow.getValuesMap[Any](row.schema.fieldNames)// res19: Map[String,Any] = Map(A -&gt; 1, B -&gt; 2.0, C -&gt; a) 这样就可以遍历row的每一列和取值了 创建DF12345678910111213141516val df = hiveContext.createDataFrame(Seq( (0.0762357,3.5,1.4,0.2,1.0), (0.226728,3.0,1.4,0.2,1.0), (4.7,3.2,1.3,0.2,1.0), (4.6,3.1,1.5,0.2,1.0), (5.0,3.6,1.4,0.2,1.0), (5.4,3.9,1.7,0.4,1.0), (4.6,3.4,1.4,0.3,1.0), (5.0,3.4,1.5,0.2,1.0), (4.4,2.9,1.4,0.2,1.0), (4.9,3.1,1.5,0.1,1.0), (5.4,3.7,1.5,0.2,1.0), (4.8,3.4,1.6,0.2,1.0), (4.8,3.0,1.4,0.1,1.0), (4.3,3.0,1.1,0.1,1.0) )).toDF(&quot;mobile_city_code&quot;, &quot;feature2&quot;, &quot;feature3&quot;, &quot;feature4&quot;, &quot;label&quot;) 读取csv文件为df123456789//读取csv，由于HUE下载的数据表头带有表名，需要去掉。例如stg_d001_t_jxl_behavior.call_cnt --&gt;&gt; call_cnt def readCsv(sc:SparkContext, csvPath:String)=&#123; val sqlContext = new SQLContext(sc) var dataset = sqlContext.read.format("com.databricks.spark.csv") .option("header","true") //这里如果在csv第一行有属性的话，没有就是"false" //.option("inferSchema",true.toString)//这是自动推断属性列的数据类型。不要打开 .load(csvPath)//文件的路径 dataset &#125;]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F08%2F06%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FOneHot%E7%BC%96%E7%A0%81%2F</url>
    <content type="text"><![CDATA[所涉及到的几种 sklearn 的二值化编码函数：OneHotEncoder(), LabelEncoder(), LabelBinarizer(), MultiLabelBinarizer()https://blog.csdn.net/gao1440156051/article/details/55096630 1.代码块12345678import pandas as pdfrom sklearn.preprocessing import OneHotEncoderfrom sklearn.preprocessing import LabelEncoderfrom sklearn.preprocessing import LabelBinarizerfrom sklearn.preprocessing import MultiLabelBinarizertestdata = pd.DataFrame(&#123;&apos;pet&apos;: [&apos;cat&apos;, &apos;dog&apos;, &apos;dog&apos;, &apos;fish&apos;],&apos;age&apos;: [4 , 6, 3, 3],&apos;salary&apos;:[4, 5, 1, 1]&#125;) 这里我们把 pet、age、salary 都看做类别特征，所不同的是 age 和 salary 都是数值型，而 pet 是字符串型。我们的目的很简单: 把他们全都二值化，进行 one-hot 编码. 2.对付数值型类别变量1OneHotEncoder(sparse = False).fit_transform( testdata.age ) # testdata.age 这里与 testdata[[&apos;age&apos;]]等价 然而运行结果是 array([[ 1., 1., 1., 1.]])，这个结果是错的，从 Warning 信息中得知，原因是 sklearn 的新版本中，OneHotEncoder 的输入必须是 2-D array，而 testdata.age 返回的 Series 本质上是 1-D array，所以要改成 1OneHotEncoder(sparse = False).fit_transform(testdata[[&apos;age&apos;]]) testdata[[‘age’]]的取值是：0,4,6所以对这个三个值进行one-hot化,第一行:[4]===&gt;得到one-hot:[0,1,0] 注意： one-hot化是每个feature的取值个数作为one-hot后的长度，对应的位置0/1的值是原数据在此位置取值的有无.不是取值的max的长度编号:如这个最大为6,按照one-hot的定义是testdata[[‘age’]]=[4]–&gt;one-hot后:[0,0,0,1,0,0]但定义直观理解是这样的,但在工程实现这样没必要,只针对有值得取值编码就可以,节约矩阵one-hot后的规模而且效果一样; 我们得到了我们想要的： 1234array([[ 0., 1., 0.], [ 0., 0., 1.], [ 1., 0., 0.], [ 1., 0., 0.]]) 可以用同样的方法对 salary 进行 OneHotEncoder, 然后将结果用 numpy.hstack() 把两者拼接起来得到变换后的结果 123a1 = OneHotEncoder(sparse = False).fit_transform( testdata[[&apos;age&apos;]] )a2 = OneHotEncoder(sparse = False).fit_transform( testdata[[&apos;salary&apos;]])final_output = numpy.hstack((a1,a2)) 结果为 1234array([[ 0., 1., 0., 0., 1., 0.], [ 0., 0., 1., 0., 0., 1.], [ 1., 0., 0., 1., 0., 0.], [ 1., 0., 0., 1., 0., 0.]]) 有时候我们除了得到最终编码结果，还想知道结果中哪几列属于 age 的二值化编码，哪几列属于 salary 的，这时候我们可以通过 OneHotEncoder() 自带的 featureindices 来实现这一要求，比如这里 featureindices 的值是[0, 3, 6]，表明 第[0:3]列是age的二值化编码，[3:6]是salary的。更多细节请参考 sklearn 文档 3.对付字符串型类别变量遗憾的是OneHotEncoder无法直接对字符串型的类别变量编码，也就是说OneHotEncoder().fit_transform(testdata[[‘pet’]])这句话会报错(不信你试试)。已经有很多人在 stackoverflow 和 sklearn 的 github issue 上讨论过这个问题，但目前为止的 sklearn 版本仍没有增加OneHotEncoder对字符串型类别变量的支持，所以一般都采用曲线救国的方式：* 方法一 先用 LabelEncoder() 转换成连续的数值型变量，再用 OneHotEncoder() 二值化* 方法二 直接用 LabelBinarizer() 进行二值化然而要注意的是，无论 LabelEncoder() 还是 LabelBinarizer()，他们在 sklearn 中的设计初衷，都是为了解决标签 y的离散化，而非输入X， 所以他们的输入被限定为 1-D array，这恰恰跟OneHotEncoder() 要求输入 2-D array 相左。所以我们使用的时候要格外小心，否则就会出现上面array([[ 1., 1., 1., 1.]])那样的错误. 12345678910111213141516171819202122232425# 方法一: LabelEncoder() + OneHotEncoder()a = LabelEncoder().fit_transform(testdata[&apos;pet&apos;])OneHotEncoder( sparse=False ).fit_transform(a.reshape(-1,1)) # 注意: 这里把 a 用 reshape 转换成 2-D array # 方法二: 直接用 LabelBinarizer() LabelBinarizer().fit_transform(testdata[&apos;pet&apos;]) 这两种方法得到的结果一致，都是 12345678910111213array([[ 1., 0., 0.], [ 0., 1., 0.], [ 0., 1., 0.], [ 0., 0., 1.]]) 正因为LabelEncoder和LabelBinarizer设计为只支持 1-D array，也使得它无法像上面 OneHotEncoder 那样批量接受多列输入，也就是说LabelEncoder().fit_transform(testdata[[‘pet’, ‘age’]])会报错。 4.无用的尝试然而执着如我怎会就此放弃，我又仔细翻了翻 sklearn 的 API 接口，果然发现有个叫 MultiLabelBinarizer() 的，看着似乎可以解决这个问题，于是尝试了一下 1MultiLabelBinarizer().fit_transform(testdata[[&apos;age&apos;,&apos;salary&apos;]].values) 输出结果如下 12345678910111213array([[0, 0, 1, 0, 0], [0, 0, 0, 1, 1], [1, 1, 0, 0, 0], [1, 1, 0, 0, 0]]) 5.另一种解决方案其实如果我们跳出 scikit-learn， 在 pandas 中可以很好地解决这个问题，用 pandas 自带的get_dummies函数即可 1pd.get_dummies(testdata,columns=testdata.columns) 结果正是我们想要的 1234567891011121314151617age_3 age_4 age_6 pet_cat pet_dog pet_fish salary_1 salary_4 salary_50 0.0 1.0 0.0 1.0 0.0 0.0 0.0 1.0 0.01 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 1.02 1.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.03 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 get_dummies的优势：* 1.本身就是 pandas 的模块，所以对 DataFrame 类型兼容很好.* 2.无论你的列是字符型还是数字型都可以进行二值编码.* 3.能根据用户指定，自动生成二值编码后的变量名.这么看来，我们找到最完美的解决方案了？ No！get_dummies千般好，万般好，但毕竟不是 sklearn 里的transformer类型，所以得到的结果得手动输入到 sklearn 里的相应模块，也无法像 sklearn 的transformer一样可以输入到pipeline中 进行流程化地机器学习过程。更重要的一点. 注意: get_dummies 不像 sklearn 的 transformer一样，有 transform方法，所以一旦测试集中出现了训练集未曾出现过的特征取值，简单地对测试集、训练集都用 get_dummies 方法将导致数据错误。]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F08%2F03%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E5%A6%82%E4%BD%95%E4%BC%98%E5%8C%96%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[https://www.leiphone.com/news/201612/77HWb6jCUovMFfCC.html 你的机器学习模型为什么会出错？奉上四大原因解析及五条改进措施 当我们评估一个机器学习模型时，首先要做的一件事就是：搞清楚这个模型的偏差和方差是否太大。 高偏差：如上图1所示，所谓高偏差就是指在取样点上模型的实际输出和预想输出不匹配，而且相差很远。出现这一问题的原因是模型并没有准确表征既定输入和预想输出之间的关系，从而造成输出结果的高错误率。 高方差：这种情况与高偏差的情况正好相反。在这一场景中，所有的取样点结果都与预期结果完全相符。看起来模型的工作状态完全正常，但其实隐藏着问题。这样的情况往往容易被忽视，就好像上文提到的，模型能够成功应用于原始输入和输出，但一旦输入新数据，结果就会漏洞百出。]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F08%2F03%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%20%E5%A6%82%E4%BD%95%E4%BC%98%E5%8C%96%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F07%2F25%2Fhadoop-spark%2Fmllib%2Fmllib%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%2F</url>
    <content type="text"><![CDATA[Spark MLlib RandomForest（随机森林）建模与预测 https://blog.csdn.net/dr_guo/article/details/53258037]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F07%2F13%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FMachine%20Learning%20Canvas%2F</url>
    <content type="text"><![CDATA[介绍 介绍中文 官网]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F07%2F13%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FUntitled%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F07%2F11%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E8%AE%A1%E7%AE%97WOE%E5%92%8CIV%2F</url>
    <content type="text"><![CDATA[1、WOE和IV计算1.1 WOE计算公式WOE可以将离散型变量转成连续型变量，假设离散化后共有N组数据，则每组的WOE计算公式为： WOE_i=\ln(\frac {P_{y1}} {P_{y0}}) = \ln( \frac {B_i/B_T} {G_i/G_T} ) , i=1,2...,N其中，$P{y1}$ 是该组中的响应客户（这里对应的就是逾期客户）占所有样本中所有响应客户的比例，$P{y0}$就是未响应的比例。$B_i$是该组中响应的数量，$B_T$是总响应数量，$G_i$是该组中未响应的数量，$G_T$是总未响应数量。 从这个公式中可以体现出，WOE表示的是“当前分组中响应客户占所有响应客户的比例”和“当前分组中没有响应的客户占所有没有响应的客户的比例”的差异。 总里程 逾期 不逾期 响应比例 WOE 0-10 10 1000 10% 0 10-100 25 2000 12.5% 0.223 100-500 15 2000 7.5% -0.287 总计 50 5000 10% WOE_i= \ln( \frac {B_i/B_T} {G_i/G_T} ) =\ln( \frac {B_i/G_i} {B_T/G_T} ) , i=1,2...,N对WOE稍作变换，可以看出WOE也可以这么理解，他表示的是当前这个组中响应的客户和未响应客户的比值，和所有样本中这个比值的差异。这个差异是用这两个比值的比值，再取对数来表示的。WOE越大，这种差异越大，这个分组里的样本响应的可能性就越大，WOE越小，差异越小，这个分组里的样本响应的可能性就越小。 总结WOE的几个性质： 1）当前分组中，响应的比例越大，WOE值越大； 2）当前分组WOE的正负，由当前分组响应和未响应的比例，与样本整体响应和未响应的比例的大小关系决定，当前分组的比例小于样本整体比例时，WOE为负，当前分组的比例大于整体比例时，WOE为正，当前分组的比例和整体比例相等时，WOE为0。 3）WOE的取值范围是全体实数。 可以看出，采用WOE进行离散转连续的好处是： 1）WOE相对比离散型变量，可疑反映出自变量与目标变量之间的非线性关系，理论上能够提升预测的准确率； 2）自变量的标准化处理。自变量内部的各个取值之间都可以直接进行比较（WOE之间的比较），而不同自变量之间的各种取值也可以通过WOE进行直接的比较。 1.2 IV计算公式 IV_i = (P_{y1}-P_{y0}) * WOE_i \\ IV=\sum_{i=0}^N IV_iIV衡量的是某一个变量的信息量，从公式来看的话，相当于是自变量woe值的一个加权求和，其值的大小决定了自变量对于目标变量的影响程度。 可见，IV是一个很好的衡量自变量对目标变量影响程度的指标，可以作为特征选择的一个判断依据。 1.3 特殊情况处理变量的任何分组中，不应该出现响应数=0或非响应数=0的情况，这样会导致WOE的值为正负无穷。 （1）如果可能，直接把这个分组做成一个规则，作为模型的前置条件或补充条件； （2）重新对变量进行离散化或分组，使每个分组的响应比例都不为0且不为100%，尤其是当一个分组个体数很小时（比如小于100个），因为本身把一个分组个体数弄得很小就不是太合理。 （3）如果上面两种方法都无法使用，建议人工把该分组的响应数和非响应的数量进行一定的调整。如果响应数原本为0，可以人工调整响应数为1，如果非响应数原本为0，可以人工调整非响应数为1。]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F07%2F11%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E5%9F%BA%E4%BA%8EChiMerge%E7%9A%84%E6%95%B0%E6%8D%AE%E5%88%86%E7%AE%B1%2F</url>
    <content type="text"><![CDATA[1、卡方检验1.1 简介假设检验就是在样本分布完全未知或只知形式、不知其参数的情况，对样本所提出的假设做出接收还是拒绝的决策。 卡方（$\chi^2$）检验是一种独立性检验（假设检验）工具，被称为20世纪科学技术所有分支的20大发明之一。 1.2 卡方检验基本思想 组别 逾期 不逾期 合计 新车 19 24 43 二手车 34 10 44 合计 53 34 87 H0：假设特征与类别无关 H1：假设特征与类别有关 $\alpha=0.05$ 首先假设H0成立，基于此前提计算出卡方值，它表示观察值与理论值之间的偏离程度。根据卡方及自由度可以确定在H0假设成立的情况下的概率。这个概率很小，说明观察值与理论值偏离程度太大，应当拒绝无效假设，也就是说特征与类别有关；否则尚不能说明样本所代表的实际情况和理论假设有差别。 1.3 计算公式 \chi^2 = \sum_{i=1}^m \sum_{j=1}^k= \frac {(A_{ij}-E_{ij})^2} {E_{ij}}其中， m是要计算的特征值个数 k是标签类别数 $A_{ij}$ 是第i个特征在第j个标签内的样本数量 $Ri=\sum{j=1}^k A_{ij}$ 第i个特征的样本数量 $Cj=\sum{i=1}^m A_{ij}$ 第j个类别的样本数量 $N=\sum_{j=1}^k C_j$ 样本数量 $E{ij}=\frac {R_i * C_j} {N}$ 理论值，$A{ij}$的期望频率 1.4 示例示例 组别 逾期 不逾期 合计 新车 19 24 43 二手车 34 10 44 合计 53 34 87 $m=2,k=2,i=0,j=0$ $A_{ij}=19, R_i=43, C_j=53,N=78$ $E_{ij}=43*53/87=26.2$ 换种方式看，我们假设汽车的类别与是否逾期是无关的，随机抽取一条样本，逾期的概率就是$53/87=0.609$，那么新车的样本数是43，则新车逾期的样本数就是$43*0.609=26.2$ 组别 逾期 不逾期 合计 新车 43 * 0.609 = 26.2 43 * 0.391 = 16.8 43 二手车 44 * 0.609 = 26.8 44 * 0.391 = 17.2 44 合计 53 34 87 \chi^2= \frac {(19-26.2)^2}{26.2}+ \frac {(34-26.8)^2}{26.8}+ \frac {(24-16.8)^2}{16.8}+ \frac {(10-17.2)^2}{17.2}=10.01我们已经得到了卡方值，但是这并不够直接明了的说明问题。因此，我们还需要将卡方值转化为 p-value。p-value是一种给定原假设为真时样本结果出现的概率，我们可以通过简单查表来进行转化。 这里需要用到一个自由度的概念，自由度等于V = (行数 - 1) * (列数 - 1)，对四格表，自由度V = 1。 查表得，$10.01&gt;9.55$，对应的p-value是0.002，也就是说在假设无关的前提下，只有0.2%的样本符合这种假设，自然就说明特征与类别是相关度很高的。 1.5 基于卡方的特征选择卡方值可以反映特征的独立程度，卡方越小越独立。如果独立性高，那么表示两者没太大关系，特征可以舍弃；如果独立性小，两者相关性高，则说明该特征会对应变量产生比较大的影响，应当选择。 12345val selector = new ChiSqSelector(). setNumTopFeatures(1). //选择TOP N个最重要的特征 setFeaturesCol("features"). setLabelCol("label"). setOutputCol("selected-feature") 2、分箱算法也称数据离散化。在建模中，需要对连续变量离散化，特征离散化后，模型会更稳定，降低模型过拟合的风险；并且离散化后的特征可以通过编码的方式更容易搭建模型。 2.1 等宽和等频分箱属于无监督分箱。此外还有基于K-Means聚类的分箱等。 等宽：从最小到最大值之间，均分成N等份。这样，每个区间的长度为$w=(max-min)/N$ 等频：每个区间包含大致相等的样本数量，比如，若$N=10$，则每个区间应该包含约10%的样本。 2.2 ChiMerge分箱属于有监督分箱，即在分箱时考虑因变量的取值，使得分箱后达到某个期望的结果。此外还有基于信息增益，基尼指数的分箱方法。 基本思想：ChiMerge算法用卡方统计量来决定相邻区间是否与类别有相关性。如果两个区间的卡方值很小，说明特征值和类别是相互独立的，就需要合并。否则可以保留分隔状态 ChiMerge算法包括两部分：1、初始化，2、自底向上合并，当满足停止条件的时候，区间合并停止。 第一步：初始化 根据要离散的属性对实例进行排序：每个实例属于一个区间 第二步：合并区间，又包括两步骤 (1) 计算每一对相邻区间的卡方值 (2) 将卡方值最小的一对区间合并 论文中的停止条件是最小的卡方值达到一定的阈值，但是比较难确定。目前采用的停止条件是设置一个最小区间数和最大迭代次数。 ChiMerge的优势： 连续型转离散型 降低了离群点的影响]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F07%2F10%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FlightGBM%2F</url>
    <content type="text"><![CDATA[LightGBM 中文文档: http://lightgbm.apachecn.org/cn/latest/ LightGBM 英文文档: http://lightgbm.apachecn.org/en/latest/ LightGBM 官方文档: https://lightgbm.readthedocs.io/en/latest/ 安装LightGBM 依赖于 OpenMP 进行编译, 然而 Apple Clang 不支持它. 请使用以下命令来安装 gcc/g++ : 12brew install cmakebrew install gcc --without-multilib 然后安装 LightGBM: 12345git clone --recursive https://github.com/Microsoft/LightGBM ; cd LightGBMexport CXX=g++-8 CC=gcc-8mkdir build ; cd buildcmake ..make -j4 cmake .. 这一步之后报错 12345678910111213&gt; CMake Error at /usr/local/Cellar/cmake/3.11.4/share/cmake/Modules/CMakeDetermineCCompiler.cmake:48 (message):&gt; Could not find compiler set in environment variable CC:&gt; &gt; gcc-7.&gt; Call Stack (most recent call first):&gt; CMakeLists.txt:9 (PROJECT)&gt; &gt; &gt; CMake Error: CMAKE_C_COMPILER not set, after EnableLanguage&gt; CMake Error: CMAKE_CXX_COMPILER not set, after EnableLanguage&gt; -- Configuring incomplete, errors occurred!&gt; See also &quot;/Users/david/david/git/LightGBM/build/CMakeFiles/CMakeOutput.log&quot;.&gt; &gt; https://github.com/Microsoft/LightGBM/issues/118官方的解答并不适用，看了下发现gcc的版本已经自动装到8了，所以将export这里改成了 12&gt; export CXX=g++-8 CC=gcc-8&gt; &gt; 然后通过 或者直接用 1pip install lightgbm 安装 训练数据格式LightGBM 支持 CSV, TSV 和 LibSVM 格式的输入数据文件。 Label 是第一列的数据，文件中是不包含 header（标题） 的。 LightGBM 可以直接使用 categorical feature（类别特征）（不需要单独编码）。 Expo data 实验显示，与 one-hot 编码相比，其速度提高了 8 倍。 权重和 Query/Group 数据LightGBM 也支持加权训练，它需要一个额外的 加权数据 。 它需要额外的 query 数据 用于排名任务。 11/3/2016 更新: 现在支持 header（标题）输入 可以指定 label 列，权重列和 query/group id 列。 索引和列都支持 可以指定一个被忽略的列的列表 参数快速查看参数格式是 key1=value1 key2=value2 ... 。 参数可以在配置文件和命令行中。 想要了解全部的参数， 请参阅 Parameters（参数）. 运行 LightGBM1./lightgbm config=your_config_file other_args ... 使用lightgbm画并保存Feature Importance 【lightgbm 使用经验总结教训】 对xgboost和lightgbm的理解及其调参应该关注的点 调参官方文档的调参http://lightgbm.apachecn.org/cn/latest/Parameters-Tuning.html 针对 Leaf-wise (最佳优先) 树的参数优化num_leaves. 这是控制树模型复杂度的主要参数 CV调参https://github.com/wanglei5205/Machine_learning/blob/master/Boosting--LightGBM/lgb-python/2.lightgbm%E8%B0%83%E5%8F%82%E6%A1%88%E4%BE%8B.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227# -*- coding: utf-8 -*-"""# 作者：wanglei5205# 邮箱：wanglei5205@126.com# 博客：http://cnblogs.com/wanglei5205# github：http://github.com/wanglei5205"""### 导入模块import numpy as npimport pandas as pdimport lightgbm as lgbfrom sklearn import metrics### 载入数据print('载入数据')dataset1 = pd.read_csv('G:/ML/ML_match/IJCAI/data3.22/3.22ICJAI/data/7_train_data1.csv')dataset2 = pd.read_csv('G:/ML/ML_match/IJCAI/data3.22/3.22ICJAI/data/7_train_data2.csv')dataset3 = pd.read_csv('G:/ML/ML_match/IJCAI/data3.22/3.22ICJAI/data/7_train_data3.csv')dataset4 = pd.read_csv('G:/ML/ML_match/IJCAI/data3.22/3.22ICJAI/data/7_train_data4.csv')dataset5 = pd.read_csv('G:/ML/ML_match/IJCAI/data3.22/3.22ICJAI/data/7_train_data5.csv')print('数据去重')dataset1.drop_duplicates(inplace=True)dataset2.drop_duplicates(inplace=True)dataset3.drop_duplicates(inplace=True)dataset4.drop_duplicates(inplace=True)dataset5.drop_duplicates(inplace=True)print('数据合并')trains = pd.concat([dataset1,dataset2],axis=0)trains = pd.concat([trains,dataset3],axis=0)trains = pd.concat([trains,dataset4],axis=0)online_test = dataset5### 数据拆分(训练集+验证集+测试集)print('数据拆分')from sklearn.model_selection import train_test_splittrain_xy,offline_test = train_test_split(trains,test_size = 0.2,random_state=21)train,val = train_test_split(train_xy,test_size = 0.2,random_state=21)# 训练集y_train = train.is_trade # 训练集标签X_train = train.drop(['instance_id','is_trade'],axis=1) # 训练集特征矩阵# 验证集y_val = val.is_trade # 验证集标签X_val = val.drop(['instance_id','is_trade'],axis=1) # 验证集特征矩阵# 测试集offline_test_X = offline_test.drop(['instance_id','is_trade'],axis=1) # 线下测试特征矩阵online_test_X = online_test.drop(['instance_id'],axis=1) # 线上测试特征矩阵### 数据转换print('数据转换')lgb_train = lgb.Dataset(X_train, y_train, free_raw_data=False)lgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train,free_raw_data=False)### 设置初始参数--不含交叉验证参数print('设置参数')params = &#123; 'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'binary_logloss', &#125;### 交叉验证(调参)print('交叉验证')min_merror = float('Inf')best_params = &#123;&#125;# 准确率print("调参1：提高准确率")for num_leaves in range(20,200,5): for max_depth in range(3,8,1): params['num_leaves'] = num_leaves params['max_depth'] = max_depth cv_results = lgb.cv( params, lgb_train, seed=2018, nfold=3, metrics=['binary_error'], early_stopping_rounds=10, verbose_eval=True ) mean_merror = pd.Series(cv_results['binary_error-mean']).min() boost_rounds = pd.Series(cv_results['binary_error-mean']).argmin() if mean_merror &lt; min_merror: min_merror = mean_merror best_params['num_leaves'] = num_leaves best_params['max_depth'] = max_depthparams['num_leaves'] = best_params['num_leaves']params['max_depth'] = best_params['max_depth']# 过拟合print("调参2：降低过拟合")for max_bin in range(1,255,5): for min_data_in_leaf in range(10,200,5): params['max_bin'] = max_bin params['min_data_in_leaf'] = min_data_in_leaf cv_results = lgb.cv( params, lgb_train, seed=42, nfold=3, metrics=['binary_error'], early_stopping_rounds=3, verbose_eval=True ) mean_merror = pd.Series(cv_results['binary_error-mean']).min() boost_rounds = pd.Series(cv_results['binary_error-mean']).argmin() if mean_merror &lt; min_merror: min_merror = mean_merror best_params['max_bin']= max_bin best_params['min_data_in_leaf'] = min_data_in_leafparams['min_data_in_leaf'] = best_params['min_data_in_leaf']params['max_bin'] = best_params['max_bin']print("调参3：降低过拟合")for feature_fraction in [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]: for bagging_fraction in [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]: for bagging_freq in range(0,50,5): params['feature_fraction'] = feature_fraction params['bagging_fraction'] = bagging_fraction params['bagging_freq'] = bagging_freq cv_results = lgb.cv( params, lgb_train, seed=42, nfold=3, metrics=['binary_error'], early_stopping_rounds=3, verbose_eval=True ) mean_merror = pd.Series(cv_results['binary_error-mean']).min() boost_rounds = pd.Series(cv_results['binary_error-mean']).argmin() if mean_merror &lt; min_merror: min_merror = mean_merror best_params['feature_fraction'] = feature_fraction best_params['bagging_fraction'] = bagging_fraction best_params['bagging_freq'] = bagging_freqparams['feature_fraction'] = best_params['feature_fraction']params['bagging_fraction'] = best_params['bagging_fraction']params['bagging_freq'] = best_params['bagging_freq']print("调参4：降低过拟合")for lambda_l1 in [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]: for lambda_l2 in [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]: for min_split_gain in [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]: params['lambda_l1'] = lambda_l1 params['lambda_l2'] = lambda_l2 params['min_split_gain'] = min_split_gain cv_results = lgb.cv( params, lgb_train, seed=42, nfold=3, metrics=['binary_error'], early_stopping_rounds=3, verbose_eval=True ) mean_merror = pd.Series(cv_results['binary_error-mean']).min() boost_rounds = pd.Series(cv_results['binary_error-mean']).argmin() if mean_merror &lt; min_merror: min_merror = mean_merror best_params['lambda_l1'] = lambda_l1 best_params['lambda_l2'] = lambda_l2 best_params['min_split_gain'] = min_split_gainparams['lambda_l1'] = best_params['lambda_l1']params['lambda_l2'] = best_params['lambda_l2']params['min_split_gain'] = best_params['min_split_gain']print(best_params)### 训练params['learning_rate']=0.01lgb.train( params, # 参数字典 lgb_train, # 训练集 valid_sets=lgb_eval, # 验证集 num_boost_round=2000, # 迭代次数 early_stopping_rounds=50 # 早停次数 )### 线下预测print ("线下预测")preds_offline = lgb.predict(offline_test_X, num_iteration=lgb.best_iteration) # 输出概率offline=offline_test[['instance_id','is_trade']]offline['preds']=preds_offlineoffline.is_trade = offline['is_trade'].astype(np.float64)print('log_loss', metrics.log_loss(offline.is_trade, offline.preds))### 线上预测print("线上预测")preds_online = lgb.predict(online_test_X, num_iteration=lgb.best_iteration) # 输出概率online=online_test[['instance_id']]online['preds']=preds_onlineonline.rename(columns=&#123;'preds':'predicted_score'&#125;,inplace=True) # 更改列名online.to_csv("./data/20180405.txt",index=None,sep=' ') # 保存结果### 保存模型from sklearn.externals import joblibjoblib.dump(lgb,'lgb.pkl')### 特征选择df = pd.DataFrame(X_train.columns.tolist(), columns=['feature'])df['importance']=list(lgb.feature_importance()) # 特征分数df = df.sort_values(by='importance',ascending=False) # 特征排序df.to_csv("./data/feature_score_20180331.csv",index=None,encoding='gbk') # 保存分数 样本集处理加载样本集，去掉app_code和日期列，保存为新样本集 12345678910111213141516171819df04 = pd.read_csv('3dtrain_aprilnew.csv', header=0)df04 = df04.drop(df04.columns[[0,-2]], axis=1)print(df04.shape)# print(df04['main_unittype_3days'] &gt; 0)df04 = df04[df04['main_unittype_3days'] &gt; 0]df04 = df04.drop(df04.columns[['all_district_entropy_avg_3days', 'all_max_distance_with_guarantee_now_unit_address_max_3days', 'all_max_distance_with_guarantee_now_unit_address_min_3days', 'all_max_distance_with_guarantee_now_unit_address_avg_3days', 'all_max_distance_with_guarantee_now_unit_address_sd_3days', 'all_max_distance_with_now_address_max_3days', 'all_max_distance_with_now_address_min_3days', 'all_max_distance_with_now_address_avg_3days', 'all_max_distance_with_now_address_sd_3days', 'all_min_time_between_accon_accoff_avg_3days', 'all_min_time_between_accon_accoff_sd_3days', 'all_overspeed120_time_before_dawn_sd_3days', 'all_overspeed120_time_daytime_sd_3days', 'all_total_time_near_now_unit_address_max_3days', 'all_total_time_near_now_unit_address_min_3days', 'all_total_time_near_now_unit_address_avg_3days', 'all_total_time_near_now_unit_address_sd_3days', 'topone_district_most_per_3days', 'main_unittype_3days', 'is_unittype_changed_3days']], axis=1)# df04 = df04[(True - (df04['days_samewith_now_address_3days'] == "-0.0507071"))]# df04 = df04[df04["is_gps_yx_lost_nighttime_district_3days"] != "-0.0507071"]# df04.loc[df04['days_samewith_now_address_3days'] != "-0.0507071"]print(df04.head)# df04.to_csv('3dtrain_april_yx.csv', index=False)df05 = pd.read_csv('3dpredict_maynew.csv', header=0)print(df05.shape)df05 = df05.drop(df05.columns[[0,-2]], axis=1)df05 = df05[df05['main_unittype_3days'] &gt; 0]print(df05.shape)df05.to_csv('3dpredict_may_yx.csv', index=False) 从最佳迭代开始预测1y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration) lightgbm算法优化-不平衡二分类问题（附代码）https://yq.aliyun.com/articles/529970 减少打印日志设置verbosity 123456params = &#123; &apos;boosting_type&apos;: &apos;gbdt&apos;, &apos;objective&apos;: &apos;binary&apos;, &apos;metric&apos;: &apos;binary_logloss&apos;, &apos;verbosity&apos;: -1, &#125; 问题记录进程被killed执行到cv时就显示Killed，被python杀死，可能是内存不够了 https://stackoverflow.com/questions/726690/what-killed-my-process-and-why Java调用https://blog.csdn.net/luoyexuge/article/details/80087952 处理离散特征而Lightgbm可以直接支持category特征的处理，在用pandas结构使用LGB时可以指定哪一列是类别型数据，省去one-hot的步骤。 12345data[item] = data[item].astype(&quot;category&quot;).cat.codes +1train, test, y_train, y_test = train_test_split(data.drop([&quot;ARRIVAL_DELAY&quot;], axis=1), data[&quot;ARRIVAL_DELAY&quot;],random_state=10, test_size=0.25)]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F07%2F10%2Fhadoop-spark%2Fspark%2Fpyspark%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[操作Dataframe与pandas 1234567891011121314151617181920212223242526272829303132333435363738394041424344# -*- coding: utf-8 -*-import pandas as pdfrom pyspark.sql import SparkSessionfrom pyspark.sql import SQLContextfrom pyspark import SparkContext #初始化数据 #初始化pandas DataFramedf = pd.DataFrame([[1, 2, 3], [4, 5, 6]], index=['row1', 'row2'], columns=['c1', 'c2', 'c3']) #打印数据print df #初始化spark DataFramesc = SparkContext()if __name__ == "__main__": spark = SparkSession\ .builder\ .appName("testDataFrame")\ .getOrCreate() sentenceData = spark.createDataFrame([ (0.0, "I like Spark"), (1.0, "Pandas is useful"), (2.0, "They are coded by Python ")], ["label", "sentence"]) #显示数据sentenceData.select("label").show() #spark.DataFrame 转换成 pandas.DataFramesqlContest = SQLContext(sc)spark_df = sqlContest.createDataFrame(df) #显示数据spark_df.select("c1").show() # pandas.DataFrame 转换成 spark.DataFramepandas_df = sentenceData.toPandas() #打印数据print pandas_df]]></content>
  </entry>
  <entry>
    <title><![CDATA[特征工程相关算法]]></title>
    <url>%2F2018%2F07%2F03%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[[TOC] 1、卡方检验1.1 简介假设检验就是在样本分布完全未知或只知形式、不知其参数的情况，对样本所提出的假设做出接收还是拒绝的决策。 卡方（$\chi^2$）检验是一种独立性检验（假设检验）工具，被称为20世纪科学技术所有分支的20大发明之一。 1.2 卡方检验基本思想 组别 逾期 不逾期 合计 新车 19 24 43 二手车 34 10 44 合计 53 34 87 H0：假设特征与类别无关 H1：假设特征与类别有关 $\alpha=0.05$ 首先假设H0成立，基于此前提计算出卡方值，它表示观察值与理论值之间的偏离程度。根据卡方及自由度可以确定在H0假设成立的情况下的概率。这个概率很小，说明观察值与理论值偏离程度太大，应当拒绝无效假设，也就是说特征与类别有关；否则尚不能说明样本所代表的实际情况和理论假设有差别。 1.3 计算公式 \chi^2 = \sum_{i=1}^m \sum_{j=1}^k= \frac {(A_{ij}-E_{ij})^2} {E_{ij}}其中， m是要计算的特征值个数 k是标签类别数 $A_{ij}$ 是第i个特征在第j个标签内的样本数量 $Ri=\sum{j=1}^k A_{ij}$ 第i个特征的样本数量 $Cj=\sum{i=1}^m A_{ij}$ 第j个类别的样本数量 $N=\sum_{j=1}^k C_j$ 样本数量 $E{ij}=\frac {R_i * C_j} {N}$ 理论值，$A{ij}$的期望频率 1.4 示例示例 组别 逾期 不逾期 合计 新车 19 24 43 二手车 34 10 44 合计 53 34 87 $m=2,k=2,i=0,j=0$ $A_{ij}=19, R_i=43, C_j=53,N=78$ $E_{ij}=43*53/87=26.2$ 换种方式看，我们假设汽车的类别与是否逾期是无关的，随机抽取一条样本，逾期的概率就是$53/87=0.609$，那么新车的样本数是43，则新车逾期的样本数就是$43*0.609=26.2$ 组别 逾期 不逾期 合计 新车 43 * 0.609 = 26.2 43 * 0.391 = 16.8 43 二手车 44 * 0.609 = 26.8 44 * 0.391 = 17.2 44 合计 53 34 87 \chi^2= \frac {(19-26.2)^2}{26.2}+ \frac {(34-26.8)^2}{26.8}+ \frac {(24-16.8)^2}{16.8}+ \frac {(10-17.2)^2}{17.2}=10.01我们已经得到了卡方值，但是这并不够直接明了的说明问题。因此，我们还需要将卡方值转化为 p-value。p-value是一种给定原假设为真时样本结果出现的概率，我们可以通过简单查表来进行转化。 这里需要用到一个自由度的概念，自由度等于V = (行数 - 1) * (列数 - 1)，对四格表，自由度V = 1。 查表得，$10.01&gt;9.55$，对应的p-value是0.002，也就是说在假设无关的前提下，只有0.2%的样本符合这种假设，自然就说明特征与类别是相关度很高的。 1.5 基于卡方的特征选择卡方值可以反映特征的独立程度，卡方越小越独立。如果独立性高，那么表示两者没太大关系，特征可以舍弃；如果独立性小，两者相关性高，则说明该特征会对应变量产生比较大的影响，应当选择。 12345val selector = new ChiSqSelector(). setNumTopFeatures(1). //选择TOP N个最重要的特征 setFeaturesCol("features"). setLabelCol("label"). setOutputCol("selected-feature") 2、分箱算法也称数据离散化。在建模中，需要对连续变量离散化，特征离散化后，模型会更稳定，降低模型过拟合的风险；并且离散化后的特征可以通过编码的方式更容易搭建模型。 2.1 等宽和等频分箱属于无监督分箱。此外还有基于K-Means聚类的分箱等。 等宽：从最小到最大值之间，均分成N等份。这样，每个区间的长度为$w=(max-min)/N$ 等频：每个区间包含大致相等的样本数量，比如，若$N=10$，则每个区间应该包含约10%的样本。 2.2 ChiMerge分箱属于有监督分箱，即在分箱时考虑因变量的取值，使得分箱后达到某个期望的结果。此外还有基于信息增益，基尼指数的分箱方法。 基本思想：ChiMerge算法用卡方统计量来决定相邻区间是否与类别有相关性。如果两个区间的卡方值很小，说明特征值和类别是相互独立的，就需要合并。否则可以保留分隔状态 ChiMerge算法包括两部分：1、初始化，2、自底向上合并，当满足停止条件的时候，区间合并停止。 第一步：初始化 根据要离散的属性对实例进行排序：每个实例属于一个区间 第二步：合并区间，又包括两步骤 (1) 计算每一对相邻区间的卡方值 (2) 将卡方值最小的一对区间合并 论文中的停止条件是最小的卡方值达到一定的阈值，但是比较难确定。目前采用的停止条件是设置一个最小区间数和最大迭代次数。 ChiMerge的优势： 连续型转离散型 降低了离群点的影响 3、WOE和IV计算3.1 WOE计算公式WOE可以将离散型变量转成连续型变量，假设离散化后共有N组数据，则每组的WOE计算公式为： WOE_i=\ln(\frac {P_{y1}} {P_{y0}}) = \ln( \frac {B_i/B_T} {G_i/G_T} ) , i=1,2...,N其中，$P{y1}$ 是该组中的响应客户（这里对应的就是逾期客户）占所有样本中所有响应客户的比例，$P{y0}$就是未响应的比例。$B_i$是该组中响应的数量，$B_T$是总响应数量，$G_i$是该组中未响应的数量，$G_T$是总未响应数量。 从这个公式中可以体现出，WOE表示的是“当前分组中响应客户占所有响应客户的比例”和“当前分组中没有响应的客户占所有没有响应的客户的比例”的差异。 总里程 逾期 不逾期 响应比例 WOE 0-10 10 1000 10% 0 10-100 25 2000 12.5% 0.223 100-500 15 2000 7.5% -0.287 总计 50 5000 10% WOE_i= \ln( \frac {B_i/B_T} {G_i/G_T} ) =\ln( \frac {B_i/G_i} {B_T/G_T} ) , i=1,2...,N对WOE稍作变换，可以看出WOE也可以这么理解，他表示的是当前这个组中响应的客户和未响应客户的比值，和所有样本中这个比值的差异。这个差异是用这两个比值的比值，再取对数来表示的。WOE越大，这种差异越大，这个分组里的样本响应的可能性就越大，WOE越小，差异越小，这个分组里的样本响应的可能性就越小。 总结WOE的几个性质： 1）当前分组中，响应的比例越大，WOE值越大； 2）当前分组WOE的正负，由当前分组响应和未响应的比例，与样本整体响应和未响应的比例的大小关系决定，当前分组的比例小于样本整体比例时，WOE为负，当前分组的比例大于整体比例时，WOE为正，当前分组的比例和整体比例相等时，WOE为0。 3）WOE的取值范围是全体实数。 可以看出，采用WOE进行离散转连续的好处是： 1）WOE相对比离散型变量，可疑反映出自变量与目标变量之间的非线性关系，理论上能够提升预测的准确率； 2）自变量的标准化处理。自变量内部的各个取值之间都可以直接进行比较（WOE之间的比较），而不同自变量之间的各种取值也可以通过WOE进行直接的比较。 3.2 IV计算公式 IV_i = (P_{y1}-P_{y0}) * WOE_i \\ IV=\sum_{i=0}^N IV_iIV衡量的是某一个变量的信息量，从公式来看的话，相当于是自变量woe值的一个加权求和，其值的大小决定了自变量对于目标变量的影响程度。 可见，IV是一个很好的衡量自变量对目标变量影响程度的指标，可以作为特征选择的一个判断依据。 3.3 特殊情况处理变量的任何分组中，不应该出现响应数=0或非响应数=0的情况，这样会导致WOE的值为正负无穷。 （1）如果可能，直接把这个分组做成一个规则，作为模型的前置条件或补充条件； （2）重新对变量进行离散化或分组，使每个分组的响应比例都不为0且不为100%，尤其是当一个分组个体数很小时（比如小于100个），因为本身把一个分组个体数弄得很小就不是太合理。 （3）如果上面两种方法都无法使用，建议人工把该分组的响应数和非响应的数量进行一定的调整。如果响应数原本为0，可以人工调整响应数为1，如果非响应数原本为0，可以人工调整非响应数为1。 参考： ChiMerge: Discretization of Numeric Attributes https://www.cnblogs.com/barrenlake/p/4354579.html https://blog.csdn.net/textboy/article/details/47008049 https://blog.csdn.net/u013421629/article/details/78416748 https://blog.csdn.net/huobanjishijian/article/details/52893357 https://blog.csdn.net/shuzfan/article/details/52993427]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>ChiMerge</tag>
        <tag>WOE</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F06%2F28%2Fhadoop-spark%2Fmllib%2Fmllib%E7%9A%84%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%2F</url>
    <content type="text"><![CDATA[http://www.cnblogs.com/xing901022/p/7152922.html 在机器学习中，一般都会按照下面几个步骤：特征提取、数据预处理、特征选择、模型训练、检验优化。那么特征的选择就很关键了，一般模型最后效果的好坏往往都是跟特征的选择有关系的，因为模型本身的参数并没有太多优化的点，反而特征这边有时候多加一个或者少加一个，最终的结果都会差别很大。 在SparkMLlib中为我们提供了几种特征选择的方法，分别是VectorSlicer、RFormula和ChiSqSelector。 下面就介绍下这三个方法的使用,强烈推荐有时间的把参考的文献都阅读下，会有所收获！ VectorSlicer这个转换器可以支持用户自定义选择列，可以基于下标索引，也可以基于列名。 如果是下标都可以使用setIndices方法 如果是列名可以使用setNames方法。使用这个方法的时候，vector字段需要通过AttributeGroup设置每个向量元素的列名。 注意1：可以同时使用setInices和setName1234567891011121314151617181920212223object VectorSlicer &#123; def main(args: Array[String]) &#123; val conf = new SparkConf().setAppName(&quot;VectorSlicer-Test&quot;).setMaster(&quot;local[2]&quot;) val sc = new SparkContext(conf) sc.setLogLevel(&quot;WARN&quot;) var sqlContext = new SQLContext(sc) val data = Array(Row(Vectors.dense(-2.0, 2.3, 0.0,1.0,2.0))) val defaultAttr = NumericAttribute.defaultAttr val attrs = Array(&quot;f1&quot;, &quot;f2&quot;, &quot;f3&quot;,&quot;f4&quot;,&quot;f5&quot;).map(defaultAttr.withName) val attrGroup = new AttributeGroup(&quot;userFeatures&quot;, attrs.asInstanceOf[Array[Attribute]]) val dataRDD = sc.parallelize(data) val dataset = sqlContext.createDataFrame(dataRDD, StructType(Array(attrGroup.toStructField()))) val slicer = new VectorSlicer().setInputCol(&quot;userFeatures&quot;).setOutputCol(&quot;features&quot;) slicer.setIndices(Array(0)).setNames(Array(&quot;f2&quot;)) val output = slicer.transform(dataset) println(output.select(&quot;userFeatures&quot;, &quot;features&quot;).first()) &#125;&#125; 注意2：如果下标和索引重复，会报重复的错：比如: 1slicer.setIndices(Array(1)).setNames(Array(&quot;f2&quot;)) 那么会遇到报错 1234567891011Exception in thread &quot;main&quot; java.lang.IllegalArgumentException: requirement failed: VectorSlicer requires indices and names to be disjoint sets of features, but they overlap. indices: [1]. names: [1:f2] at scala.Predef$.require(Predef.scala:233) at org.apache.spark.ml.feature.VectorSlicer.getSelectedFeatureIndices(VectorSlicer.scala:137) at org.apache.spark.ml.feature.VectorSlicer.transform(VectorSlicer.scala:108) at xingoo.mllib.VectorSlicer$.main(VectorSlicer.scala:35) at xingoo.mllib.VectorSlicer.main(VectorSlicer.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:497) at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144) 注意3：如果下标不存在1slicer.setIndices(Array(6)) 如果数组越界也会报错 1234567891011121314151617181920Exception in thread &quot;main&quot; java.lang.ArrayIndexOutOfBoundsException: 6 at org.apache.spark.ml.feature.VectorSlicer$$anonfun$3$$anonfun$apply$2.apply(VectorSlicer.scala:110) at org.apache.spark.ml.feature.VectorSlicer$$anonfun$3$$anonfun$apply$2.apply(VectorSlicer.scala:110) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244) at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) at scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:156) at scala.collection.TraversableLike$class.map(TraversableLike.scala:244) at scala.collection.mutable.ArrayOps$ofInt.map(ArrayOps.scala:156) at org.apache.spark.ml.feature.VectorSlicer$$anonfun$3.apply(VectorSlicer.scala:110) at org.apache.spark.ml.feature.VectorSlicer$$anonfun$3.apply(VectorSlicer.scala:109) at scala.Option.map(Option.scala:145) at org.apache.spark.ml.feature.VectorSlicer.transform(VectorSlicer.scala:109) at xingoo.mllib.VectorSlicer$.main(VectorSlicer.scala:35) at xingoo.mllib.VectorSlicer.main(VectorSlicer.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:497) at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144) 注意4：如果名称不存在也会报错1234567891011121314151617181920Exception in thread &quot;main&quot; java.lang.IllegalArgumentException: requirement failed: getFeatureIndicesFromNames found no feature with name f8 in column StructField(userFeatures,org.apache.spark.mllib.linalg.VectorUDT@f71b0bce,false). at scala.Predef$.require(Predef.scala:233) at org.apache.spark.ml.util.MetadataUtils$$anonfun$getFeatureIndicesFromNames$2.apply(MetadataUtils.scala:89) at org.apache.spark.ml.util.MetadataUtils$$anonfun$getFeatureIndicesFromNames$2.apply(MetadataUtils.scala:88) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244) at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108) at scala.collection.TraversableLike$class.map(TraversableLike.scala:244) at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108) at org.apache.spark.ml.util.MetadataUtils$.getFeatureIndicesFromNames(MetadataUtils.scala:88) at org.apache.spark.ml.feature.VectorSlicer.getSelectedFeatureIndices(VectorSlicer.scala:129) at org.apache.spark.ml.feature.VectorSlicer.transform(VectorSlicer.scala:108) at xingoo.mllib.VectorSlicer$.main(VectorSlicer.scala:35) at xingoo.mllib.VectorSlicer.main(VectorSlicer.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:497) at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144) 注意5：经过特征选择后，特征的顺序与索引和名称的顺序相同RFormula这个转换器可以帮助基于R模型，自动生成feature和label。比如说最常用的线性回归，在先用回归中，我们需要把一些离散化的变量变成哑变量，即转变成onehot编码，使之数值化，这个我之前的文章也介绍过，这里就不多说了。 如果不是用这个RFormula，我们可能需要经过几个步骤： 1StringIndex...OneHotEncoder... 而且每个特征都要经过这样的变换，非常繁琐。有了RFormula，几乎可以一键把所有的特征问题解决。 id coutry hour clicked 7 US 18 1.0 8 CA 12 0.0 9 NZ 15 0.0 然后我们只要写一个类似这样的公式clicked ~ country + hour + my_test，就代表clicked为label，coutry、hour、my_test是三个特征 比如下面的代码： 123456789101112131415161718192021object RFormulaTest &#123; def main(args: Array[String]) &#123; val conf = new SparkConf().setAppName(&quot;RFormula-Test&quot;).setMaster(&quot;local[2]&quot;) val sc = new SparkContext(conf) sc.setLogLevel(&quot;WARN&quot;) var sqlContext = new SQLContext(sc) val dataset = sqlContext.createDataFrame(Seq( (7, &quot;US&quot;, 18, 1.0,&quot;a&quot;), (8, &quot;CA&quot;, 12, 0.0,&quot;b&quot;), (9, &quot;NZ&quot;, 15, 0.0,&quot;a&quot;) )).toDF(&quot;id&quot;, &quot;country&quot;, &quot;hour&quot;, &quot;clicked&quot;,&quot;my_test&quot;) val formula = new RFormula() .setFormula(&quot;clicked ~ country + hour + my_test&quot;) .setFeaturesCol(&quot;features&quot;) .setLabelCol(&quot;label&quot;) val output = formula.fit(dataset).transform(dataset) output.show() output.select(&quot;features&quot;, &quot;label&quot;).show() &#125;&#125; 得到的结果 123456789101112131415+---+-------+----+-------+-------+------------------+-----+| id|country|hour|clicked|my_test| features|label|+---+-------+----+-------+-------+------------------+-----+| 7| US| 18| 1.0| a|[0.0,0.0,18.0,1.0]| 1.0|| 8| CA| 12| 0.0| b|[1.0,0.0,12.0,0.0]| 0.0|| 9| NZ| 15| 0.0| a|[0.0,1.0,15.0,1.0]| 0.0|+---+-------+----+-------+-------+------------------+-----++------------------+-----+| features|label|+------------------+-----+|[0.0,0.0,18.0,1.0]| 1.0||[1.0,0.0,12.0,0.0]| 0.0||[0.0,1.0,15.0,1.0]| 0.0|+------------------+-----+ ChiSqSelector这个选择器支持基于卡方检验的特征选择，卡方检验是一种计算变量独立性的检验手段。具体的可以参考维基百科，最终的结论就是卡方的值越大，就是我们越想要的特征。因此这个选择器就可以理解为，再计算卡方的值，最后按照这个值排序，选择我们想要的个数的特征。 代码也很简单 12345678910111213141516171819202122232425262728object ChiSqSelectorTest &#123; def main(args: Array[String]) &#123; val conf = new SparkConf().setAppName(&quot;ChiSqSelector-Test&quot;).setMaster(&quot;local[2]&quot;) val sc = new SparkContext(conf) sc.setLogLevel(&quot;WARN&quot;) var sqlContext = new SQLContext(sc) val data = Seq( (7, Vectors.dense(0.0, 0.0, 18.0, 1.0), 1.0), (8, Vectors.dense(0.0, 1.0, 12.0, 0.0), 0.0), (9, Vectors.dense(1.0, 0.0, 15.0, 0.1), 0.0) ) val beanRDD = sc.parallelize(data).map(t3 =&gt; Bean(t3._1,t3._2,t3._3)) val df = sqlContext.createDataFrame(beanRDD) val selector = new ChiSqSelector() .setNumTopFeatures(2) .setFeaturesCol(&quot;features&quot;) .setLabelCol(&quot;clicked&quot;) .setOutputCol(&quot;selectedFeatures&quot;) val result = selector.fit(df).transform(df) result.show() &#125; case class Bean(id:Double,features:org.apache.spark.mllib.linalg.Vector,clicked:Double)&#123;&#125;&#125; 这样得到的结果： 1234567+---+------------------+-------+----------------+| id| features|clicked|selectedFeatures|+---+------------------+-------+----------------+|7.0|[0.0,0.0,18.0,1.0]| 1.0| [18.0,1.0]||8.0|[0.0,1.0,12.0,0.0]| 0.0| [12.0,0.0]||9.0|[1.0,0.0,15.0,0.1]| 0.0| [15.0,0.1]|+---+------------------+-------+----------------+ 总结下面总结一下三种特征选择的使用场景： VectorSilcer，这个选择器适合那种有很多特征，并且明确知道自己想要哪个特征的情况。比如你有一个很全的用户画像系统，每个人有成百上千个特征，但是你指向抽取用户对电影感兴趣相关的特征，因此只要手动选择一下就可以了。 RFormula，这个选择器适合在需要做OneHotEncoder的时候，可以一个简单的代码把所有的离散特征转化成数值化表示。 ChiSqSelector，卡方检验选择器适合在你有比较多的特征，但是不知道这些特征哪个有用，哪个没用，想要通过某种方式帮助你快速筛选特征，那么这个方法很适合。 以上的总结纯属个人看法，不代表官方做法，如果有其他的见解可以留言~ 多交流！ 参考1 Spark特征处理 2 Spark官方文档 3 如何优化逻辑回归 4 数据挖掘中的VI和WOE 5 Spark卡方选择器 6 卡方分布 7 皮尔逊卡方检验 8 卡方检验原理]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F06%2F27%2Fhadoop-spark%2Fmllib%2Fmllib%E6%BA%90%E7%A0%81%E4%BF%AE%E6%94%B9%2F</url>
    <content type="text"><![CDATA[模仿mllib的pipeline写法。 1、继承Estimate会遇到很多重写的问题，先只继承Params，使得传参更方便。后面要转成DateFrame输出时才用Estimate 2、extends Model[WeightOfEvidence]这个也先去掉了]]></content>
  </entry>
  <entry>
    <title><![CDATA[Shell日期计算]]></title>
    <url>%2F2018%2F06%2F27%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FSHELL%2F%E6%97%A5%E6%9C%9F%E8%AE%A1%E7%AE%97%2F</url>
    <content type="text"><![CDATA[日期循环12345678910#!/bin/sh startdate=`date -d &quot;$1&quot; +%Y-%m-%d` enddate=`date -d &quot;$2&quot; +%Y-%m-%d` while [[ $startdate &lt; $enddate ]] do echo &quot;########$startdate#########&quot; startdate=`date -d &quot;+1 day $startdate&quot; +%Y-%m-%d` done 上个月1last_month_str=`date -d &quot;$(date +%Y-%m-%d) last month&quot; +%Y-%m-%d` 昨天1yesterday_str=`date -d &quot;yesterday&quot; +%Y-%m-%d` N天前1last_half_month_str=`date -d &quot;1 days ago&quot; +%Y-%m-%d`]]></content>
      <categories>
        <category>Shell</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F06%2F26%2Fhadoop-spark%2Fmllib%2Fmllib%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[dataframe多列合并成vector12345678910111213var spark: SparkSession = SparkSession .builder .appName(&quot;deviceC&quot;) .config(&quot;spark.default.parallelism&quot;, 10) .master(&quot;local[4]&quot;) .getOrCreate() val sc = spark.sqlContext val data = sc.createDataFrame(List( (1.0, 2.0, 3.0), (11.0, 21.0, 31.0) )).toDF(&quot;tag1&quot;, &quot;tag2&quot;, &quot;tag3&quot;) val assembler = new VectorAssembler() .setInputCols(Array(&quot;tag1&quot;, &quot;tag2&quot;, &quot;tag2&quot;)) .setOutputCol(&quot;features&quot;) val output = assembler.transform(data)]]></content>
  </entry>
  <entry>
    <title><![CDATA[Scala问题总结]]></title>
    <url>%2F2018%2F06%2F25%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FSCALA%2Fscala%E9%97%AE%E9%A2%98%E9%9B%86%2F</url>
    <content type="text"><![CDATA[1234Error:(52, 30) missing parameter type for expanded functionThe argument types of an anonymous function must be fully known. (SLS 8.5)Expected type was: (?, Double) =&gt; ? disFeatures.reduceLeft &#123; case (left, right) =&gt; 这里多了一个case，去掉就可以 fastjson问题1ambiguous reference to overloaded definition when call method in java library https://stackoverflow.com/questions/40519743/ambiguous-reference-to-overloaded-definition-when-call-method-in-java-library For some reason, Scala overloading logic does not match Java logic. You have to call it like this: 1JSON.toJSONString(map, SerializerFeature.PrettyFormat)]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F06%2F22%2Fhadoop-spark%2Fmllib%2Fmllib%E7%BB%BC%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[包重做 mllib中有很多private的方法无法访问，于是从源码中重做了一个包，修改了POM文件使其可用。目前的版本是1.6.0，项目放在carthage-spark-mllib]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F06%2F22%2Fhadoop-spark%2Fmllib%2F%E8%B5%84%E6%96%99%E9%9B%86%2F</url>
    <content type="text"><![CDATA[gitbook教程 https://endymecy.gitbooks.io/spark-ml-source-analysis/content/%E7%89%B9%E5%BE%81%E6%8A%BD%E5%8F%96%E5%92%8C%E8%BD%AC%E6%8D%A2/chi-square-selector.html]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F06%2F22%2Fhadoop-spark%2Fmllib%2F%E5%8D%A1%E6%96%B9%2F</url>
    <content type="text"><![CDATA[1.6版本下可行的例子 参考http://dblab.xmu.edu.cn/blog/1475-2/ 12345678910111213141516// 创造实验数据，这是一个具有三个样本，四个特征维度的数据集，标签有1，0两种。1.6中，标签必须是double型val df = sqlContext.createDataFrame(Seq( (1, Vectors.dense(0.0, 0.0, 18.0, 1.0), 1.0), (2, Vectors.dense(0.0, 1.0, 12.0, 0.0), 0.0), (3, Vectors.dense(1.0, 0.0, 15.0, 0.1), 0.0) )).toDF(&quot;id&quot;, &quot;features&quot;, &quot;label&quot;) df.show() val selector = new ChiSqSelector(). setNumTopFeatures(1). setFeaturesCol(&quot;features&quot;). setLabelCol(&quot;label&quot;). setOutputCol(&quot;selected_features&quot;) val select_model = selector.fit(df) val result = select_model.transform(df) result.show() 源码的主要逻辑在 ChiSqTest.chiSquaredFeatures 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162def chiSquaredFeatures(data: RDD[LabeledPoint], methodName: String = PEARSON.name): Array[ChiSqTestResult] = &#123; val maxCategories = 10000 val numCols = data.first().features.size val results = new Array[ChiSqTestResult](numCols) var labels: Map[Double, Int] = null // at most 1000 columns at a time val batchSize = 1000 var batch = 0 while (batch * batchSize &lt; numCols) &#123; // The following block of code can be cleaned up and made public as // chiSquared(data: RDD[(V1, V2)]) val startCol = batch * batchSize val endCol = startCol + math.min(batchSize, numCols - startCol) val pairCounts = data.mapPartitions &#123; iter =&gt; val distinctLabels = mutable.HashSet.empty[Double] val allDistinctFeatures: Map[Int, mutable.HashSet[Double]] = Map((startCol until endCol).map(col =&gt; (col, mutable.HashSet.empty[Double])): _*) var i = 1 iter.flatMap &#123; case LabeledPoint(label, features) =&gt; if (i % 1000 == 0) &#123; if (distinctLabels.size &gt; maxCategories) &#123; throw new SparkException(s"Chi-square test expect factors (categorical values) but " + s"found more than $maxCategories distinct label values.") &#125; allDistinctFeatures.foreach &#123; case (col, distinctFeatures) =&gt; if (distinctFeatures.size &gt; maxCategories) &#123; throw new SparkException(s"Chi-square test expect factors (categorical values) but " + s"found more than $maxCategories distinct values in column $col.") &#125; &#125; &#125; i += 1 distinctLabels += label features.toArray.view.zipWithIndex.slice(startCol, endCol).map &#123; case (feature, col) =&gt; allDistinctFeatures(col) += feature (col, feature, label) &#125; &#125; &#125;.countByValue() if (labels == null) &#123; // Do this only once for the first column since labels are invariant across features. labels = pairCounts.keys.filter(_._1 == startCol).map(_._3).toArray.distinct.zipWithIndex.toMap &#125; val numLabels = labels.size pairCounts.keys.groupBy(_._1).map &#123; case (col, keys) =&gt; val features = keys.map(_._2).toArray.distinct.zipWithIndex.toMap val numRows = features.size val contingency = new BDM(numRows, numLabels, new Array[Double](numRows * numLabels)) keys.foreach &#123; case (_, feature, label) =&gt; val i = features(feature) val j = labels(label) contingency(i, j) += pairCounts((col, feature, label)) &#125; results(col) = chiSquaredMatrix(Matrices.fromBreeze(contingency), methodName) &#125; batch += 1 &#125; results &#125; 报错 1Chi-squared statistic undefined for input matrix due to0 sum in column [1]. 某个特征都是0的时候，卡方不能计算]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F06%2F21%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B-ChiMerge%2F</url>
    <content type="text"><![CDATA[ChiMerge 是监督的、自底向上的(即基于合并的)数据离散化方法。它依赖于卡方分析：具有最小卡方值的相邻区间合并在一起，直到满足确定的停止准则。 基本思想：对于精确的离散化，相对类频率在一个区间内应当完全一致。因此，如果两个相邻的区间具有非常类似的类分布，则这两个区间可以合并；否则，它们应当保持分开。而低卡方值表明它们具有相似的类分布。 参考： ChiMerge:Discretization of numeric attributs Chi算法 参考1的要点： 1、 最简单的离散算法是： 等宽区间。 从最小值到最大值之间,，均分为N等份， 这样， 如果 A, B为最小最大值， 则每个区间的长度为w=(B-A) / N, 则区间边界值为 A+W, A+2W, …. A+(N-1)W. 2、 还有一种简单算法，等频区间。区间的边界值要经过选择，使得每个区间包含大致相等的实例数量。比如说 N=10，每个区间应该包含大约10%的实例。 3、 以上两种算法有弊端：比如，等宽区间划分，划分为5区间，最高工资为50000，则所有工资低于10000的人都被划分到同一区间。等频区间可能正好相反，所有工资高于 50000的人都会被划分到50000这一区间中。这两种算法都忽略了实例所属的类型，落在正确区间里的偶然性很大。 4、 C4、CART、PVM算法在离散属性时会考虑类信息，但是是在算法实施的过程中间，而不是在预处理阶段。例如，C4算法（ID3决策树系列的一种），将数值属性离散为两个区间，而取这两个区间时，该属性的信息增益是最大的。 5、 评价一个离散算法是否有效很难，因为不知道什么是最高效的分类。 6、 离散化的主要目的是：消除数值属性以及为数值属性定义准确的类别。 7、 高质量的离散化应该是：区间内一致，区间之间区分明显。 8、 ChiMerge算法用卡方统计量来决定相邻区间是否一致或者是否区别明显。如果经过验证，类别属性独立于其中一个区间，则这个区间就要被合并。 9、 ChiMerge算法包括2部分：1、初始化，2、自底向上合并，当满足停止条件的时候，区间合并停止。 第一步：初始化 根据要离散的属性对实例进行排序：每个实例属于一个区间 第二步：合并区间，又包括两步骤 (1) 计算每一对相邻区间的卡方值 (2) 将卡方值最小的一对区间合并 预先设定一个卡方的阈值，在阈值之下的区间都合并，阈值之上的区间保持分区间。 卡方的计算公式：%5E2%7D%7BE_%7Bij%7D%7D) 参数说明； m=2（每次比较的区间数是2个） k=类别数量 Aij=第i区间第j类的实例的数量 Ri=第i区间的实例数量 Cj=第j类的实例数量 N=总的实例数量 Eij= Aij的期望频率 10、卡方阈值的确定：先选择显著性水平，再由公式得到对应的卡方值。得到卡方值需要指定自由度，自由度比类别数量小1。例如，有3类，自由度为2，则90%置信度（10%显著性水平)下，卡方的值为4.6。阈值的意义在于，类别和属性独立时，有90%的可能性，计算得到的卡方值会小于4.6，这样，大于阈值的卡方值就说明属性和类不是相互独立的，不能合并。如果阈值选的大，区间合并就会进行很多次，离散后的区间数量少、区间大。用户可以不考虑卡方阈值，此时，用户可以考虑这两个参数：最小区间数，最大区间数。用户指定区间数量的上限和下限，最多几个区间，最少几个区间。 11、 ChiMerge算法推荐使用.90、.95、.99置信度，最大区间数取10到15之间. 举例： 取鸢尾花数据集作为待离散化的数据集合，使用ChiMerge算法，对四个数值属性分别进行离散化，令停机准则为max_interval=6。（韩家炜 数据挖掘概念与技术 第三版 习题3.12） 下面是我用Python写的程序，大致分两步： 第一步，整理数据 读入鸢尾花数据集，构造可以在其上使用ChiMerge的数据结构，即, 形如 [(‘4.3’, [1, 0, 0]), (‘4.4’, [3, 0, 0]),…]的列表，每一个元素是一个元组，元组的第一项是字符串，表示区间左端点，元组的第二项是一个列表，表示在此区间各个类别的实例数目； 第二步，离散化 使用ChiMerge方法对具有最小卡方值的相邻区间进行合并，直到满足最大区间数(max_interval)为6 程序最终返回区间的分裂点 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140__author__ = "Yinlong Zhao (zhaoyl[at]sjtu[dot]edu[dot]cn)"__date__ = "$Date: 2013/03/25 $"from time import ctimedef read(file): '''read raw date from a file ''' Instances=[] fp=open(file,'r') for line in fp: line=line.strip('\n') #discard '\n' if line!='': Instances.append(line.split(',')) fp.close() return(Instances)def split(Instances,i): ''' Split the 4 attibutes, collect the data of the ith attributs, i=0,1,2,3 Return a list like [['0.2', 'Iris-setosa'], ['0.2', 'Iris-setosa'],...]''' log=[] for r in Instances: log.append([r[i],r[4]]) return(log)def count(log): '''Count the number of the same record Return a list like [['4.3', 'Iris-setosa', 1], ['4.4', 'Iris-setosa', 3],...]''' log_cnt=[] log.sort(key=lambda log:log[0]) i=0 while(i&lt;len(log)): cnt=log.count(log[i])#count the number of the same record record=log[i][:] record.append(cnt) # the return value of append is None log_cnt.append(record) i+=cnt#count the next diferent item return(log_cnt)def build(log_cnt): '''Build a structure (a list of truples) that ChiMerge algorithm works properly on it ''' log_dic=&#123;&#125; for record in log_cnt: if record[0] not in log_dic.keys(): log_dic[record[0]]=[0,0,0] if record[1]=='Iris-setosa': log_dic[record[0]][0]=record[2] elif record[1]=='Iris-versicolor': log_dic[record[0]][1]=record[2] elif record[1]=='Iris-virginica': log_dic[record[0]][2]=record[2] else: raise TypeError("Data Exception") log_truple=sorted(log_dic.items()) return(log_truple)def collect(Instances,i): ''' collect data for discretization ''' log=split(Instances,i) log_cnt=count(log) log_tuple=build(log_cnt) return(log_tuple)def combine(a,b): ''' a=('4.4', [3, 1, 0]), b=('4.5', [1, 0, 2]) combine(a,b)=('4.4', [4, 1, 2]) ''' c=a[:] # c[0]=a[0] for i in range(len(a[1])): c[1][i]+=b[1][i] return(c)def chi2(A): ''' Compute the Chi-Square value ''' m=len(A); k=len(A[0]) R=[] for i in range(m): sum=0 for j in range(k): sum+=A[i][j] R.append(sum) C=[] for j in range(k): sum=0 for i in range(m): sum+=A[i][j] C.append(sum) N=0 for ele in C: N+=ele res=0 for i in range(m): for j in range(k): Eij=R[i]*C[j]/N if Eij!=0: res=res+(A[i][j]-Eij)**2/Eij return resdef ChiMerge(log_tuple,max_interval): ''' ChiMerge algorithm ''' ''' Return split points ''' num_interval=len(log_tuple) while(num_interval&gt;max_interval): num_pair=num_interval-1 chi_values=[] for i in range(num_pair): arr=[log_tuple[i][1],log_tuple[i+1][1]] chi_values.append(chi2(arr)) min_chi=min(chi_values) # get the minimum chi value for i in range(num_pair-1,-1,-1): # treat from the last one if chi_values[i]==min_chi: log_tuple[i]=combine(log_tuple[i],log_tuple[i+1]) # combine the two adjacent intervals log_tuple[i+1]='Merged' while('Merged' in log_tuple): # remove the merged record log_tuple.remove('Merged') num_interval=len(log_tuple) split_points=[record[0] for record in log_tuple] return(split_points)def discrete(path): ''' ChiMerege discretization of the Iris plants database ''' Instances=read(path) max_interval=6 num_log=4 for i in range(num_log): log_tuple=collect(Instances,i) # collect data for discretization split_points=ChiMerge(log_tuple,max_interval) # discretize data using ChiMerge algorithm print(split_points) if __name__=='__main__': print('Start: ' + ctime()) discrete('c:\\Python33\\iris.data') print('End: ' + ctime()) 函数说明： collect(Instances,i) 读入鸢尾花数据集，取第i个特征构造一个数据结构，以便使用ChiMerge算法。这个数据结构 形如 [(‘4.3’, [1, 0, 0]), (‘4.4’, [3, 0, 0]),…]的列表，每一个元素是一个元组，元组的第一项是字符串，表示区间左端点，元组的第二项是一个列表，表示在此区间各个类别的实例数目 ChiMerge(log_tuple,max_interval) ChiMerge算法，返回区间的分裂点 程序运行结果： [python] view plain copy >&gt;&gt; ================================ RESTART ================================ >&gt;&gt; Start: Mon Mar 25 21:31:40 2013 [‘4.3’, ‘4.9’, ‘5.0’, ‘5.5’, ‘5.8’, ‘7.1’] [‘2.0’, ‘2.3’, ‘2.5’, ‘2.9’, ‘3.0’, ‘3.4’] [‘1.0’, ‘3.0’, ‘4.5’, ‘4.8’, ‘5.0’, ‘5.2’] [‘0.1’, ‘1.0’, ‘1.4’, ‘1.7’, ‘1.8’, ‘1.9’] End: Mon Mar 25 21:31:40 2013 >&gt;&gt; 本文出自： http://blog.csdn.net/zhaoyl03/article/details/8689440]]></content>
  </entry>
  <entry>
    <title><![CDATA[【转】地图轨迹平滑]]></title>
    <url>%2F2018%2F05%2F31%2F%E5%9C%B0%E7%90%86%E4%BF%A1%E6%81%AF%E6%8C%96%E6%8E%98%2F%E5%9C%B0%E5%9B%BE%E8%BD%A8%E8%BF%B9%E5%B9%B3%E6%BB%91%2F</url>
    <content type="text"><![CDATA[https://blog.csdn.net/lion888/article/details/52109101]]></content>
      <categories>
        <category>地理信息处理和挖掘</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[卡尔曼滤波]]></title>
    <url>%2F2018%2F05%2F31%2F%E5%9C%B0%E7%90%86%E4%BF%A1%E6%81%AF%E6%8C%96%E6%8E%98%2F%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2%2F</url>
    <content type="text"><![CDATA[《computing with spatial trajectories》 1.9.1 度量模型 度量向量measurement vector $Z_i$，包括$z_i^{(x)}$和$z_i^{(y)}$。这是包含噪音的度量坐标。 给出状态向量state vector的估计。在这里，状态向量$X_i$包括物体的位置和速度。 $x_i$和$y_i$是i时刻的真实坐标，$s_i^{(x)}$和$s_i^{(y)}$是真实速度。卡尔曼滤波会生成$X_i$的估计。 $Z_i$和$X_i$的关系是 Z_i=H_iX_i+v_iH是度量矩阵，表达了我们通过测量$x_i$和$y_i$得到$z_i^{(x)}$和$z_i^{(y)}$ 。H的下标表示会随时间改变。 噪音向量$v_i$是一个服从高斯分布的。高斯噪音可以作为GPS噪音的一个简单模型。 1.9.2 动态模型 卡尔曼滤波模型有一半是度量，另一半是动态的。动态模型估计了状态向量是如何随时间变化的。动态模型使用矩阵并增加了噪音： X_i=\Phi_{i-1}X_{i-1}+w_{i-1}系统矩阵$\Phi_{i-1}$给出了上一个与下一个的线性关系 $\sigma$估计了GPS的传感器误差。$\sigma_s$是速度误差。 通俗理解卡尔曼滤波及其算法实现（实例解析） 很详细 https://blog.csdn.net/tiandijun/article/details/72469471 卡尔曼滤波（Kalman Filter） 没仔细看，也比较详细 https://blog.csdn.net/baimafujinji/article/details/50646814 python实现的卡尔曼 https://github.com/devincody/pyKalman/blob/master/kalman.py https://github.com/allanjg89/Taxi-Trajectory-Smoothing]]></content>
      <categories>
        <category>地理信息处理和挖掘</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[IP获取城市]]></title>
    <url>%2F2018%2F05%2F31%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FJAVA%2FIP%E8%8E%B7%E5%8F%96%E5%9F%8E%E5%B8%82%2F</url>
    <content type="text"><![CDATA[12345&lt;dependency&gt; &lt;groupId&gt;com.buzzinate&lt;/groupId&gt; &lt;artifactId&gt;buzz-ip&lt;/artifactId&gt; &lt;version&gt;1.9.4-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; 12345678910111213141516171819202122232425262728293031import com.buzzinate.common.util.ip.IpLocation;import com.buzzinate.common.util.ip.IPUtils;public static void main(String[] args) &#123; try &#123; BufferedReader br = null; br = new BufferedReader(new InputStreamReader( Thread.currentThread().getContextClassLoader().getResourceAsStream(&quot;ipds.txt&quot;), &quot;utf8&quot;)); String line = &quot;&quot;; IpLocation location = null; while ((line = br.readLine()) != null) &#123; line = line.replace(&quot; &quot;, &quot;&quot;); if (line.indexOf(&quot;,&quot;) &gt; -1) &#123; String[] strArr = line.split(&quot;,&quot;); List&lt;String&gt; citys = new ArrayList&lt;String&gt;(); for (String str : strArr) &#123; location = IPUtils.getIpLocation(str); citys.add(location.getCity()); &#125; System.out.println(StringUtils.join(citys, &quot;, &quot;)); &#125; else &#123; location = IPUtils.getIpLocation(line); System.out.println(location.getCity()); &#125; &#125; &#125; catch (Exception e) &#123; &#125; IpLocation location = IPUtils.getIpLocation(&quot;27.184.135.14&quot;); System.out.printf(location.getCity()); &#125;]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>自用代码记录</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F05%2F30%2F%E6%80%BB%E7%BB%93%E4%B8%8E%E6%80%9D%E8%80%83%2F%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[面试 map、mapPartitions区别 map：用于遍历RDD,将函数f应用于每一个元素，返回新的RDD(transformation算子)。 mapPartitions:用于遍历操作RDD中的每一个分区，返回生成一个新的RDD（transformation算子）。 groupbykey，combinebykey 如何划分stage 根据宽依赖。指的是多个子RDD的Partition会依赖同一个父RDD的Partition。例如reduceByKey,groupByKey的算子，会导致宽依赖的产生。 spark常用的配置参数有哪些，设置某个参数值的依据是什么，如何验证你的设置 如何防止内存溢出 CNN最成功的应用是在CV，那为什么NLP和Speech的很多问题也可以用CNN解出来？ RNN和LSTM的区别 监督学习和迁移学习的区别]]></content>
  </entry>
  <entry>
    <title><![CDATA[轨迹聚类]]></title>
    <url>%2F2018%2F05%2F30%2F%E5%9C%B0%E7%90%86%E4%BF%A1%E6%81%AF%E6%8C%96%E6%8E%98%2F%E8%BD%A8%E8%BF%B9%E8%81%9A%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[应用场景单车辆多天的轨迹异常发现TRACLUS《computing with spatial trajectories》P184 输入：多条轨迹 如何定义轨迹对象 除了point的list，还有time，方向 输出：轨迹能聚成几个簇，每个簇有几条轨迹 算法原理： 分段：将轨迹进行分段以作为下一阶段的输入； 归组：相似的线段归为一类。归类的算法采用基于密度的聚类算法，本文采用的是DBSCAN算法 https://blog.csdn.net/jsc9410/article/details/51008444 http://www.doc88.com/p-7092826196880.html https://max.book118.com/html/2014/0425/7901598.shtm git https://github.com/luborliu/TraClusAlgorithm 1java boliu.Main deer_1995.tra testOut.txt 代码原理： 1、MDL 2、估计参数 3、聚类 12&gt; findOptimalPartition(Trajectory pTrajectory)&gt; MDL。由两个部分组成：L(H)和L(D|H)。H是假设hypothesis，D是数据。L(H)是假设的长度。L(D|H)在这个假设下数据的长度。算法目的是要找到使L(H)和L(D|H)最小的H和D。 在轨迹分组问题中，H就对应了一组轨迹分组的指定集合。类似于这样的定义 算法核心是partition-and-group 定义距离 partition 多车辆同一天的轨迹聚集发现一天的停留点中有哪些是与其他轨迹产生交集的 同一个县级的多辆车在每天的轨迹是否有相似的 相关算法轨迹相似度检测Hausdorff距离https://en.wikipedia.org/wiki/Hausdorff_distance DTW距离https://en.wikipedia.org/wiki/Dynamic_time_warping 轨迹聚类TRACLUShttps://blog.csdn.net/jsc9410/article/details/51008444 Trajectory clustering via deep representation learning基于编码-解码的序列到序列架构 https://www.cnblogs.com/dpwow/p/7884971.html 轨迹点聚类DBSCAN]]></content>
      <categories>
        <category>地理信息处理和挖掘</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F05%2F25%2Fhadoop-spark%2Fspark%2Fspark%E6%97%A5%E5%BF%97%2F</url>
    <content type="text"><![CDATA[Spark配置log4j日志输出 https://blog.csdn.net/u012036736/article/details/78599832 Spark的日志配置]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F05%2F18%2F%E6%80%BB%E7%BB%93%E4%B8%8E%E6%80%9D%E8%80%83%2F%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86%E7%BB%8F%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[每周的周报除了]]></content>
  </entry>
  <entry>
    <title><![CDATA[DBSCAN]]></title>
    <url>%2F2018%2F05%2F14%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FDBSCAN%2F</url>
    <content type="text"><![CDATA[编写基于dbscan的GPS数据热点区域分析（二）算法的实现 https://blog.csdn.net/wu5151/article/details/50634265 DBSCAN聚类算法原理及其实现 http://shiyanjun.cn/archives/1288.html 空间数据聚类 DBSCAN (各种geo处理的python包) https://blog.csdn.net/weiyudang11/article/details/52684333 https://blog.csdn.net/lincolnmi/article/details/51175246 http://www.cnblogs.com/aijianiula/p/4339960.html wiki的伪代码 https://zh.wikipedia.org/wiki/DBSCAN 1234567891011121314151617181920212223242526272829303132DBSCAN(D, eps, MinPts) &#123; C = 0 for each point P in dataset D &#123; if P is visited continue next point mark P as visited NeighborPts = regionQuery(P, eps) if sizeof(NeighborPts) &lt; MinPts mark P as NOISE else &#123; C = next cluster expandCluster(P, NeighborPts, C, eps, MinPts) &#125; &#125;&#125;expandCluster(P, NeighborPts, C, eps, MinPts) &#123; add P to cluster C for each point P&apos; in NeighborPts &#123; if P&apos; is not visited &#123; mark P&apos; as visited NeighborPts&apos; = regionQuery(P&apos;, eps) if sizeof(NeighborPts&apos;) &gt;= MinPts NeighborPts = NeighborPts joined with NeighborPts&apos; &#125; if P&apos; is not yet member of any cluster add P&apos; to cluster C &#125;&#125;regionQuery(P, eps) return all points within P&apos;s eps-neighborhood (including P) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172package com.mljr.carthage.common.geo.search;import com.mljr.carthage.common.geo.bean.LonLatPoint;import java.util.ArrayList;public class DBScan &#123; public void process(ArrayList&lt;LonLatPoint&gt; points, double radius, int minPts) &#123; int size = points.size(); int idx = 0; int cluster = 1; while (idx &lt; size) &#123; LonLatPoint p = points.get(idx); // 找到一个没访问过的p if (!p.isVisit()) &#123; // 设为已访问 p.setVisit(true); ArrayList&lt;LonLatPoint&gt; adjacentPoints = getAdjacentPoints(p, points, radius); // 如果小于邻域密度阈值，则标记为离群点 if (adjacentPoints != null &amp;&amp; adjacentPoints.size() &lt; minPts) &#123; p.setNoised(true); &#125; else &#123; p.setCluster(cluster); for (int i = 0; i &lt; adjacentPoints.size(); i++) &#123; LonLatPoint adjacentPoint = adjacentPoints.get(i); // 只遍历未访问的点，只有未访问的点才可能作为新的可达点 if (! adjacentPoint.isVisit()) &#123; adjacentPoint.setVisit(true); ArrayList&lt;LonLatPoint&gt; adjacentAdjacentPoints = getAdjacentPoints(adjacentPoint, points, radius); // 如果邻近点也是核心对象，将其的邻域点也加到中心点的邻域点中 if (adjacentAdjacentPoints != null &amp;&amp; adjacentAdjacentPoints.size() &gt;= minPts) &#123; adjacentPoints.addAll(adjacentAdjacentPoints); &#125; &#125; if (adjacentPoint.getCluster() == 0) &#123; adjacentPoint.setCluster(cluster); if (adjacentPoint.isNoised()) &#123; adjacentPoint.setNoised(false); &#125; &#125; &#125; cluster ++; &#125; &#125; &#125; &#125; /** * 获取中心点邻域内的所有点 * @param centerPoint * @param points * @param radius * @return */ private ArrayList&lt;LonLatPoint&gt; getAdjacentPoints(LonLatPoint centerPoint, ArrayList&lt;LonLatPoint&gt; points, double radius) &#123; ArrayList&lt;LonLatPoint&gt; adjacentPoints = new ArrayList&lt;LonLatPoint&gt;(); for (LonLatPoint point: points) &#123; // 这也包括中心点自身 double distance = centerPoint.getDistance(point); if (distance &lt;= radius) &#123; adjacentPoints.add(point); &#125; &#125; return adjacentPoints; &#125;&#125; 用python做聚类dbscanForLonlat.py https://www.cnblogs.com/pinard/p/6217852.html 12345678910111213141516import pandas as pdimport mathimport gpsTransform as transimport numpy as npimport matplotlib.pyplot as pltdf = pd.read_csv(&apos;/Users/david/david/00projects/01风控模型/轨迹数据处理/carthage_fact_gps_rawdata0510.csv&apos;, sep=&apos;,&apos;)df[&apos;gpstime&apos;] = pd.to_datetime(df[&apos;gpstime&apos;])df[&apos;lon&apos;] = df[&apos;lon&apos;].astype(float)df[&apos;lat&apos;] = df[&apos;lat&apos;].astype(float)plt.scatter(df[&apos;lon&apos;], df[&apos;lat&apos;], marker=&apos;o&apos;)plt.show()# 先看kmeans效果from sklearn.cluster import KMeans 经纬度聚类https://blog.csdn.net/weiyudang11/article/details/52684333]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F05%2F10%2Fhadoop-spark%2Fspark%2F%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5%2F</url>
    <content type="text"><![CDATA[1234567891011121314scala&gt; val cm = c.map(e =&gt; (e._1, (e._2, 0)))cm: org.apache.spark.rdd.RDD[(String, (Int, Int))] = MapPartitionsRDD[25] at map at &lt;console&gt;:23 scala&gt; val cr = cm.reduceByKey((e1, e2) =&gt; (e1._1 + e2._1, e1._1/2 + e2._1/2))cr: org.apache.spark.rdd.RDD[(String, (Int, Int))] = ShuffledRDD[26] at reduceByKey at &lt;console&gt;:25 scala&gt; val cz = cr.map(e =&gt; (e._1, e._2._1, e._2._2))cz: org.apache.spark.rdd.RDD[(String, Int, Int)] = MapPartitionsRDD[27] at map at &lt;console&gt;:27 scala&gt; cz.collectres15: Array[(String, Int, Int)] = Array((b,3,1), (a,6,2), (c,1,0)) scala&gt; val c = sc.parallelize(List((&quot;a&quot;, 1), (&quot;a&quot;, 2), (&quot;a&quot;, 3), (&quot;b&quot;, 1), (&quot;b&quot;, 2), (&quot;c&quot;, 1)))c: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[28] at parallelize at &lt;console&gt;:21 sorthttps://blog.csdn.net/greahuang/article/details/46551649 12345678910111213scala&gt; val data = List((1,4),(4,8),(0,4),(12,8))scala&gt; val rdd = sc.parallelize(data)scala&gt; rdd.sortByKey().collectArray[(Int, Int)] = Array( (0,4), (1,4), (4,8),(12,8))重写 orderingimplicit val st = new Ordering[Int]&#123;override def compare(a:Int,b:Int)=a.toString.compare(b.toString)&#125;Array[(Int, Int)] = Array((0,4), (1,4), (12,8), (4,8)) 1234567val rdd = rdd1.union(rdd2)if (!deleteDirIfExisted(fileSystem, new Path(&quot;hdfs:/**/result_t1&quot;))) &#123; throw new BusinessRuntimeException(&quot;Output path in hdfs :%s delete failed!&quot;)&#125;rdd.sortBy(line =&gt;sortBasicRddAlgo(line),true,1) .saveAsTextFile(&quot;hdfs:/**/result_t1&quot;) https://blog.csdn.net/jiangpeng59/article/details/52938465 123456789val conf = new SparkConf() val sc = new SparkContext(conf) val array = Array((1, 6, 3), (2, 3, 3), (1, 1, 2), (1, 3, 5), (2, 1, 2)) val rdd1 = sc.parallelize(array) //设置元素(e1,e3)为key,value为原来的整体 val rdd2 = rdd1.map(f =&gt; ((f._1, f._3), f)) //利用sortByKey排序的对key的特性 val rdd3 = rdd2.sortByKey() val rdd4 = rdd3.values.collect groupbykeyhttps://www.2cto.com/net/201704/623475.html 12345678910111213141516171819202122232425262728293031323334353637/* 同类合并、计算 */ val source = Source.fromFile(&quot;E:test.txt&quot;).getLines.toArray val sourceRDD = sc.parallelize(source) /* spark单机读取数据 */ sourceRDD.map &#123; line =&gt; val lines = line.split(&quot;,&quot;) /* 拆分数据 */ (s&quot;$&#123;lines(0)&#125;&quot;, s&quot;$&#123;lines(1)&#125;,$&#123;lines(2)&#125;,$&#123;lines(3)&#125;&quot;) /* 找出同样的数据为K，需要进行计算的为V，拼成map */ &#125;.groupByKey.map &#123; /* 分组，最重要的就是这，同类的数据分组到一起，后面只需要计算V了 */ case (k, v) =&gt; var a, b, c = 0 /* 定义几个存数据的变量，恩，这很java，一般scala中很少见到var */ v.foreach &#123; /* 遍历需要计算的V */ x =&gt; val r = x.split(&quot;,&quot;) /* 将V拆分 */ a += r(0).toInt /* 计算 */ b += r(1).toInt c += r(2).toInt &#125; s&quot;$k,$a,$b,$c&quot; /* 拼字符串，返回数据 */ &#125;.foreach(println) map排序http://www.aboutyun.com/thread-22942-1-1.html spark 怎么对一个内容为一批Iterable[Double]的Rdd中的每一个Iterable[Double]做增值修改]]></content>
  </entry>
  <entry>
    <title><![CDATA[combinebykey-groupbykey]]></title>
    <url>%2F2018%2F05%2F10%2Fhadoop-spark%2Fspark%2Fcombinebykey-groupbykey%2F</url>
    <content type="text"><![CDATA[combineByKeyhttps://blog.csdn.net/bitcarmanlee/article/details/77573560 函数原型 123456def combineByKey[C]( createCombiner: V =&gt; C, mergeValue: (C, V) =&gt; C, mergeCombiners: (C, C) =&gt; C): RDD[(K, C)] = self.withScope &#123; combineByKeyWithClassTag(createCombiner, mergeValue, mergeCombiners)(null) &#125; 可以看出，combineByKey是典型的K-V类型的算子，而且是一个transformation操作。与其他transformation操作一样，combineByKey也不会触发作业的提交。combineByKey函数主要有三个参数，而且这三个参数都是函数：createCombiner: V =&gt; C 产生一个combiner的函数，将RDD[K,V]中的V转换成一个新的值C1mergeValue: (C, V) =&gt; C 合并value的函数，将一个C1类型的值与一个V合并成一个新的C类型的值，假设这个新的C类型的值为C2mergeCombiners: (C, C) =&gt; C) 将两个C类型的值合并为一个C类型的值 整个函数最后的输出为RDD[(K, C)] 示例 123456789101112131415161718192021222324252627282930def t1(sc: SparkContext) = &#123; val inputpath = &quot;XXX&quot; sc.textFile(inputpath). filter &#123; x =&gt; val lines = x.split(&quot;\t&quot;) lines.length == 2 &amp;&amp; lines(1).length &gt; 0 &#125; .map &#123; x =&gt; val lines = x.split(&quot;\t&quot;) val (city, uuid) = (lines(0), lines(1)) (city, uuid) &#125; .combineByKey((v: String) =&gt; &#123; val set = new java.util.HashSet[String]() set.add(v) set &#125;, (x: java.util.HashSet[String], v: String) =&gt; &#123; x.add(v) x &#125;, (x: java.util.HashSet[String], y: java.util.HashSet[String]) =&gt; &#123; x.addAll(y) x &#125;) .map(x =&gt; (x._1, x._2.size())) .sortBy(_._2, false) .take(10) .foreach(println) &#125; 转自https://www.iteblog.com/archives/1357.html 你可以想象一个非常大的数据集，在使用 reduceByKey 和 groupByKey 时他们的差别会被放大更多倍。以下函数应该优先于 groupByKey ： （1）、combineByKey组合数据，但是组合之后的数据类型与输入时值的类型不一样。 （2）、foldByKey合并每一个 key 的所有值，在级联函数和“零值”中使用。]]></content>
      <categories>
        <category>spark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SparkSession-SparkContext]]></title>
    <url>%2F2018%2F05%2F04%2Fhadoop-spark%2Fspark%2FSparkSession-SparkContext%2F</url>
    <content type="text"><![CDATA[参考：SparkContext、SparkConf和SparkSession的初始化 SparkContext和SparkConf任何Spark程序都是SparkContext开始的，SparkContext的初始化需要一个SparkConf对象，SparkConf包含了Spark集群配置的各种参数。 SparkSessionSince: 2.0.0 SparkSession实质上是SQLContext和HiveContext的组合（未来可能还会加上StreamingContext），所以在SQLContext和HiveContext上可用的API在SparkSession上同样是可以使用的。SparkSession内部封装了sparkContext，所以计算实际上是由sparkContext完成的。 123456789val warehouseLocation = &quot;D:\\workSpace_IDEA_NEW\\day2017-10-12\\spark-warehouse&quot; //todo:1、创建sparkSession val spark: SparkSession = SparkSession.builder() .appName(&quot;HiveSupport&quot;) .master(&quot;local[2]&quot;) .config(&quot;spark.sql.warehouse.dir&quot;, warehouseLocation) .enableHiveSupport() //开启支持hive .getOrCreate() spark.sparkContext.setLogLevel(&quot;WARN&quot;) //设置日志输出级别 SparkSession从csv读数据 1val dq = sparkSession.read.option(&quot;header&quot;, &quot;true&quot;).csv(&quot;src/main/resources/scala.csv&quot;)]]></content>
      <categories>
        <category>spark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spark读写hive]]></title>
    <url>%2F2018%2F05%2F03%2Fhadoop-spark%2Fspark%2Fspark%E8%AF%BB%E5%86%99hive%2F</url>
    <content type="text"><![CDATA[需求是从hive读取后处理，再写到hive的表。 https://blog.csdn.net/kjdsgh/article/details/75006960 1、添加依赖包 spark-hive_2.10的添加为了能创建hivecontext对象 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-hive_2.10&lt;/artifactId&gt; &lt;version&gt;1.6.1&lt;/version&gt;&lt;/dependency&gt; mysql驱动链接元数据 123456&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.38&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt;&lt;/dependency&gt; 2.添加hive-site.xml文件内容如下其中mysql中hive库是hive的元数据库 12345678910111213141516171819202122&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!--Autogenerated by Cloudera Manager--&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://master:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 读取示例 12345678910111213141516object App &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf().setAppName(&quot;test&quot;).setMaster(&quot;local[2]&quot;) val sc = new SparkContext(conf) val sqlContext = new HiveContext(sc) sqlContext.table(&quot;test.person&quot;) // 库名.表名 的格式 .registerTempTable(&quot;person&quot;) // 注册成临时表 sqlContext.sql( &quot;&quot;&quot; | select * | from person | limit 10 &quot;&quot;&quot;.stripMargin).show() sc.stop() &#125;&#125; 看代码https://github.com/fansy1990/spark_hive_source_destination Spark记录-本地Spark读取Hive数据简单例子 Spark-SQL和Hive on Spark, SqlContext和HiveContexthttps://blog.csdn.net/cymvp/article/details/53815043 hiveContext演示https://blog.csdn.net/u012432611/article/details/50455009 将DataFrame数据如何写入到Hive表中Spark 之DataFrame与RDD 转换https://blog.csdn.net/lxhandlbb/article/details/52367605 RDD转换为DataFramehttps://blog.csdn.net/Dillon2015/article/details/68068552 Rdd没法调用toDF原因https://blog.csdn.net/u013013024/article/details/64124075]]></content>
      <categories>
        <category>spark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[mac安装office2016]]></title>
    <url>%2F2018%2F04%2F24%2F%E5%B7%A5%E5%85%B7%E5%92%8C%E7%8E%AF%E5%A2%83%2Fmac%E5%AE%89%E8%A3%85office2016%2F</url>
    <content type="text"><![CDATA[转自https://www.jianshu.com/p/3c5dc4f3c96e 安装 下载 Office 2016 for mac下载地址：http://www.pc6.com/mac/137931.html ， 其他下载方式亦可。 安装下载的 Office 2016 for mac 在安装完成以后，请不要先登录自己的账户。 图 01.png 如果在此之前已经在 Office 中登录了微软账户的话，请先在 Mac 上删除 Office 以后，再重新安装。 破解 安装 FWMSO2016VLU2.0.dmg （破解软件）破解程序下载 百度云盘地址 提取密码: a89r 安装破解软件.png 运行 FWMSO2016VLU2.0.dmg 运行破解软件.png 如果系统提示因不明开发者不能打开此软件的话，请在系统偏好设置的【安全性与隐私】中打开它。 因不明开发者不能打开此软件.png 在【安全性与隐私】窗口中，可以先在左下角解锁，再选择【任何来源】这个选项，就可以成功打开了。 没有【任何来源】怎么办（这个与系统版本有关，打开方法如下：） 打开终端，在终端运行以下指令（需要输入密码）：Sudo spctl —master-disable &lt;密码是开机密码！密码是开机密码！密码是开机密码！&gt; 如何打开【任何来源】.png 2.进入【安全性与隐私】更改为【任意来源】选项 更改为【任意来源】.png 接下来打开以后，请 点击 窗口中 锁图标 继续，如图所示。 点击锁图标.png 随后如果提示当前已经授权激活的话，请继续点击【继续】按钮。 点继续.png Succeed 随后会提示需要输入当前 Mac 上用户的密码，然后就可以激活了。 可以愉快的玩耍了.png 在激活以前，打开 Office2016 以后是这样的，提示需要激活。 未激活状态.png 当激活成功以后，便不会再提示需要激活了，功能也会全部解锁。 激活状态.png]]></content>
      <categories>
        <category>工具和环境</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[【转】Echarts地图可视化]]></title>
    <url>%2F2018%2F04%2F23%2F%E5%9C%B0%E7%90%86%E4%BF%A1%E6%81%AF%E6%8C%96%E6%8E%98%2F%E5%8F%AF%E8%A7%86%E5%8C%96%2F</url>
    <content type="text"><![CDATA[Echarts中BMapAPI地图绘制功能（BMapExt）的实现步骤https://blog.csdn.net/u014136513/article/details/50580617 echarts的绘图参数 http://echarts.baidu.com/echarts2/doc/doc.html#%E5%AE%9E%E4%BE%8B%E6%96%B9%E6%B3%95]]></content>
      <categories>
        <category>地理信息处理和挖掘</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[工作思考]]></title>
    <url>%2F2018%2F04%2F13%2F%E6%80%BB%E7%BB%93%E4%B8%8E%E6%80%9D%E8%80%83%2F%E5%B7%A5%E4%BD%9C%E6%80%9D%E8%80%83%2F</url>
    <content type="text"><![CDATA[一次沟通的结果是什么：有哪些工作，目前什么状态，需要推进到什么状态，需要解决哪些问题？分工是什么？时间节点？ 如何开展一次Brain-Storm？ 1、针对会议的议题，先拆分出几个大的主题。 2、每个人围绕这几个大的主题进行发散，阐述自己的想法。 3、依次对每个想法进行讨论，判断其是否通过或打分。判断的原则包括且不限于： 1）是否make-sense？即是否符合公司业务，是否符合市场需求，是否适合我们团队做。 2）根据团队的条件，项目是否能够最终落地？ 3）实现项目所耗费的团队资源是否能为公司带来正比的收益。 4、对筛选后的项目，制定实现的优先级，安排具体工作。 2018年09月24日 怎么跟其他部门沟通的？ 怎么推进项目？ 目的：落地自己的一个预测模型 先是想在分一杯羹 2018年09月26日 制定一个A/Btest的方案 如刚才开会沟通，这周我们重点出一版方案，不要继续调模型了，模型的准确率节后再调。方案包括： 对于预测出来的结果，A/B test，如何抽样划分A和B（肯定不能简单随机抽，否则会受运气影响，比如抽到的都是某个地域，或者历史上还款状况较差的合同） 对于要做动作的A集合，是账单日前哪一天或哪几天做action 做什么样的Action 如何评价A和B合同集合后面的表现指标 成本大概是多少 2018年09月30日 今天下午复盘了一年多与反欺诈在GPS上打的交道 1、先回溯一年的时间，每个时间段做了什么事 2、分析造成目前的情况，可能的原因是什么。有没有了解清楚真正的问题所在。 3、讨论解决方案和下一步计划 会议讨论后安排事项的邮件 1、要按照优先级排期 2018年10月10日 复盘 故障复盘，项目复盘 70:20:10法则：70来自实践，20来自沟通，10来自培训 复盘的误区： 1、对别人的事情做复盘 要自己做复盘。因为自己最清楚当时的情况 2、复盘就是项目总结 复盘更提倡个人的学习，从总结中找到哪些地方走了弯路，哪些地方忽视。 目的不一样，总结为了呈现总结报告，哪些成果。复盘更深入，分析走的弯路。 3、把复盘当做一种解决问题的思路和方法 不等于解决问题，而是从问题中学习。 希望过程的本身，而不是具体的解决问题。 4、把复盘开成甩锅会 复盘的目的是客观公正。 复盘会的首要因素： 1、聆听。防止代入自己的思维，以偏概全。2、发表观点。3、探讨 复盘的观察者：记录过程，帮助引导想法，要知道哪些地方比较关键。 复盘之后，看有没有todo项，在jira上更新下一步任务d’d 一个积极而有建设性的人则是另一种做事方法（下面列的每一条，都是我从我司一些人身上总结出来的） 始终相信积极的行动本身会带来最终问题的解决，即使在行动时并不能预期最后的解决方案 始终相信团队的力量大于个人的能力，以团队团结作为行动的出发点 保持自己的心身健康，在压力大的时候也保持健康和锻炼，保持朝气 成为团队的情绪和心理的粘合剂和纽带，即使自己没有完全想通也鼓励和激发其他人 坚持执行，不会停下来坐而论道，在执行中想办法——通常会发现一个副产品成为最终的前进道路 不是想通了再干，而是在干的过程中总结 充分听取其他人的意见，而不是上来就带帽子，甚至搞人身攻击 保持沟通（书面的和口头的），尤其是在困难的时候 及时回复，不在出问题的时候消失 不斤斤计较于一件事上的名利，以把事情做成了为主要的出发点 即使发现前人的工作有缺点，也先肯定其中有价值的成绩部分，再根据优先级一点一点分解改进 经常做自我总结和反思，和其他人讨论自己的不足的地方 打仗没有总是打胜仗的，所有的“名将”都是在一次一次的失败中成长起来的。一个人是不是有不被卡住的能力，就在于他/她在困难面前，做出何种选择。这些选择，都是很小的事情。事业成败，往往就是在这些小事情，而不是宏大的叙事。 埋点项目推进首先的第一步是， 得有个明确的方向，就是如何提高现在的单页取现流程的转化率。首先需要一个明确的漏斗，前端+后端数据，确保数据没有bug，能全方位无死角的反映这个单页流程里，流量进来以后在各部分的明确耗损，不能有模棱两可的地方。这是第一步，下一步才是针对耗损进行ab测试优化 明确一个时间点，解决所有已经存在的问题，有一个精确的漏斗。能反映每个流程的流量耗损。 2019年01月14日 做一个事情之前，先想好这个事情的预期，里程碑。还是定目标，针对目标，按照这个事情的流程，梳理一遍要做哪些事。 需求讨论和需求评审格林威治的第二版，我们早早的就开了需求评审会，但做的时候才发现有很多还没有确定的。 所以当时开的起始是需求讨论会，之后要给出2-3天的调研时间，把所有功能和方案都确定下来且不能修改，再开需求评审会确定需求，确定后就不能更改了。 测试立项就应该测试介入 跟其他部门沟通的邮件统一的命名风格，便于检索。]]></content>
      <categories>
        <category>总结与思考</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F04%2F13%2Fhadoop-spark%2F%E7%9B%91%E6%8E%A7%2F</url>
    <content type="text"><![CDATA[集群容量、最高使用占比监控]]></content>
  </entry>
  <entry>
    <title><![CDATA[Git技巧]]></title>
    <url>%2F2018%2F04%2F11%2F%E5%B7%A5%E5%85%B7%E5%92%8C%E7%8E%AF%E5%A2%83%2FGit%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[git config配置1234567891011121314151617[core] symlinks = false repositoryformatversion = 0 filemode = false logallrefupdates = true[remote &quot;pullRemote&quot;] url = ssh://git@06.7.12.50:5310/gongsi/data-platform.git fetch = +refs/heads/*:refs/remotes/pullRemote/*[remote &quot;pushRemote&quot;] url = ssh://git@06.7.12.50:5310/da/data-platform.git fetch = +refs/heads/*:refs/remotes/pushRemote/*[branch &quot;master&quot;] remote = pullRemote merge = refs/heads/master[branch &quot;feature-data-platform-sparkVersionUpdate&quot;] remote = pushRemote merge = refs/heads/feature-data-platform-sparkVersionUpdate git ignore配置若没有，在项目下新建.gitignore 1234567891011/*/*/target/*/*/*/*/target/*/bin/out/*.iml.classpath.idea/.project.settings/.gitignore.cache-main 撤销已经push的commit12345678910111213141516先在本地回退到相应的版本：git reset --hard &lt;版本号&gt;// 注意使用 --hard 参数会抛弃当前工作区的修改// 使用 --soft 参数的话会回退到之前的版本，但是保留当前工作区的修改，可以重新提交123如果此时使用命令：git push origin &lt;分支名&gt;会提示本地的版本落后于远端的版本； 为了覆盖掉远端的版本信息，使远端的仓库也回退到相应的版本，需要加上参数--forcegit push origin &lt;分支名&gt; --force 实际的操作是 12git reset --soft 5bbafdbgit push pushRemote --force 然后再重新提交 git开发上线流程]]></content>
      <categories>
        <category>工具和环境</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[地图开放平台]]></title>
    <url>%2F2018%2F04%2F10%2F%E5%9C%B0%E7%90%86%E4%BF%A1%E6%81%AF%E6%8C%96%E6%8E%98%2F%E5%9C%B0%E5%9B%BE%E5%BC%80%E6%94%BE%E5%B9%B3%E5%8F%B0%2F</url>
    <content type="text"><![CDATA[百度地图开放平台http://lbsyun.baidu.com/index.php?title=open/dev-res 高德开放平台http://lbs.amap.com/api/webservice/guide/api/search]]></content>
      <categories>
        <category>地理信息处理和挖掘</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[IDEA技巧]]></title>
    <url>%2F2018%2F04%2F09%2F%E5%B7%A5%E5%85%B7%E5%92%8C%E7%8E%AF%E5%A2%83%2Fidea%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[cannot resolve symbol用idea跑spark的单机项目，总是提示 1cannot resolve symbol apache 但maven的.m2目录下是有这个文件夹和jar包的，试过网上各种办法，包括 点击File | Invalidate Caches，清理了缓存重启IDEA 最后结果是靠 项目右键-&gt;maven -&gt; Reimport go to classcommand+o 代码片段快捷键https://blog.csdn.net/tiantiandjava/article/details/42269173 更多的提示可以CTRL + j 可以查看，mac系统下是command＋j。]]></content>
      <categories>
        <category>工具和环境</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[使用spark DStream的foreachRDD时要注意哪些坑]]></title>
    <url>%2F2018%2F04%2F08%2Fhadoop-spark%2Fspark%2F%E4%BD%BF%E7%94%A8spark%20DStream%E7%9A%84foreachRDD%E6%97%B6%E8%A6%81%E6%B3%A8%E6%84%8F%E5%93%AA%E4%BA%9B%E5%9D%91%2F</url>
    <content type="text"><![CDATA[转自使用spark DStream的foreachRDD时要注意哪些坑？ 答案： 两个坑， 性能坑和线程坑 DStream是抽象类，它把连续的数据流拆成很多的小RDD数据块， 这叫做“微批次”， spark的流式处理， 都是“微批次处理”。 DStream内部实现上有批次处理时间间隔，滑动窗口等机制来保证每个微批次的时间间隔里， 数据流以RDD的形式发送给spark做进一步处理。因此， 在一个为批次的处理时间间隔里， DStream只产生一个RDD。 可以利用dstream.foreachRDD把数据发送给外部系统。 但是想要正确地， 有效率的使用它， 必须理解一下背后的机制。通常向外部系统写数据需要一个Connection对象（通过它与外部服务器交互）。程序员可能会想当然地在spark上创建一个connection对象， 然后在spark线程里用这个对象来存RDD。比如下面的程序： 123456dstream.foreachRDD &#123; rdd =&gt; val connection = createNewConnection() // executed at the driver rdd.foreach &#123; record =&gt; connection.send(record) // executed at the worker &#125;&#125; 这个代码会产生执行错误， 因为rdd是分布式存储的，它是一个数据结构，它是一组指向集群数据的指针， rdd.foreach会在集群里的不同机器上创建spark工作线程， 而connection对象则不会在集群里的各个机器之间传递， 所以有些spark工作线程就会产生connection对象没有被初始化的执行错误。 解决的办法可以是在spark worker里为每一个worker创建一个connection对象， 但是如果你这么做， 程序要为每一条record创建一次connection，显然效率和性能都非常差。 另一种改进方法是为每个spark分区创建一个connection对象，同时维护一个全局的静态的连接池对象， 这样就可以最好的复用connection。 另外需要注意： 虽然有多个connection对象， 但在同一时间只有一个connection.send(record)执行， 因为在同一个时间里， 只有 一个微批次的RDD产生出来。 12345678dstream.foreachRDD &#123; rdd =&gt; rdd.foreachPartition &#123; partitionOfRecords =&gt; // ConnectionPool is a static, lazily initialized pool of connections val connection = ConnectionPool.getConnection() partitionOfRecords.foreach(record =&gt; connection.send(record)) ConnectionPool.returnConnection(connection) // return to the pool for future reuse &#125;&#125; 有人问了个问题，为什么foreachRDD里有两层嵌套的foreach? 为什么dstream.foreachRDD里还要再套一层rdd.foreach 可以这么理解, DStream.foreachRDD 是一个输出操作符，它返回的不是RDD里的一行数据， 而是输出DStream后面的RDD,在一个时间间隔里， 只返回一个RDD的“微批次”， 为了访问这个“微批次”RDD里的数据， 我们还需要在RDD数据对象上做进一步操作.。 参考下面的代码实例， 更容易理解。 给顶一个 RDD [Security, Prices]数据结构 1234567891011121314151617181920212223242526272829303132333435dstream.foreachRDD &#123; pricesRDD =&gt; // Loop over RDD val x= pricesRDD.count if (x &gt; 0) // RDD has data &#123; for(line &lt;- pricesRDD.collect.toArray) // Look for each record in the RDD &#123; var index = line._2.split(&apos;,&apos;).view(0).toInt // That is the index var timestamp = line._2.split(&apos;,&apos;).view(1).toString // This is the timestamp from source var security = line._2.split(&apos;,&apos;).view(12.toString // This is the name of the security var price = line._2.split(&apos;,&apos;).view(3).toFloat // This is the price of the security if (price.toFloat &gt; 90.0) &#123; // Do something here // Sent notification, write to HDFS etc &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>spark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spark问题集]]></title>
    <url>%2F2018%2F04%2F07%2Fhadoop-spark%2Fspark%2Fspark%E9%97%AE%E9%A2%98%E9%9B%86%2F</url>
    <content type="text"><![CDATA[读本地文件报错1234567Exception in thread &quot;main&quot; java.lang.VerifyError: class com.fasterxml.jackson.module.scala.ser.ScalaIteratorSerializer overrides final method withResolved.(Lcom/fasterxml/jackson/databind/BeanProperty;Lcom/fasterxml/jackson/databind/jsontype/TypeSerializer;Lcom/fasterxml/jackson/databind/JsonSerializer;)Lcom/fasterxml/jackson/databind/ser/std/AsArraySerializerBase; at org.apache.spark.rdd.RDDOperationScope$.&lt;init&gt;(RDDOperationScope.scala:81) at org.apache.spark.rdd.RDDOperationScope$.&lt;clinit&gt;(RDDOperationScope.scala) at org.apache.spark.SparkContext.withScope(SparkContext.scala:714) at org.apache.spark.SparkContext.textFile(SparkContext.scala:830) at com.iclick.word_segmentation.WordTest$.main(WordTest.scala:40) at com.iclick.word_segmentation.WordTest.main(WordTest.scala) 在spark版本是 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.10&lt;/artifactId&gt; &lt;version&gt;1.6.1&lt;/version&gt; &lt;/dependency&gt; 必须要要需要引入jackson的scala版本，这里的jackson-module-scala_2.10原来我用的2.4.4还依旧报错，改为2.7.3就OK了。 1234567891011121314151617181920&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.module&lt;/groupId&gt; &lt;artifactId&gt;jackson-module-scala_2.10&lt;/artifactId&gt; &lt;version&gt;2.7.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.module&lt;/groupId&gt; &lt;artifactId&gt;jackson-module-jaxb-annotations&lt;/artifactId&gt; &lt;version&gt;2.7.4&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.7.4&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-annotations&lt;/artifactId&gt; &lt;version&gt;2.7.4&lt;/version&gt; &lt;/dependency&gt; 报错 1Caused by: java.lang.UnsatisfiedLinkError: no snappyjava in java.library.path 1SnappyError: [FAILED_TO_LOAD_NATIVE_LIBRARY] null maven引用的版本问题，一开始用1.6.0就报错，改成1.6.1就可以了 报错 1java.net.BindException: Address already in use 默认端口是4040，被占用了就会递增，几次都不行就报错，我是手动设置的 1conf.set(&quot;spark.ui.port&quot;,&quot;4049&quot;) 也可以这样https://blog.csdn.net/a921122/article/details/45095845不过我没有成功 报错 1Javax.servlet.FilterRegistration&quot;&apos;s signer information does not match signer information of other classes in the same package 出现的原因是在spark项目里面引用了一个hive udf的jar包。 一直在Accepted一致处于 118/05/24 16:33:03 INFO yarn.Client: Application report for application_1524027932434_173978 (state: ACCEPTED) 多次重复后任务失败，报错 12345678910111213141516171818/05/24 16:40:08 INFO yarn.Client: client token: N/A diagnostics: Application application_1524027932434_173980 failed 2 times due to AM Container for appattempt_1524027932434_173980_000002 exited with exitCode: 15For more detailed output, check application tracking page:http://hlt-data-171.mljr.com:8088/proxy/application_1524027932434_173980/Then, click on links to logs of each attempt.Diagnostics: Exception from container-launch.Container id: container_e79_1524027932434_173980_02_000001Exit code: 15Stack trace: ExitCodeException exitCode=15: at org.apache.hadoop.util.Shell.runCommand(Shell.java:601) at org.apache.hadoop.util.Shell.run(Shell.java:504) at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786) at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213) at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302) at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745) http://www.itkeyword.com/doc/4214879305995466x649/application-report-for-application-state-accepted-never-ends-for-spark-submi 可以尝试，用yarn-cluster代替yarn或yarn-client 或者减小申请的内存，可能此时资源不够 执行包括hive的任务是OOM1java.lang.OutOfMemoryError: PermGen space https://blog.csdn.net/xiao_jun_0820/article/details/45038205 在Spark中使用hql方法执行hive语句时，由于其在查询过程中调用的是Hive的获取元数据信息、SQL解析，并且使用Cglib等进行序列化反序列化，中间可能产生较多的class文件，导致JVM中的持久代使用较多，如果配置不当，可能引起OOM问题。 原因是实际使用时，如果用的是JDK1.6版本，Server模式的持久代默认大小是64M，Client模式的持久代默认大小是32M，而Driver端进行SQL处理时，其持久代的使用可能会达到90M，导致OOM溢出，任务失败。 解决方法就是在Spark的conf目录中的spark-defaults.conf里，增加对Driver的JVM配置，因为Driver才负责SQL的解析和元数据获取。 因此增加了 1--driver-java-options &quot;-Xms2048m -Xmx2048m -XX:PermSize=1024M -XX:MaxPermSize=1024M -Xss10m&quot; spark提交后task数量过多num-executors 参数说明：该参数用于设置Spark作业总共要用多少个Executor进程来执行。Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。 参数调优建议：每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。 executor-memory 参数说明：该参数用于设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。 参数调优建议：每个Executor进程的内存设置4G~8G较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少，num-executors乘以executor-memory，就代表了你的Spark作业申请到的总内存量（也就是所有Executor进程的内存总和），这个量是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，那么申请的总内存量最好不要超过资源队列最大总内存的1/3~1/2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。 executor-cores 参数说明：该参数用于设置每个Executor进程的CPU core数量。这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。 参数调优建议：Executor的CPU core数量设置为2~4个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么num-executors * executor-cores不要超过队列总CPU core的1/3~1/2左右比较合适，也是避免影响其他同学的作业运行。 spark.default.parallelism 参数说明：该参数用于设置每个stage的默认task数量。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。 参数调优建议：Spark作业的默认task数量为500~1000个较为合适。很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此Spark官网建议的设置原则是，设置该参数为num-executors * executor-cores的2~3倍较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。 spark未序列化1org.apache.spark.SparkException: Task not serializable https://blog.csdn.net/javastart/article/details/51206715 spark读不了资源文件错误的写法是 12InputStream is = ScriptReader.class.getClassLoader().getResourceAsStream( &quot;/gps.sql&quot;); 正确的写法是 12ScriptReader.class.getResourceAsStream( &quot;/gps.sql&quot;); 报错 1scala.collection.mutable.ArrayBuffer cannot be cast to scala.collection.mutable.ListBuffer spark提交任务还没RUNNING就报错123456789101112131415161718192021222324252627282930313233343536Exit code: 10Stack trace: ExitCodeException exitCode=10: at org.apache.hadoop.util.Shell.runCommand(Shell.java:601) at org.apache.hadoop.util.Shell.run(Shell.java:504) at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786) at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213) at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302) at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)Container exited with a non-zero exit code 10Failing this attempt. Failing the application. ApplicationMaster host: N/A ApplicationMaster RPC port: -1 queue: root.infra.dmp start time: 1528194816590 final status: FAILED tracking URL: http://hlt-data-171.mljr.com:8088/cluster/app/application_1524027932434_235300 user: dmpException in thread &quot;main&quot; org.apache.spark.SparkException: Application application_1524027932434_235300 finished with failed status at org.apache.spark.deploy.yarn.Client.run(Client.scala:1035) at org.apache.spark.deploy.yarn.Client$.main(Client.scala:1082) at org.apache.spark.deploy.yarn.Client.main(Client.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731) at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181) at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206) at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121) at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala) 123456789101112131415161718Application application_1524027932434_235300 failed 2 times due to AM Container for appattempt_1524027932434_235300_000002 exited with exitCode: 10For more detailed output, check application tracking page:http://hlt-data-171.mljr.com:8088/proxy/application_1524027932434_235300/Then, click on links to logs of each attempt.Diagnostics: Exception from container-launch.Container id: container_e79_1524027932434_235300_02_000003Exit code: 10Stack trace: ExitCodeException exitCode=10:at org.apache.hadoop.util.Shell.runCommand(Shell.java:601)at org.apache.hadoop.util.Shell.run(Shell.java:504)at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786)at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213)at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)at java.util.concurrent.FutureTask.run(FutureTask.java:262)at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)at java.lang.Thread.run(Thread.java:745)Container exited with a non-zero exit code 10Failing this attempt. Failing the application. 1234567891011121314151617181920212218/06/05 18:33:48 ERROR yarn.ApplicationMaster: Uncaught exception: java.lang.ClassNotFoundException: com.mljr.spark.gps.sample.GPSMetricInfo at java.net.URLClassLoader$1.run(URLClassLoader.java:366) at java.net.URLClassLoader$1.run(URLClassLoader.java:355) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:354) at java.lang.ClassLoader.loadClass(ClassLoader.java:425) at java.lang.ClassLoader.loadClass(ClassLoader.java:358) at org.apache.spark.deploy.yarn.ApplicationMaster.startUserApplication(ApplicationMaster.scala:536) at org.apache.spark.deploy.yarn.ApplicationMaster.runDriver(ApplicationMaster.scala:319) at org.apache.spark.deploy.yarn.ApplicationMaster.run(ApplicationMaster.scala:185) at org.apache.spark.deploy.yarn.ApplicationMaster$$anonfun$main$1.apply$mcV$sp(ApplicationMaster.scala:653) at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:69) at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:68) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698) at org.apache.spark.deploy.SparkHadoopUtil.runAsSparkUser(SparkHadoopUtil.scala:68) at org.apache.spark.deploy.yarn.ApplicationMaster$.main(ApplicationMaster.scala:651) at org.apache.spark.deploy.yarn.ApplicationMaster.main(ApplicationMaster.scala)18/06/05 18:33:48 INFO yarn.ApplicationMaster: Final app status: FAILED, exitCode: 10, (reason: Uncaught exception: java.lang.ClassNotFoundException: com.mljr.spark.gps.sample.GPSMetricInfo)18/06/05 18:33:49 INFO util.ShutdownHookManager: Shutdown hook called 仅仅是main函数所在class的package变更，而提交任务时忘了修改 Container killed on request1ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e79_1524027932434_251117_02_000006 on host: hlt-data-195.mljr.com. Exit status: 137. Diagnostics: Container killed on request. Exit code is 137 可能是内存设置太大。 12Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Number of dynamic partitions created is 1381, which is more than 1000. To solve this try to set hive.exec.max.dynamic.partitions to at least 1381. at org.apache.hadoop.hive.ql.metadata.Hive.loadDynamicPartitions(Hive.java:1604) 1hiveContext.sql(&quot;SET hive.exec.max.dynamic.partitions=1500&quot;) 12User class threw exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 42 in stage 4.0 failed 4 times, most recent failure: Lost task 42.3 in stage 4.0 (TID 130, hlt-data-166.mljr.com, executor 12): java.lang.ArrayIndexOutOfBoundsExceptionDriver stacktrace: 最后一个日期字段没有加上 1User class threw exception: java.lang.reflect.InvocationTargetException 12345678910111213141516Container marked as failed: container_e79_1524027932434_277197_01_000355 on host: hlt-data-166.mljr.com. Exit status: 50. Diagnostics: Exception from container-launch.ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e79_1524027932434_277197_01_000355 on host: hlt-data-166.mljr.com. Exit status: 50. Diagnostics: Exception from container-launch.Container id: container_e79_1524027932434_277197_01_000355Exit code: 50Stack trace: ExitCodeException exitCode=50: at org.apache.hadoop.util.Shell.runCommand(Shell.java:601) at org.apache.hadoop.util.Shell.run(Shell.java:504) at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786) at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213) at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302) at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745) 虽然Executor上有这个报错，但是不影响任务运行 空指针错误1User class threw exception: java.lang.NullPointerException 没有定位到具体的行，往往是 no snappyjava1no snappyjava in java.library.path 参考https://blog.csdn.net/fansy1990/article/details/53954347 1、输出maven依赖包，找到snappy的jar包，如snappy-java-1.0.4.1.jar 2、unzip snappy.java.jar解压后，找到libsnappyjava.jnilib，重命名为libsnappyjava.dylib，复制文件所在目录 3、配置idea的项目环境，在VM options加上 1-Djava.library.path=/Users/david/david/tmp 后面路径就是libsnappyjava的路径 bad symbolic reference123456789101112131415161718192021222324Error:scalac: Error: bad symbolic reference. A signature in Vectors.class refers to term linalgin value breeze which is not available.It may be completely missing from the current classpath, or the version onthe classpath might be incompatible with the version used when compiling Vectors.class.scala.reflect.internal.Types$TypeError: bad symbolic reference. A signature in Vectors.class refers to term linalgin value breeze which is not available.It may be completely missing from the current classpath, or the version onthe classpath might be incompatible with the version used when compiling Vectors.class. at scala.reflect.internal.pickling.UnPickler$Scan.toTypeError(UnPickler.scala:847) at scala.reflect.internal.pickling.UnPickler$Scan$LazyTypeRef.complete(UnPickler.scala:854) at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1231) at scala.reflect.internal.Symbols$Symbol.tpe(Symbols.scala:1202) at scala.tools.nsc.transform.SpecializeTypes$$a Compiler.scala:111) at sbt.internal.inc.AnalyzingCompiler.compile(AnalyzingCompiler.scala:90) at org.jetbrains.jps.incremental.scala.local.IdeaIncrementalCompiler.compile(IdeaIncrementalCompiler.scala:40) at org.jetbrains.jps.incremental.scala.local.LocalServer.compile(LocalServer.scala:30) at org.jetbrains.jps.incremental.scala.remote.Main$.make(Main.scala:68) at org.jetbrains.jps.incremental.scala.remote.Main$.nailMain(Main.scala:25) at org.jetbrains.jps.incremental.scala.remote.Main.nailMain(Main.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at com.martiansoftware.nailgun.NGSession.run(NGSession.java:319) 尚未找到具体原因，把POM文件覆盖过去后就不会报错了。 mapValue后不能serializablehttps://stackoverflow.com/questions/32900862/map-can-not-be-serializable-in-scala 1rdd.groupBy(_.segment).mapValues(v =&gt; ...).map(identity) UDF：Schema for type Any is not supportedUDF的返回值不能是any 插入表报错ArrayIndexOutOfBoundsException在create表的时候报错 1234567val hiveSql = &quot;create table carthage_dev.gps_feature_info_wx2 as select * from tmp_feature_info_days&quot; hiveContext.sql(&quot;use carthage&quot;) hiveContext.sql(&quot;SET mapreduce.job.reduces=40&quot;) hiveContext.createDataFrame(gpsFeatureInfoRDD, schema).registerTempTable(tempTableName) hiveContext.sql(&quot;drop table if exists carthage_dev.gps_feature_info_wx2&quot;) hiveContext.sql(hiveSql) 执行到hiveContext.sql(hiveSql)时，报错 119/07/03 10:55:25 ERROR yarn.ApplicationMaster: User class threw exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 5.0 failed 4 times, most recent failure: Lost task 16.3 in stage 5.0 (TID 4136, hlt-data-186.mljr.com, executor 20): java.lang.ArrayIndexOutOfBoundsException 解决 不一定是插入表的问题，要细到stage中看，比如在这里看到 实际上是之前逻辑的问题]]></content>
      <categories>
        <category>spark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[map-flatMap-mapPartitions]]></title>
    <url>%2F2018%2F04%2F07%2Fhadoop-spark%2Fspark%2Fmap-flatMap-mapPartitions%2F</url>
    <content type="text"><![CDATA[用map实现 123456789101112131415161718192021import org.apache.spark.&#123; SparkConf, SparkContext &#125;object DoubleNum &#123; def mapDoubleFunc(a: Int): (Int, Int) = &#123; (a, a * 2) &#125; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf().setAppName("Samples").setMaster("local[1]"). set("spark.sql.shuffle.partitions", "1").set("spark.network.timeout", "30s") val sc = new SparkContext(conf) sc.setLogLevel("WARN") val a = sc.parallelize(1 to 9, 3) val mapResult = a.map(mapDoubleFunc) println(mapResult.collect().mkString) &#125;&#125; 如果这里改成flatMap？ 12 用mapPartitions实现 123456789101112val a = sc.parallelize(1 to 9, 3)def doubleFunc(iter: Iterator[Int]) : Iterator[(Int,Int)] = &#123; var res = List[(Int,Int)]() while (iter.hasNext) &#123; val cur = iter.next; res .::= (cur,cur*2) &#125; res.iterator &#125;val result = a.mapPartitions(doubleFunc)println(result.collect().mkString) 写成匿名函数 12345678val result = a.mapPartitions(iter =&gt; &#123; var res = List[(Int, Int)]() while (iter.hasNext) &#123; val cur = iter.next res.::=(cur, cur * 2) &#125; res.iterator &#125;) 也可以声明iter的数据类型(iter: Iterator[Int]) 12345678val result = a.mapPartitions( (iter: Iterator[Int]) =&gt; &#123; var res = List[(Int, Int)]() while (iter.hasNext) &#123; val cur = iter.next res.::=(cur, cur * 2) &#125; res.iterator &#125;) foreachPartitions]]></content>
      <categories>
        <category>spark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[spark任务提交]]></title>
    <url>%2F2018%2F04%2F06%2Fhadoop-spark%2Fspark%2Fspark%E4%BB%BB%E5%8A%A1%E6%8F%90%E4%BA%A4%2F</url>
    <content type="text"><![CDATA[1234567891011121314export CLASSPATH=.:$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar:$CLASSPATHexport JAVA_HOME=/opt/jdk1.7.0_67export PATH=$JAVA_HOME/bin:$PATHexport HADOOP_MAPRED_HOME=/usr/lib/hadoop-mapreducenohup /usr/lib/spark/bin/spark-submit --master spark://srv-buzz-bosk1:7077 --jars $CLASSPATH --class com.iclick.data.main.PredictAgeGender --executor-memory 100g --driver-memory 100g --total-executor-cores 100 --conf spark.akka.frameSize=2000 $&#123;PIG_HOME&#125;/spark-data-1.0.jar /shortdata/vid_url/ 10 $&#123;Today&#125; /rawdata/genome/$&#123;Yesterday&#125; /nlp/ageIndex.model /shortdata/vid_profile/$&#123;Today&#125; &gt;&gt;$&#123;PIG_EXT_LOG&#125; 2&gt;&amp;1 &amp; spark submit参数调优 官方文档Spark Configuration 用submit提交一个任务后，会启动一个对应的Driver进程，根据你使用的部署模式（deploy-mode）不同，Driver进程可能在本地启动，也可能在集群中某个工作节点上启动。Driver进程本身会根据我们设置的参数，占有一定数量的内存和CPU core。而Driver进程要做的第一件事情，就是向集群管理器（可以是Spark Standalone集群，也可以是其他的资源管理集群，美团•大众点评使用的是YARN作为资源管理集群）申请运行Spark作业需要使用的资源，这里的资源指的就是Executor进程。YARN集群管理器会根据我们为Spark作业设置的资源参数，在各个工作节点上，启动一定数量的Executor进程，每个Executor进程都占有一定数量的内存和CPU core。 1.num-executors 参数说明：该参数用于设置Spark作业总共要用多少个Executor进程来执行。Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。 参数调优建议：每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。 2.executor-memory 参数说明：该参数用于设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。 参数调优建议：每个Executor进程的内存设置4G~8G较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少，num-executors乘以executor-memory，是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，那么申请的内存量最好不要超过资源队列最大总内存的1/3~1/2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。 3.executor-cores 参数说明：该参数用于设置每个Executor进程的CPU core数量。这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。 参数调优建议：Executor的CPU core数量设置为2~4个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么num-executors * executor-cores不要超过队列总CPU core的1/3~1/2左右比较合适，也是避免影响其他同学的作业运行。 4.driver-memory 参数说明：该参数用于设置Driver进程的内存。 参数调优建议：Driver的内存通常来说不设置，或者设置1G左右应该就够了。唯一需要注意的一点是，如果需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。 5.spark.default.parallelism 参数说明：该参数用于设置每个stage的默认task数量。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。 参数调优建议：Spark作业的默认task数量为500~1000个较为合适。很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此Spark官网建议的设置原则是，设置该参数为num-executors * executor-cores的2~3倍较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。 6.spark.storage.memoryFraction 参数说明：该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。 参数调优建议：如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。 7.spark.shuffle.memoryFraction 参数说明：该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。 参数调优建议：如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。 8.total-executor-cores 参数说明：Total cores for all executors. 9.资源参数参考示例以下是一份spark-submit命令的示例： 123456789101112131415161718 --master spark://192.168.1.1:7077 \ --master yarn-cluster \ --master yarn-client \ ./bin/spark-submit \ --master spark://192.168.1.1:7077 \ --num-executors 100 \ --executor-memory 6G \ --executor-cores 4 \ --total-executor-cores 400 \ ##standalone default all cores --driver-memory 1G \ --conf spark.default.parallelism=1000 \ --conf spark.storage.memoryFraction=0.5 \ --conf spark.shuffle.memoryFraction=0.3 \]]></content>
      <categories>
        <category>spark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python指数、幂函数拟合]]></title>
    <url>%2F2018%2F04%2F04%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FPYTHON%2F%E6%8C%87%E6%95%B0%E3%80%81%E5%B9%82%E5%87%BD%E6%95%B0%E6%8B%9F%E5%90%88%2F</url>
    <content type="text"><![CDATA[转自：python指数、幂数拟合curve_fit 1、一次二次多项式拟合 一次二次比较简单，直接使用numpy中的函数即可，polyfit(x, y, degree)。 2、指数幂数拟合curve_fit 使用scipy.optimize 中的curve_fit，幂数拟合例子如下： 12345678910111213141516from scipy.optimize import curve_fit import matplotlib.pyplot as plt import numpy as np def func(x, a, b, c): return a * np.exp(-b * x) + c xdata = np.linspace(0, 4, 50) y = func(xdata, 2.5, 1.3, 0.5) ydata = y + 0.2 * np.random.normal(size=len(xdata)) plt.plot(xdata,ydata,&apos;b-&apos;) popt, pcov = curve_fit(func, xdata, ydata) #popt数组中，三个值分别是待求参数a,b,c y2 = [func(i, popt[0],popt[1],popt[2]) for i in xdata] plt.plot(xdata,y2,&apos;r--&apos;) print popt 下面是指数拟合例子： 123456789101112def fund(x, a, b): return x**a + b xdata = np.linspace(0, 4, 50) y = fund(xdata, 2.5, 1.3) ydata = y + 4 * np.random.normal(size=len(xdata)) plt.plot(xdata,ydata,&apos;b-&apos;) popt, pcov = curve_fit(fund, xdata, ydata) #popt数组中，三个值分别是待求参数a,b,c y2 = [fund(i, popt[0],popt[1]) for i in xdata] plt.plot(xdata,y2,&apos;r--&apos;) print popt 其他相关文章 Python科学计算——任意波形拟合 用Python开始机器学习（3：数据拟合与广义线性回归）]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[computing with spatial trajectories]]></title>
    <url>%2F2018%2F04%2F03%2F%E5%9C%B0%E7%90%86%E4%BF%A1%E6%81%AF%E6%8C%96%E6%8E%98%2Fcomputing%20with%20spatial%20trajectories%2F</url>
    <content type="text"><![CDATA[专著computing with spatial trajectories.pdf 第一章：数据预处理。对于位置点的高采样可以生成精确轨迹，但会数据量过大影响性能。所以要设计轨迹数据的压缩方法。同时，轨迹数据会生成离群点和噪音，要过滤这些。 对于压缩，有离线数据的批处理模式，和在线数据的在线模式。 对于消除噪音，包括mean and median filtering，the Kalman filter, the particle filter. 第二章：数据量大，处理时间长，数据集没有很好的组织。比如，要查找跨十字路口的轨迹原理简单，但是在在线系统中采用直接扫描的方式遍历大量数据是不可行的。同时，我们需要搜索满足特定条件的轨迹数据。比如我们可以检索旅行者在特定时间穿过指定区域的数据，以帮助旅行规划。 第二章介绍了在轨迹数据库中经常用到的查询类型和查询方法。 第三章：一般的GPS设备有10米以上的误差，这些移动的轨迹数据就比较难精确定位到POI上，尤其在高密度的城市区域。同时，对象在连续的移动，而位置只能按时间离线的更新，则两个更新点之间是不确定的。导致这种long-interval updates的原因是save energy consumption和communication bandwidth。当interval逐渐增大时，对轨迹搜索会带来新的挑战。 为解决上述的不确定性问题，第三章介绍了对不确定性表达和建模的Moving Objects Databases(MOD)模型。同时讨论了处理时空查询的高效算法。要注意的是，第二章介绍的查询方法没有考虑不确定性。 第四章：LBS的隐私问题。通常有两种类型的LBS。 snapshot LBS，移动用户只需要在得到想要的信息时汇报当前位置。事实上，当使用此类服务时，用户并不需要告诉LBS他的当前位置。比如，找到附近的宾馆只需要一个粗的地理范围。 continuous LBS，用户必须持续汇报位置，比如导航。保护用户隐私更困难。 第五章：分析轨迹模式。比如一个个体的轨迹包含什么模式，一组轨迹是否有相似模式。也可以轨迹聚类。比如，找到有相似空间形状的轨迹的clusters，能够帮助检测热门驾驶路线。另外，判断一群一起移动的人可用于探索社交关系。 第六章：用户行为认知。空间轨迹隐含了用户的行为和活动。通过多人的活动信息，能够估计相关性。 第七章：车辆轨迹挖掘。可以挖掘路网信息，交通状况，司机行为。用于优化路线等。 第八九章：用户喜欢分享位置信息，可以挖掘用户关系。 压缩车的轨迹一天1400条，若分析一个月的数据才需要压缩？ 批量压缩batched compression techniques 在线压缩on-line data reduction 1） 使用线段拟合尽可能多的轨迹点。主要关注空间属性 2）预测轨迹，并上报偏离预测的点。为了预测，需要关注速度、朝向等。 压缩的时候计算误差 1）perpendicular Euclidean distance 2）synchronized Euclidean distance 安装时间计算投影的误差 去噪音P46 Kalman Filter Particle Filter CH2 轨迹索引应用场景： 1）轨迹和点（查询某段时间在某个地点附近的轨迹点） 2）轨迹和区域（查询某段时间某个轨迹经过的区域） 3）轨迹和轨迹（某段时间内相似路径的轨迹） 轨迹查询的类别CH5 轨迹模式挖掘Introduction轨迹模式的应用案例： 1、交通优化应用[25]需要找到一组相似的轨迹，来指示一起行驶物体的相关性。例如，拼车应用需要找到相同轨迹组的司机，从而降低交通费用。或者检测相同轨迹的送货卡车以更好指定送货规划。 2、预测方法[50]能够探索轨迹的知识，用于理解物体行为。这些知识可用于提供有效通知effective notifications，用于精准推送广告，提供定制化LBS服务。 3、相关专业需要研究动物的迁徙轨迹。 4、团队体育运动能够提供运动员的轨迹数据。 5、交通应用，使用轨迹研究聚集和离群crowds and outliers。挖掘离群点可用于检测和移除错误数据，或者用于判断危险驾驶行为。 Overview of Trajectory Patterns]]></content>
      <categories>
        <category>地理信息处理和挖掘</category>
      </categories>
      <tags>
        <tag>论文整理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark算子分类及功能]]></title>
    <url>%2F2018%2F04%2F03%2Fhadoop-spark%2Fspark%2Fspark%E7%AE%97%E5%AD%90%E5%88%86%E7%B1%BB%E5%8F%8A%E5%8A%9F%E8%83%BD%2F</url>
    <content type="text"><![CDATA[大致分为3类： 1、Value数据类型的Transformation算子。不触发提交作业，针对处理的数据项是Value型的数据。 2、Key-Value型的Transformation算子。针对Key-Value数据对。 3、Action算子。触发SparkContext提交Job。 Value型Transformation算子可以根据RDD变换算子的输入分区与输出分区关系分为以下几个类型： 输入分区和输出分区一对一型； 多对一型； 多对多型； 输出为输入子集型。 一对一型map等action触发提交后，在同一个stage中运算。 flatMapmapPartitionsmap是对rdd中的每一个元素进行操作，而mapPartitions(foreachPartition)则是对rdd中的每个分区的迭代器进行操作。 如果在map过程中需要频繁创建额外的对象(例如将rdd中的数据通过jdbc写入数据库,map需要为每个元素创建一个链接而mapPartition为每个partition创建一个链接),则mapPartitions效率比map高的多。 SparkSql或DataFrame默认会对程序进行mapPartition的优化。 与map的区别https://blog.csdn.net/lsshlsw/article/details/48627737 使用map 1234567val a = sc.parallelize(1 to 9, 3)def mapDoubleFunc(a : Int) : (Int,Int) = &#123; (a,a*2)&#125;val mapResult = a.map(mapDoubleFunc)println(mapResult.collect().mkString) 使用mapPartitions 123456789101112val a = sc.parallelize(1 to 9, 3) def doubleFunc(iter: Iterator[Int]) : Iterator[(Int,Int)] = &#123; var res = List[(Int,Int)]() while (iter.hasNext) &#123; val cur = iter.next; res .::= (cur,cur*2) &#125; res.iterator &#125;val result = a.mapPartitions(doubleFunc)println(result.collect().mkString) glom将每个RDD分区形成一个数组。 多对一型union++符号相当于union cartesian对两个RDD中所有元素进行笛卡尔积操作。 多对多型groupBy例如 1）将用户数据预处理 1val cleanF = sc.clean(f) 2）对数据map进行函数操作，最后再对groupByKey进行分组操作 1this.map(t =&gt; (cleanF(t), t)).groupByKey(p) 其中，p确定了分区个数和分区函数，也就决定了并行化的程度。 子集型filterdistinctsubstract集合的差操作。RDD1去除RDD2交集中的所有元素。 sample采样。可以设定有放回的采样、百分比、随机种子。 takeSample不使用相对比例采样，而是按设定的采样个数进行采样。同时返回结果也不是RDD，而是相当于对采样后的数据进行Collect()，返回单机的数组。 Cache型cache将RDD从磁盘缓存到内存，相当于persist(MEMORY_ONLY) 的功能。 persist缓存到哪里，由StorageLevel枚举类型决定。有以下几种类型的组合：DISK磁盘，MEMORY内存，SER数据是否序列化存储。可以缓存的模式有 Key-Value型Transformation算子输入和输出分区一对一mapValues针对(Key, Value)型数据中的Value进行Map操作，而不对key进行处理。 比如a=&gt;a+2只对value进行+2操作 对单个或两个RDD聚集单个RDD聚集 combineByKey123456combineByKey[C](createCombiner:(V)=&gt; C,mergeValue:(C, V)=&gt; C,mergeCombiners:(C, C)=&gt; C,partitioner: PartitionermapSideCombine: Boolean = true,serializer: Serializer =null): RDD[(K, C)] 其中， createCombiner:(V)=&gt;C 会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过，要么就和之前的某个元素的键相同。如果这是一个新的元素， combineByKey() 会使用一个叫作 createCombiner() 的函数来创建 。 mergeValue 当C已经存在的情况下,需要merge,如把item V加到seq C中,或者叠加。 mergeCombiners 合并两个C partitioner 分区器，Shuffle时需要通过Partitioner的分区策略进行分区。 mapSideCombine 为了减小传输量,很多combine可以在map端先做。例如,叠加可以先在一个partition中把所有相同的Key的Value叠加，再shuffle。 serializer 传输序列化 整个过程相当于将元素为(int,int)的转变为了(int, Seq[Int])的RDD。 reduceByKey是combineByKey的一种简单情况。 123def reduceByKey(partitioner: Partitioner, func: (V, V) =&gt; V): RDD[(K, V)] = &#123; combineByKey[V]((v: V) =&gt; v, func, func, partitioner)&#125; combineByKey和reduceByKey区别转自请教Spark 中 combinebyKey 和 reduceByKey的传入函数参数的区别？例如： 123val testData = sc.parallelize(Seq(("t1", 1), ("t1", 2), ("t1", 3), ("t2", 2), ("t2", 5)))val testDataCombine = testData.combineByKey(x=&gt;x,(x:Int,y:Int)=&gt;x+y,(x:Int,y:Int)=&gt;x+y)val testDataReduce = testData.reduceByKey((x,y)=&gt;x+y) combineByKey要指定x和y的Int类型，不然会报错无法识别”+”这个方法符，而reduceByKey不用指派类型。 从两者的方法签名 12345678class PairRDDFunctions[K, V](...) &#123; def reduceByKey(func: (V, V) =&gt; V): RDD[(K, V)] def combineByKey[C]( createCombiner: V =&gt; C, mergeValue: (C, V) =&gt; C, mergeCombiners: (C, C) =&gt; C): RDD[(K, C)]&#125; 可以看到 reduceByKey 的 func 参数的类型只依赖于 PairRDDFunction 的类型参数 V，在这个例子里也就是 Int。于是 func 的类型已经确定为 (Int, Int) =&gt; Int，所以就不需要额外标识类型了。 而 combineByKey 比 reduceByKey 更加通用，它允许各个 partition 在 shuffle 前先做 local reduce 得到一个类型为 C 的中间值，待 shuffle 后再做合并得到各个 key 对应的 C。也就是说C可以是任意定义的数据类型，所以必须要指定数据类型，在上面的例子中比较简单，看不出来。 以求均值为例，我们可以让每个 partiton 先求出单个 partition 内各个 key 对应的所有整数的和 sum 以及个数 count，然后返回一个 pair (sum, count)。在 shuffle 后累加各个 key 对应的所有 sum 和 count，再相除得到均值： 1234567891011121314val sumCountPairs: RDD[(String, (Int, Long))] = testData.combineByKey( (_: Int) =&gt; (0, 0L), (pair: (Int, Long), value: Int) =&gt; (pair._1 + value, pair._2 + 1L), (pair1: (Int, Long), pair2: (Int, Long)) =&gt; (pair1._1 + part2._1, pair2._2 + pair2._2))val averages: RDD[String, Double] = sumCountPairs.mapValues &#123; case (sum, 0L) =&gt; 0D case (sum, count) =&gt; sum.toDouble / count&#125; 这里的C就通过V定义成了(Int, Long) 另一个，学生平均成绩的例子 https://blog.csdn.net/t1dmzks/article/details/70249743 https://www.edureka.co/blog/apache-spark-combinebykey-explained partitionBy对RDD进行分区操作。如果原有RDD的partitioner与现有的一致，则不变；否则相当于根据partitioner生成一个新的ShuffledRDD。 对两个RDD进行聚集 ？cogroup将连个RDD协同划分。每个RDD中相同Key的元素分别聚合为一个集合，并返回两个RDD中对应Key的元素集合的迭代器。 1(K, (Iterable[V], Iterable[W])) 连接join对两个需要连接的RDD进行cogroup操作。之后形成新的RDD，对每个key下的元素进行笛卡尔积操作，返回的结果再flat。函数实现为 12this.cogroup(other, partitioner).flatMapValues &#123; case (vs, ws) =&gt;for (v &lt;- vs; w &lt;- ws) yield (v, w) &#125; 本质就是通过cogroup先进行协同划分，再通过flatMapValues将合并的数据打散。 例如 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112import org.apache.spark.sql.types.&#123;DataTypes, StructField, StructType&#125;import org.apache.spark.sql.&#123;Row, SQLContext&#125;import org.apache.spark.&#123;SparkConf, SparkContext&#125;object Run &#123; def main(args: Array[String]) &#123; val conf = new SparkConf().setAppName("test").setMaster("local") val sc = new SparkContext(conf) sc.setLogLevel("ERROR") val sqlContext = new SQLContext(sc) /** * id name * 1 zhangsan * 2 lisi * 3 wangwu */ val idName = sc.parallelize(Array((1, "zhangsan"), (2, "lisi"), (3, "wangwu"))) /** * id age * 1 30 * 2 29 * 4 21 */ val idAge = sc.parallelize(Array((1, 30), (2, 29), (4, 21))) /** *******************************RDD **********************************/ println("*********************************RDD**********************************") println("\n内关联（inner join）\n") // 内关联（inner join） // 只保留两边id相等的部分 /** * (1,(zhangsan,30)) * (2,(lisi,29)) */ idName.join(idAge).collect().foreach(println) println("\n左外关联（left out join）\n") // 左外关联（left out join） // 以左边的数据为标准, 左边的数据一律保留 // 右边分三情况: // 一: 左边的id, 右边有, 则合并数据; (1,(zhangsan,Some(30))) // 二: 左边的id, 右边没有, 则右边为空; (3,(wangwu,None)) // 三: 右边的id, 左边没有, 则不保留; 右边有id为4的行, 但结果中并未保留 /** * (1,(zhangsan,Some(30))) * (2,(lisi,Some(29))) * (3,(wangwu,None)) */ idName.leftOuterJoin(idAge).collect().foreach(println) println("\n右外关联（right outer join）\n") // 右外关联（right outer join） // 以右边的数据为标准, 右边的数据一律保留 // 左边分三种情况: // 一: 右边的id, 左边有, 则合并数据; (1,(Some(zhangsan),30)) // 二: 右边的id, 左边没有, 则左边为空; (4,(None,21)) // 三: 左边的id, 右边没有, 则不保留; 左边有id为3的行, 但结果中并为保留 /** * (1,(Some(zhangsan),30)) * (2,(Some(lisi),29)) * (4,(None,21)) */ idName.rightOuterJoin(idAge).collect().foreach(println) println("\n全外关联（full outer join）\n") // 全外关联（full outer join） /** * * (1,(Some(zhangsan),Some(30))) * (2,(Some(lisi),Some(29))) * (3,(Some(wangwu),None)) * (4,(None,Some(21))) */ idName.fullOuterJoin(idAge).collect().foreach(println) /** *******************************DataFrame **********************************/ val schema1 = StructType(Array(StructField("id", DataTypes.IntegerType, nullable = true), StructField("name", DataTypes.StringType, nullable = true))) val idNameDF = sqlContext.createDataFrame(idName.map(t =&gt; Row(t._1, t._2)), schema1) val schema2 = StructType(Array(StructField("id", DataTypes.IntegerType, nullable = true), StructField("age", DataTypes.IntegerType, nullable = true))) val idAgeDF = sqlContext.createDataFrame(idAge.map(t =&gt; Row(t._1, t._2)), schema2) println("*********************************DataFrame**********************************") println("\n内关联（inner join）\n") // 相当于调用, idNameDF.join(idAgeDF, Seq("id"), "inner").collect().foreach(println) // 这里只是调用了封装的API idNameDF.join(idAgeDF, "id").collect().foreach(println) println("\n左外关联（left out join）\n") idNameDF.join(idAgeDF, Seq("id"), "left_outer").collect().foreach(println) println("\n右外关联（right outer join）\n") idNameDF.join(idAgeDF, Seq("id"), "right_outer").collect().foreach(println) println("\n全外关联（full outer join）\n") idNameDF.join(idAgeDF, Seq("id"), "outer").collect().foreach(println) println("\nleft semi join\n") // left semi join // 左边的id, 在右边有, 就保留左边的数据; 右边的数据不保留, 只有id的有意义的 /** * [1,zhangsan] * [2,lisi] */ idNameDF.join(idAgeDF, Seq("id"), "leftsemi").collect().foreach(println) &#125;&#125; 当出现相同Key时, join会出现笛卡尔积, 而cogroup的处理方式不同 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475import org.apache.spark.sql.SQLContextimport org.apache.spark.&#123;SparkConf, SparkContext&#125;object Run &#123; def main(args: Array[String]) &#123; val conf = new SparkConf().setAppName("test").setMaster("local") val sc = new SparkContext(conf) sc.setLogLevel("ERROR") val sqlContext = new SQLContext(sc) /** * id name * 1 zhangsan * 2 lisi * 3 wangwu */ val idName = sc.parallelize(Array((1, "zhangsan"), (2, "lisi"), (3, "wangwu"))) /** * id age * 1 30 * 2 29 * 4 21 */ val idAge = sc.parallelize(Array((1, 30), (2, 29), (4, 21))) println("\ncogroup\n") /** * (1,(CompactBuffer(zhangsan),CompactBuffer(30))) * (2,(CompactBuffer(lisi),CompactBuffer(29))) * (3,(CompactBuffer(wangwu),CompactBuffer())) * (4,(CompactBuffer(),CompactBuffer(21))) */ idName.cogroup(idAge).collect().foreach(println) println("\njoin\n") // fullOuterJoin于cogroup的结果类似, 只是数据结构不一样 /** * (1,(Some(zhangsan),Some(30))) * (2,(Some(lisi),Some(29))) * (3,(Some(wangwu),None)) * (4,(None,Some(21))) */ idName.fullOuterJoin(idAge).collect().foreach(println) /** * id score * 1 100 * 2 90 * 2 95 */ val idScore = sc.parallelize(Array((1, 100), (2, 90), (2, 95))) println("\ncogroup, 出现相同id时\n") /** * (1,(CompactBuffer(zhangsan),CompactBuffer(100))) * (2,(CompactBuffer(lisi),CompactBuffer(90, 95))) * (3,(CompactBuffer(wangwu),CompactBuffer())) */ idName.cogroup(idScore).collect().foreach(println) println("\njoin, 出现相同id时\n") /** * (1,(Some(zhangsan),Some(100))) * (2,(Some(lisi),Some(90))) * (2,(Some(lisi),Some(95))) * (3,(Some(wangwu),None)) */ idName.fullOuterJoin(idScore).collect().foreach(println) &#125;&#125; Actions算子无输出foreachforeachPartitionsForeach与ForeachPartition都是在每个partition中对iterator进行操作, 不同的是,foreach是直接在每个partition中直接对iterator执行foreach操作,而传入的function只是在foreach内部使用, 而foreachPartition是在每个partition中把iterator给传入的function,让function自己对iterator进行处理（可以避免内存溢出）. HDFSsaveAsTextFile将数据输出，存储到HDFS的指定目录。 saveAsObjectFile将分区中每10个元素组成一个Array，然后将这个Array序列化，映射为(Null, BytesWritable(Y))的元素，写入HDFS为SequenceFile的格式。 Scala集合和数据类型collect相当于toArray，但toArray已经过时不推荐使用。collect将RDD返回为一个单机的scala Array数组。 collectAsMap返回一个单机的HashMap。对于重复Key的RDD，后面覆盖前面的。 reduceByKeyLocally先对RDD整体进行reduce再collectAsMap。 lookupLookup函数对(Key,Value)型的RDD操作，返回指定Key对应的元素形成的Seq。这 个函数处理优化的部分在于，如果这个RDD包含分区器，则只会对应处理K所在的分区，然 后返回由(K,V)形成的Seq。如果RDD不包含分区器，则需要对全RDD元素进行暴力扫描 处理，搜索指定K对应的元素。 counttop返回最大的k个元素 take返回最小的k个 takeOrdered返回最小的k个元素，并且在返回的数组中保持元素的顺序。 first相当于top(1)，可以定义排序的方式Ordering[T] reduce相当于对RDD中的元素进行reduceLeft函数的操作。 1Some(iter.reduceLeft(cleanF)) 先对每个分区的集合进行reduceLeft，结果形成一个元素，再对这个结果做reduceLeft。 例如，用户自定义函数为 1f:(A,B)=&gt;(A._1+&quot;@&quot;+B._1,A._2+B._2) fold和reduce原理相同，接收与reduce接收的函数签名相同的函数，不同点是，另外再加上一个初始值作为第一次调用的结果。 aggregate可以对两个不同类型的元素进行聚合，即支持异构。它先聚合每一个分区里的元素，然后将所有结果返回回来，再用一个给定的conbine方法以及给定的初始值zero value进行聚合。 没懂 aggreagate与fold和reduce的不同之处在于,aggregate相当于采用归并的方式进行数据聚集,这种聚集是并行化的。而在fold和reduce函数的运算过程中,每个分区中需要进行串行处理,每个分区串行计算完结果,结果再按之前的方式进行聚集,并返回最终聚集结果。 1def aggregate [U: ClassTag] (zeroValue: U) (seqOp: (U,T)=&gt;U，combOp: (U,U)=&gt;U):U 由以上可以看到，(zeroValue: U)是给定一个初值，后半部分有两个函数，seqOp与combOp。seqOp相当于是在各个分区里进行的聚合操作，它支持(U,T)=&gt;U，也就是支持不同类型的聚合。combOp是将seqOp后的结果再进行聚合，此时的结果全部是U类，只能进行同构聚合。 两个特殊变量broadcast用于广播Map Side Join中的小表，以及广播大变量等场景。这些数据集合在单节点内存能够容纳,不需要像RDD那样在节点之间打散存储。Spark运行时把广播变量数据发到各个节点,并保存下来,后续计算可以复用。 相比Hadoop的distributed cache，广播的内容可以跨作业共享。 accumulator允许做全局累加操作，如accumulator变量广泛使用在应用中记录当前的运行指标的情景。]]></content>
      <categories>
        <category>spark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[spark工作机制]]></title>
    <url>%2F2018%2F04%2F03%2Fhadoop-spark%2Fspark%2Fspark%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[spark应用执行机制Spark执行机制总览1、RDD的Action算子触发Job的提交； 2、提交到Spark的Job生成RDD DAG； 3、由DAGSchedule转化为Stage DAG； 4、每个Stage中产生相应的Task集合； 5、TaskScheduler将Task分发到Executor执行。 6、每个Task对应一个数据块，使用用户定义的函数处理数据块。 Spark执行的底层实现原理对RDD的块管理通过BlockManager完成。BlockManager将数据抽象为数据块，在内存或磁盘存储。 在Executor中会创建线程池，tasks通过线程池并发执行。 Spark应用的概念执行模式有Local，Standalone，YARN，Mesos。 Spark的三种分布式部署模式：Standalone, Mesos,Yarn Standalone可以单独部署到一个集群中，无需依赖任何其他资源管理系统。 借鉴Spark开发模式，我们可以得到一种开发新型计算框架的一般思路：先设计出它的standalone模式，为了快速开发，起初不需要考虑服务（比如master/slave）的容错性，之后再开发相应的wrapper，将stanlone模式下的服务原封不动的部署到资源管理系统yarn或者mesos上，由资源管理系统负责服务本身的容错。 目前Spark在standalone模式下是没有任何单点故障问题的，这是借助zookeeper实现的，思想类似于Hbase master单点故障解决方案。将Spark standalone与MapReduce比较，会发现它们两个在架构上是完全一致的。 集群启动后，用jps在主节点会看到Master进程，从节点看到Worker进程。其中Master负责接收客户端提交的作业，管理Worker。 Mesos官方推荐模式。有粗粒度和细粒度模式。 YARN。目前仅支持粗粒度模式。 如果使用Standalone模式，只需要提供Hadoop的HDFS支持；如果使用Yarn模式，则同时需要提供Hadoop的Yarn。 根据Driver Program是否在集群中运行，又可以分为Cluster和Client模式。 一个应用的基本组件包括： Application：用户写的Spark程序。 Driver Program：运行Application的main函数并创建SparkContext RDD Graph：当RDD遇到Action算子时，将之前所有算子形成一个DAG，也就是RDD Graph。再在Spark中转化为Job，提交到集群执行。一个App可以包含多个Job。 Job：一个RDD Graph触发的作业。 Stage：每个Job根据RDD的宽依赖关系被切分成多个Stage，每个Stage包含一组Task。 Task：一个分区对应一个Task，Task封装好后装入Executor的线程池执行。 spark作业提交以WordCount为例说明RDD从转换到作业提交的过程 1sc.textFile("/User/david/key.txt").flatMap(line=&gt;line.split(" ")).map(word=&gt;(word,1)).reduceByKey(_+_) 步骤1：val rawFile = sc.textFile(&quot;/User/david/key.txt&quot;) textFile先生成HadoopRDD，然后再通过map操作生成MappedRDD。在spark-shell中可以看到 123scala&gt; val rawFile = sc.textFile(&quot;/User/david/key.txt&quot;)rawFile: org.apache.spark.rdd.RDD[String] = /User/david/key.txt MapPartitionsRDD[3] at textFile at &lt;console&gt;:271.6.3版本变成了MapPartitionsRDD 步骤2： 1val splittedText = rawFile.flatMap(line=&gt;line.split(&quot; &quot;)) flatMap将原来的MappedRDD转换为FlatMappedRDD。 步骤3 1val wordCount = splittedText.map(word=&gt;(word,1)) 步骤4：reduceByKey 作业执行spark执行中相关概念 Spark中Task，Partition，RDD、节点数、Executor数、core数目的关系和Application，Driver，Job，Task，Stage理解 若干个block合并成一个输入分片InputSplit，一个InputSplit对应一个Task，一个Task生成一个Partition。 随后这些具体的Task每个都会被分配到集群上的某个节点的某个Executor去执行。 每个节点可以启一个或多个Executor。 每个Executor由若干core组成，每个Executor的每个core一次只能执行一个Task。 每个Task执行的结果就是生成了目标RDD的一个partiton。每个partition再下一步又由一个task来执行。 注意: 这里的core是虚拟的core而不是机器的物理CPU核，可以理解为就是Executor的一个工作线程。 而 Task被执行的并发度 = Executor数目 * 每个Executor核数。 所以，如果一共要执行8个task，但只有一个Executor，2个core，则并发度是2。那么需要分成4个批次，每次并发执行两个Task。 至于partition的数目： 对于数据读入阶段，例如sc.textFile，输入文件被划分为多少InputSplit就会需要多少初始Task。 在Map阶段partition数目保持不变。 在Reduce阶段，RDD的聚合会触发shuffle操作，聚合后的RDD的partition数目跟具体操作有关，例如repartition操作会聚合成指定分区数，还有一些算子是可配置的。 在任务提交中主要涉及Driver和Executor两个节点。 Driver可以理解为我们自己编写的程序。主要解决 RDD依赖性分析，以生成DAG 根据RDD DAG将Job分割为多个stage Stage确认后，生成相应的task，分发到Executor执行。 Executor：在每个WorkerNode上为某应用启动的一个进程，是一个执行task的容器。一个Executor执行多个Task。 另外 Job：包含很多task的并行计算，可以认为是Spark RDD 里面的action,每个action的计算会生成一个job。 用户提交的Job会提交给DAGScheduler，Job会被分解成Stage和Task。 Spark中的Job和MR中Job不一样。MR中Job主要是Map或者Reduce Job。而Spark的Job其实很好区别，一个action算子就算一个Job，比方说count，first等。 Stage： 一个Job会被拆分为多组Task，每组任务被称为一个Stage就像Map Stage， Reduce Stage。 Stage的划分在RDD的论文中有详细的介绍，简单的说是以shuffle和result这两种类型来划分。在Spark中有两类task，一类是shuffleMapTask，一类是resultTask，第一类task的输出是shuffle所需数据，第二类task的输出是result，stage的划分也以此为依据，shuffle之前的所有变换是一个stage，shuffle之后的操作是另一个stage。比如 rdd.parallize(1 to 10).foreach(println) 这个操作没有shuffle，直接就输出了，那么只有它的task是resultTask，stage也只有一个；如果是rdd.map(x =&gt; (x, 1)).reduceByKey( + ).foreach(println), 这个job因为有reduce，所以有一个shuffle过程，那么reduceByKey之前的是一个stage，执行shuffleMapTask，输出shuffle所需的数据，reduceByKey到最后是一个stage，直接就输出结果了。如果job中有多次shuffle，那么每个shuffle之前都是一个stage。 Task 即 stage 下的一个任务执行单元，一般来说，一个 rdd 有多少个 partition，就会有多少个 task，因为每一个 task 只是处理一个 partition 上的数据. 依赖性分析和stage划分RDD之间的依赖分为窄依赖和宽依赖。 窄依赖是指父RDD所有输出都会被执行的子RDD消费，也就是输出路径固定。例如如下的Transformation： map、flatMap、filter、sample 宽依赖是指父RDD输出会由不同子RDD消费，输出路径不固定。例如： sortByKey、reduceByKey、groupByKey、cogroupByKey、join、cartensian 调度器（Scheduler）会计算RDD之间的依赖关系，将窄依赖的RDD归并到同一个stage，而宽依赖则作为划分不同Stage的判断标准。宽依赖和窄依赖的边界就是stage的划分点 任务的创建和分发由Executor执行的Task分为ShuffleMapTask和ResultTask两种，相当于Map和Reduce。]]></content>
      <categories>
        <category>spark</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F04%2F01%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8%2F</url>
    <content type="text"><![CDATA[转自《程序员代码面试指南 IT名企算法与数据结构题目最优解 ,左程云著》 如果碰到网页黑名单系统、爬虫的网址判重等，如果系统容忍一定程度的失误率，但对空间要求比较严格，往往是要求了解布隆过滤器。 一个布隆过滤器精确代表一个集合，并可以精确判断一个元素是否在集合中。但是有多精确，取决于具体的设计，但不可能完全正确。 哈希函数输入是任意，输出是固定范围，假设为S，并具有如下性质： 1、典型的哈希函数都有无限的输入值域。 2、传入相同的输入值，返回值一样。 3、传入不同输入值，返回值可能一样，也可能不一样。 4、最重要的性质：很多不同输入值得到的返回值会均匀分布在S上。 第4点是评价一个哈希函数优劣的关键，这种均匀分布与输入值出现的规律无关。比如MD5和SHA1算法。 布隆过滤器原理假设有一个长度为m的bit类型的数组，即数组中每个位置只占一个bit，每个bit只有0和1两种状态 再假设一共有k个哈希函数，这些函数的输出域S都大于或等于m，且这些哈希函数彼此独立。那么对同一个输入对象，经过k个哈希函数算出来的结果也是独立的。对算出来的每个结果都对m取余，然后再bit array上把相应位置设置1。 把bit类型的数组记为bitMap。处理完所有输入对象后，可能bitMap中已经有很多位置被涂黑。至此，一个布隆过滤器生成完毕，这个代表之前所有输入对象组成的集合。 在检查阶段，如何检查某一个对象是否是之前的某一个输入对象？ 一个输入对象，通过k个哈希算出k个值，把k个值对m取余，在bitMap上看这些位置是不是都是黑，如果有一个不为黑，则一定不在集合。如果都是黑，说明在集合，但是这里存在误判。 实现如果bitMap的大小m相比于输入对象的个数n过小，失误率会变大。 假设黑名单中样本个数为100亿个，记为n；失误率不希望超过0.01%，记为p；每个样本（url）大小64B，当然这个大小不会影响布隆过滤器的大小。 所以n=100亿，p=0.01%，布隆过滤器的大小m由以下公式确定： m = - \frac{n*\ln p}{(\ln 2)^2}根据公式计算出m=19.19n，大约20n，即需要2000亿个bit，也即是25G。 哈希函数的个数由以下公式决定： k = \ln 2 * \frac {m} {n}计算出约为14个。 然后用25GB的bitMap再单独实现14个哈希函数，根据如上描述生成布隆过滤器。 失误率的计算 失误率就是某个不认识的url，经过k个哈希函数后，在布隆过滤器取余后发现这些位置上都是黑，这样的情况称为一个失误。那么失误率就是看k个位置都是黑的概率。 假设k个哈希函数独立，对于某一个bit位，一个输入对象在被k个哈希函数散列后，这个位置依然没有被涂黑的概率为： (1-\frac 1 m)^k经过n个输入对象后，这个位置依然没有被涂黑的概率为： (1-\frac 1 m)^{kn}被涂黑的概率就是 1-(1-\frac 1 m)^{kn}则在检查阶段，检查k个位置都为黑的概率为： (1-(1-\frac 1 m)^{kn})^k = (1-(1-\frac 1 m)^{-m*- \frac {-kn} {m}})^k在$x \to 0$时，$(1+x)^{\frac 1 x} \to e$ 。这里m是很大的数，所以$-\frac 1m \to 0$，则简化为 (1-e^{\frac {-nk} {m}})^k误报的解决通过建立白名单防止误报。比如已经发现某个样本不在布隆过滤器，但每次计算后结果都显示其在布隆过滤器中，则可以把该样本加入白名单中，以后就知道的确不在过滤器中。 代码爬虫项目中用到了布隆过滤器 [SimpleBloomFilter.java](../../../../gitlab/data-platform/page-feature/rtbreq-frontier/src/main/java/com/buzzinate/util/SimpleBloomFilter.java) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193package com.buzzinate.util;import java.io.File;import java.io.FileInputStream;import java.io.FileOutputStream;import java.io.ObjectInputStream;import java.io.ObjectOutputStream;import java.util.BitSet;import org.apache.commons.lang.StringUtils;import org.apache.log4j.Logger;public class SimpleBloomFilter&#123; private static Logger logger = Logger.getLogger(SimpleBloomFilter.class); private static SimpleBloomFilter instance = null; private static final int DEFAULT_SIZE = Integer.MAX_VALUE; private static final int[] seeds = new int[] &#123; 5, 7, 11, 13, 31, 37, 61 &#125;; private BitSet bits = new BitSet(DEFAULT_SIZE); private SimpleHash[] func = new SimpleHash[seeds.length]; public static String filename = "/opt/tomcat/bloomfilter"; private SimpleBloomFilter() &#123; for (int i = 0; i &lt; seeds.length; i++) &#123; func[i] = new SimpleHash(DEFAULT_SIZE, seeds[i]); &#125; &#125; public BitSet getBits() &#123; return bits; &#125; public void setBits(BitSet bits) &#123; this.bits = bits; &#125; public void add(String value) &#123; for (SimpleHash f : func) &#123; bits.set(f.hash(value), true); &#125; &#125; public boolean contains(String value) &#123; if (value == null) &#123; return false; &#125; boolean ret = true; for (SimpleHash f : func) &#123; ret = ret &amp;&amp; bits.get(f.hash(value)); &#125; return ret; &#125; public void saveBit() &#123; try &#123; int count = 0; for(int i = 0;i &lt; DEFAULT_SIZE;i ++)&#123; if(bits.get(i))&#123; count ++ ; &#125; &#125; logger.info("bloomfilter true bits:"+count); File file = new File(filename+".tmp"); logger.info("save bloomfilter, path "+ file.getAbsolutePath()); ObjectOutputStream oos = new ObjectOutputStream( new FileOutputStream(file, false)); oos.writeObject(bits); oos.flush(); oos.close(); file.renameTo(new File(filename)); logger.info("saved bloomfilter to " + file.getAbsolutePath()); &#125; catch (Exception e) &#123; logger.error(e, e); &#125; &#125; public BitSet readBit(String path) &#123; BitSet bits = new BitSet(DEFAULT_SIZE); File file = new File(filename); if(StringUtils.isNotBlank(path))&#123; file = new File(path); &#125; logger.info("read bloomfilter, path "+ file.getAbsolutePath()); if (!file.exists()) &#123; logger.info(file.getAbsolutePath()+" not exists!"); return bits; &#125; try &#123; ObjectInputStream ois = new ObjectInputStream(new FileInputStream( file)); bits = (BitSet) ois.readObject(); ois.close(); int count = 0; for(int i = 0;i &lt; DEFAULT_SIZE;i ++)&#123; if(bits.get(i))&#123; count ++ ; &#125; &#125; logger.info("bloomfilter true bits:"+count); &#125; catch (Exception e) &#123; logger.error("read bloomfilter fail!", e); &#125; return bits; &#125; // 内部类，simpleHash public static class SimpleHash &#123; private int cap; private int seed; public SimpleHash(int cap, int seed) &#123; this.cap = cap; this.seed = seed; &#125; public int hash(String value) &#123; int result = 0; int len = value.length(); for (int i = 0; i &lt; len; i++) &#123; result = seed * result + value.charAt(i); &#125; return (cap - 1) &amp; result; &#125; &#125; public static SimpleBloomFilter getInstance()&#123; if(instance == null)&#123; instance = new SimpleBloomFilter(); if(new File(filename).exists())&#123; instance.setBits(instance.readBit("")); &#125; &#125; return instance; &#125; /** * 改变实例 * @param path */ public static boolean changeInstance(String path)&#123; SimpleBloomFilter tmpInstance = new SimpleBloomFilter(); File file = new File(path); if(System.currentTimeMillis() - file.lastModified() &lt; 3600 * 1000)&#123; tmpInstance.setBits(tmpInstance.readBit(path)); instance = tmpInstance; return true; &#125; return false; &#125; /** * 返回bit为1的数量 * @return */ public static int trueBits()&#123; int count = 0; for(int i = 0;i &lt; DEFAULT_SIZE;i ++)&#123; if(instance.bits.get(i))&#123; count ++ ; &#125; &#125; return count; &#125; public static void main(String[] args) &#123; SimpleBloomFilter filter = new SimpleBloomFilter(); for (int i = 0; i &lt;100000; i++) &#123; filter.add(i + ""); &#125; long t1 = System.currentTimeMillis(); filter.saveBit(); long t2 = System.currentTimeMillis(); System.out.println("it takes " + (t2 - t1)+"ms"); // System.out.println("add end");// int cnt = 0;// for (int i = 200000; i &gt; 100000; i--) &#123;// if (filter.contains(i + "")) &#123;// cnt++;// &#125;// &#125;//// System.out.println("total:" + cnt / 300000000.0); &#125;&#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[java技巧总结（不断更新中）]]></title>
    <url>%2F2018%2F03%2F31%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FJAVA%2Fjava%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[list转array123List&lt;Double&gt; scores = new ArrayList&lt;Double&gt;();Double[] arrays = scores.toArray(new Double[0]); log4j打印catch堆栈信息1logger.error(e.getMessage(),e); 判断字符类型判断中文字符1Character.isLetter() 判断是否为英文或数字123456789public static boolean isAlphaOrDigit(char ch) &#123; if (ch &gt;= 'a' &amp;&amp; ch &lt;= 'z') return true; if (ch &gt;= 'A' &amp;&amp; ch &lt;= 'Z') return true; if (ch &gt;= '0' &amp;&amp; ch &lt;= '9') return true; return false;&#125; 文件读写123456789101112String encoding = &quot;GBK&quot;; String lineText = null; File file = new File(&quot;D:\\00projects\\20buzz-cookie-info\\chinaz.json&quot;); if (file.isFile() &amp;&amp; file.exists()) &#123; BufferedReader read = new BufferedReader(new InputStreamReader( new FileInputStream(file), encoding)); while((lineText=read.readLine())!=null)&#123; &#125; read.close(); &#125; 读tar.gz12List&lt;String&gt; content = GzipUtil.readAllLines(Thread.currentThread().getContextClassLoader() .getResourceAsStream(filename)); 读resource的文件1234567891011121314151617181920212223242526BufferedReader br = null; try &#123; br = new BufferedReader(new InputStreamReader(Thread.currentThread().getContextClassLoader() .getResourceAsStream(modelFilePath), &quot;utf8&quot;)); /* br = new BufferedReader(new InputStreamReader(new FileInputStream(modelFilePath), &quot;utf8&quot;));*/ String line = &quot;&quot;; while ((line = br.readLine()) != null) &#123; String[] dataArr = line.split(&quot;\t&quot;); if(dataArr.length == 2)&#123; enToCh.put(dataArr[0], dataArr[1]); chToEn.put(dataArr[1], dataArr[0]); &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; br.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; 写文件123456BufferedWriter bw = null;File f = new File(path);bw = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(f),&quot;gbk&quot;));bw.write(JSON.toJSONString(lookalikeNode));bw.write(&quot;\n&quot;);bw.close(); 日期日期和字符串互转字符串转日期 123//2017-05-26-16-40-13SimpleDateFormat simpleDateFormat = new SimpleDateFormat(&quot;yyyy-MM-dd-HH-mm-ss&quot;);Date date = simpleDateFormat.parse(xmoEvent.getOpxdate()); 日期输出为字符串 12simpleDateFormat = new SimpleDateFormat(&quot;yyyyMMdd&quot;);System.out.println(simpleDateFormat.format(date)); 当前日期 12345678System.currentTimeMillis()或者Date date = new Date(System.currentTimeMillis());字符串表示SimpleDateFormat simpleDateFormat = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);Date date = new Date(System.currentTimeMillis());System.out.println(simpleDateFormat.format(date)); 两个日期之间的天数和周数http://www.jb51.net/article/100441.htm 123456789Calendar end = Calendar.getInstance();//取日期是一年中第几天d1 = end.get(Calendar.DAY_OF_YEAR)//两个日期相隔几周(d2-d1)/7//星期中相隔几天Date转为Calendar 日期加减12345678910111213Calendar calendar = Calendar.getInstance();//此时打印它获取的是系统当前时间 calendar.add(Calendar.DATE, -1); //得到前一天String yestedayDate= new SimpleDateFormat("yyyy-MM-dd").format(cal.getTime()); calendar.add(Calendar.MONTH, -1); //得到前一个月 int year = calendar.get(Calendar.YEAR); int month = calendar.get(Calendar.MONTH)+1; //输出前一月的时候要记得加1 String Date Calendar之间的转换12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970711.Calendar 转化 StringCalendar calendat = Calendar.getInstance();SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd");String dateStr = sdf.format(calendar.getTime()); 2.String 转化CalendarString str="2012-5-27";SimpleDateFormat sdf= new SimpleDateFormat("yyyy-MM-dd");Date date =sdf.parse(str);Calendar calendar = Calendar.getInstance();calendar.setTime(date); 3.Date 转化StringSimpleDateFormat sdf= new SimpleDateFormat("yyyy-MM-dd");String dateStr=sdf.format(new Date()); 4.String 转化DateString str="2012-5-27";SimpleDateFormat sdf= new SimpleDateFormat("yyyy-MM-dd");Date date= sdf.parse(str); 5.Date 转化CalendarCalendar calendar = Calendar.getInstance();calendar.setTime(new java.util.Date()); 6.Calendar转化DateCalendar calendar = Calendar.getInstance();java.util.Date date =calendar.getTime(); 7.String 转成 TimestampTimestamp ts = Timestamp.valueOf("2012-1-14 08:11:00"); 8.Date 转 TimeStampSimpleDateFormat df = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");String time = df.format(new Date());Timestamp ts = Timestamp.valueOf(time); 读写json常规解析对于某个json 1&#123;"classic_payload":&#123;"opxvrsn":"ut","opxuid":"0","opxclientid":"1127","opxcounter":"1","rnum":"4208937089424580.5","re":"http%3A%2F%2Fwww.liba.com%2Findex.shtml","opxpid":"201705221928320003451270013251741","opxsid":"2ebfa12ef6fe88985d2c1b04ea8d22df","opxdate":"2017-05-26-16-40-13","opxip":"120.92.161.27","opxleadsite":"http://www.liba.com/index.shtml","opxreferringsite":"http%3A%2F%2Fshuaxinfuwu.nipponpaint.com.cn%2Findex.aspx%3Futm_medium%3DVM%26utm_source%3DLB","useragent":"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.101 Safari/537.36","language":"zh-CN,zh;q=0.8,zh-TW;q=0.6,en-US;q=0.4,en;q=0.2","accept":"*/*","connection":"close","encoding":"gzip, deflate","hversion":"HTTP/1.0","frompaidsearch":0,"hascookie":0,"opxeventid":"13057"&#125;,"machine":"10.11.10.12","uuid":"318553958122302938064698429658924956664"&#125; 123456789101112131415//取其中某个字段JSONObject jsonObj = JSONObject.parseObject(content);String uuid = jsonObj.getString(&quot;uuid&quot;);//如果是数组JSONArray array = json.getJSONArray(&quot;result&quot;); List&lt;Keyword&gt; keywords = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; array.size(); i++) &#123; JSONObject jo = array.getJSONObject(i); assertEquals(topic.getId(), jo.getInteger(&quot;topicId&quot;)); keywords.add(new Keyword(topic, jo.getInteger(&quot;id&quot;), jo.getString(&quot;keyword&quot;), null, jo.getJSONArray(&quot;spiderTopics&quot;), jo.getString(&quot;updatedAt&quot;), jo.getString(&quot;createdAt&quot;))); &#125; //如果是某个类 解析JSONArray123456789101112List&lt;IP&gt; iplist = new ArrayList&lt;IP&gt;();iplist.add(new IP("1.4.255.255"));iplist.add(new IP("1.0.255.255"));iplist.add(new IP("1.2.255.255"));String ipliststr = JSON.toJSONString(iplist);List&lt;IP&gt; ipl = new ArrayList&lt;IP&gt;(JSONArray.parseArray(ipliststr,IP.class));for (IP ip : ipl) &#123; System.out.println(ip.getIp_address());&#125; 解析的类中有嵌套的ArrayList&lt;类&gt;，可以直接解析，但对ArrayList的命名有要求，例如 1private ArrayList&lt;KeywordItem&gt; keywordItems; 解析为复杂集合1234567TypeReference&lt;HashMap&lt;String, ArrayList&lt;WebPageInfo&gt;&gt;&gt; ty = new TypeReference&lt;HashMap&lt;String, ArrayList&lt;WebPageInfo&gt;&gt;&gt;()&#123;&#125;; try &#123; d2wps = JSON.parseObject(info,ty); &#125; catch (Exception e) &#123; continue; &#125; 数据类型比较两个double是否相等123long thisBits = Double.doubleToLongBits(d1); long anotherBits = Double.doubleToLongBits(d2); return (thisBits == anotherBits ? 0 : // Values are equal 正则1234567//匹配的是括号中的内容，匹配时，先matcher.find()，然后找group，group（0）是本身。group（1）是第一个括号内容//这里匹配的是可能包括&amp;，然后有q=，括号是提取的内容，用懒惰模式。最后要么是结尾，要么是&amp;，Pattern pattern = Pattern.compile("&amp;?(q|p|wd|word|query)=(.*?)(&amp;|$)");Matcher matcher = pattern.matcher(url.getQuery());while (matcher.find()) &#123; System.out.println(matcher.group(1));&#125; String转char1char[] chars = word.toCharArray(); hdfs读数据 123FSDataInputStream dis = null;FileSystem fs = FileSystem.get(HDFSHandler.conf);dis = fs.open(new Path(modelFile)); 集合ArrayList拼接成String12ArrayList&lt;String&gt; wordSet = new ArrayList&lt;String&gt;();StringUtils.join(wordSet, " "); ArrayList一句话初始化12345678910111213ArrayList&lt;String&gt; list = new ArrayList&lt;String&gt;() &#123;&#123; add("A"); add("B"); add("C");&#125;&#125;List&lt;String&gt; places = Arrays.asList("Buenos Aires", "Córdoba", "La Plata");// 初始化一个 List，而不是 ArrayListList&lt;String&gt; list = ["A", "B", "C"];// 初始化的更好看些List&lt;Long&gt; cptIds = Lists.newArrayList(); 优先级队列java中PriorityQueue优先级队列使用方法 优先级队列是不同于先进先出队列的另一种队列。每次从队列中取出的是具有最高优先权的元素。如果不提供Comparator的话，优先队列中元素默认按自然顺序排列，也就是数字默认是小的在队列头，字符串则按字典序排列。 如果想实现按照自己的意愿进行优先级排列的队列的话，需要实现Comparator接口。下面的方法，实现了根据某个变量，来进行优先级队列的建立。 1234567891011Comparator&lt;test&gt; OrderIsdn = new Comparator&lt;test&gt;()&#123; public int compare(test o1, test o2) &#123; // TODO Auto-generated method stub int numbera = o1.getPopulation(); int numberb = o2.getPopulation(); if(numberb &gt; numbera) return 1; else if(numberb&lt;numbera) return -1; else return 0; &#125; &#125;; Queue&lt;test&gt; priorityQueue = new PriorityQueue&lt;test&gt;(11,OrderIsdn); TreeSetTreeSet中的对象要实现comparable 1234567891011121314151617181920212223public class KeywordItem implements Comparable&lt;KeywordItem&gt; &#123; public String name; public Double score; public KeywordItem(String name, Double score) &#123; this.name = name; this.score = score; &#125; @Override public String toString() &#123; return this.name + &quot;\t&quot; + score; &#125; @Override public int compareTo(KeywordItem o) &#123; if (this.score &lt; o.score) &#123; return 1; &#125; else &#123; return -1; &#125; &#125;&#125; 插入的时候自动比较 1234567TreeSet&lt;KeywordItem&gt; looklikeCodeTree = new TreeSet&lt;KeywordItem&gt;();if (entry.getValue() &gt; this.minLookLike) &#123; looklikeCodeTree.add(new KeywordItem(entry.getKey().toString(), entry.getValue())); if (looklikeCodeTree.size() &gt; maxLookLikeTagCnt) &#123; looklikeCodeTree.pollLast(); &#125; &#125; 随机数12345678import java.util.Random;private Random random = new Random();//随机打印一些日志if (random.nextDouble() &lt; 0.00001 ) &#123; logger.info(JSON.toJSONString(obj));&#125; 二维数组1int[][] arr = new int[3][5]; URLDecoder Illegal hex characters in escape (%) pattern - For input string: “u0” Incomplete trailing escape (%) pattern 1searchString=URLDecoder.decode(searchString.replaceAll(&quot;%&quot;, &quot;%25&quot;),&quot;utf-8&quot;); List对象的去重123456789101112def dedupBy[T](list: List[T])(f: T =&gt; String): List[T] = &#123; val buf = new ListBuffer[T] val set = new HashSet[String] list foreach &#123; e =&gt; val key = f(e) if (!set.contains(key)) &#123; buf += e set += key &#125; &#125; buf.result &#125; 并发CountDownLatchBlockingQueue两者的使用参考InterestTimerTask.java http://www.importnew.com/15731.html Redis锁setnx是「SET if Not eXists」的缩写，可以用来实现锁。如果setnx存在，则执行更新，更新后删除这个key。在锁的时候，其他对redis的访问要么等待，要么用过期的缓存 注意要加上锁的过期时间，防止请求意外退出而锁得不到释放。 检测语言是否为繁体 1234String simpleText = TraditionalChineseDictionary.convertToSimplifiedChinese(text);if (!simpleText.equals(text)) &#123; lang = &quot;zh-hk&quot;; &#125; 是否为英文 123456789public static boolean isEnglish(String str) &#123; String regEx = &quot;[a-zA-Z]+&quot;; Pattern p = Pattern.compile(regEx); Matcher m = p.matcher(str); if (m.find()) &#123; return true; &#125; return false; &#125; 是否为中文 123456789public static boolean isChinese(String str) &#123; String regEx = &quot;[\\u4e00-\\u9fa5]+&quot;; Pattern p = Pattern.compile(regEx); Matcher m = p.matcher(str); if (m.find()) &#123; return true; &#125; return false; &#125; 枚举定义一个枚举类 12345678910111213public enum TypeGroup &#123; CN(&quot;cn&quot;), EN(&quot;en&quot;), CN_EN(&quot;cn_en&quot;),BR(&quot;br&quot;),ALL(&quot;all&quot;); private final String type; private TypeGroup(String type) &#123; this.type = type; &#125; public String getType() &#123; return type; &#125;&#125; jsoup获取url的html123String url = &quot;http://www.jb51.net/softjc/414254.html&quot;;String html = Jsoup.connect(url).userAgent(&quot;Mozilla&quot;).get().toString();System.out.println(html); get请求1234567891011121314151617181920212223242526272829303132333435363738394041/** * 向指定URL发送GET方法的请求 * */ public static String get(String url) &#123; BufferedReader in = null; try &#123; URL realUrl = new URL(url); // 打开和URL之间的连接 URLConnection connection = realUrl.openConnection(); // 设置通用的请求属性 connection.setRequestProperty(&quot;accept&quot;, &quot;*/*&quot;); connection.setRequestProperty(&quot;connection&quot;, &quot;Keep-Alive&quot;); connection.setRequestProperty(&quot;user-agent&quot;, &quot;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1;SV1)&quot;); connection.setConnectTimeout(5000); connection.setReadTimeout(5000); // 建立实际的连接 connection.connect(); // 定义 BufferedReader输入流来读取URL的响应 in = new BufferedReader(new InputStreamReader(connection.getInputStream())); StringBuffer sb = new StringBuffer(); String line; while ((line = in.readLine()) != null) &#123; sb.append(line); &#125; return sb.toString(); &#125; catch (Exception e) &#123; LOG.error(&quot;Exception occur when send http get request!&quot;, e); &#125; // 使用finally块来关闭输入流 finally &#123; try &#123; if (in != null) &#123; in.close(); &#125; &#125; catch (Exception e2) &#123; e2.printStackTrace(); &#125; &#125; return null; &#125; list排序12static List&lt;Integer&gt; intList = Arrays.asList(2, 3, 1);Collections.sort(intList); 结果默认是正序排序 11,2,3 如何实现逆序呢，这就要使用第二种方式了，即通过实现Comparator接口的compare方法来完成自定义排序，代码如下： 12345678Collections.sort(intList,new Comparator&lt;Integer&gt;() &#123; @Override public int compare(Integer o1, Integer o2) &#123; // 返回值为int类型，大于0表示正序，小于0表示逆序 return o2-o1; &#125; &#125;); 随机打乱1Collections.shuffle 多线程最简单的多线程如果要测试多线程 1234567891011121314151617181920212223class MyThread extends Thread &#123; public MyThread(String name) &#123; super(name); &#125; public void run() &#123; for (int i = 0; i &lt; 20; i++) &#123; System.out.println(Thread.currentThread().getName()); try &#123; Thread.sleep(1000l); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125;public static void main(String[] args) &#123; MyThread myThread1 = new MyThread(&quot;myThread1&quot;); MyThread myThread2 = new MyThread(&quot;myThread2&quot;); myThread1.start(); myThread2.start(); &#125; 能看到两个线程交替打印 Atomic保证在高并发的情况下只有一个线程能够访问这个属性值。 AtomicBoolean使用 AtomicBoolean 高效并发处理 “只初始化一次” 的功能要求： 12345678private static AtomicBoolean initialized = new AtomicBoolean(false);public void init()&#123; if( initialized.compareAndSet(false, true) ) &#123; // 这里放置初始化代码.... &#125;&#125; 多线程下的类/geo-poi/src/main/java/com/iclick/geo/search/GeoPois.java 12345678private static ConcurrentHashMap&lt;String, GeoPois&gt; mRegistry = new ConcurrentHashMap&lt;String, GeoPois&gt;();public static synchronized GeoPois getInstance(String cityname) &#123; if (mRegistry.get(cityname) == null) &#123; mRegistry.put(cityname, new GeoPois(cityname)); &#125; return mRegistry.get(cityname); &#125; jar包替换文件1jar -uvf pig-ext-lite-1.0-SNAPSHOT.jar sensitive_words.txt 这里的sensitive_words.txt会放到jar包的根目录，若存在则会替换 执行java任务 Double判断相等1if (new java.math.BigDecimal(distance).compareTo(new java.math.BigDecimal(-1.0)) == 0) POJO命名规则https://www.cnblogs.com/yxnchinahlj/archive/2012/02/24/2366110.html POJO是DO/DTO/BO/VO的统称。 POJO：POJO（Plain Ordinary Java Object）简单的Java对象，实际就是普通JavaBeans，是为了避免和EJB混淆所创造的简称。通指没有使用Entity Beans的普通java对象，可以把POJO作为支持业务逻辑的协助类。 POJO实质上可以理解为简单的实体类，顾名思义POJO类的作用是方便程序员使用数据库中的数据表，对于广大的程序员，可以很方便的将POJO类当做对象来进行使用，当然也是可以方便的调用其get,set方法。POJO类也给我们在struts框架中的配置带来了很大的方便。 一个POJO持久化以后就是PO 直接用它传递、传递过程中就是DTO直接用来对应表示层就是VO java如何计算程序运行时间1234567long startTime = System.currentTimeMillis(); //获取开始时间doSomething(); //测试的代码段long endTime = System.currentTimeMillis(); //获取结束时间System.out.println(&quot;程序运行时间：&quot; + (endTime - startTime) + &quot;ms&quot;); //输出程序运行时间 用泛型来解析jsonhttps://www.cnblogs.com/jpfss/p/9928747.html 123456789public static &lt;T&gt; T parseJson(String json, Class&lt;T&gt; object) &#123; T t = null; try &#123; t = JacksonUtil.getObjectMapper().readValue(json, object); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return t; &#125; jackson解析json12345678910111213141516171819202122public class JacksonUtil &#123; public ObjectMapper getObjectMapper() &#123; ObjectMapper objectMapper = new ObjectMapper(); //序列化的时候序列对象的所有属性 objectMapper.setSerializationInclusion(JsonInclude.Include.ALWAYS); //反序列化的时候如果多了其他属性,不抛出异常 objectMapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false); //如果是空对象的时候,不抛异常 objectMapper.configure(SerializationFeature.FAIL_ON_EMPTY_BEANS, false); return objectMapper; &#125; public &lt;T&gt; T parseJson(String json, Class&lt;T&gt; object) &#123; T t = null; try &#123; t = getObjectMapper().readValue(json, object); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return t; &#125;&#125; 反射和泛型123456// 获取方法 scala语法gdav.app_code = clazz.getDeclaredMethod(&quot;getAppCode&quot;).invoke(x).asInstanceOf[String]// java语法// getDeclaredMethod，第一个参数是方法名，后面是形参；invoke第一个是类的实例，第二个实参ResponseResult ret = (ResponseResult)AnalysisController.class.getDeclaredMethod(methodName, SearchStopAreaVO.class).invoke(analysisController, searchStopAreaVO);]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[经纬度判断城市]]></title>
    <url>%2F2018%2F03%2F31%2F%E5%9C%B0%E7%90%86%E4%BF%A1%E6%81%AF%E6%8C%96%E6%8E%98%2F%E7%BB%8F%E7%BA%AC%E5%BA%A6%E5%88%A4%E6%96%AD%E5%9F%8E%E5%B8%82%2F</url>
    <content type="text"><![CDATA[1/geo-poi/src/main/java/com/iclick/geo/search/Geolocation.java 有数据district_v2.tar.gz 包括每个行政区划的边界经纬度和中心经纬度 district_v2的结构爬去的高德的数据 每行对应一个城市的区县 citycode 城市代码 adcode是全国行政区划代码 code编码数据来源于国家标准行政区划代码（adcode）映射表《》 解析数据，将每个district的边界坐标保存到districts 。对于一个经纬度，遍历每个district，判断点是否在多边形内，找到所属行政区划。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566/** * 判断目标点是否在多边形内(由多个点组成)&lt;br/&gt; * * @param px 目标点的经度坐标 * @param py 目标点的纬度坐标 * @param polygonXA 多边形的经度坐标集合 * @param polygonYA 多边形的纬度坐标集合 * @return */ public static boolean isPointInPolygon ( double px , double py , ArrayList&lt;Double&gt; polygonXA , ArrayList&lt;Double&gt; polygonYA ) &#123; boolean isInside = false; double ESP = 1e-9; int count = 0; double linePoint1x; double linePoint1y; double linePoint2x = 180; double linePoint2y; linePoint1x = px; linePoint1y = py; linePoint2y = py; for (int i = 0; i &lt; polygonXA.size() - 1; i++) &#123; double cx1 = polygonXA.get(i); double cy1 = polygonYA.get(i); double cx2 = polygonXA.get(i + 1); double cy2 = polygonYA.get(i + 1); //如果目标点在任何一条线上 if ( isPointOnLine(px, py, cx1, cy1, cx2, cy2) ) &#123; return true; &#125; //如果线段的长度无限小(趋于零)那么这两点实际是重合的，不足以构成一条线段 if ( Math.abs(cy2 - cy1) &lt; ESP ) &#123; continue; &#125; //第一个点是否在以目标点为基础衍生的平行纬度线 if ( isPointOnLine(cx1, cy1, linePoint1x, linePoint1y, linePoint2x, linePoint2y) ) &#123; //第二个点在第一个的下方,靠近赤道纬度为零(最小纬度) if ( cy1 &gt; cy2 ) count++; &#125; //第二个点是否在以目标点为基础衍生的平行纬度线 else if ( isPointOnLine(cx2, cy2, linePoint1x, linePoint1y, linePoint2x, linePoint2y) ) &#123; //第二个点在第一个的上方,靠近极点(南极或北极)纬度为90(最大纬度) if ( cy2 &gt; cy1 ) count++; &#125; //由两点组成的线段是否和以目标点为基础衍生的平行纬度线相交 else if ( isIntersect(cx1, cy1, cx2, cy2, linePoint1x, linePoint1y, linePoint2x, linePoint2y) ) &#123; count++; &#125; &#125; if ( count % 2 == 1 ) &#123; isInside = true; &#125; return isInside; &#125; 也可以用geojson配合qgis的api，geojson文件为BOUNT_poly.geojson]]></content>
      <categories>
        <category>地理信息处理和挖掘</category>
      </categories>
      <tags>
        <tag>自用代码记录</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据仓库框架对比]]></title>
    <url>%2F2018%2F03%2F31%2Fhadoop-spark%2F%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%A1%86%E6%9E%B6%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[presto、druid、sparkSQL、kylin的对比分析，如性能、架构等，有什么异同？ 商业系统 InfoBright Greenplum（已开源）、HP Vertica、TeraData、Palo、ExaData、RedShift、BigQuery（Dremel） 开源实现 Impala、Presto、Spark SQL、Drill、Hawq Druid、Pinot Kylin presto、druid、sparkSQL、kylin可以分为三类。 presto和spark sql都是解决分布式查询问题，提供SQL查询能力，但数据加载不一定能保证实时。 Druid是保证数据实时写入，但查询上不支持SQL，或者说目前只支持部分SQL，我个人觉得适合用于工业大数据，比如一堆传感器实时写数据的场景。 Kylin是MOLAP，就是将数据先进行预聚合，然后把多维查询变成了key-value查询。基本思路是预先对数据作多维索引，查询时只扫描索引而不访问原始数据从而提速。 大数据查询目前来讲可以大体分为三类： 1、基于hbase预聚合的。适合相对固定的业务报表类需求。需要指定预聚合的指标，在数据接入的时候根据指定的指标进行聚合运算，只需要统计少量维度即可满足业务报表需求。比如Opentsdb,Kylin,Druid等 2、基于Parquet列式存储的，基本是完全基于内存的并行计算，Parquet系能降低存储空间，提高IO效率，以离线处理为主，很难提高数据写的实时性，超大表的join支持可能不够好。spark sql也算类似，但它在内存不足时可以spill disk来支持超大数据查询和join。比如Presto, Drill，Impala等 3、基于lucene外部索引的，比如ElasticSearch和Solr,能够满足的的查询场景远多于传统的数据库存储，但对于日志、行为类时序数据，所有的搜索请求都也必须搜索所有的分片，另外，对于聚合分析场景的支持也是软肋]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python技巧总结]]></title>
    <url>%2F2018%2F03%2F31%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FPYTHON%2Fpython%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[json操作1.1 json转字符串1result = json.loads(s) 1.2 遍历json的key将json当做dict，用dict的方法遍历 12for k,v in result.items(): print result[k] 1.3 读json文件1234567import jsonf = open(&quot;/home/david/keywordjson&quot;)result = json.load(f,encoding=&apos;utf-8&apos;)for k,v in result.items(): print result[k] 1.4 报错1Python json.loads shows ValueError: Extra data https://stackoverflow.com/questions/21058935/python-json-loads-shows-valueerror-extra-data 1.5 判断是否包含key12345jsonObject 是个jsonif (key in jsonObject) : print &apos;有&apos;else: print &apos;没有&apos; 文件操作2.1 读CSV文件csv文件的格式如 12id,click,hour,C1,C2,C3,C4,C5123,1,14091123,a,b,c,d,e 读取的方式为： 12345from csv import DictReader# t是每行的index，row是每行的具体数据，dict型for t, row in enumerate(DictReader(open(path))): ID = row[&apos;id&apos;] 2.2 按行读普通文件12345678910for line in open('../result/ctrout-featurelist-1-10.dat'): #要去掉换行符 arr = line.strip('\n').split(',') if len(arr) &gt; 0: try: prectr.append(float(arr[0])) onlinectr.append(float(arr[1])) isclick.append(1 if arr[2]=='true' else 0) except ValueError: continue 上述方法不够严谨，为防止读取时出错，应该用 1234with codecs.open(fname, &quot;r&quot;) as f: for line in f.readlines(): # 按行读是带换行符的，要用strip去掉 data = line.strip(&apos;\n&apos;).split() 读文件转成二维数组12345678910def read(file): '''read raw date from a file ''' Instances = [] fp = open(file, 'r') for line in fp: line = line.strip('\n') # discard '\n' if line != '': Instances.append(line.split(',')) fp.close() return (Instances) 写文件12345678910filename = &apos;write_data.txt&apos;with open(filename,&apos;w&apos;) as f: # 如果filename不存在会自动创建， &apos;w&apos;表示写数据，写之前会清空文件中的原有数据！ f.write(&quot;I am Meringue.\n&quot;) f.write(&quot;I am now studying in NJTECH.\n&quot;) # orf = open(&apos;test.txt&apos;, &apos;w&apos;) # 若是&apos;wb&apos;就表示写二进制文件f.write(&apos;Hello, world!&apos;)f.close() 判断文件是否存在123456import osos.path.exists(test_file.txt)#Trueos.path.exists(no_exist_file.txt)#False 内置函数3.1 hash返回对象的hash值，返回的哈希值是使用一个整数表示，通常使用在字典里，以便实现快速查询键值。参数object输入是数字类型时，是根据数值来计算的，比如1和1.0计算出来是一样的哈希值，因此说这个函数是不区分不同的数值类型。 日期操作123456789101112131415161718192021222324252627282930313233343536373839404142import datetimedstr = '20170228'd2 = datetime.datetime.strptime(dstr,'%Y%m%d') + datetime.timedelta(days=1)d2str = d2.strftime('%Y%m%d')# 结果是20170301# 获取当前日期并格式化import timetime.strftime('%Y%m%d',time.localtime(time.time()))time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time()))# 如果要获取datetime类型的当前时间datetime.datetime.now()# 日期加减#datetime.timedelta(days, seconds, microseconds)d1 = datetime.date.today()# 加一天：d2 = d1 + datetime.timedelta(1)# 減一天：d2 = d1 + datetime.timedelta(-1)# 当天日期的格式化datetime.date.today().strftime('%Y%m%d')# 字符串转日期import datetimedetester = ‘2017-01-01'date = datetime.datetime.strptime(detester,’%Y-%m-%d') # 日期转字符串import datetime# python3的date = datetime.now()detester = date.strftime(‘%Y-%m-%d') # 比较两个时间之差nowtime=datetime.datetime.today()(nowtime-gpstime_datetime).seconds 字符串字符串拼接1234runToday=date.today().strftime(&apos;%y-%m-%d&apos;)b=time.strptime(runToday,&apos;%y-%m-%d&apos;)btime = datetime(*b[:3])&quot;,&quot;.join([&quot;/shortdata/persona/xid_present_info/&quot; + (btime - timedelta(x)).strftime(&apos;%y-%m-%d&apos;) for x in range(2,13)]) 1&apos;/shortdata/persona/xid_present_info/17-05-08,/shortdata/persona/xid_present_info/17-05-07,/shortdata/persona/xid_present_info/17-05-06,/shortdata/persona/xid_present_info/17-05-05,/shortdata/persona/xid_present_info/17-05-04,/shortdata/persona/xid_present_info/17-05-03,/shortdata/persona/xid_present_info/17-05-02,/shortdata/persona/xid_present_info/17-05-01,/shortdata/persona/xid_present_info/17-04-30,/shortdata/persona/xid_present_info/17-04-29,/shortdata/persona/xid_present_info/17-04-28&apos; 字符串定位1234567891011121314151617181920212223242526272829303132333435str_1='wo shi yi zhi da da niu 'char_1='i'nPos=str_1.index(char_1)print(nPos)运行结果：7========是使用find==========str_1='wo shi yi zhi da da niu 'char_1='i'nPos=str_1.find(char_1)print(nPos)结果：5========如何查找所有‘i’在字符串中位置呢？===========#开挂模式str_1='wo shi yi zhi da da niu 'char_1=str(input('Please input the Char you want:'))count=0str_list=list(str_1)for each_char in str_list: count+=1 if each_char==char_1: print(each_char,count-1) 运行结果：Please input the Char you want:ii 0i 1i 2i 3 数据类型除法后转float12# 加个点即可scale = state.shape[1] / 720. 除法后保留intpython的int做除法后会变float，难道python3才有的特性？ 12 循环12345678documents = ["Human machine interface for lab abc computer applications", "A survey of user opinion of computer system response time", "The EPS user interface management system", "System and human system engineering testing of EPS"]stoplist = set('for a of the and to in'.split())texts = [[word for word in document.lower().split() if word not in stoplist] for document in documents] hdfs操作https://snakebite.readthedocs.io/en/latest/client.html 代码规范定义类123456class model_evaluation(object): def __init__(self, prectrArr, onlinectrArr, isclickArr): self.prectrArr = prectrArr self.onlinectrArr = onlinectrArr self.isclickArr = isclickArr main函数123456def main(argv=None): if argv == None: argv = sys.argv if __name__ == &quot;__main__&quot;: main() nohup执行nohup python -u download_bing_api.py &gt;&gt; result.log &amp; 可以输出print的内容 max初级技巧 12tmp = max(1,2,4)print(tmp) 1234#可迭代对象a = [1, 2, 3, 4, 5, 6]tmp = max(a)print(tmp) 中级技巧：key属性的使用 当key参数不为空时，就以key的函数对象为判断的标准。如果我们想找出一组数中绝对值最大的数，就可以配合lamda先进行处理，再找出最大值 123a = [-9, -8, 1, 3, -4, 6]tmp = max(a, key=lambda x: abs(x))print(tmp) 高级技巧：找出字典中值最大的那组数据 如果有一组商品，其名称和价格都存在一个字典中，可以用下面的方法快速找到价格最贵的那组商品： 12345678910prices = &#123; &apos;A&apos;:123, &apos;B&apos;:450.1, &apos;C&apos;:12, &apos;E&apos;:444,&#125;# 在对字典进行数据操作的时候，默认只会处理key，而不是value# 先使用zip把字典的keys和values翻转过来，再用max取出值最大的那组数据max_prices = max(zip(prices.values(), prices.keys()))print(max_prices) # (450.1, &apos;B&apos;) 画图画圆 12345678import numpy as np import matplotlib.pyplot as plt from matplotlib.patches import Polygon import matplotlib.patches as mpatches for i in range(0,r,600): plt.plot(np.cos(t)*i, np.sin(t)*i)plt.show() 去掉坐标轴 123ax=plt.subplot(111)ax.set_xticks([]) ax.set_yticks([]) 直方图教你利用Python玩转histogram直方图的五种方法 进程subprocess运行python的时候，我们都是在创建并运行一个进程。像Linux进程那样，一个进程可以fork一个子进程，并让这个子进程exec另外一个程序。在Python中，我们通过标准库中的subprocess包来fork一个子进程，并运行一个外部的程序。 如 1cat = subprocess.Popen([&quot;hadoop&quot;, &quot;fs&quot;, &quot;-du&quot;, abspath], stdout=subprocess.PIPE) Popen对象创建后，主程序不会自动等待子进程完成。我们必须调用对象的wait()方法，父进程才会等待 (也就是阻塞block)，举例： 123&gt;&gt;&gt; import subprocess&gt;&gt;&gt; child = subprocess.Popen([&apos;ping&apos;,&apos;-c&apos;,&apos;4&apos;,&apos;blog.linuxeye.com&apos;])&gt;&gt;&gt; print &apos;parent process&apos; 从运行结果中看到，父进程在开启子进程之后并没有等待child的完成，而是直接运行print。对比等待的情况: 1234&gt;&gt;&gt; import subprocess&gt;&gt;&gt; child = subprocess.Popen(&apos;ping -c4 blog.linuxeye.com&apos;,shell=True)&gt;&gt;&gt; child.wait()&gt;&gt;&gt; print &apos;parent process&apos; 从运行结果中看到，父进程在开启子进程之后并等待child的完成后，再运行print。此外，你还可以在父进程中对子进程进行其它操作，比如我们上面例子中的child对象: 1234child.poll() # 检查子进程状态child.kill() # 终止子进程child.send_signal() # 向子进程发送信号child.terminate() # 终止子进程 子进程的PID存储在child.pid，子进程的标准输入、标准输出和标准错误如下属性分别表示: 123child.stdinchild.stdoutchild.stderr 例如lbspoi.py 123456789101112131415161718192021def generate_poi_content(self): city_list = [] cmd_out = subprocess.Popen( " hadoop fs -ls /shortdata/mobile_poi_raw/day_id=%s|awk -F'city=' 'NF==2&#123;print $2&#125;'" % (self.today), stdout=subprocess.PIPE, shell=True) for line in cmd_out.stdout: line = line.strip() if len(line) &gt; 0: city_list.append(line) for city_code in city_list: sql = "pig -p MOBILE_POI_RAW=/shortdata/mobile_poi_raw/day_id=%s/city=%s " \ "-p CITY_CODE=%s -p MOBILE_POI_INFO=/shortdata/persona/mobile_poi/%s/%s " \ "/opt/pig_home/mobile_poi/GeneratePoiContent.pig" % ( self.today, city_code, city_code, self.today, city_code) taskCmd(sql) sql = "alter table mobile_poi " \ "add partition (day_id='%s',city_code='%s') " \ "location '/shortdata/persona/mobile_poi/%s/%s';" % (self.today, city_code, self.today, city_code) cmd = 'hive -e "use persona; %s"' % (sql) taskCmd(cmd) Python模块安装方法一、方法1： 单文件模块直接把文件拷贝到 $python_dir/Lib 二、方法2： 多文件模块，带setup.py 下载模块包，进行解压，进入模块文件夹，执行：python setup.py install 三、 方法3：easy_install 方式 先下载ez_setup.py,运行python ez_setup 进行easy_install工具的安装，之后就可以使用easy_install进行安装package了。easy_install packageNameeasy_install package.egg 四、 方法4：pip 方式 先进行pip工具的安裝：easy_install pip（pip 可以通过easy_install 安裝，而且也会装到 Scripts 文件夹下。） 安裝：pip install PackageName 更新：pip install -U PackageName 移除：pip uninstall PackageName 搜索：pip search PackageName 帮助：pip help 输出日志123456789101112131415161718192021logger = logging.getLogger("eventToKafka")logger.setLevel(logging.DEBUG)# 建立一个filehandler来把日志记录在文件里，级别为debug以上fh = logging.FileHandler("/home/hdbatch/fh.log")fh.setLevel(logging.DEBUG)# 建立一个streamhandler来把日志打在CMD窗口上，级别为error以上ch = logging.StreamHandler()ch.setLevel(logginng.ERROR)# 设置日志格式formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")ch.setFormatter(formatter)fh.setFormatter(formatter)#将相应的handler添加在logger对象中logger.addHandler(ch)logger.addHandler(fh)# 开始打日志logger.debug("debug message")logger.info("info message")logger.warn("warn message")logger.error("error message")logger.critical("critical message") 输出到excel1234567891011121314151617181920212223242526import xlwtfrom datetime import datetime style0 = xlwt.easyxf(&apos;font: name Times New Roman, color-index red, bold on&apos;,num_format_str=&apos;#,##0.00&apos;)style1 = xlwt.easyxf(num_format_str=&apos;D-MMM-YY&apos;) wb = xlwt.Workbook(encoding=&apos;utf-8&apos;)ws = wb.add_sheet(&apos;A Test Sheet&apos;)ln = 1for line in open(&apos;/home/david/taxo.txt&apos;): arr = line.strip(&apos;\n&apos;).split(&apos;|&apos;) if len(arr) &gt; 0: try: print arr[0] ws.write(ln,0,arr[0]) print arr[1] ws.write(ln,1,arr[1]) print arr[2] ws.write(ln,2,arr[2]) ln += 1 except ValueError: print ValueError.message continue wb.save(&apos;/home/david/example.xls&apos;) 用nohup执行python程序时，print无法输出123456789nohup python test.py &gt; nohup.out 2&gt;&amp;1 &amp;发现nohup.out中显示不出来python程序中print的东西。这是因为python的输出有缓冲，导致nohup.out并不能够马上看到输出。python 有个-u参数，使得python不启用缓冲。nohup python -u test.py &gt; nohup.out 2&gt;&amp;1 &amp; latin1转utf8很多语料库下载后都是latin1编码，是乱码 判断变量是否为None123456789三种主要的写法有：第一种：if X is None;第二种：if not X；当X为None, False, 空字符串&quot;&quot;, 0, 空列表[], 空字典&#123;&#125;, 空元组()这些时，not X为真，即无法分辨出他们之间的不同。第三种：if not X is None; 在Python中，None、空列表[]、空字典{}、空元组()、0等一系列代表空和无的对象会被转换成False。除此之外的其它对象都会被转化成True。 在命令if not 1中，1便会转换为bool类型的True。not是逻辑运算符非，not 1则恒为False。因此if语句if not 1之下的语句，永远不会执行。 对比：foo is None 和 foo == None 示例： 123456789&gt;&gt;&gt; class Foo(object): def __eq__(self, other): return True&gt;&gt;&gt; f = Foo()&gt;&gt;&gt; f == NoneTrue&gt;&gt;&gt; f is NoneFalse 输入参数判断12345678import argparseparser = argparse.ArgumentParser()parser.add_argument(&quot;-v&quot;, &quot;--verbosity&quot;, help=&quot;increase output verbosity&quot;)args = parser.parse_args()if args.verbosity: print &quot;verbosity turned on&quot; 一种是通过一个-来指定的短参数，如-h； 一种是通过--来指定的长参数，如--help 这两种方式可以同存，也可以只存在一个。通过解析后，其值保存在args.verbosity变量中用法如下： 1234567891011121314151617yarving@yarving-VirtualBox /tmp $ python prog.py -v 1verbosity turned onyarving@yarving-VirtualBox /tmp $ python prog.py --verbosity 1verbosity turned onyarving@yarving-VirtualBox /tmp $ python prog.py -h usage: prog.py [-h] [-v VERBOSITY]optional arguments: -h, --help show this help message and exit -v VERBOSITY, --verbosity VERBOSITY increase output verbosityyarving@yarving-VirtualBox /tmp $ python prog.py -v usage: prog.py [-h] [-v VERBOSITY]prog.py: error: argument -v/--verbosity: expected one argument 或者这样使用 12345678910parser = argparse.ArgumentParser() parser.add_argument('--phone', default='Android', choices=['Android', 'IOS'], type=str, help='mobile phone OS') parser.add_argument('--sensitivity', default=2.045, type=float, help='constant for press time') parser.add_argument('--serverURL', default='http://localhost:8100', type=str, help='ServerURL for wda Client') parser.add_argument('--resource', default='resource', type=str, help='resource dir') parser.add_argument('--debug', default=None, type=str, help='debug mode, specify a directory for storing log files.') args = parser.parse_args() # print(args) AI = WechatAutoJump(args.phone, args.sensitivity, args.serverURL, args.debug, args.resource) type定义了输入的数据类型，如果不符合会报错 后面也可以给args增加新的参数 1args.vocab_size=50 搜索指定目录下的文件123import glob# 搜索目录下的所有png文件，返回的是所有路径的列表glob.glob(os.path.join(self.resource_dir, &apos;circle/*.png&apos;)) 复制文件12import shutilshutil.copyfile(&apos;state.png&apos;, os.path.join(self.debug, &apos;state_&#123;:03d&#125;.png&apos;.format(self.step))) codecs普通的open打开会有编码问题，用codecs.open打开后，再写入数据就不会有问题 https://www.cnblogs.com/buptldf/p/4805879.html 这种方法可以指定一个编码打开文件，使用这个方法打开的文件读取返回的将是unicode。写入时，如果参数 是unicode，则使用open()时指定的编码进行编码后写入；如果是str，则先根据源代码文件声明的字符编码，解码成unicode后再进行前述 操作。相对内置的open()来说，这个方法比较不容易在编码上出现问题。 12with codecs.open(input_file, &quot;r&quot;, encoding=self.encoding) as f: data = f.read() 这里的read就是读取整个文本，换行符为\r\n with12with codecs.open(input_file, &quot;r&quot;, encoding=self.encoding) as f: data = f.read() 使用with后不管with中的代码出现什么错误，都会进行对当前对象进行清理工作。 例如file的file.close()方法，无论with中出现任何错误，都会执行file.close（）方法 with语句类似 try : except: finally: 的功能：但是with语句更简洁。而且更安全。代码量更少。 collectionscollections模块自Python 2.4版本开始被引入，包含了dict、set、list、tuple以外的一些特殊的容器类型，分别是： OrderedDict类：排序字典，是字典的子类。引入自2.7。 namedtuple()函数：命名元组，是一个工厂函数。引入自2.6。 Counter类：为hashable对象计数，是字典的子类。引入自2.7。 deque：双向队列。引入自2.4。 defaultdict：使用工厂函数创建字典，使不用考虑缺失的字典键。引入自2.5。 CounterCounter类的目的是用来跟踪值出现的次数。它是一个无序的容器类型，以字典的键值对形式存储，其中元素作为key，其计数作为value。计数值可以是任意的Interger（包括0和负数）。Counter类和其他语言的bags或multisets很相似。 1c = collections.Counter(&apos;sdfsadfag&apos;) Counter({‘a’: 2, ‘s’: 2, ‘d’: 2, ‘f’: 2, ‘g’: 1}) counter对象的常用操作 123456789sum(c.values()) # 所有计数的总数c.clear() # 重置Counter对象，注意不是删除list(c) # 将c中的键转为列表set(c) # 将c中的键转为setdict(c) # 将c中的键值对转为字典c.items() # 转为(elem, cnt)格式的列表Counter(dict(list_of_pairs)) # 从(elem, cnt)格式的列表转换为Counter类对象c.most_common()[:-n:-1] # 取出计数最少的n-1个元素c += Counter() # 移除0和负值 sorted默认是升序 1234567&gt;&gt;&gt; L=[(&apos;b&apos;,2),(&apos;a&apos;,1),(&apos;c&apos;,3),(&apos;d&apos;,4)]&gt;&gt;&gt; sorted(L, cmp=lambda x,y:cmp(x[1],y[1])) # 利用cmp函数[(&apos;a&apos;, 1), (&apos;b&apos;, 2), (&apos;c&apos;, 3), (&apos;d&apos;, 4)]&gt;&gt;&gt; sorted(L, key=lambda x:x[1]) # 利用key[(&apos;a&apos;, 1), (&apos;b&apos;, 2), (&apos;c&apos;, 3), (&apos;d&apos;, 4)]# 变成降序sorted(L, key=lambda x:-x[1]) zip123456789&gt;&gt;&gt; a = [1,2,3]&gt;&gt;&gt; b = [4,5,6]&gt;&gt;&gt; c = [4,5,6,7,8]&gt;&gt;&gt; zipped = zip(a,b) # 打包为元组的列表[(1, 4), (2, 5), (3, 6)]&gt;&gt;&gt; zip(a,c) # 元素个数与最短的列表一致[(1, 4), (2, 5), (3, 6)]&gt;&gt;&gt; zip(*zipped) # 与 zip 相反，可理解为解压，返回二维矩阵式[(1, 2, 3), (4, 5, 6)] zip循环1234567891011121314151617181920212223242526272829303132333435&gt;&gt;&gt; a = [1,2,3,4,5]&gt;&gt;&gt; b = [9,8,7,6,5]&gt;&gt;&gt; length = len(a) if len(a)&lt;len(b) else len(b)&gt;&gt;&gt; length5 &gt;&gt;&gt; c = zip(a,b)&gt;&gt;&gt; c[(1, 9), (2, 8), (3, 7), (4, 6), (5, 5)] &gt;&gt;&gt;zip(*c)[(1,2,3,4,5), (9,8,7,6,5)]&gt;&gt;&gt; d = []&gt;&gt;&gt; for x,y in zip(a,b): d.append(x+y)&gt;&gt;&gt; d[10, 10, 10, 10, 10] 持久化http://blog.csdn.net/bh20077/article/details/6070278 如果希望透明地存储 Python 对象，而不丢失其身份和类型等信息，则需要某种形式的对象序列化：它是一个将任意复杂的对象转成对象的文本或二进制表示的过程。同样，必须能够将对象经过序列化后的形式恢复到原有的对象。在 Python 中，这种序列化过程称为 pickle，可以将对象 pickle 成字符串、磁盘上的文件或者任何类似于文件的对象，也可以将这些字符串、文件或任何类似于文件的对象 unpickle 成原来的对象。 12345678# 写入# 这里的chars是字符的列表with open(vocab_file, 'wb') as f: cPickle.dump(chars, f)# 读取with open(vocab_file, 'rb') as f: self.chars = cPickle.load(f) mapmap() 会根据提供的函数对指定序列做映射。 第一个参数 function 以参数序列中的每一个元素调用 function 函数，返回包含每次 function 函数返回值的新列表。 1234567891011&gt;&gt;&gt;def square(x) : # 计算平方数... return x ** 2... &gt;&gt;&gt; map(square, [1,2,3,4,5]) # 计算列表各个元素的平方[1, 4, 9, 16, 25]&gt;&gt;&gt; map(lambda x: x ** 2, [1, 2, 3, 4, 5]) # 使用 lambda 匿名函数[1, 4, 9, 16, 25] # 提供了两个列表，对相同位置的列表数据进行相加&gt;&gt;&gt; map(lambda x, y: x + y, [1, 3, 5, 7, 9], [2, 4, 6, 8, 10])[3, 7, 11, 15, 19] 记录shell执行的结果如果用os.system执行shell命令，不会保存执行的结果，要保存结果，可以用 12345678city_list = [] cmd_out = subprocess.Popen( " hadoop fs -ls /shortdata/mobile_poi_raw/day_id=%s|awk -F'city=' 'NF==2&#123;print $2&#125;'" % (self.today), stdout=subprocess.PIPE, shell=True) for line in cmd_out.stdout: line = line.strip() if len(line) &gt; 0: city_list.append(line) 其中， shell=True如果 args 是字符串，它将作为命令行字符串通过shell 执行．如果是一个序列， 它的第一个条目将作为命令行字符串，后面的条目作为附加的shell参数。 stdout=subprocess.PIPE如果stdout=PIPE,这个属性是个文件对象，提供子进程的输出，否则它是None 这样cmd_out就可以保存shell的结果 统计程序的执行时间（python3）1234567x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]### VECTORIZED DOT PRODUCT OF VECTORS ###tic = time.process_time()dot = np.dot(x1,x2)toc = time.process_time()print ("dot = " + str(dot) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms") pass如果初始化的时候不需要什么，可以写pass当占位符。 1234class NaiveBayes(object): def __init__(self): pass 基于数组A对数组B排序1234567import numpy as npa = np.array([0,1,3,2,6,4,5])b = np.array([0,1,2,3,4,5,6])# 数据格式为[(0, 0), (1, 1), (2, 3), (3, 2), (4, 5), (5, 6), (6, 4)]zipped=sorted(zip(a, b), key=lambda x:x[0])# 拆分aa,bb = zip(*zipped) Python爬虫模拟登录还没有试成功 123456789101112131415161718192021222324252627282930313233343536373839404142def setup(): browser = webdriver.Safari() return browserdef login(username, password, browser=None): browser.get("http://www.car900.com/login/") pwd_btn = browser.find_element_by_id("userPwd") act_btn = browser.find_element_by_id("userName") submit_btn = browser.find_element_by_id("login") act_btn.send_keys(username) pwd_btn.send_keys(password) submit_btn.send_keys(Keys.ENTER) return browserdef set_sessions(browser): request = requests.Session() headers = &#123; "User-Agent": "Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 " "(KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36" &#125; request.headers.update(headers) cookies = browser.get_cookies() for cookie in cookies: request.cookies.set(cookie['name'], cookie['value']) return request #request = requests.Session()#headers = &#123;# "User-Agent":# "Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 "# "(KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36"#&#125;#request.headers.update(headers)#request.cookies.set("http://www.car900.com/", "sessionId=7957926f-8aa7-40a9-be91-d264500c4659")#browser = login(u"创辉测试","123456", setup())#rq = set_sessions(browser) https://www.jianshu.com/p/cc70e35b47fc 1&apos;chromedriver&apos; executable needs to be in PATH https://blog.csdn.net/tymatlab/article/details/78649727 使用brew安装chromedriver1.brew安装chromedriver 1$ brew install chromedriver 2.安装完成后，再次运行： 12from selenium import webdriverdriver = webdriver.Chrome() python带cookie爬先从chrome获取cookie 1234cookies=&#123;&#125;cookies[&apos;sessionId&apos;]=&apos;7957926f-8aa7-40a9-be91-d264500c4659&apos;response = requests.get(&quot;http://www.car900.com/http/TwoChargeVehicle/QueryMortgagePoint.json?city=%E6%B7%AE%E5%8D%97%E5%B8%82&amp;_=1527554506552&quot;, cookies=cookies)print(response.text) 随机挑选N个列表中的数12345678import random as rdrand_num_list = [1,2,3,4,5]rand_num = rd.choice(rand_num_list)col_names = [&apos;all_avg_speed_nighttime_sd_3days&apos;, &apos;all_distance_in_10km_around_now_unit_address_avg_3days&apos;, &apos;all_distance_in_5km_around_address_avg_3days&apos;]rd.sample(col_names,rand_num) PyHivepandas读取hive并转成dataframe 123456789101112131415from pyhive import hiveimport pandas as pddef getData(): conn = hive.Connection(host=&quot;1.0.1.38&quot;, auth=&quot;CUSTOM&quot;, username=&apos;hive&apos;, password=&quot;pvXxHTsdqrt8&quot;, port=10000, database=&apos;tapro_atg&apos;) df = pd.read_sql(&quot;select * from sales_data_leisure_view&quot;, conn) records = df.head(n=100000) print(records.to_json(orient=&apos;records&apos;))getData();作者：Helen_Cat链接：https://www.jianshu.com/p/cb2b864b4aca來源：简书简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。 https://blog.csdn.net/qq_41664845/article/details/80775319 插入数据 1234cursor.execute(&quot;&quot;&quot;LOAD DATA LOCAL INPATH &apos;%s&apos; INTO TABLE %s&quot;&quot;&quot; % (&apos;aaa.csv&apos;, &apos;carthage_dev.tmp_local_to_hive&apos;))sql = &apos;&apos;&apos; LOAD DATA LOCAL INPATH &apos;/home/infra_pub/david.xu/aaa.csv&apos; OVERWRITE INTO TABLE carthage_dev.tmp_local_to_hive &apos;&apos;&apos;cursor.execute(sql, async=True) 数据问题setup没有权限123python setup.py installerror: [Errno 13] Permission denied: &apos;/Users/david/anaconda3/lib/python3.6/site-packages/easy-install.pth&apos; 12sudo chown ysbecca easy-install.pth chmod +x easy-install.pth python多线程、多进程详细1https://blog.csdn.net/u011734144/article/details/55519272 批量执行shell命令1234567891011import oscmdtxt = &apos;&apos;&apos;hadoop fs -text /user/dmp/yyfq_car_gps/saigerealtime/180910/10/part-r-00000.gz | grep &apos;&quot;gpsTime\&quot;:\&quot;2018-09-08 %s&apos; &gt; 2018-09-08-%srecov;gzip 2018-09-08-%srecov;hadoop fs -put 2018-09-08-%srecov.gz /user/dmp/yyfq_car_gps/saigerealtime/180908/%s&apos;&apos;&apos;for i in range(3, 24, 1): hour_str = str(i) if i &lt; 10: hour_str = &apos;0&apos; + str(i) cmd_new = cmdtxt % (hour_str, hour_str, hour_str, hour_str, hour_str) print cmd_new os.system(cmd_new) python2 get、post请求1234567891011121314151617181920212223242526def saige_last_gpstime(is_wx, gps_no): post_json = &#123; 'callLetter': gps_no, 'flag': 'false', 'sign': '335BB919C5476417E424FF6F0BC5AD6F' &#125; unittype = "YX" requrl = "http://218.17.3.228:8008/mljrserver/vehicle/queryYXGpsInfo" if is_wx: unittype = "WX" requrl = "http://218.17.3.228:8008/mljrserver/vehicle/queryWXGpsInfo" headers = &#123;'Content-Type': 'application/json'&#125; req = urllib2.Request(url=requrl, headers=headers, data=json.dumps(post_json)) res_data = urllib2.urlopen(req) res = json.loads(res_data.read()) if res['datas']: gpstime_str = res['datas']['list'][0]['gpsTime'] if random.random() &lt; 0.001: print('random print' + gps_no + ',' + gpstime_str) gpstime_date = get_time(gpstime_str) if (datetime.datetime.now() - gpstime_date).seconds &gt; 10800 and (gpstime_date.date() - yester_date).days == 1: print(unittype + ',' + gps_no + ',' + gpstime_str) 随机数1234import random# 0-1之间if random.random() &lt; 0.001: print(&apos;random print&apos; + gps_no + &apos;,&apos; + gpstime_str) 星号和双星号用法在Python中，星号除了用于乘法数值运算和幂运算外，还有一种特殊的用法”在变量前添加单个星号或两个星号”，实现多参数的传入或变量的拆解，本文将详细介绍”星号参数”的用法。 最初，星号变量是用在函数的参数传递上的，在下面的实例中，单个星号代表这个位置接收任意多个非关键字参数，在函数的b位置上将其转化成元组，而双星号代表这个位置接收任意多个关键字参数，在b位置上将其转化成字典*： * 该位置接受任意多个非关键字（non-keyword）参数，在函数中将其转化为元组（1,2,3,4） \ 该位置接受任意多个关键字（keyword）参数，在函数位置上转化为词典 [key:value, key:value ]** 除法转成百分比12345def get_per(cnt, sum_cnt): if 0 == sum_cnt: return &apos;&apos; percentage = cnt / (sum_cnt + 0.0) return &apos;%.2f%%&apos; % (percentage) mpl_toolkits没有basemap包1conda install basemap]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[前缀树]]></title>
    <url>%2F2018%2F03%2F31%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E5%89%8D%E7%BC%80%E6%A0%91%2F</url>
    <content type="text"><![CDATA[这段代码应用于GeoHash 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255package com.iclick.geo.search;import java.util.HashMap;import java.util.LinkedList;import java.util.Map.Entry;/** * 前缀树的实现 * * @author li * */public class TrieTree &#123; public int total; public int totalNode; private class Node &#123; private int prefixNum;// 以该字符串为前缀的字符数 private Node[] childNodes; private boolean isLeaf;// 是否为单词 private int dumplicateNum;// 该字符串重复出现的次数(单词重复出现次数) private LinkedList&lt;POINode&gt; pois ; public Node() &#123; prefixNum = 0; isLeaf = false; dumplicateNum = 0; childNodes = new Node[36]; pois = null; &#125; &#125; private Node root;// 根节点 public TrieTree() &#123; root = new Node(); &#125; public void insert(String word) &#123; insert(this.root, word, null); &#125; public void insert(String word, POINode poi) &#123; insert(this.root, word, poi); &#125; /** * 插入单词 * * @param root * @param word */ public void insert(Node root, String word,POINode poi) &#123; word = word.toLowerCase(); char[] chars = word.toCharArray(); for (int i = 0; i &lt; chars.length; i++) &#123; int index = getNodeIndex(chars[i]); if (root.childNodes[index] == null) &#123; root.childNodes[index] = new Node(); totalNode += 1; &#125; root.childNodes[index].prefixNum++; if (i == chars.length - 1) &#123; root.childNodes[index].isLeaf = true;// 单词 if(poi != null) &#123; if(root.childNodes[index].pois == null) &#123; root.childNodes[index].pois = new LinkedList&lt;POINode&gt;(); &#125; root.childNodes[index].pois.add(poi); total += 1; &#125; root.childNodes[index].dumplicateNum++;// 该单词出现的个数 &#125; root = root.childNodes[index]; &#125; &#125; /** * 获取以prefixStr为前缀的字符串 * * @param prefixStr * @return */ private HashMap&lt;String, LinkedList&lt;POINode&gt;&gt; getPrefixString(Node root, String prefixStr) &#123; prefixStr = prefixStr.toLowerCase(); HashMap&lt;String, LinkedList&lt;POINode&gt;&gt; map = new HashMap&lt;String, LinkedList&lt;POINode&gt;&gt;(); char[] prefixs = prefixStr.toCharArray(); for (int i = 0; i &lt; prefixs.length; i++) &#123; int index = getNodeIndex(prefixs[i]); if (root.childNodes[index] != null) &#123; root = root.childNodes[index]; &#125; else &#123; return map; &#125; &#125; dfs(root, prefixStr, map); return map; &#125; public HashMap&lt;String, LinkedList&lt;POINode&gt;&gt; getPrefixString(String prefixStr) &#123; return getPrefixString(root, prefixStr); &#125; /** * 深度优先遍历 * * @param root * :根节点 * @param map */ private HashMap&lt;String, LinkedList&lt;POINode&gt;&gt; dfs(Node root, String prefixs, HashMap&lt;String, LinkedList&lt;POINode&gt;&gt; map) &#123; if (root == null) &#123; return null; &#125; if (root.isLeaf) &#123; map.put(prefixs, root.pois); &#125; Node[] nodes = root.childNodes; for (int i = 0; i &lt; nodes.length; i++) &#123; if (nodes[i] != null) &#123; char c = '0'; if(i &lt; 10) &#123; c = (char) (i + '0'); &#125; else &#123; c = (char) (i + 'a' - 10); &#125; String tempStr = prefixs + c; dfs(root.childNodes[i], tempStr, map); &#125; &#125; return map; &#125; /** * 遍历所有的字符串 * * @return */ public HashMap&lt;String, LinkedList&lt;POINode&gt;&gt; dfs() &#123; HashMap&lt;String, LinkedList&lt;POINode&gt;&gt; map = new HashMap&lt;String, LinkedList&lt;POINode&gt;&gt;(); return dfs(this.root, "", map); &#125; /** * 删除节点 * * @param word */ public boolean delete(String word) &#123; // 不存在该单词则删除 Node p = root; if (!searchWord(word)) &#123; return false; &#125; word = word.toLowerCase(); char[] chars = word.toCharArray(); for (int i = 0; i &lt; chars.length; i++) &#123; int index = getNodeIndex(chars[i]); Node[] nodes = p.childNodes; if (nodes[index].prefixNum == 1) &#123; nodes[index] = null; return true; &#125; else &#123; nodes[index].prefixNum--; p = nodes[index]; &#125; &#125; if (p.dumplicateNum == 1) &#123; p.isLeaf = false; &#125; p.dumplicateNum--; return true; &#125; /** * * @param root * @param word * @return */ private boolean searchWord(Node root, String word) &#123; if (root == null) &#123; return false; &#125; word = word.toLowerCase(); char[] chars = word.toCharArray(); for (int i = 0; i &lt; chars.length; i++) &#123; int index = getNodeIndex(chars[i]); Node[] nodes = root.childNodes; if (nodes[index] != null) &#123; root = nodes[index]; &#125; else &#123; return false; &#125; &#125; if (!root.isLeaf) &#123; return false; &#125; return true; &#125; public boolean searchWord(String word) &#123; return searchWord(root, word); &#125; public static int getNodeIndex(char c) &#123; int index = 0; if (c &gt;= '0' &amp;&amp; c &lt;= '9') &#123; index = c - '0'; &#125; else &#123; index = (c - 'a') + 10; &#125; return index; &#125; public static void main(String[] args) &#123;// char[] chars = &#123;'0','1','2','3','4','5','9','a','b','z'&#125;;// // for (int i = 0; i &lt; chars.length; i++) &#123;// int index = getNodeIndex(chars[i]);// System.out.println(chars[i] + "\t" + index);// &#125; TrieTree trieTree = new TrieTree(); trieTree.insert("gab"); trieTree.insert("gb"); trieTree.insert("gb"); trieTree.insert("gbc"); trieTree.insert("gbc"); trieTree.insert("go"); trieTree.insert("go"); trieTree.insert("goa"); for (Entry&lt;String, LinkedList&lt;POINode&gt;&gt; entry : trieTree.getPrefixString("g") .entrySet()) &#123; System.out.println(entry.getKey() + " : " + entry.getValue()); &#125; System.out.println(trieTree.total + "\t" + trieTree.totalNode); System.out.println(trieTree.delete("gb")); System.out.println(trieTree.delete("gb")); System.out.println("***************"); for (Entry&lt;String, LinkedList&lt;POINode&gt;&gt; entry : trieTree.getPrefixString("g") .entrySet()) &#123; System.out.println(entry.getKey() + " : " + entry.getValue()); &#125; &#125;&#125;]]></content>
      <categories>
        <category>算法与数据结构</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[GeoHash]]></title>
    <url>%2F2018%2F03%2F31%2F%E5%9C%B0%E7%90%86%E4%BF%A1%E6%81%AF%E6%8C%96%E6%8E%98%2FGeoHash%2F</url>
    <content type="text"><![CDATA[geohash学习及优化 GeoHash 经纬度坐标编码与解码算法 geohash在实际使用时，需要注意： 1、一个geohash编码对应一个区域 2、geohash编码越长，精度越高 3、同一个区域内的点的距离不一定就比不同区域的距离短。这个很容易理解，这就像你有两个朋友A和B，A和你都在中国，B在俄罗斯，你和A的编码是一样的，但是不一定A离你近，还是B离你近。 4、由于存在#3的问题，搜索范围poi时，必须至少扩展一圈。 5、#4带来一个扩展策略的问题，就是精度选择的问题。精度太低，划分的区域很大，会选择出很多多余的点，降低性能。精度太高，有可能需要进行多次扩展，扩展得到的区域数将会快速的增长(2*n+1)^2，性能也会降低。 GeoHash的优化用打表的方式解决求Geohash当前区域周围8个区域编码https://blog.csdn.net/u014520745/article/details/52619796 订制经纬度范围标准的geohash算法的经度范围是(-180,180)，纬度范围为(-90,90)，这个范围是适用于全球的地理位置的。但是我们目前的应用数据点仅局限于国内，所以可以将范围缩小。 减少计算的次数提高性能 降低geohash有效值的位数 自定义经纬度范围可选定一个趋于正方形的范围，当计算结果为一个圆形区域，这样能更好的和圆契合。 ​ 可选定经纬度范围， 经度(75, 135)，纬度(18, 53)。 GeoHash获取矩形的四个顶点 1234567891011121314151617181920212223242526272829303132333435363738394041424344Geohash.bounds = function(geohash) &#123; if (geohash.length === 0) throw new Error(&apos;Invalid geohash&apos;); geohash = geohash.toLowerCase(); var evenBit = true; var latMin = -90, latMax = 90; var lonMin = -180, lonMax = 180; for (var i=0; i&lt;geohash.length; i++) &#123; var chr = geohash.charAt(i); var idx = Geohash.base32.indexOf(chr); if (idx == -1) throw new Error(&apos;Invalid geohash&apos;); for (var n=4; n&gt;=0; n--) &#123; var bitN = idx &gt;&gt; n &amp; 1; if (evenBit) &#123; // longitude var lonMid = (lonMin+lonMax) / 2; if (bitN == 1) &#123; lonMin = lonMid; &#125; else &#123; lonMax = lonMid; &#125; &#125; else &#123; // latitude var latMid = (latMin+latMax) / 2; if (bitN == 1) &#123; latMin = latMid; &#125; else &#123; latMax = latMid; &#125; &#125; evenBit = !evenBit; &#125; &#125; var bounds = &#123; sw: &#123; lat: latMin, lon: lonMin &#125;, ne: &#123; lat: latMax, lon: lonMax &#125;, &#125;; return bounds;&#125;; 参考 Geohash介绍及针对具体需求的改良]]></content>
      <categories>
        <category>地理信息处理和挖掘</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[hive技巧总结]]></title>
    <url>%2F2018%2F03%2F31%2Fhadoop-spark%2Fhive%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[插入hive表控制part文件数量http://blog.sina.com.cn/s/blog_604c7cdd0102wbsw.html 12345-- 每个文件上限500Mset hive.exec.reducers.bytes.per.reducer=512000000;insert overwrite table carthage.gps_address_info_weekly_bak PARTITION(DATA_DATE=&apos;2019-01-15&apos;)select * from carthage.gps_address_info DISTRIBUTE by RAND();-- DISTRIBUTE by RAND()主要靠这个控制reduce的文件数 strict模式1set hive.mapred.mode=strict 有助于前置解决一些语法和可能的逻辑错误。 限制小文件数量123set mapred.max.split.size=256000000; -- 决定每个map处理的最大的文件大小，单位为Bset mapred.min.split.size.per.node=10000000; -- 节点中可以处理的最小的文件大小set mapred.min.split.size.per.rack=10000000; -- 机架中可以处理的最小的文件大小 查询时如何去掉重复数据假设数据为 1234name adx tran_id cost tsck 5 125.168.10.0 33.00 1407234660ck 5 187.18.99.00 33.32 1407234661ck 5 125.168.10.0 33.24 1407234661 1select * from (select *,row_number() over (partition by tran_id order by timestamp asc) num from table) t where t.num=1; 附上：ROW_NUMBER() OVER函数的基本用法 语法：ROW_NUMBER() OVER(PARTITION BY COLUMN ORDER BY COLUMN) 简单的说row_number()从1开始，为每一条分组记录返回一个数字，这里的ROW_NUMBER() OVER (ORDER BY xlh DESC) 是先把xlh列降序，再为降序以后的没条xlh记录返回一个序号。示例： 123456&gt; xlh row_num &gt; 1700 1 &gt; 1500 2 &gt; 1085 3 &gt; 710 4 &gt; &gt; row_number() OVER (PARTITION BY COL1 ORDER BY COL2) 表示根据COL1分组，在分组内部根据 COL2排序，而此函数计算的值就表示每组内部排序后的顺序编号（组内连续的唯一的) split后的数组长度1size(split(driving_districts,&apos;;&apos;)) 切换队列1set mapred.job.queue.name=data; sqoop切换队列是 1-D mapred.job.queue.name=data 加载hdfs的udf12ADD JAR hdfs://iclick/zyz/udf/zyz_udf2.jar;CREATE TEMPORARY FUNCTION get_region as &apos;org.apache.hadoop.hive.ql.udf.Ip2GeoCodeUDF&apos;; Hive Trashhive删除表时，会移除表的元数据和数据，而HDFS上的数据，如果配置了Trash，会移到.Trash/Current目录下。删除外部表时，表中的数据不会被删除。 性能用groupby代替distinct，少用orderby 123456789101112131415161718192021222324同事写了个hive的sql语句，执行效率特别慢，跑了一个多小时程序只是map完了，reduce进行到20%。该Hive语句如下：select count(distinct ip) from (select ip as ip from comprehensive.f_client_boot_daily where year=&quot;2013&quot; and month=&quot;10&quot; union all select pub_ip as ip from f_app_boot_daily where year=&quot;2013&quot; and month=&quot;10&quot; union all select ip as ip from format_log.format_pv1 where year=&quot;2013&quot; and month=&quot;10&quot; and url_first_id=1 ) d 分析：select ip as ip from comprehensive.f_client_boot_daily where year=&quot;2013&quot; and month=&quot;10&quot;这个语句筛选出来的数据约有10亿条，select pub_ip as ip from f_app_boot_daily where year=&quot;2013&quot; and month=&quot;10&quot;约有10亿条条，select ip as ip from format_log.format_pv1 where year=&quot;2013&quot; and month=&quot;10&quot; and url_first_id=1 筛选出来的数据约有10亿条，总的数据量大约30亿条。这么大的数据量，使用disticnt函数，所有的数据只会shuffle到一个reducer上，导致reducer数据倾斜严重。 解决办法： 首先，通过使用groupby，按照ip进行分组。改写后的sql语句如下：select count(*) from (select ip from(select ip as ip from comprehensive.f_client_boot_daily where year=&quot;2013&quot; and month=&quot;10&quot; union all select pub_ip as ip from f_app_boot_daily where year=&quot;2013&quot; and month=&quot;10&quot; union all select ip as ip from format_log.format_pv1 where year=&quot;2013&quot; and month=&quot;10&quot; and url_first_id=1) d group by ip ) b 然后，合理的设置reducer数量，将数据分散到多台机器上。set mapred.reduce.tasks=50; 经过优化后，速度提高非常明显。整个作业跑完大约只需要20多分钟的时间。 提高order by的性能https://blog.csdn.net/djd1234567/article/details/51917603 12345678910111213141516171819202122232425262728293031323334353637383940Hive中的order by跟传统的sql语言中的order by作用是一样的，会对查询的结果做一次全局排序，所以说，只有hive的sql中制定了order by所有的数据都会到同一个reducer进行处理（不管有多少map，也不管文件有多少的block只会启动一个reducer）。但是对于大量数据这 将会消耗很长的时间去执行。 这里跟传统的sql还有一点区别：如果指定了hive.mapred.mode=strict（默认值是nonstrict）,这时就必须指定limit 来限制输出条数，原因是：所有的数据都会在同一个reducer端进行，数据量大的情况下可能不能出结果，那么在这样的严格模式下，必须指定输出的条数。 所以数据量大的时候能不用order by就不用，可以使用sort by结合distribute by来进行实现。sort by是局部排序，而distribute by是控制map怎么划分reducer。 Hive中指定了sort by，那么在每个reducer端都会做排序，也就是说保证了局部有序（每个reducer出来的数据是有序的，但是不能保证所有的数据是有序的，除非只有一个reducer），好处是：执行了局部排序之后可以为接下去的全局排序提高不少的效率（其实就是做一次归并排序就可以做到全局排序了） ditribute by是控制map的输出在reducer是如何划分的，举个例子，我们有一张表，mid是指这个store所属的商户，money是这个商户的盈利，name是这个store的名字store:mid money nameAA 15.0 商店1AA 20.0 商店2BB 22.0 商店3CC 44.0 商店4 执行hive语句：[sql] view plain copyselect mid, money, name from store distribute by mid sort by mid asc, money asc 我 们所有的mid相同的数据会被送到同一个reducer去处理，这就是因为指定了distribute by mid，这样的话就可以统计出每个商户中各个商店盈利的排序了（这个肯定是全局有序的，因为相同的商户会放到同一个reducer去处理）。这里需要注意 的是distribute by必须要写在sort by之前。cluster by cluster by的功能就是distribute by和sort by相结合，如下2个语句是等价的：[sql] view plain copyselect mid, money, name from store cluster by mid select mid, money, name from store distribute by mid sort by mid 如果需要获得与上面的中语句一样的效果：[sql] view plain copyselect mid, money, name from store cluster by mid sort by money 注意被cluster by指定的列只能是降序，不能指定asc和desc。 问题集查询ES表报错1Failed with exception java.io.IOException:org.elasticsearch.hadoop.rest.EsHadoopInvalidRequest: The number of slices [1126] is too large. It must be less than [1024]. This limit can be set by changing the [index.max_slices_per_scroll] index level settin 修改es的设置 1234567891011PUT /megacorp/_settings&#123; &quot;index&quot;: &#123; &quot;max_slices_per_scroll&quot; : 1126 &#125;&#125; 上传hive UDF包后重启hive server报错hive udf没有权限执行1Error while compiling statement: FAILED: SemanticException No valid privileges User dmp does not have privileges for CREATEFUNCTION The required privileges: Server=server1-&gt;URI=file:///home/hive/aux_libs/carthage-common-udf-hive-test.jar-&gt;action=*; 没有找到jar包的报错1Error while compiling statement: FAILED: SemanticException [Error 10014]: Line 1:7 Wrong arguments &apos;70.0&apos;: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to execute method public java.lang.String com.mljr.carthage.common.geo.udf.hive.UDFGeoLocation.evaluate(java.lang.Double,java.lang.Double) on object com.mljr.carthage.common.geo.udf.hive.UDFGeoLocation@54ee9573 of class com.mljr.carthage.common.geo.udf.hive.UDFGeoLocation with arguments &#123;50.0:java.lang.Double, 70.0:java.lang.Double&#125; of size 2 其他都是可以的，就这个udf的第二个参数一直报错。经测试，还是UDF本身的问题，跟参数的设置没有关系。 最后发现问题是udf的jar包上传后，关联的一些jar包没有打进去，手动加上 1234567891011121314151617181920212223&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;1.6&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;artifactSet&gt; &lt;includes&gt; &lt;include&gt;com.mljr.carthage:carthage-common-geo&lt;/include&gt; &lt;include&gt;com.alibaba:fastjson&lt;/include&gt; &lt;include&gt;com.github.davidmoten:geo&lt;/include&gt; &lt;include&gt;com.github.davidmoten:grumpy-core&lt;/include&gt; &lt;/includes&gt; &lt;/artifactSet&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; hive用高版本的UDF在hive2.0中有类似于months_between的函数，可以实现2个时间之间的月份差。但是低版本没有这个函数 解决： 下载hive-2.1源码包 http://mirrors.hust.edu.cn/apache/hive/hive-2.2.0/ 导入eclipse，查找months_between 在org.apache.hadoop.hive.ql.udf.generic包下找到GenericUDFMonthsBetween类，移植即可 /ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFMonthsBetween.java String转date1select cast(to_date(from_unixtime(unix_timestamp(&apos;12-05-2010&apos;, &apos;dd-MM-yyyy&apos;))) as date) MapJoin异常问题处理总结https://yq.aliyun.com/articles/64306 替换hive分隔符1sed -i &apos;.bak&apos; &apos;s/^A/,/g&apos; baseinfo05.csv ^A要用ctrl+V+A打出来 Beeline导出csv后特殊符号导出csv后，由于某个字段（比如经纬度），本身就包括逗号，于是在导出时加上了特殊字符。 sublime打开是 于是搜索替换的方法，用vim打开后， 1:%s/\%x00/&quot;/g 就替换为了双引号，这样在读取csv的时候会将双引号自动转义 LOAD DATA1LOAD DATA [LOCAL] INPATH &apos;filepath&apos; [OVERWRITE] INTO TABLE tablename[PARTITION (partcol1=val1,partcol2=val2,…)] 最好不要用LOCAL，要从hadoop加载数据。local读的是hive服务器的本地路径。 12345678910111213141516171819# coding:utf-8from pyhive import hivefrom TCLIService.ttypes import TOperationState# 打开hive连接hiveConn = hive.connect(host=&apos;192.168.83.135&apos;,port=11111,username=&apos;hadoop&apos;)cursor = hiveConn.cursor()# 执行sql语句sql = &apos;&apos;&apos; LOAD DATA LOCAL INPATH &apos;/home/hadoop/HivePy/employee.txt&apos; OVERWRITE INTO TABLE userdbbypy.employee &apos;&apos;&apos;cursor.execute(sql, async=True)# 得到执行语句的状态status = cursor.poll().operationStateprint &quot;status:&quot;,status# 关闭hive连接cursor.close()hiveConn.close() return code 3试试 1set hive.vectorized.execution.enabled=false; hive锁表1Completed compiling command 若卡在上面的语句，说明锁表 123456show locks carthage_dev.baseinfo_personal_info;-- 如果是unlock table dwh_dml_risk_dev.rec_car_operation;show locks carthage_dev.baseinfo_personal_info partition(data_date=&apos;20180517&apos;);unlock table dwh_dml_risk_dev.rec_car_operation partition(data_date=&apos;2018-09-30&apos;); hive解锁的脚本是all_hive_unlock.sh hive新增列报错在添加字段是可以通过CASCADE关键字来，避免出现这种问题。如alter table table_name add columns(age int) CASCADE https://qubole.zendesk.com/hc/en-us/articles/115002396646-Hive-Null-Pointer-Exception-in-select-query-after-modifying-table-definition This can happen in the scenario where table definition and specific partition definition is different, and the underlying data matches table definition but not partition definition. When a table with partitions is altered to add a column using statement: ALTER TABLE ADD COLUMNS (c1 int); The table definition for existing partitions don’t get modified as per the above statement. As a result of this there is a mismatch between partition and table definition. This is ok if the partition data matches the definition of partition, but if the data matches definition of table itself, NPE is thrown as there is a mismatch in data vs definition. To avoid this issue, this statement should be used in hadoop2 ALTER TABLE ADD COLUMNS (c1 int) CASCADE; In case of hadoop1, CASCADE option is not available. Hence, as long as the table is external table, following can be done: Drop and recreate partitions for this table Alter partition definition for specific partition having issues 用了cascade 无效。 找到原因： hive表是ORC格式的，因此cascade无效，若改成text格式则成功。 解决方案： 若必须是ORC格式，建表是先预留若干字段，后期改名字 hive分区解锁1234show locks carthage_dev.baseinfo_personal_info;unlock table carthage_dev.baseinfo_personal_info;show locks carthage_dev.baseinfo_personal_info partition(data_date=&apos;20180517&apos;);unlock table carthage_dev.baseinfo_personal_info partition(data_date=&apos;20180517&apos;); 解锁的技巧： 1、定位哪张表锁住，可以分批执行sql，定位关键表 2、show locks并下载，观察锁表的状态，通过 show locks table extends可以看依赖的表 3、用脚本all_hive_unlock.sh解锁 hive column rename1alter table carthage_dev.gps_wx_stop_status CHANGE stop_region_center_lon stop_status_center_lon string hive配置12345-- 输出为gzipset hive.exec.compress.output=true; set mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec;-- 输出为一个文件set mapred.reduce.tasks=1; hive timestamp转时间123from_unixtime(unix_timestamp(),‘yyyy/MM/dd HH:mm:ss’);from_unixtime(cast(cast(time as bigint)/1000 as bigint),&apos;yyyy/MM/dd HH:mm:ss&apos;) 常用日期函数1234567891011121314151617to_date：日期时间转日期函数 select to_date(&apos;2018-04-02 13:34:12&apos;); 输出：2018-04-02 from_unixtime：转化unix时间戳到当前时区的时间格式 select from_unixtime(1524573762,&apos;yyyy-MM-dd HH:mm:ss&apos;); 输出：2018-04-24 20:42:42unix_timestamp：获取当前unix时间戳 select unix_timestamp(); 输出：1524573762 select unix_timestamp(&apos;2018-04-01 13:01:20&apos;); 输出：1522558880datediff：返回开始日期减去结束日期的天数 select datediff(&apos;2018-04-09&apos;,&apos;2018-04-01&apos;); 输出：8 date_sub：返回日期前n天的日期 select date_sub(&apos;2018-04-09&apos;,4); 输出：2018-04-05 date_add：返回日期后n天的日期 select date_add(&apos;2018-04-09&apos;,4); 输出：2018-04-13 add_months：月份增加函数 select add_months(&apos;2018-02-10&apos;, 2 ); 输出：2018-04-10 last_day：返回当月底日期 select last_day(&apos;2018-02-21&apos;); 输出：2018-02-28 hive 字符串函数1. 字符串长度函数：length 语法: length(string A) 返回值: int 说明：返回字符串A的长度 举例： hive&gt; select length(‘abcedfg’) from lxw_dual; 7 2. 字符串反转函数：reverse 语法: reverse(string A) 返回值: string 说明：返回字符串A的反转结果 举例： hive&gt; select reverse(abcedfg’) from lxw_dual; gfdecba 3. 字符串连接函数：concat 语法: concat(string A, string B…) 返回值: string 说明：返回输入字符串连接后的结果，支持任意个输入字符串 举例： hive&gt; select concat(‘abc’,’def’,’gh’) from lxw_dual; abcdefgh 4. 带分隔符字符串连接函数：concat_ws 语法: concat_ws(string SEP, string A, string B…) 返回值: string 说明：返回输入字符串连接后的结果，SEP表示各个字符串间的分隔符 举例： hive&gt; select concat_ws(‘,’,’abc’,’def’,’gh’) from lxw_dual; abc,def,gh 5. 字符串截取函数：substr,substring 语法: substr(string A, int start),substring(string A, int start) 返回值: string 说明：返回字符串A从start位置到结尾的字符串 举例： hive&gt; select substr(‘abcde’,3) from lxw_dual; cde hive&gt; select substring(‘abcde’,3) from lxw_dual; cde hive&gt; selectsubstr(‘abcde’,-1) from lxw_dual; （和ORACLE相同） e 6. 字符串截取函数：substr,substring 语法: substr(string A, int start, int len),substring(string A, intstart, int len) 返回值: string 说明：返回字符串A从start位置开始，长度为len的字符串 举例： hive&gt; select substr(‘abcde’,3,2) from lxw_dual; cd hive&gt; select substring(‘abcde’,3,2) from lxw_dual; cd hive&gt;select substring(‘abcde’,-2,2) from lxw_dual; de 7. 字符串转大写函数：upper,ucase 语法: upper(string A) ucase(string A) 返回值: string 说明：返回字符串A的大写格式 举例： hive&gt; select upper(‘abSEd’) from lxw_dual; ABSED hive&gt; select ucase(‘abSEd’) from lxw_dual; ABSED 8. 字符串转小写函数：lower,lcase 语法: lower(string A) lcase(string A) 返回值: string 说明：返回字符串A的小写格式 举例： hive&gt; select lower(‘abSEd’) from lxw_dual; absed hive&gt; select lcase(‘abSEd’) from lxw_dual; absed 9. 去空格函数：trim 语法: trim(string A) 返回值: string 说明：去除字符串两边的空格 举例： hive&gt; select trim(‘ abc ‘) from lxw_dual; abc 10. 左边去空格函数：ltrim 语法: ltrim(string A) 返回值: string 说明：去除字符串左边的空格 举例： hive&gt; select ltrim(‘ abc ‘) from lxw_dual; abc 11. 右边去空格函数：rtrim 语法: rtrim(string A) 返回值: string 说明：去除字符串右边的空格 举例： hive&gt; select rtrim(‘ abc ‘) from lxw_dual; abc 12. 正则表达式替换函数：regexp_replace 语法: regexp_replace(string A, string B, string C) 返回值: string 说明：将字符串A中的符合java正则表达式B的部分替换为C。注意，在有些情况下要使用转义字符,类似oracle中的regexp_replace函数。 举例： hive&gt; select regexp_replace(‘foobar’, ‘oo|ar’, ‘’) from lxw_dual; fb 13. 正则表达式解析函数：regexp_extract 语法: regexp_extract(string subject, string pattern, int index) 返回值: string 说明：将字符串subject按照pattern正则表达式的规则拆分，返回index指定的字符。 举例： hive&gt; select regexp_extract(‘foothebar’, ‘foo(.*?)(bar)’, 1) fromlxw_dual; the hive&gt; select regexp_extract(‘foothebar’, ‘foo(.*?)(bar)’, 2) fromlxw_dual; bar hive&gt; select regexp_extract(‘foothebar’, ‘foo(.*?)(bar)’, 0) fromlxw_dual; foothebar 注意，在有些情况下要使用转义字符，下面的等号要用双竖线转义，这是java**正则表达式的规则。** select data_field, ​ regexp_extract(data_field,’.*?bgStart\=(&+)’,1) as aaa, ​ regexp_extract(data_field,’.*?contentLoaded_headStart\=(&+)’,1) as bbb, ​ regexp_extract(data_field,’.*?AppLoad2Req\=(&+)’,1) as ccc ​ from pt_nginx_loginlog_st ​ where pt = ‘2012-03-26’limit 2; 14. URL解析函数：parse_url 语法: parse_url(string urlString, string partToExtract [, stringkeyToExtract]) 返回值: string 说明：返回URL中指定的部分。partToExtract的有效值为：HOST, PATH, QUERY, REF, PROTOCOL, AUTHORITY, FILE, and USERINFO. 举例： hive&gt; selectparse_url(‘http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1‘, ‘HOST’) fromlxw_dual; facebook.com hive&gt; selectparse_url(‘http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1‘, ‘QUERY’,’k1’) from lxw_dual; v1 15. json解析函数：get_json_object 语法: get_json_object(string json_string, string path) 返回值: string 说明：解析json的字符串json_string,返回path指定的内容。如果输入的json字符串无效，那么返回NULL。 举例： hive&gt; select get_json_object(‘{“store”: > {“fruit”:[{“weight”:8,”type”:”apple”},{“weight”:9,”type”:”pear”}], > “bicycle”:{“price”:19.95,”color”:”red”} > }, > “email”:”amy@only_for_json_udf_test.net”, > “owner”:”amy” > } > ‘,’$.owner’) from lxw_dual; amy 16. 空格字符串函数：space 语法: space(int n) 返回值: string 说明：返回长度为n的字符串 举例： hive&gt; select space(10) from lxw_dual; hive&gt; select length(space(10)) from lxw_dual; 10 17. 重复字符串函数：repeat 语法: repeat(string str, int n) 返回值: string 说明：返回重复n次后的str字符串 举例： hive&gt; select repeat(‘abc’,5) from lxw_dual; abcabcabcabcabc 18. 首字符ascii函数：ascii 语法: ascii(string str) 返回值: int 说明：返回字符串str第一个字符的ascii码 举例： hive&gt; select ascii(‘abcde’) from lxw_dual; 97 19. 左补足函数：lpad 语法: lpad(string str, int len, string pad) 返回值: string 说明：将str进行用pad进行左补足到len位 举例： hive&gt; select lpad(‘abc’,10,’td’) from lxw_dual; tdtdtdtabc 注意：与GP**，ORACLE**不同，pad 不能默认 20. 右补足函数：rpad 语法: rpad(string str, int len, string pad) 返回值: string 说明：将str进行用pad进行右补足到len位 举例： hive&gt; select rpad(‘abc’,10,’td’) from lxw_dual; abctdtdtdt 21. 分割字符串函数: split 语法: split(string str, stringpat) 返回值: array 说明: 按照pat字符串分割str，会返回分割后的字符串数组 举例： hive&gt; select split(‘abtcdtef’,’t’) from lxw_dual; [“ab”,”cd”,”ef”] 22. 集合查找函数:find_in_set 语法: find_in_set(string str, string strList) 返回值: int 说明: 返回str在strlist第一次出现的位置，strlist是用逗号分割的字符串。如果没有找该str字符，则返回0 举例： hive&gt; select find_in_set(‘ab’,’ef,ab,de’) from lxw_dual; 2 hive&gt; select find_in_set(‘at’,’ef,ab,de’) from lxw_dual; 0 instr group后拼接group_concat( [distinct] 要连接的字段 [order by 排序字段 asc/desc ] [separator ‘分隔符’] ) 123select userid,bankid,group_concat(cast(creditlimit as string))from vdm_fin.cc_user_bill_0724group by userid,bankid hive实现相同的功能： 1234SELECT id,concat_ws(&apos;|&apos;, collect_set(str)) FROM t GROUP BY id;1234 主意:collect_set 只能返回不重复的集合若要返回带重复的要用collect_list 2、collect_list 展示子表排序后结果，collect_set 不受子表排序影响select phone,collect_list(user_id) ,collect_set(user_id) from(select * from a order by order_time asc)bgroup by phone结果：123456789 [1,1,3,2,2] [1,3,2] a表数据如下phone user_id order_time123456789 1 2018/8/23123456789 3 2018/8/24123456789 2 2018/8/25123456789 1 2018/8/22123456789 2 2018/8/26]]></content>
      <categories>
        <category>大数据平台工具</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[经纬度找POI]]></title>
    <url>%2F2018%2F03%2F31%2F%E5%9C%B0%E7%90%86%E4%BF%A1%E6%81%AF%E6%8C%96%E6%8E%98%2F%E7%BB%8F%E7%BA%AC%E5%BA%A6%E6%89%BEPOI%2F</url>
    <content type="text"><![CDATA[定时任务每天有定时任务 100 07 * * * source /etc/profile; python /opt/pig_home/mobile_poi/lbspoi.py &gt;&gt; /opt/pig_home/mobile_poi/lbs.log 2&gt;&amp;1 generate_mobile_geo_info1234567891011121314151617181920212223242526USE default; ADD JAR hdfs://iclick/zyz/udf/zyz_udf2.jar; CREATE TEMPORARY FUNCTION get_region as 'org.apache.hadoop.hive.ql.udf.Ip2GeoCodeUDF'; INSERT OVERWRITE TABLE shortdata.mobile_poi_raw partition(day_id='%s',city) SELECT zuid, gps, ip, ts, slot_id, city_code AS city FROM ( SELECT region_code, city_code FROM persona.pois_city_dict ) a join ( SELECT zuid,gps,ip,ts,slot_id,get_region(ip) region_code,day_id FROM nobid WHERE day_id='%s' AND gps!='' AND dt!=0 AND length(zuid)=32 AND ip!='' AND slot_id !='' ) b on(a.region_code=b.region_code); 从persona.pois_city_dict 取地理编码region_code和城市区号city_code，从nobid中取zuid，gps等，通过region_code做join，结果放到shortdata.mobile_poi_raw ，包括zuid, gps, ip, ts, slot_id, city_code check_raw_data检查 1/shortdata/mobile_poi_raw/day_id=%s 是否有数据 create_mobile_es_indexES中创建索引 generate_poi_content1hadoop fs -ls /shortdata/mobile_poi_raw/day_id=%s|awk -F&apos;city=&apos; &apos;NF==2&#123;print $2&#125;&apos; 目录下找出所有的城市的区号。对于每个区号，执行 1pig -p MOBILE_POI_RAW=/shortdata/mobile_poi_raw/day_id=%s/city=%s -p CITY_CODE=%s -p MOBILE_POI_INFO=/shortdata/persona/mobile_poi/%s/%s /opt/pig_home/mobile_poi/GeneratePoiContent.pig MOBILE_POI_RAW的字段包括数据如mid,geo,ip,timestamp,ban，ban不知道是什么。数据例如 123456789(0072e62dcb59f4193a87c57b52f72f40,0.000000_-0.000000,223.104.106.193,1522251854,9641490506058567536)(00a416a2bf6155061378554128e6d34e,0.000000_-0.000000,223.104.63.204,1522177840,2079384033841789443)(00a78ac01d879f209f0d9ca1c44c8e7f,116.272537_39.905327,114.248.119.130,1522242470,12150475897216517376)(012ddc44478c6d828ab151b6ca683916,0.000000_-0.000000,117.157.192.13,1522193641,11414853793502166973)(025f2ed81db51d3d99d9a47bf9aeb9a0,0.000000_-0.000000,47.94.140.78,1522187326,18293937070879238797)(02bf4d8bdc38ac1f6fdd52209e84039c,115.123116_35.816261,223.104.108.158,1522242477,17879387064476989398)(02c18f312fa72708909183b040e74e13,120.034416_32.183392,36.149.39.178,1522181536,16092911831591004172)(03279dff09e62f9eaba637fa43744dd6,0.000000_-0.000000,223.72.77.105,1522201115,692495996220981070)(034cd47c18e030379ac5bdfaa4c2f1cb,106.306679_26.233530,223.104.95.36,1522191134,2612) 后面会对mid,geo,ip,timestamp去重，ban就去掉了。 有一个关键的UDF 123DEFINE getPoiContent com.buzzinate.pig.udf.util.SearchPoiContentHdfs();mobile_poi_city = FOREACH mobile_poi GENERATE mid,FLATTEN(getPoiContent('$CITY_CODE',mid,timestamp,ip,geo,50.0)) as content; SearchPoiContentHdfs 首先初始化/geo-poi/src/main/java/com/iclick/geo/search/GeoPois.java 。初始化时要加载POI文件，如010.csv，然后添加到前缀树TrieTree 中。 然后给定一个经纬度坐标和搜索范围，搜索POI点。 123456789101112131415161718192021222324252627282930313233343536373839404142/** * @param rlng WGS84坐标系的经度 * @param rlat WGS84坐标系的纬度 * @param accuracy 匹配精度 * @return 匹配到的POI列表 */public List&lt;POINode&gt; search(double rlng, double rlat, double accuracy, TrieTree trieTree) &#123; // 转成火星坐标系 Double[] gc = GPSTransform.wgs84Togcj02(rlng, rlat); Double lng = gc[0]; Double lat = gc[1]; List&lt;POINode&gt; pois = new ArrayList&lt;POINode&gt;(); HashSet&lt;String&gt; fn = new HashSet&lt;String&gt;(); // 根据精度误差，得到geohash编码长度 int rlen = getAccuracyRange(accuracy); String shash = ""; try &#123; shash = GeoHash.encodeHash(lat, lng, rlen); fn.add(shash); &#125; catch (IllegalArgumentException e) &#123; return pois; &#125; for (String nbour : GeoHash.neighbours(shash)) &#123; fn.add(nbour.substring(0, rlen)); &#125; for (String thash : fn) &#123; for (Entry&lt;String, LinkedList&lt;POINode&gt;&gt; entry : trieTree.getPrefixString(thash).entrySet()) &#123; if (entry.getValue() != null) &#123; for (POINode poi : entry.getValue()) &#123; Double distance = PointUtils.distance(lng, lat, poi.lng, poi.lat); if (distance &lt;= accuracy) &#123; pois.add(poi); poi.setDistance(distance); &#125; &#125; &#125; &#125; &#125; return pois;&#125; 首先要进行坐标转换，GPSTransform getAccuracyRange的代码 1234567891011121314151617181920212223242526272829303132333435 /* * 根据精度误差，得到geohash编码长度 * * accuracy: 精度误差 * */ public static int getAccuracyRange(double accuracy) &#123; /* ** 具体的计算方法：Latitude的范围是：-90 到 +90Longitude的范围：-180 到 +180地球参考球体的周长：40075016.68米geohash长度 Lat位数 Lng位数 Lat误差 Lng误差 km误差1 2 3 ±23 ±23 ±25002 5 5 ± 2.8 ±5.6 ±6303 7 8 ± 0.70 ± 0.7 ±784 10 10 ± 0.087 ± 0.18 ±205 12 13 ± 0.022 ± 0.022 ±2.46 15 15 ± 0.0027 ± 0.0055 ±0.617 17 18 ±0.00068 ±0.00068 ±0.0768 20 20 ±0.000086 ±0.000172 ±0.019119 22 23 ±0.000021 ±0.000021 ±0.0047810 25 25 ±0.00000268 ±0.00000536 ±0.000597111 27 28 ±0.00000067 ±0.00000067 ±0.000149212 30 30 ±0.00000008 ±0.00000017 ±0.0000186*/ if (accuracy &gt; 76) &#123; //当精度要求大于76米的时候，geohash长度为６ return 6; &#125; else if (accuracy &gt; 19) &#123; //当精度要求19米 &lt; 76米的时候，geohash长度为７ return 7; &#125; else &#123; //当精度要求 &lt; 19米的时候，geohash长度为8 return 8; &#125; &#125; 将经纬度转成geohash，找到邻近的geohash。对每个邻近的遍历前缀树，找到邻近的POI点，提取符合要求的。 看每个cookie的经纬度最近的POI是什么，最后得出每个POI的分布 3.1 ES的index1、pois_v2 ：所有的POI信息 123456789&#123; &quot;name&quot;: &quot;华光庙大街二巷8号&quot;, &quot;citycode&quot;: &quot;020&quot;, &quot;city&quot;: &quot;广州市&quot;, &quot;district&quot;: &quot;番禺区&quot;, &quot;street&quot;: &quot;钟村镇&quot;, &quot;location&quot;: &quot;22.984817,113.309482&quot;, &quot;slatlng&quot;: &quot;22.984817,113.309482&quot;&#125; 2、lbs_v2 ：每个cookie的位置属于哪个POI 12345678910111213&#123; &quot;id&quot;: &quot;17fa0f7bc0e06a5f9cac64547ce5b24b&quot;, &quot;ip&quot;: &quot;223.73.116.54&quot;, &quot;citycode&quot;: &quot;020&quot;, &quot;location&quot;: &quot;23.1024,113.28197&quot;, &quot;slatlng&quot;: &quot;23.1024,113.28197&quot;, &quot;city&quot;: &quot;广州市&quot;, &quot;district&quot;: &quot;海珠区&quot;, &quot;street&quot;: &quot;滨江街道&quot;, &quot;name&quot;: &quot;万寿门诊部&quot;, &quot;create_time&quot;: &quot;2017-12-05 21:03:25&quot;, &quot;index_time&quot;: &quot;2017-12-05&quot;&#125; 3.2 查询LBS api的使用： 3.2.1 通过POI点的关键词、城市找到按经纬度分布的POI数量123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354GET pois_v2/_search&#123; "size" : 10000, "query" : &#123; "bool" : &#123; "filter" : [ &#123; "terms" : &#123; "citycode" : [ "010" ], "boost" : 1.0 &#125; &#125; ], "should" : [ &#123; "match_phrase" : &#123; "name" : &#123; "query" : "星巴克", "slop" : 0, "boost" : 1.0 &#125; &#125; &#125; ], "disable_coord" : true, "adjust_pure_negative" : true, "minimum_should_match" : "1", "boost" : 1.0 &#125; &#125;, "aggregations" : &#123; "geo" : &#123; "terms" : &#123; "field" : "slatlng", "size" : 10000, "shard_size" : -1, "min_doc_count" : 1, "shard_min_doc_count" : 0, "show_term_doc_count_error" : false, "order" : [ &#123; "_count" : "desc" &#125;, &#123; "_term" : "asc" &#125; ] &#125; &#125; &#125;, "ext" : &#123; &#125;&#125; 3.2.2 通过关键词、城市，查询每个区，每个街道的POI点有多少PV和UVDSL为： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141GET lbs_v2/_search&#123; "size" : 0, "query" : &#123; "bool" : &#123; "filter" : [ &#123; "terms" : &#123; "citycode" : [ "010" ], "boost" : 1.0 &#125; &#125; ], "should" : [ &#123; "match_phrase" : &#123; "name" : &#123; "query" : "星巴克", "slop" : 0, "boost" : 1.0 &#125; &#125; &#125; ], "disable_coord" : true, "adjust_pure_negative" : true, "minimum_should_match" : "1", "boost" : 1.0 &#125; &#125;, "aggregations" : &#123; "city" : &#123; "terms" : &#123; "field" : "city", "size" : 1000, "shard_size" : -1, "min_doc_count" : 1, "shard_min_doc_count" : 0, "show_term_doc_count_error" : false, "order" : [ &#123; "_count" : "desc" &#125;, &#123; "_term" : "asc" &#125; ] &#125;, "aggregations" : &#123; "district" : &#123; "terms" : &#123; "field" : "district", "size" : 30, "shard_size" : -1, "min_doc_count" : 1, "shard_min_doc_count" : 0, "show_term_doc_count_error" : false, "order" : [ &#123; "_count" : "desc" &#125;, &#123; "_term" : "asc" &#125; ] &#125;, "aggregations" : &#123; "street" : &#123; "terms" : &#123; "field" : "street", "size" : 30, "shard_size" : -1, "min_doc_count" : 1, "shard_min_doc_count" : 0, "show_term_doc_count_error" : false, "order" : [ &#123; "_count" : "desc" &#125;, &#123; "_term" : "asc" &#125; ] &#125;, "aggregations" : &#123; "uniq" : &#123; "cardinality" : &#123; "field" : "id" &#125; &#125; &#125; &#125;, "uniq" : &#123; "cardinality" : &#123; "field" : "id" &#125; &#125; &#125; &#125;, "poi" : &#123; "terms" : &#123; "field" : "slatlng", "size" : 10000, "shard_size" : -1, "min_doc_count" : 1, "shard_min_doc_count" : 0, "show_term_doc_count_error" : false, "order" : [ &#123; "_count" : "desc" &#125;, &#123; "_term" : "asc" &#125; ] &#125;, "aggregations" : &#123; "uniq" : &#123; "cardinality" : &#123; "field" : "id" &#125; &#125; &#125; &#125;, "uniq" : &#123; "cardinality" : &#123; "field" : "id" &#125; &#125; &#125; &#125;, "uniq" : &#123; "cardinality" : &#123; "field" : "id" &#125; &#125; &#125;, "ext" : &#123; &#125;&#125;]]></content>
      <categories>
        <category>地理信息处理和挖掘</category>
      </categories>
      <tags>
        <tag>自用代码记录</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[fastText]]></title>
    <url>%2F2018%2F03%2F31%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FNLP%2Ffasttext%2F</url>
    <content type="text"><![CDATA[fastText 源码分析 fastText原理和实践 带监督的文本分类算法FastText 文本分类（六）：使用fastText对文本进行分类—小插曲 本地测试代码``]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>分类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何阅读代码]]></title>
    <url>%2F2018%2F03%2F25%2F%E6%80%BB%E7%BB%93%E4%B8%8E%E6%80%9D%E8%80%83%2F%E5%A6%82%E4%BD%95%E9%98%85%E8%AF%BB%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[从宏观到微观的原则。 即明确读代码的目的，找到关键问题，忽略中间的细节。 比如今天读w2v的增量学习代码，花了太多时间读小模型的具体实现，又看了不少理论，其实增量学习的逻辑只在后面一点，浪费了大量时间。]]></content>
      <categories>
        <category>总结与思考</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[相关文章]]></title>
    <url>%2F2018%2F03%2F23%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A%2F%E7%9B%B8%E5%85%B3%E6%96%87%E7%AB%A0%2F</url>
    <content type="text"><![CDATA[腾讯2017广告大赛的代码https://github.com/BladeCoda/Tencent2017_Final_Coda_Allegro 在线广告中基于转化率提升的竞价策略https://zhuanlan.zhihu.com/p/24801130]]></content>
      <categories>
        <category>计算广告</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python读取文件内容并写到kafka]]></title>
    <url>%2F2018%2F03%2F23%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FPYTHON%2F%E8%AF%BB%E5%8F%96%E6%96%87%E4%BB%B6%E5%86%85%E5%AE%B9%E5%B9%B6%E5%86%99%E5%88%B0kafka%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200#!/usr/bin/env python2# -*- coding: utf-8 -*-from kafka.producer import KafkaProducer import sysimport loggingimport osimport datetimeimport randomimport threading import timeparentPath = "/opt/git/tracking_ingest_HK/app/event"finishedLogPath = parentPath+"/finished_log.dat"logger = logging.getLogger("eventToKafka")logger.setLevel(logging.DEBUG)# 建立一个filehandler来把日志记录在文件里，级别为debug以上fh = logging.FileHandler(parentPath+"/event_to_kafka.log")fh.setLevel(logging.DEBUG)# 建立一个streamhandler来把日志打在CMD窗口上，级别为error以上ch = logging.StreamHandler()ch.setLevel(logging.ERROR)# 设置日志格式formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")ch.setFormatter(formatter)fh.setFormatter(formatter)#将相应的handler添加在logger对象中logger.addHandler(ch)logger.addHandler(fh)class Kafka_producer(): """ 使用kafka的生产模块 """ def __init__(self, kafkatopic, kafkapartition): self.kafkaTopic = kafkatopic self.kafkaPartition=kafkapartition self.producer = KafkaProducer(bootstrap_servers = ['10.11.70.141:9092','10.11.70.142:9092','10.11.70.143:9092']) def sendjsondata(self, jsonData): try: producer = self.producer producer.send(self.kafkaTopic, jsonData) #producer.flush() except Exception, e: logger.error(e) def flush(self): producer = self.producer producer.flush() def sendBatchJsonData(self, jsonData): try: curcount = len(jsonData)/self.kafkaPartition for i in range(0, self.kafkaPartition): start = i * curcount if i != (self.kafkaPartition - 1): end = (i+1) * curcount curdata = jsonData[start:end] self.producer.send(self.kafkaTopic, curdata) self.producer.flush() else: curdata = jsonData[start:] self.producer.send(self.kafkaTopic, curdata) self.producer.flush() except Exception, e: logger.error(e)def searchFile(path, keyword): fpList = [] for filename in os.listdir(path): fp = os.path.join(path, filename) if os.path.isfile(fp) and keyword in filename: fpList.append(fp) return fpListdef insertIntoSet(filePath): file = open(filePath) try: tempSet = set() for line in file: tempSet.add(line.replace('\n','')) except Exception, e: logger.error(e) finally: file.close() return tempSet class calthread(threading.Thread): #初始化函数 def __init__(self,threadname,cond,startN,endN,files): threading.Thread.__init__(self,name = threadname) self.cond = cond self.startN = startN self.endN = endN self.files = files #业务函数 def run(self): for i in range(self.startN,self.endN + 1): filePath = self.files[i] logger.info("current file is " + filePath) producer = Kafka_producer("event", 1) file = open(filePath) try: fileLines = 0 for line in file: arr = line.strip().split('\t') if len(arr) &gt; 0: try: producer.sendjsondata(arr[2]) producer.flush() #随机打印日志 if random.random() &lt; 0.00001: logger.info(arr[2]) fileLines += 1 except Exception, e: logger.error("current wrong file is %s" % (filePath)) logger.error("The wrong event log is %s" % (arr[2])) logger.error(e) continue logger.info("insert into kafka %s lines" % (str(fileLines))) except Exception, e: logger.error(e) finally: file.close()def main(argv=None): if argv == None: argv = sys.argv #获取线程锁 cond = threading.Condition() #已处理日志 finishedLog = set() finishedFile = open(finishedLogPath) try: for line in finishedFile: finishedLog.add(line.strip('\n')) finally: finishedFile.close() #incoming日志 incomingLog = set(searchFile("/mapr/hkidc.hadoop.iclick/staging/tracking/incoming/", "event.HK")) #待处理日志写入finished_log.dat todoLog = incomingLog - finishedLog if len(todoLog) == 0: return for i in todoLog: print(i) outfile = open(finishedLogPath, 'a') try: for i in todoLog: outfile.write(i + "\n") finally: outfile.close() todoList = list(todoLog) alen = len(todoList) threadN = alen #执行线程对象列表 threadL = [] t = alen / threadN logger.info( "初始化线程" ) for x in range(0,threadN): startN = x*t endN = 0 if((x+1)*t &gt;= alen): endN = alen - 1 else: if(x == threadN - 1): endN = alen - 1 else: endN = (x+1)*t - 1 #向列表中存入新线程对象 threadTT = calthread("Thread--"+str(x),cond,startN,endN,todoList) threadL.append(threadTT) #总计完成线程计数器 logger.info("Start time of threadCal--"+ str(time.time())) for a in threadL: a.start() logger.info("done")if __name__ == "__main__": main()]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas技巧总结]]></title>
    <url>%2F2018%2F03%2F23%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FPYTHON%2Fpandas%2F</url>
    <content type="text"><![CDATA[整理pandas操作 用 Python 做数据处理必看：12 个使效率倍增的 Pandas 技巧 创建数据随机数据创建一个Series，pandas可以生成一个默认的索引 1s = pd.Series([1,3,5,np.nan,6,8]) 通过numpy创建DataFrame，包含一个日期索引，以及标记的列 123456789101112dates = pd.date_range(&apos;20170101&apos;, periods=6)df = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list(&apos;ABCD&apos;))dfOut[4]: A B C D2016-10-10 0.630275 1.081899 -1.594402 -2.5716832016-10-11 -0.211379 -0.166089 -0.480015 -0.3467062016-10-12 -0.416171 -0.640860 0.944614 -0.7566512016-10-13 0.652248 0.186364 0.943509 0.0532822016-10-14 -0.430867 -0.494919 -0.280717 -1.3274912016-10-15 0.306519 -2.103769 -0.019832 0.035211 其中，np.random.randn可以返回一个随机数组 通过Dict创建12345678910111213df2 = pd.DataFrame(&#123; 'A' : 1., 'B' : pd.Timestamp('20130102'), 'C' : pd.Series(1,index=list(range(4)),dtype='float32'), 'D' : np.array([3] * 4,dtype='int32'), 'E' : pd.Categorical(["test","train","test","train"]), 'F' : 'foo' &#125;) Out[20]: A B C D E F0 1.0 2013-01-02 1.0 3 test foo1 1.0 2013-01-02 1.0 3 train foo2 1.0 2013-01-02 1.0 3 test foo3 1.0 2013-01-02 1.0 3 train foo 通过nparray创建12345678910111213data = [[2000,1,2],[2001,1,3]]df = pd.DataFrame(data, index=['one','two'], columns=['year','state','pop']) # 也可以转置后创建out = array([data_real_np, ydz_np]).Tdf = pd.DataFrame(out)df.to_csv('final.csv', encoding='utf-8', index=0, header=None) 创建TimeStamp有几个方法可以构造一个Timestamp对象 pd.Timestamp 1234567891011121314151617181920import pandas as pdfrom datetime import datetime as dtp1=pd.Timestamp(2017,6,19)p2=pd.Timestamp(dt(2017,6,19,hour=9,minute=13,second=45))p3=pd.Timestamp(&quot;2017-6-19 9:13:45&quot;)print(&quot;type of p1:&quot;,type(p1))print(p1)print(&quot;type of p2:&quot;,type(p2))print(p2)print(&quot;type of p3:&quot;,type(p3))print(p3)(&apos;type of p1:&apos;, &lt;class &apos;pandas.tslib.Timestamp&apos;&gt;)2017-06-19 00:00:00(&apos;type of p2:&apos;, &lt;class &apos;pandas.tslib.Timestamp&apos;&gt;)2017-06-19 09:13:45(&apos;type of p3:&apos;, &lt;class &apos;pandas.tslib.Timestamp&apos;&gt;)2017-06-19 09:13:45 to_datetime() 123456789101112131415import pandas as pdfrom datetime import datetime as dtp4=pd.to_datetime(&quot;2017-6-19 9:13:45&quot;)p5=pd.to_datetime(dt(2017,6,19,hour=9,minute=13,second=45))print(&quot;type of p4:&quot;,type(p4))print(p4)print(&quot;type of p5:&quot;,type(p5))print(p5)(&apos;type of p4:&apos;, &lt;class &apos;pandas.tslib.Timestamp&apos;&gt;)2017-06-19 09:13:45(&apos;type of p5:&apos;, &lt;class &apos;pandas.tslib.Timestamp&apos;&gt;)2017-06-19 09:13:45 读取数据读取csv1df = pd.read_csv(&apos;x.csv&apos;) 读取压缩包12345import zipfilewith zipfile.ZipFile(&apos;x.csv.zip&apos;, &apos;r&apos;) as z: f = z.open(&apos;x.csv&apos;) df = pd.read_csv(f, header=0) 保存为csv12out_df.to_csv(&apos;predict_result.csv&apos;, encoding=&apos;utf-8&apos;, index=0, header=None)# index=0即不要序号的列，header=None即不要表头 查看数据参考Basics section 查看数据类型12345678910df2.dtypesOut[30]: A float64B datetime64[ns]C float32D int32E categoryF objectdtype: object 查看head和tail12df.head(1)df.tail(3) 判断是否为空1df.empty 查看index、column和数据获取表头df.columns 1234df.indexdf.columnsdf.values df.count 显示数据的快速统计1234567891011df.describe()Out[19]: A B C Dcount 6.000000 6.000000 6.000000 6.000000mean 0.073711 -0.431125 -0.687758 -0.233103std 0.843157 0.922818 0.779887 0.973118min -0.861849 -2.104569 -1.509059 -1.13563225% -0.611510 -0.600794 -1.368714 -1.07661050% 0.022070 -0.228039 -0.767252 -0.38618875% 0.658444 0.041933 -0.034326 0.461706max 1.212112 0.567020 0.276232 1.071804 找缺失值https://blog.csdn.net/u012387178/article/details/52571725 1print(df_base_dpd[df_base_dpd.isnull().values == True]) 筛选数据转置1df.T 遍历traj_plot.py 123df = df.set_index(&apos;gpstime&apos;)for index, row in df.iterrows(): locationF.write(&quot;p%s | %s | %s | %s | %s &quot; % (str(cnt), index, str(row[0]), str(row[1]), str(row[2])) + &apos;\n&apos; ) 1234567891011121314151617181920212223242526import numpy as npimport pandas as pddef _map(data, exp): for index, row in data.iterrows(): # 获取每行的index、row for col_name in data.columns: row[col_name] = exp(row[col_name]) # 把结果返回给data return datadef _1map(data, exp): _data = [[exp(row[col_name]) # 把结果转换成2级list for col_name in data.columns] for index, row in data.iterrows() ] return _dataif __name__ == "__main__": inp = [&#123;'c1':10, 'c2':100&#125;, &#123;'c1':11,'c2':110&#125;, &#123;'c1':12,'c2':120&#125;] df = pd.DataFrame(inp) temp = _map(df, lambda ele: ele+1 ) print temp _temp = _1map(df, lambda ele: ele+1) res_data = pd.DataFrame(_temp) # 对2级list转换成DataFrame print res_data 排序通过列名来排序12#对于矩阵，axis=0表示行，1表示列df.sort_index(axis=1, ascending=False) 通过某一列的数值排序1df.sort_values(by=&apos;B&apos;) 1234567import pandas as pddf = pd.read_csv(&apos;./query_result.csv&apos;, sep=&apos;,&apos;)# 转为日期型df[&apos;gpstime&apos;] = pd.to_datetime(df[&apos;gpstime&apos;])# 按某一列排序df.sort_values([&apos;gpstime&apos;]) 选择series选择某行1df_app_no[0] #不需要跟列名，因为只有一列 选择某个行列的值1bad_predict_label.iloc[i, 0] 表示选择第i行，第0列的值 选择某一列1df[&apos;A&apos;] 某几列 123col_n = [&apos;名称&apos;,&apos;收盘价&apos;,&apos;日期&apos;]a = pd.DataFrame(df,columns = col_n) 选择某几行123df[0:3]#也可以通过行的索引来选择，但是不能单独写某一行df[&apos;20130102&apos;:&apos;20130104&apos;] 通过条件过滤行1df_pred.label[df_pred[&apos;label&apos;]&lt;1] 选择几列转为矩阵1coords=dftest.as_matrix(columns=[&apos;longitude&apos;,&apos;latitude&apos;]) 抽样1df_train_p = df_train_p.sample(frac=0.7) 过滤找到为null的1print(dfall[dfall.isnull().values == True]) pandas如何去掉、过滤数据集中的某些值或者某些行？ 删除某列1234567891011方法一：直接del DF[&apos;column-name&apos;]方法二：采用drop方法，有下面三种等价的表达式：1. DF= DF.drop(&apos;column_name&apos;, 1)；2. DF.drop(&apos;column_name&apos;,axis=1, inplace=True)# inplace=true表示对原DF操作，否则将结果生成在一个新的DF中3. DF.drop(DF.columns[ : ], axis=1,inplace=True) # Note: zero indexedx = [1,2] #删除多列需给定列表，否则参数过多df.drop(df.columns[x],axis=1,inplace=True) pandas删除列 根据时间范围过滤12df = df.set_index(&apos;gpstime&apos;)df[&apos;2018-04-22 01:00:00&apos;: &apos;2018-04-22 05:00:00&apos;] 某一列按条件过滤1234nightdf = nightdf[nightdf[&apos;speed&apos;]&lt;1]df06 = df04.loc[True - (float(df04.columns[-6]) &gt; 0.0)] 12# 验证有效的方法df_pred_raw.due_type3[(df_pred_raw[&apos;due_type3&apos;]&lt;12) &amp; (df_pred_raw[&apos;due_type3&apos;]&gt;5)] 行列同时过滤12wrong_idx = result[(result[&apos;pred&apos;]&lt;0.95) &amp; (result[&apos;label&apos;]&gt;0.05)].indexdf_pred_wrong = df_pred_raw.loc[wrong_idx, [&apos;yq30_term_rate_3&apos;,&apos;due_type3&apos;,&apos;zd_term_rate&apos;]] 用正则过滤123456df.filter(regex=(&quot;d.*&quot;))&gt;&gt; d1 d20 2 31 3 42 4 5 123456df.select(lambda col: col.startswith(&apos;d&apos;), axis=1)&gt;&gt; d1 d20 2 31 3 42 4 5 1df_trains = df_trains.filter(regex=(&quot;.*3days|label|.*7days&quot;)) groupby利用pandas进行数据分组及可视化 pandas聚合和分组运算——GroupBy技术(1) 例1 1234567891011121314from sklearn.datasets.samples_generator import make_blobsfrom matplotlib import pyplotfrom pandas import DataFrame# generate 2d classification datasetX, y = make_blobs(n_samples=100, centers=3, n_features=2)# dict中定义三个key，分别是坐标和label，再通过dict创建DataFramedf = DataFrame(dict(x=X[:,0], y=X[:,1], label=y))colors = &#123;0:'red', 1:'blue', 2:'green'&#125;fig, ax = pyplot.subplots()#groupby可以通过传入需要分组的参数实现对数据的分组grouped = df.groupby('label')for key, group in grouped: group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])pyplot.show() 例2 123456789101112131415161718192021import pandas as pdimport matplotlib.pyplot as plt# 把数据划分到自定义的区间中def cla(n,lim): return'[%.f,%.f)'%(lim*(n//lim),lim*(n//lim)+lim) # map function# 默认第一行是标题，从第二行开始是数据。sep是分隔符df = pd.read_csv('/home/david/iaudience-plan-statistics.csv', sep=',')# 设置某列的数据类型df['precent'] = df['precent'].astype('float64')# 对planid做group，group后对precent做sumgrouped = df['precent'].groupby(df['planid']).sum()c = pd.DataFrame(grouped)# 用c.precent或c['precent']都可以addone = pd.Series([cla(s,1) for s in c.precent])c['addone'] = addonegroups3 = c.groupby(['addone']).count()groups3['precent'].plot('bar')plt.show() 去重https://blog.csdn.net/xinxing__8185/article/details/48022401 12345678910111213from pandas import Series, DataFrame data = DataFrame(&#123;&apos;k&apos;: [1, 1, 2, 2]&#125;) print data IsDuplicated = data.duplicated() print IsDuplicated print type(IsDuplicated) data = data.drop_duplicates() print data DataFrame的duplicated方法返回一个布尔型Series,表示各行是否重复行。 而 drop_duplicates方法，它用于返回一个移除了重复行的DataFrame 这两个方法会判断全部列，你也可以指定部分列进行重复项判段。 例如，希望对名字为k2的列进行去重， data.drop_duplicates([‘k2’]) 多个DataFrame操作合并1234567objs 是需要拼接的对象集合，一般为列表或者字典axis=0 是行拼接，拼接之后行数增加，列数也根据join来定，join=’outer’时，列数是两表并集。同理join=’inner’,列数是两表交集。在默认情况下，axis=0为纵向拼接，此时有concat([df1,df2]) 等价于 df1.append(df2)在axis=1 时为横向拼接 ，此时有concat([df1,df2],axis=1) 等价于 merge(df1,df2,left_index=True,right_index=True,how=&apos;outer&apos;) 处理数据填充缺失值使用一个全局常量填充缺失值：将缺失值用同一个常数（如Unknown或﹣∞）替换。如果缺失值都用Unknown替换，则挖掘程序可能误认为它们形成了一个有趣的概念，因为它们都具有相同的值“Unknown”。因此此方法虽然简单但不可靠。 123data.Cabin.fillna(&apos;unknown&apos;,inplace=True)df_all.fillna(0, inplace=True) 使用属性的均值填充缺失值：例如，假定顾客的平均收入为56000美元，则使用该值替换income中的缺失值。 1data.Age.fillna(data.Age.mean(),inplace=True) 新建列并赋值1frame[&apos;panduan&apos;] = frame.city.apply(lambda x: 1 if &apos;ing&apos; in x else 0) Drop列1df1t.drop([&apos;app&apos;], axis=1, inplace=True) 选择列12col_imp = [&apos;mobile_city_code&apos;,&apos;label&apos;]df_train = pd.DataFrame(df_train, columns=col_imp) 改列名1a.rename(columns=&#123;&apos;A&apos;:&apos;a&apos;, &apos;C&apos;:&apos;c&apos;&#125;, inplace = True) 应用用kmeans聚类1234567891011121314151617181920212223242526272829303132import pandas as pdimport matplotlib.pyplot as plt#读取文本数据到DataFrame中，将数据转换为matrix，保存在dataSet中df = pd.read_table('d:/22.txt')dataSet = df.as_matrix(columns=None)# n_clusters=4，参数设置需要的分类这里设置成4类kmeans = KMeans(n_clusters=4, random_state=0).fit(dataSet)#center为各类的聚类中心，保存在df_center的DataFrame中给数据加上标签center = kmeans.cluster_centers_df_center = pd.DataFrame(center, columns=['x', 'y'])#标注每个点的聚类结果labels = kmeans.labels_#将原始数据中的索引设置成得到的数据类别，根据索引提取各类数据并保存df = pd.DataFrame(dataSet, index=labels, columns=['x', 'y'])df1 = df[df.index==0]df2 = df[df.index==1]df3 = df[df.index==2]df4 = df[df.index==3]#绘图plt.figure(figsize=(10,8), dpi=80)axes = plt.subplot()#s表示点大小，c表示color，marker表示点类型，DataFrame数据列引用参考博客其他文章type1 = axes.scatter(df1.loc[:,['x']], df1.loc[:,['y']], s=50, c='red', marker='d')type2 = axes.scatter(df2.loc[:,['x']], df2.loc[:,['y']], s=50, c='green', marker='*')type3 = axes.scatter(df3.loc[:,['x']], df3.loc[:,['y']], s=50, c='brown', marker='p')type4 = axes.scatter(df4.loc[:,['x']], df4.loc[:,['y']], s=50, c='black')#显示聚类中心数据点type_center = axes.scatter(df_center.loc[:,'x'], df_center.loc[:,'y'], s=40, c='blue')plt.xlabel('x', fontsize=16)plt.ylabel('y', fontsize=16)axes.legend((type1, type2, type3, type4, type_center), ('0','1','2','3','center'), loc=1)plt.show() 转ndarrayhttps://blog.csdn.net/flyfrommath/article/details/69388675 onehot后转dataframe12345678910111213141516171819202122232425262728293031323334353637383940414243444546import pandas as pdimport numpy as npfrom sklearn.datasets import load_irissns.set()%matplotlib inline#Iris Plotiris = load_iris()n_samples, m_features = iris.data.shape#Load DataX, y = iris.data, iris.targetD_target_dummy = dict(zip(np.arange(iris.target_names.shape[0]), iris.target_names))DF_data = pd.DataFrame(X,columns=iris.feature_names)DF_data[&quot;target&quot;] = pd.Series(y).map(D_target_dummy)#sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) \#0 5.1 3.5 1.4 0.2 #1 4.9 3.0 1.4 0.2 #2 4.7 3.2 1.3 0.2 #3 4.6 3.1 1.5 0.2 #4 5.0 3.6 1.4 0.2 #5 5.4 3.9 1.7 0.4 DF_dummies = pd.get_dummies(DF_data[&quot;target&quot;])#setosa versicolor virginica#0 1 0 0#1 1 0 0#2 1 0 0#3 1 0 0#4 1 0 0#5 1 0 0from sklearn.preprocessing import OneHotEncoder, LabelEncoderdef f1(DF_data): Enc_ohe, Enc_label = OneHotEncoder(), LabelEncoder() DF_data[&quot;Dummies&quot;] = Enc_label.fit_transform(DF_data[&quot;target&quot;]) DF_dummies2 = pd.DataFrame(Enc_ohe.fit_transform(DF_data[[&quot;Dummies&quot;]]).todense(), columns = Enc_label.classes_) return(DF_dummies2)%timeit pd.get_dummies(DF_data[&quot;target&quot;])#1000 loops, best of 3: 777 µs per loop%timeit f1(DF_data)#100 loops, best of 3: 2.91 ms per loop 问题记录Pycharm Pandas无法绘图最近用了pycharm，感觉还不错，就是pandas中Series、DataFrame的plot()方法不显示图片就给我结束了,但是我在ipython里就能画图 以前的代码是这样的 1234import matplotlib.pyplot as pltfrom pandas import DataFrame,SeriesSeries([4,5,7]).plot() 发现只要加个 12plt.show()就可以显示图像了了]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python容器总结]]></title>
    <url>%2F2018%2F03%2F23%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FPYTHON%2FPython%E5%AE%B9%E5%99%A8%2F</url>
    <content type="text"><![CDATA[列表[]列表是可变的，这是它区别于字符串和元组的最重要的特点，一句话概括即：列表可以修改，而字符串和元组不能。 创建直接创建 12list1 = [&apos;a&apos;,&apos;b&apos;]list2 = [1,2] list函数创建 12list3 = list(&quot;hello&quot;)print list3 输出 [‘h’, ‘e’, ‘l’, ‘l’, ‘o’] 1lista = [0] * 6 过滤1234[elem for elem in li if len(elem) &gt; 1]# 用filter过滤list(filter(lambda x: x[1]&lt;0, exp.local_exp[1])) 划分1234567a=[1,2,3,4]#不包括1a[:1]#输出[1]#包括2a[2:]#输出[3、4] 判断数组为空12if not nums: return None 遍历索引和元素12345678910#遍历列表, 打印索引和元素names = ['Tom', 'Jerry', 'Marry']for index, name in enumerate(names): print('names[&#123;&#125;] = &#123;&#125;'.format(index, name)) 打印结果:names[0] = Tomnames[1] = Jerrynames[2] = Marry 遍历元素1234567891011121314# 方法1print &apos;遍历列表方法1：&apos;for i in list: print(&quot;序号：%s 值：%s&quot; % (list.index(i) + 1, i))# 方法2print &apos;\n遍历列表方法2：&apos;for i in range(len(list)): print(&quot;序号：%s 值：%s&quot; % (i + 1, list[i]))# 方法3print &apos;\n遍历列表方法3：&apos;for i, val in enumerate(list): print(&quot;序号：%s 值：%s&quot; % (i + 1, val)) 遍历嵌套list123456789101112131415#coding=utf-8 listA = ['today','is','thursday']listB = ['today','is','fine','day','!']list1 = [listA,'恩哼？',listB]for i in list1: if isinstance(i, list): #查看list元素是否是list for j in i: #遍历嵌套的list print j, else : print i,--------------------- 作者：世界看我我看世界 来源：CSDN 原文：https://blog.csdn.net/seetheworld518/article/details/46756355 版权声明：本文为博主原创文章，转载请附上博文链接！ 把列表中某个值划分出去123if featureVec[axis] == value: reducedFeatVec = featureVec[:axis] reducedFeatVec.extend(featureVec[axis+1:]) 二维列表12345dataSet = [[1, 1, &apos;yes&apos;], [1, 1, &apos;yes&apos;], [1, 0, &apos;no&apos;], [0, 1, &apos;no&apos;], [0, 1, &apos;no&apos;]] 定义一个5×4的都是0的二维数组 1c=[[0 for i in range(4)] for j in range(5)] 合并1234circle_file = glob.glob(os.path.join(self.resource_dir, &apos;circle/*.png&apos;))table_file = glob.glob(os.path.join(self.resource_dir, &apos;table/*.png&apos;)) # 直接相加 self.jump_file = [cv2.imread(name, 0) for name in circle_file + table_file] generator转list1234import jieba# jieba的cut返回的是一个generatora = jieba.cut(&apos;我喜欢吃土豆&apos;)b = list(a) 列表扩展的两种方式123456789a=[1,2,3]b=[4,5,6]a.append(b)[1,2,3,[4,5,6]]a.extend(b)[1,2,3,4,5,6] 保存为csv元组()元组与列表一样，也是一种序列，唯一不同的是元组不能被修改（字符串其实也有这种特点）。 创建123456t1=1,2,3t2="jeffreyzhao","cnblogs"t3=(1,2,3,4)t4=()t5=(1,)print t1,t2,t3,t4,t5 输出： (1, 2, 3) (‘jeffreyzhao’, ‘cnblogs’) (1, 2, 3, 4) () (1,) 从上面我们可以分析得出： a、用逗号分隔一些值，元组自动创建完成； b、元组大部分时候是通过圆括号括起来的； c、空元组可以用没有包含内容的圆括号来表示； d、只含一个值的元组，必须加个逗号（,）； list转元组tuple函数和序列的list函数几乎一样：以一个序列作为参数并把它转换为元组。如果参数就是元组，那么该参数就会原样返回 12345678t1=tuple([1,2,3])t2=tuple(&quot;jeff&quot;)t3=tuple((1,2,3))print t1print t2print t3t4=tuple(123)print t4 输出： (1, 2, 3)(‘j’, ‘e’, ‘f’, ‘f’)(1, 2, 3) t4=tuple(123)TypeError: ‘int’ object is not iterable 元组保存到list和取出12345imgs = []imgs.append((words[0], int(words[1])))# 取出fn, label = self.imgs[index] 词典{}12345678prices = &#123; &apos;A&apos;:123, &apos;B&apos;:450.1, &apos;C&apos;:12, &apos;E&apos;:444,&#125;prices[&apos;A&apos;] 创建词典123456789&gt;&gt;&gt;dict() # 创建空字典&#123;&#125;&gt;&gt;&gt; dict(a='a', b='b', t='t') # 传入关键字&#123;'a': 'a', 'b': 'b', 't': 't'&#125;&gt;&gt;&gt; dict(zip(['one', 'two', 'three'], [1, 2, 3])) # 映射函数方式来构造字典&#123;'three': 3, 'two': 2, 'one': 1&#125; &gt;&gt;&gt; dict([('one', 1), ('two', 2), ('three', 3)]) # 可迭代对象方式来构造字典&#123;'three': 3, 'two': 2, 'one': 1&#125;&gt;&gt;&gt; 读取文件创建词典12345678910111213141516#读取代码fr = open(&apos;dic.txt&apos;,&apos;r&apos;)dic = &#123;&#125;keys = [] #用来存储读取的顺序for line in fr: v = line.strip().split(&apos;:&apos;) dic[v[0]] = v[1] keys.append(v[0])fr.close()print(dic)#写入文件代码 通过keys的顺序写入fw = open(&apos;wdic.txt&apos;,&apos;w&apos;)for k in keys: fw.write(k+&apos;:&apos;+dic[k]+&apos;\n&apos;) fw.close() 转list1li = dict.items() 结果类似于 1[(u&apos;11&apos;, 50808340), (u&apos;1101&apos;, 9842378)] 排序转为list后再排序 12 判断key是否存在12345678910#生成一个字典d = &#123;&apos;name&apos;:&#123;&#125;,&apos;age&apos;:&#123;&#125;,&apos;sex&apos;:&#123;&#125;&#125;#打印返回值print d.has_key(&apos;name&apos;)#结果返回True#python3if adict.has_key(key1): 改为if key1 in adict: 判断词典是否包含某个元素1234labelCount=&#123;&#125;for feature in dataSet: label = feature[-1] if label not in labelCount[label]: labelCount[label] = 0 词典的遍历iteritems 12345678910sentences = [&quot;我喜欢吃土豆&quot;,&quot;土豆是个百搭的东西&quot;,&quot;我不喜欢今天雾霾的北京&quot;]words = []for doc in sentences: words.append(list(jieba.cut(doc)))dic = corpora.Dictionary(words)for word,index in dic.token2id.iteritems(): print word + &apos;, index: &apos; + str(index) 在3.x 里 用 items()替换iteritems() 增加元素1234567891011121314#比如有个词典action = &#123; &quot;_index&quot;: elastic_urls_index, &quot;_type&quot;: doc_type_name, &quot;_id&quot;: data[0], &quot;_source&quot;: &#123; &quot;iclick_id&quot;: data[0], &quot;onsite_id&quot;: data[1], &quot;create_time&quot;: self.today_2 &#125;&#125;#要增加元素data[&apos;_soupyrce&apos;][&apos;age&apos;] = &apos;aa&apos; 提取文本的高频词1234567891011121314documents = ["Human machine interface for lab abc computer applications", "A survey of user opinion of computer system response time"]stoplist = set('for in and'.split())texts = [ [word for word in document.lower().split() if word not in stoplist ] for document in documents]from collections import defaultdictfrequency = defaultdict(int)for text in texts: for word in text: frequency[word]+=1texts = [ [word for word in text if frequency[word]&gt;1] for text in texts ] 映射mapping集合set定义1aaa = set() 增加1aaa.add(1) 判断是否在集合1if 1 in aaa: 数组转集合12a = [11,22,33,44,11,22] b = set(a) 通过set去除停用词123456documents = [&quot;Human machine interface for lab abc computer applications&quot;, &quot;A survey of user opinion of computer system response time&quot;]stoplist = set(&apos;for in and&apos;.split())texts = [ [word for word in document.lower().split() if word not in stoplist ] for document in documents] set增加数据123vocabSet = set([])for document in dataSet: vocabSet = vocabSet | set(document)]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala和java混编项目创建]]></title>
    <url>%2F2018%2F03%2F23%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FJAVA%2Fscala%E5%92%8Cjava%E6%B7%B7%E7%BC%96%E9%A1%B9%E7%9B%AE%E5%88%9B%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[现在有一个能用的scala和java混编项目，于是可以在这个基础上创建新的。步骤如下： 1、新建一个java的maven项目。 2、拷贝原项目中pom.xml的build和dependencies到新的pom.xml 3、在src/main下面新建scala目录 4、复制原项目在src/main下面的assembly目录 5、修改scala编译的版本到2.10.5]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[java出错总结]]></title>
    <url>%2F2018%2F03%2F23%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FJAVA%2Fjava%E5%87%BA%E9%94%99%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[MD5线程不安全MessageDigest线程不安全，多线程下会报错 12345678910111213java.lang.ArrayIndexOutOfBoundsException at java.lang.System.arraycopy(Native Method) at sun.security.provider.DigestBase.engineUpdate(DigestBase.java:114) at java.security.MessageDigest$Delegate.engineUpdate(MessageDigest.java:584) at java.security.MessageDigest.update(MessageDigest.java:335) at com.buzzinate.common.util.hash.MD5Util.getMD5String(MD5Util.java:98) at com.buzzinate.common.util.hash.MD5Util.getMD5String(MD5Util.java:94) 改用apache的commons-codec 123456789&lt;dependency&gt; &lt;groupId&gt;commons-codec&lt;/groupId&gt; &lt;artifactId&gt;commons-codec&lt;/artifactId&gt; &lt;version&gt;1.10&lt;/version&gt; &lt;/dependency&gt; 使用方法 1234567public static String encodeMD5Hex(String data) &#123; return DigestUtils.md5Hex(data); &#125; 该方法是线程安全的 Eclipse闪退每次闪退后都提示查看\workspace.metadata.log，发现有如下异常信息记录： 1234!ENTRY org.eclipse.e4.ui.workbench.swt 4 2 2016-08-23 08:42:49.516 !MESSAGE Problems occurred when invoking code from plug-in: &quot;org.eclipse.e4.ui.workbench.swt&quot;. !STACK 0 java.lang.IllegalArgumentException: Argument cannot be null 出现该问题的原因是：由于项目没有正常关闭运行而导致”workbench.xmi”中的”persistedState”标签还保持在运行时的配置造成的。 解决办法： 找到&lt;workspace&gt;/.metadata/.plugins/org.eclipse.e4.workbench/workbench.xmi文件，将其删掉，再重启Eclipse，恢复正常。 ConcurrentModificationExceptionhttp://www.cnblogs.com/dolphin0520/p/3933551.html]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>总结</tag>
        <tag>错题集</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[敏感词过滤]]></title>
    <url>%2F2018%2F03%2F23%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E6%95%8F%E6%84%9F%E8%AF%8D%E8%BF%87%E6%BB%A4%2F</url>
    <content type="text"><![CDATA[转自 敏感词过滤的算法原理之DFA算法]]></content>
      <categories>
        <category>算法与数据结构</category>
      </categories>
      <tags>
        <tag>DFA算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[所有回文子串]]></title>
    <url>%2F2018%2F03%2F23%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E6%89%80%E6%9C%89%E5%9B%9E%E6%96%87%E5%AD%90%E4%B8%B2%2F</url>
    <content type="text"><![CDATA[一种是用了3重循环的方法 1234567891011121314151617181920212223242526272829#include&lt;stdio.h&gt;#include&lt;string.h&gt;int main(int argc, char *argv[])&#123; char a[505]; int n,len,begin,maxBegin,i,j; freopen("29.in","r",stdin); scanf("%s",a); n=strlen(a); for(len=2;len&lt;=n;len++)//枚举子串的所有可能的长度 &#123; maxBegin=n-len; for(begin=0;begin&lt;=maxBegin;begin++)//枚举子串的开始点 &#123; j=begin+len-1; for(i=begin;i&lt;j;i++,j--) //遍历当前子串（a[i]~a[begin+len-1]）,判断是否回文串 &#123; if(a[i]!=a[j]) break; &#125; if(i&gt;=j)//是回文串 &#123; j=begin+len-1; for(i=begin;i&lt;=j;i++) printf("%c",a[i]); printf("\n"); &#125; &#125; &#125; return 0;&#125;]]></content>
      <categories>
        <category>算法与数据结构</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最长公共子序列]]></title>
    <url>%2F2018%2F03%2F23%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97%2F</url>
    <content type="text"><![CDATA[转自http://blog.csdn.net/littlethunder/article/details/25637173 序列a共同拥有m个元素，序列b共同拥有n个元素，假设a[m-1]==b[n-1]，那么a[:m]和b[:n]的最长公共子序列长度就是a[:m-1]和b[:n-1]的最长公共子序列长度+1；假设a[m-1]!=b[n-1]，那么a[:m]和b[:n]的最长公共子序列长度就是MAX（a[:m-1]和b[:n]的最长公共子序列长度，a[:m]和b[:n-1]的最长公共子序列长度）。 12345678910111213141516171819202122232425262728293031323334353637383940def lcs(a,b): lena=len(a) lenb=len(b) c=[[0 for i in range(lenb+1)] for j in range(lena+1)] flag=[[0 for i in range(lenb+1)] for j in range(lena+1)] for i in range(lena): for j in range(lenb): if a[i]==b[j]: c[i+1][j+1]=c[i][j]+1 flag[i+1][j+1]='ok' elif c[i+1][j]&gt;c[i][j+1]: c[i+1][j+1]=c[i+1][j] flag[i+1][j+1]='left' else: c[i+1][j+1]=c[i][j+1] flag[i+1][j+1]='up' return c,flagdef printLcs(flag,a,i,j): if i==0 or j==0: return if flag[i][j]=='ok': printLcs(flag,a,i-1,j-1) print(a[i-1],end='') elif flag[i][j]=='left': printLcs(flag,a,i,j-1) else: printLcs(flag,a,i-1,j) a='ABCBDAB'b='BDCABA'c,flag=lcs(a,b)for i in c: print(i)print('')for j in flag: print(j)print('')printLcs(flag,a,len(a),len(b))print('') 上图是执行结果，第一个矩阵是计算公共子序列长度的，能够看到最长是4；第二个矩阵是构造这个最优解用的；最后输出一个最优解BCBA。]]></content>
      <categories>
        <category>算法与数据结构</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F03%2F23%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E6%A0%91%E7%9A%84%E7%AE%97%E6%B3%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[二叉树插入12345678910111213141516171819202122232425262728class TreeNode: def __init__(self, val, left, right): self.val = val self.left = left self.right = right class BinarySearchTree: def insert(self, root, val): if root == None: root = TreeNode(val, None, None) else: if val &lt; root.val: root.left = self.insert(root.left, val) if val &gt; root.val: root.right = self.insert(root.right, val) return root def preOrder(self, root): if root: print root.val self.preOrder(root.left) self.preOrder(root.right)Tree = BinarySearchTree()root = Nonefor i in [1,2,3]: root = Tree.insert(root, i)Tree.preOrder(root) 有序数组创建二叉树123456789101112class Solution: def sortedArrayToBST(self, num): if not num: return None mid = len(num)//2 #“//”表示整数除法；“/”浮点数除法； root = TreeNode(num[mid]) left = num[:mid] right = num[mid+1:] root.left = self.sortedArrayToBST(left) root.right = self.sortedArrayToBST(right) return root 遍历二叉树algorithms/ 前序 根左右 中序 左根右 后序 左右根 递归方式 1234567891011121314151617181920class Tree(object): def __init__(self,data,left,right): self.data=data self.left=left self.right=rightdef post_visit(Tree): if Tree: post_visit(Tree.left) post_visit(Tree.right) print Tree.datadef pre_visit(Tree): if Tree: print Tree.data pre_visit(Tree.left) pre_visit(Tree.right)def in_visit(Tree): if Tree: in_visit(Tree.left) print Tree.data in_visit(Tree.right) 非递归 1234567891011121314151617181920212223class TreeNode: def __init__(self,value=None,leftNode=None,rightNode=None): self.value = value self.leftNode = leftNode self.rightNode = rightNode class Tree: def __init__(self,root=None): self.root = root def preOrder(self): if not self.root: return stackNode = [] stackNode.append(self.root) while stackNode: node = stackNode.pop() print node.value, if node.rightNode: stackNode.append(node.rightNode) if node.leftNode: stackNode.append(node.leftNode) 平衡二叉树输入一棵二叉树，判断该二叉树是否是平衡二叉树。 1234567891011121314151617class Solution: def getDepth(self , Root): if Root == None: return 0; lDepth = self.getDepth(Root.left); rDepth = self.getDepth(Root.right); return max(lDepth , rDepth) + 1; def IsBalanced_Solution(self, pRoot): if not pRoot: return True lDepth = self.getDepth(pRoot.left); rDepth = self.getDepth(pRoot.right); diff = lDepth - rDepth; if diff &lt; -1 or diff &gt; 1: return False; return self.IsBalanced_Solution(pRoot.left) and self.IsBalanced_Solution(pRoot.right); 判断一棵树是否为另一棵的子树给定两棵二叉树，判断T2是否是T1某棵子数的结构。 T1序列化成字符串str1; T2序列化成字符串str2; 用KMP算法判断str1中是否包含str2:如果str1包含str2,说明T1包含于T2结构一致的子树。 KMP算法 12345678910111213141516171819202122232425#KMP def kmp_match(s, p): m = len(s); n = len(p) cur = 0#起始指针cur table = partial_table(p) while cur&lt;=m-n: for i in range(n): if s[i+cur]!=p[i]: cur += max(i - table[i-1], 1)#有了部分匹配表,我们不只是单纯的1位1位往右移,可以一次移动多位 break else: return True return False #部分匹配表 def partial_table(p): '''''partial_table("ABCDABD") -&gt; [0, 0, 0, 0, 1, 2, 0]''' prefix = set() postfix = set() ret = [0] for i in range(1,len(p)): prefix.add(p[:i]) postfix = &#123;p[j:i+1] for j in range(1,i+1)&#125; ret.append(len((prefix&amp;postfix or &#123;''&#125;).pop())) return ret]]></content>
  </entry>
  <entry>
    <title><![CDATA[JAVA面试题总结]]></title>
    <url>%2F2018%2F03%2F23%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FJAVA%2FJAVA%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[HashMap和HashTableHashMap通过hashcode对其内容进行快速查找，而 TreeMap中所有的元素都保持着某种固定的顺序，如果你需要得到一个有序的结果你就应该使用TreeMap（HashMap中元素的排列顺序是不固定的）。 HashMap和TreeMaphttp://blog.csdn.net/fujiakai/article/details/51585767 1、继承的父类不同 Hashtable继承自Dictionary类，而HashMap继承自AbstractMap类。但二者都实现了Map接口。 2、线程安全性不同 ​ Hashtable 中的方法是Synchronize的，而HashMap中的方法在缺省情况下是非Synchronize的。在多线程并发的环境下，可以直接使用Hashtable，不需要自己为它的方法实现同步，但使用HashMap时就必须要自己增加同步处理。 3、contains方法 HashMap把Hashtable的contains方法去掉了，改成containsValue和containsKey，因为contains方法容易让人引起误解。 Hashtable则保留了contains，containsValue和containsKey三个方法，其中contains和containsValue功能相同。 4、key和value是否允许为null Hashtable中，key和value都不允许出现null值。HashMap中，null可以作为键，这样的键只有一个；可以有一个或多个键所对应的值为null。因此，在HashMap中不能由get()方法来判断HashMap中是否存在某个键， 而应该用containsKey()方法来判断。 5、遍历方式的内部实现上不同 Hashtable、HashMap都使用了 Iterator。而由于历史原因，Hashtable还使用了Enumeration的方式 。 6、hash值不同 HashTable直接使用对象的hashCode。而HashMap重新计算hash值。 hashCode是jdk根据对象的地址或者字符串或者数字算出来的int类型的数值。 Hashtable在求hash值对应的位置索引时，用取模运算，而HashMap在求位置索引时，则用与运算，且这里一般先用hash&amp;0x7FFFFFFF后，再对length取模，&amp;0x7FFFFFFF的目的是为了将负的hash值转化为正值，因为hash值有可能为负数，而&amp;0x7FFFFFFF后，只有符号外改变，而后面的位都不变。]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试经历]]></title>
    <url>%2F2018%2F03%2F23%2F%E6%80%BB%E7%BB%93%E4%B8%8E%E6%80%9D%E8%80%83%2F%E9%9D%A2%E8%AF%95%E7%BB%8F%E5%8E%86%2F</url>
    <content type="text"><![CDATA[触宝 20180314提的一些问题： 转化如何反馈到投放中？ DSP广告追求的平滑投放有什么意义？ 转化可以更加均衡，也能防止一些作弊行为。 公司的广告投放流程中有哪些是比较有特色的？ CVR和CTR预测有什么不同的难点，侧重点？ 如何检验CTR预测的效果？ 因为没有均衡采样，线下用AUC会不准。为什么不准？ 线上如何检验？直接看实际CTR有什么问题？ LR中的w和b分别是如何优化的？ 性别预测用贝叶斯，特征之间有依赖如何解决？ 这种时候用贝叶斯的效果就不好了，可以改用随机森林，GBDT等优秀分类器试试。 加上惩罚项。 每个算法还有什么可以改进的地方？ LR样本不均衡如何处理？对AUC有什么影响？如何均衡？从不均衡到均衡的过程中权重会如何变化？ 2345 20180315求一个字符串中所有的回文子串 一次调查表明意大利三分之一的汽车都是法拉利，并且在那之中一半的车都是红色的。如果你在意大利的街头看到一辆红色的汽车驶来，请问它是法拉利的可能性有多大？ 一位农民想搞明白是什么因素影响了他的牛奶产量。他记录了每天的气温（30 - 40 度）、湿度（60 - 90%）、饲料消耗（2000 - 2500 千克）以及牛奶产量（500 - 1000 升）。假设问题是要预测每天的牛奶产量，你会如何处理数据并建立模型？ 你有一个电商网站，当用户点击一个商品打开详情页面时，你想基于商品特征和用户的购买历史为用户推荐 5 个其他的商品显示在页面的底部。你需要哪些服务和数据表来实现这个功能？请写一个查询语句或一段过程式代码来返回所要推荐的 5 个商品。 特征比样本多的情况怎么办 矩阵就不是满秩的，如果数据集的特征比样本点还多怎么办？是否还可以使用线性回归来做预测？答案是否定的，因为在计算$(X^TX)^{-1}$ 的时候会出错。 为了解决这个问题，统计学家引入了岭回归（ridge regression）的概念。简单说来，岭回归就是在矩阵 $X^TX$加一个 $λI$ 使得矩阵非奇异，进而能对 $X^X+\lambda I$ 求逆。在这种情况下，回归系数的计算公式变为： w=(X^TX+\lambda I)^{-1}X^Ty 支持向量机中核函数的目的？如何选择？多分类怎么做？样本不平衡的影响？如何解决样本不平衡？ 核函数，他们公司首先用线性核函数，速度快。径向基慢。 数据量增大后，模型会遇到哪些瓶颈，如何解决？ 分为在线模型和离线模型。在线模型快速响应，离线模型扔到hadoop跑全量。 回归模型的优缺点？回归模型遇到分类样本如何解决？ 比较排序算法时间复杂度下界为nlogn的证明 排序算法的比较是两两进行的，所以可以抽象成一棵二叉树，相互比较的数分别是左右叶子结点，，比较的结果存储在父节点中，依此类推。那么算法的时间复杂度就是取决于树的深度。如果要对n个数字进行比较排序，则需要进行n!次，即该二叉树有n!片叶子。 一棵深度为d的二叉树拥有的叶子结点数最大为2d个，则具有n!片叶子的二叉树的深度为logn!。 logn!=logn+log(n-1)+log(n-1)+…+log(2)+log(1)≥logn+log(n-1)+log(n-2)+…+log(n/) ≥(n/2)log(n/2)≥(n/2)log(n/10) ≥(n/2)logn-(n/2)log10=(n/2)logn=O(nlogn) 所以比较排序的算法时间复杂度为O(nlogn) 爱奇艺 20180316 中序遍历二叉树（非递归） 二叉树的插入 从1-200中任意选出101个自然数,其中一个数必是另一个数的整数倍 1 0-1背包问题的变形2 LCS最长公共子序列问题3 平衡二叉树判定 word2vec是有监督还是无监督 头条 word2vec为什么快 word2vec与神经网络的区别 负采样模型，采样率对向量的影响]]></content>
      <categories>
        <category>总结与思考</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[eclipse技巧]]></title>
    <url>%2F2018%2F03%2F23%2F%E5%B7%A5%E5%85%B7%E5%92%8C%E7%8E%AF%E5%A2%83%2Feclipse%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[1234打开 Eclipse -&gt; Window -&gt; Perferences -&gt; Java -&gt; Editor -&gt; Content Assist，在右边最下面一栏找到 auto-Activation ，下面有三个选项，找到第二个“Auto activation triggers for Java：”选项在其后的文本框中会看到一个“.”存在。这表示：只有输入“.”之后才会有代码提示和自动补全，我们要修改的地方就是这里。把该文本框中的“.”换掉，换成“if.”，这样，你在Eclipse里面写Java代码就可以做到按“abcdefghijklmnopqrstuvwxyz.”中的任意一个字符都会有代码提示。 打开时an error has occurred方法很简单 就是删除 .metadata目录下.plugins/org.eclipse.e4.workbench即可]]></content>
      <categories>
        <category>工具和环境</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[maven技巧总结]]></title>
    <url>%2F2018%2F03%2F23%2F%E5%B7%A5%E5%85%B7%E5%92%8C%E7%8E%AF%E5%A2%83%2Fmaven%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[本地安装jar包12345mvn install:install-file -DgroupId=com.swagger -DartifactId=swagger-ui -Dversion=1.9.4 -Dpackaging=jar -Dfile=swagger-ui-1.9.4.jarmvn install:install-file -Dfile=a.jar -DgroupId=gid -DartifactId=aid -Dversion=0.0.1 -Dpackaging=jar -Dclassifier=sourcesmvn install:install-file -Dfile=a.jar -DgroupId=gid -DartifactId=aid -Dversion=0.0.1 -Dpackaging=jar -Dclassifier=javadoc 打包时输出jar包可以用 1mvn dependency:copy-dependencies 或者 123456789101112131415161718192021&lt;build&gt;&lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;copy&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;copy-dependencies&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;outputDirectory&gt; $&#123;project.build.directory&#125;/lib &lt;/outputDirectory&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 打包时包含lib的jar包fat jar方式打包 需要在pom文件的build节点的plugins节点内添加一个plugin，plugin内容如下：maven-shade-plugin 里面可以配置exclude和include的包 1234567891011121314151617181920212223242526272829&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;1.6&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;artifactSet&gt; &lt;includes&gt; &lt;include&gt;com.mljr.carthage:carthage-common-geo&lt;/include&gt; &lt;include&gt;com.mljr.carthage:carthage-common-utils&lt;/include&gt; &lt;include&gt;com.alibaba:fastjson&lt;/include&gt; &lt;include&gt;com.github.davidmoten:geo&lt;/include&gt; &lt;include&gt;com.github.davidmoten:grumpy-core&lt;/include&gt; &lt;include&gt;com.metamx:java-util&lt;/include&gt; &lt;/includes&gt; &lt;/artifactSet&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 如果只要某个jar包 1234567891011121314151617181920&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;1.6&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;artifactSet&gt; &lt;includes&gt; &lt;include&gt;com.mljr.carthage:carthage-common-geo&lt;/include&gt; &lt;/includes&gt; &lt;/artifactSet&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; include里面填jar包的groupId和artifactId maven上传至中央仓库1、确认settings.xml中是否有用户密码 123456789101112&lt;servers&gt; &lt;server&gt; &lt;id&gt;buzzinate&lt;/id&gt; &lt;username&gt;buzzinate&lt;/username&gt; &lt;password&gt;buzzinate&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;buzzinate-snapshots&lt;/id&gt; &lt;username&gt;buzzinate&lt;/username&gt; &lt;password&gt;buzzinate&lt;/password&gt; &lt;/server&gt;&lt;/servers&gt; 2、如果是SNAPSHOT版本，在POM中增加 12345678 &lt;distributionManagement&gt; &lt;snapshotRepository&gt; &lt;id&gt;buzzinate-snapshots&lt;/id&gt; &lt;name&gt;snapshots name&lt;/name&gt; &lt;url&gt;http://117.121.96.174:8081/nexus/content/repositories/snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt; &lt;/distributionManagement&gt; &lt;/project&gt; 重点是id需要跟settings中一致 3、增加源码插件，上传后可以看到源码和javadoc 123456789101112131415&lt;plugin&gt; &lt;!-- 源码插件 --&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-source-plugin&lt;/artifactId&gt; &lt;version&gt;2.1&lt;/version&gt; &lt;!-- 发布时自动将源码同时发布的配置 --&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;attach-sources&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;jar&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; 4、执行mvn deploy 5、如果有时候编译失败，可能是因为网络不通，导致元数据读取失败了，用 mvn compile -U 强制更新 本地jar包的deploy1mvn deploy:deploy-file -Dmaven.test.skip=true -Dfile=/Users/david/david/code/jar/json-serde-1.1.9.9-Hive1.2-jar-with-dependencies.jar -DgroupId=json -DartifactId=serde -Dversion=1.1.9.9-Hive1.2-jar-with-dependencies -Dpackaging=jar -DrepositoryId=mljr-releases -Durl=http://maven.youjie.com/content/repositories/releases/ 常见错误 1[ERROR] Failed to execute goal org.apache.maven.plugins:maven-deploy-plugin:2.7:deploy-file (default-cli) on project standalone-pom: Failed to deploy artifacts: Could not transfer artifact json:serde:jar:1.1.9.9-Hive1.2-jar-with-dependencies from/to mljr-releases (http://maven.youjie.com/content/groups/public): Failed to transfer file: http://maven.youjie.com/content/groups/public/json/serde/1.1.9.9-Hive1.2-jar-with-dependencies/serde-1.1.9.9-Hive1.2-jar-with-dependencies.jar. Return code is: 400, ReasonPhrase: Bad Request. -&gt; [Help 1] 下载source1mvn clean package -DdownloadSources=true setting.xml文件参考：maven设置———setting.xml文件学习 部署到服务器1、mvn clean package打包，如果要修改生成jar包的名称，要改 build -&gt;finalname属性 2、服务器上，复制jar包，新建conf，复制配置文件新建lib，复制lib下的jar包新建shell，编写启动脚本 123456export JAVA_HOME=/usr/local/lib/jvm/jdk1.7.0_71/CLASSPATH=conf:log4jtest.jarfor f in lib/*.jar; do CLASSPATH=$&#123;CLASSPATH&#125;:$fdone$JAVA_HOME/bin/java -server -Dfile.encoding=UTF-8 -Xms2G -Xmx2G -XX:+UseConcMarkSweepGC -XX:+CMSIncrementalMode -XX:+CMSIncrementalPacing -XX:CMSIncrementalDutyCycleMin=0 -XX:CMSIncrementalDutyCycle=10 -XX:MaxNewSize=1024M -XX:MaxPermSize=256M -XX:+DisableExplicitGC -cp $CLASSPATH com.iclick.rocket.examples.main.log4jMain &gt; test.log 其中，Xms1G JVM初始分配的堆内存 报错和解决setting问题12No marketplace entries found to handle maven-compiler-plugin:2.3.2:compile in Eclipse. Please see Help for more information.No marketplace entries found to handle maven-compiler-plugin:2.3.2:testCompile in Eclipse. Please see Help for more information. 在window-preferences-maven-user settings改为本地的maven配置如果还有问题，检查settings.xml文件 导入项目时报错安装weka-stable失败解决：用mvn install -Dmaven.test.skip=true跳过单元测试 jdk-tools12Description Resource Path Location TypeThe container &apos;Maven Dependencies&apos; references non existing library &apos;/home/david/.m2/repository/jdk/tools/jdk.tools/1.7/jdk.tools-1.7.jar&apos; pig-ext Build path Build Path Problem 解决：直接在terminal输入 1mvn install:install-file -DgroupId=jdk.tools -DartifactId=jdk.tools -Dpackaging=jar -Dversion=1.7 -Dfile=tools.jar -DgeneratePom=true 在目标路径新增了相关目录，接着下载jdk.tools-1.7.jar并cp过去类似的增加cascading、riffle、thirdparty 然后再mvn clean package 之后报错 12Description Resource Path Location Typemaven-dependency-plugin (goals &quot;copy-dependencies&quot;, &quot;unpack&quot;) is not supported by m2e. pom.xml /pig-ext line 175 Maven Project Build Lifecycle Mapping Problem 解决：在plugins之前，build下面添加如下pluginManagement 12345678910111213141516171819202122232425262728&lt;pluginManagement&gt;&lt;plugins&gt;&lt;plugin&gt;&lt;groupId&gt;org.eclipse.m2e&lt;/groupId&gt;&lt;artifactId&gt;lifecycle-mapping&lt;/artifactId&gt;&lt;version&gt;1.0.0&lt;/version&gt;&lt;configuration&gt;&lt;lifecycleMappingMetadata&gt;&lt;pluginExecutions&gt;&lt;pluginExecution&gt;&lt;pluginExecutionFilter&gt;&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;&lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;&lt;versionRange&gt;[2.0,)&lt;/versionRange&gt;&lt;goals&gt;&lt;goal&gt;copy-dependencies&lt;/goal&gt;&lt;/goals&gt;&lt;/pluginExecutionFilter&gt;&lt;action&gt;&lt;ignore /&gt;&lt;/action&gt;&lt;/pluginExecution&gt;&lt;/pluginExecutions&gt;&lt;/lifecycleMappingMetadata&gt;&lt;/configuration&gt;&lt;/plugin&gt;&lt;/plugins&gt;&lt;/pluginManagement&gt; 如果还不行然后在plugins中增加一个 123456789101112131415161718&lt;plugin&gt;&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;&lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;&lt;executions&gt;&lt;execution&gt;&lt;id&gt;copy-dependencies&lt;/id&gt;&lt;phase&gt;package&lt;/phase&gt;&lt;goals&gt;&lt;goal&gt;copy-dependencies&lt;/goal&gt;&lt;/goals&gt;&lt;configuration&gt;&lt;outputDirectory&gt;$&#123;project.build.directory&#125;/lib&lt;/outputDirectory&gt;&lt;excludeTransitive&gt;false&lt;/excludeTransitive&gt;&lt;stripVersion&gt;true&lt;/stripVersion&gt;&lt;/configuration&gt;&lt;/execution&gt;&lt;/executions&gt;&lt;/plugin&gt; 并把原来的maven-dependency-plugin注释掉 报错 Missing artifact jdk.tools:jdk.tools:jar:1.7 输入 1mvn install:install-file -DgroupId=jdk.tools -DartifactId=jdk.tools -Dpackaging=jar -Dversion=1.7 -Dfile=tools.jar -DgeneratePom=true 并且在pom中加上 1&lt;dependency&gt; &lt;groupId&gt;jdk.tools&lt;/groupId&gt; &lt;artifactId&gt;jdk.tools&lt;/artifactId&gt; &lt;version&gt;1.7&lt;/version&gt;&lt;/dependency&gt; 或者把java Build Path里面的Libraries中的JRE System Libraries(JRE7)改为jdk7 编码GBK的不可映射字符在POM中加上 1234567891011121314&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.7&lt;/source&gt; &lt;target&gt;1.7&lt;/target&gt; &lt;encoding&gt;utf8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; maven几种打包的方式https://blog.csdn.net/u011955252/article/details/78927175 当你使用 Maven 对项目打包时，你需要了解以下 3 个打包 plugin，它们分别是 plugin function maven-jar-plugin maven 默认打包插件，用来创建 project jar maven-shade-plugin 用来打可执行包，executable(fat) jar maven-assembly-plugin 支持定制化打包方式，例如 apache 项目的打包方式 depedency排除12345678910111213141516171819&lt;dependency&gt; &lt;groupId&gt;org.lightgbm.predict4j&lt;/groupId&gt; &lt;artifactId&gt;lightgbm_predict4j&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-over-slf4j&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-over-slf4j&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; 找到setting的路径12# 获取setting文件的读取顺序mvn -X aliyun配置之前是这么配的 12345&lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;external:cloudera&lt;/mirrorOf&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;/mirror&gt; 现在配置成 12345&lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;/mirror&gt; 不太理解这两种的区别，好像都可以]]></content>
      <categories>
        <category>工具和环境</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sublime技巧]]></title>
    <url>%2F2018%2F03%2F23%2F%E5%B7%A5%E5%85%B7%E5%92%8C%E7%8E%AF%E5%A2%83%2Fsublime%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[空格代替Tab123456菜单栏里点击 Preferences-&gt; Setting-User//把 tab 转换成4个空格 &quot;tab_size&quot;: 4, //把tab 转换成 空格 &quot;translate_tabs_to_spaces&quot;: true pretty json格式化快捷键：control+cmd+j 去掉空行CTRL+H打开replace功能，勾选上左侧的regular expression，并填写 find what栏 : \s+$ （正则表达式） replace with栏 : （这行留空） 接着点replace all即可 放大字体command+ 选择列按住option和触摸板左下角，然后拖拉 修改快捷键在preference -&gt; key bindings 123[ &#123; &quot;keys&quot;: [&quot;super+d&quot;], &quot;command&quot;: &quot;run_macro_file&quot;, &quot;args&quot;: &#123;&quot;file&quot;: &quot;res://Packages/Default/Delete Line.sublime-macro&quot;&#125; &#125;] 查找括号ctrl+m sql格式化https://blog.csdn.net/avaloon/article/details/78639848 SqlBeautifier Mac先按command+K，再按command+F，即可格式化SQL。 大写转小写Ctrl+KU 改为大写 Ctrl+KL 改为小写]]></content>
      <categories>
        <category>工具和环境</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[kafka的log存储]]></title>
    <url>%2F2018%2F03%2F23%2Fhadoop-spark%2Fkafka%E7%9A%84log%E5%AD%98%E5%82%A8%2F</url>
    <content type="text"><![CDATA[转自https://www.cnblogs.com/dorothychai/p/6181058.html]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[新词发现]]></title>
    <url>%2F2018%2F03%2F23%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FNLP%2F%E6%96%B0%E8%AF%8D%E5%8F%91%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[代码在 1/spark-buzzads/src/main/scala/com/iclick/word_segmentation/WordSegment.scala 自己测试的代码在 1/spark-test/src/main/scala/com/iclick/word_segmentation/WordTest.scala 新词发现的原理是 中文新词发现算法解析 互联网时代的社会语言学：基于SNS的文本数据挖掘 从凝固度和自由度两个角度考虑你通过一元分词的相邻词组合而成的“新词”是否是语境中真正的一个词。 算法原理把可能成词的文本片段全部提取出来，再跟已有词库比较，找出新词。 怎样的文本片段可以认为是词？ 凝固度光看词频是不够的，比如“的电影”出现300次，“电影院”出现100次。因为电影和院凝固的更紧一些。 首先要枚举凝固方式 令 p(x) 为文本片段 x 在整个语料中出现的概率，那么我们定义“电影院”的凝合程度就是 p(电影院) 与 p(电) · p(影院) 比值和 p(电影院) 与 p(电影) · p(院) 的比值中的较小值，“的电影”的凝合程度则是 p(的电影) 分别除以 p(的) · p(电影) 和 p(的电) · p(影) 所得的商的较小值。 自由度把一个文本片段的自由运用程度定义为它的左邻字信息熵和右邻字信息熵中的较小值。如果太小，就不能成单独的词（因为都是跟着左、右一起出现的）。 抽词后，按照频度从高到低排序。 定义候选词的最大长度，再为出现频数、凝固程度和自由程度各设定一个阈值，然后只需要提取出所有满足阈值要求的候选词即可。 左右邻字的熵取较小值。 熵较小，说明该词往往是固定搭配，不能成词。 示例文本是 ab我 ef cd 步骤1、对语料用标点分隔，这里不需要去掉停用词。 2、在设定的最长词长度内，提取所有可能成词的词（至少大于2,小于max长度），并保存每个词的左邻近和右邻近字。 3、计算 123val conf = new SparkConf().setAppName(&quot;wordSegname&quot;).setMaster(&quot;local[4]&quot;). set(&quot;spark.sql.shuffle.partitions&quot;,&quot;10&quot;).set(&quot;spark.network.timeout&quot;,&quot;30s&quot;) local[4]是指在本地运行，用4核CPU。 spark.sql.shuffle.partitions是指partition的数量。SparkSQL在运行时，将一个查询任务分解成多个task，一个task就是一个partition。默认是200个partition，而如果实际集群只能并行3个task，则跑完200个partition要200/3=67次。 spark.network.timeout是指所有网络通信的超时时间，默认是120s 123456val word1=sc.textFile(path).map&#123;x=&gt; val x_filter=x.replaceAll(&quot;\p&#123;Punct&#125;&quot;, &quot; &quot;).replaceAll(&quot;\pP&quot;, &quot; &quot;) .replaceAll(&quot; &quot;, &quot; &quot;).replaceAll(&quot;[&quot; + AtomsUitl.stopwords + &quot;]&quot;, &quot; &quot;).replaceAll(&quot;\p&#123;Blank&#125;&quot;, &quot; &quot;).replaceAll(&quot;\p&#123;Space&#125;&quot;, &quot; &quot;).replaceAll(&quot;\p&#123;Cntrl&#125;&quot;, &quot; &quot;) x_filter &#125; 停用词，标点等转换为空格。 replaceAll中是正则表达式。上文中，是将所有的特殊字符都用空格代替 AtomsUitl.stopwords停用词是”的很了么呢是嘛个都也比还这于不与才上用就好在和对挺去后没说” sc.textFile读取文件后，生成一个RDD，以行为单位，所以后面的map是对每行的操作 1val sum_document = word1.count() 打印出所有的元素，用 word1.foreach { x =&gt; println(x) } 12345678val word_document = word1.zipWithIndex.filter &#123; x =&gt; !StringUtils.isBlank(x._1) &#125;.flatMap &#123; x =&gt; val arr = ArrayBuffer[(String, Int)]() val line = x._1.split(&quot; &quot;) //对于每一行，都用空格分割 for (i &lt;- line) &#123; arr += ((i, x._2.toInt)) //分割后，每一个tuple加到数组中 &#125; arr &#125;.map &#123; x =&gt; (x._1.trim, x._2) &#125;.filter(x =&gt; !StringUtils.isBlank(x._1)) zipWithIndex用带有index的来压缩RDD，索引从0开始 word1.zipWithIndex.foreach { x =&gt; println(x) } (ab ef,0) (cd,1) 上述代码得到的结果是 (ab,0) (ef,0) (cd,1) 1234567891011121314151617val wordleft = word.map(x =&gt; AtomsUitl.reverse(x)).map &#123; x =&gt; &quot;&quot; + x + &quot;&quot; &#125;.flatMap &#123; x =&gt; var arr = ArrayBufferString for (y &lt;- 1 to AtomsUitl.len(x) - 2) &#123; // arr+=x.substring(y, Math.min(maxLen + y, x.length())) arr += AtomsUitl.substring(x, y, Math.min(maxLen + y, AtomsUitl.len(x))) &#125; arr&#125;.sortBy(x =&gt; x) 将每个句子倒序排列，提取每个子集 今$ 四期星天今 处言语然自 天今$ 星天今$ 期星天今$ 然自$ 理处言语然 自$ 言语然自 语然自$ $ \123456789101112131415161718val wordleft_caculate = wordleft.map &#123;​ s =&gt;​ val first = AtomsUitl.substring(s, 0, 1).toString​ (first, s)​ &#125;.groupBy(f =&gt; f._1).map &#123;​ x =&gt; x._2​ &#125;wordleft_caculate.foreach&#123;x=&gt; println(x.iterator.next())&#125; \ groupBy之后得到 (期, CompactBuffer((期,期星天今$))等 这个是Iterable，可迭代的。可以转换为一个迭代器x.iterator.next()。迭代出来就是 (期,期星天今$) 等 结果如： [維他命,105,5.103775510263332,2.120730528309974,41,3362341] [红庙路口,10,3.969704104467617,1.3592367006650063,6,3362341] [红裙艳丽,8,4.516740790602718,1.2554823251787535,4,3362341] [绛侯,85,3.85163302224936,1.8033243982880292,37,3362341] 其中，第一列是新词，第二列是总词频，第三列是凝聚度，第四列是左熵右熵取最小值，第五列是出现该词的文档数，最后一列是总文档数]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[GBDT]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FGBDT%2F</url>
    <content type="text"><![CDATA[tips 论文中GBDT的参数，树的数量最多500颗（500以上就没有提升了），每棵树的节点不多于12。 https://blog.csdn.net/shine19930820/article/details/71713680 原理算法原理整理 简单易学的GBDT原理 GBDT公式推导 GBDT算法原理深入解析 GBDT详解 梯度提升算法Freidman提出了梯度提升算法，该方法是利用最速下降法的近似方法，其关键是利用损失函数的负梯度在当前模型的值 -[{\partial L(y,f(x_i)) \over \partial f(x_i)}]_{f(x) = f_{m-1}(x)} 这个公式中，每个参数什么意思 作为回归问题算法中的残差的近似值，拟合一个回归模型。 其算法流程如下： $F0(x) = argmin\rho \sum _{i=1}^N L(y_i, \rho)$ For $m = 1$ to $M$ do: $\qquad \tilde yi = -[{\partial L(y,F(x_i)) \over \partial F(x_i)}]{F(x) = F_{m-1}(x)}, i = 1, N$ $\qquad am = argmin{a,\beta}\sum_{i=1}^N[\tilde y_i - \beta h(x_i; a)]^2$ $\qquad \rhom = argmin\rho \sum{i=1}^N L(y_i, F{m-1}(x_i) + \rho h(x_i; a_m))$ $\qquad Fm(x) = F{m-1}(x) + \rho_m h(x;a_m)$ endForendAlgorighm 其中$h(x_i;a_m)$表示基本分类器（weak learner or base learner），4中$a_m$表示拟合负梯度能力最好的分类器参数负梯度只是表示下降的方向，但是下降多少没有确定，5中$\rho_m$可以认为是下降最快的步长，可以让Loss最小，可以用线性搜索的方式来估计$\rho_m$的值 为何这里不直接利用负梯度来调节，而是需要用一个分类器来拟合呢？因为这里的负梯度是在训练集上求出的，不能被泛化测试集中。我们的参数是在一个函数空间里面，不能使用例如SGD这样的求解方式。使用一个分类器来拟合，是一个泛化的方式。 回归树当我们的基本分类器是一个包含J个节点的回归树时，回归树模型可以表示为 h(x;\{b_j, R_j\}_1^J) = \sum_{b=j}^Jb_jI(x\in R_j) \qquad (8)其中${ R_j }_1^J$不相交的区域，它们的集合覆盖了预测值的空间，${ b_j }_1^J$是叶子节点的值，可以认为是模型$h$的系数 利用回归树模型，算法流程6中的公式可以被替换为：F_m(x) = F_{m-1}(x) + \rho_m \sum_{j=1}^J b_{jm}I(x \in R_{jm})\qquad (9) 其中${ R_{jm} }_1^J$是第m次迭代生成的树所产生的区域。第m次迭代的树用来预测流程3中由流程4中平方误差产生的${\tilde y_i}_i^N$ ${ b{jm}}$可以被表示为 $$b{jm} = ave{x_i \in R{jm}} \tilde y_i$$ 即用平均值表示该叶子节点拟合的值 有了下降的方向，我们还需要最好的步长，缩放因子$\rho_m$是流程5中线性搜索方式的一种解决方案 从上面可以看出，我们是先求的$b{jm}$，然后在求解$\rho_m$，我们能否同时求解呢？另$\gamma{jm} = \rho{m}b{jm}$，公式9可以被表示为：F_m(x) = F_{m-1}(x) + \sum_{j=1}^J \gamma_{jm}I(x \in R_{jm})\qquad (10) 通过优化如下公式来获取最优的系数$\gamma_{jm}$： \{\gamma_{jm}\}_1^J = argmin_{\ \gamma_j {\ _1^J}}\sum_{i=1}^N L\left(y_i, F_{m-1}(x_i) + \sum_{j=1}^J\gamma_jI(x \in R_{jm})\right)\qquad 1)由于回归树产生的叶子节点各个区域之间是不相交的，且所有的样本最终都会属于某个叶子节点，所以公式11可以表示为： \gamma_{jm} = argmin_\gamma \sum_{x_i\in R_{jm}} L(y_i, F_{m-1}(x_i) + \gamma)给定当前$F{m-1}(i)$，$\gamma{jm}$可以作为叶子节点的值，该值可以看做是基于损失函数L的每个叶子节点的最理想的常数更新值，也可以认为$\gamma_{jm}$是即有下降方向又有下降步长的值。 综上，用回归树作为基本分类器的梯度提升算法流程可以如下表示： $F0(x) = argmin\rho \sum _{i=1}^N L(y_i, \rho)$ For $m = 1$ to $M$ do: $\qquad \tilde yi = -[{\partial L(y,F(x_i)) \over \partial F(x_i)}]{F(x) = F_{m-1}(x)}, i = 1, N$ $\qquad {R_{jm}}_1^J = J-terminal\, node\, tree({ \tilde y_i, x_i }_i^N)$ $\qquad \gamma{jm} = argmin\gamma \sum{x_i\in R{jm}} L(yi, F{m-1}(x_i) + \gamma)$ $\qquad Fm(x) = F{m-1}(x) + \sum{j=1}^J \gamma{jm}I(x \in R_{jm})$ endForendAlgorighm 其中3是计算残差（利用损失函数的负梯度在当前模型的值作为残差的近似值），4是拟合一颗含有J个叶子节点的回归树，5是估计回归树叶子节点的值 下面我们看一下二元分类、多元分类、回归中残差的计算、叶子节点值的估计。 Two-class logistic regression and classification我们用negative binomial log-likehood作为我们的损失函数： L(y, F) = log(1 + exp(-2yF)), y \in {-1, 1}\qquad (12)其中F(x) = {1\over2}log\left[{Pr(y=1|x) \over Pr(y=-1|x)}\right]\qquad (13)公式13是logit函数，log odds 如上公式是Freidman的论文中使用的公式，我认为使用在逻辑回归中常见的$L(y, F) = ylogF + (1-y)log(1-F)$，其中$F(z) ={ 1\over{1+exp(-z)}}$也可以 计算残差：\tilde y_i = -[{\partial L(y,F(x_i)) \over \partial F(x_i)}]_{F(x) = F_{m-1}(x)} = {2y_i\over 1+exp(2y_iF_{m-1}(x_i))}\qquad(14) 叶子节点值的估计： \gamma_jm = argmin_\gamma \sum_{x_i \in R_{jm}} log(1+exp(-2y_i(F_{m-1}(x_i) + \gamma)))\qquad (15)可以通过一步Newton-Raphson来近似公式15，估计结果为： \gamma_{jm} = {\sum_{x_i \in R_{jm}}\tilde y_i \over {\sum_{x_i \in R_{jm}}}|\tilde y_i|(2-|\tilde y_i|)}最终得到的$F_M(x)$与对数几率 log-odds相关，我们可以用来进行概率估计 F(x) = {1\over2}log\left({p \over 1-p}\right)e^{2F(x)} = {p\over(1-p)}P_+(x) = p = {e^{2F(x)}\over 1+e^{2F(x)}} = {1\over1+e^{-2F(x)}}P_-(x) = 1-p = {1\over1+e^{2F(x)}}有了概率之后，我们接下来就可以利用概率进行分类 Multi-class logistic regression and classification我们使用multi-class log-loss作为损失函数： L(\{y_k, F_k(x)\}_1^K) = -\sum_{k=1}^K y_klogp_k(x)\qquad(16)其中使用softmax来计算概率：p_k(x) = exp(F_k(x)) / \sum_{l=1}^Kexp(F_l(x))\qquad(17) 从公式17可以得出，对于多分类问题，我们需要为每个类别创建一颗回归树$F_l(x)\, l=1,2,…,k$ 计算残差： \tilde y_{ik} = -[{\partial L(\{y_{il},F_l(x_i)\}_{l=1}^K) \over \partial F_k(x_i)}]_{\{F_l(x) = F_{l, m-1(x)}\}_1^K} = y_{ik} - p_{k,m-1(i)}\qquad (18)我们假定共分为3类，那么logloss为： L = -y_1log{exp(F_1(x))\over exp(F_1(x)) + exp(F_1(x)) + exp(F_1(x))} -y_2log{exp(F_2(x))\over exp(F_1(x)) + exp(F_1(x)) + exp(F_1(x))} _3log{exp(F_3(x))\over exp(F_1(x)) + exp(F_1(x)) + exp(F_1(x))}{\partial L \over \partial F_1(x)} = -y_1 + y_1p_1 + y_2p_2 + y_3p_3{\partial L \over \partial F_2(x)} = y_1p_1 - y_2 + y_2p_2 + y_3p_3{\partial L \over \partial F_3(x)} = y_1p_1 + y_2p_2 - y_3 + y_3p_3如果当期样本的类别为(1,0,0)，那么 {\partial L \over \partial F_1(x)} = -1 + p1{\partial L \over \partial F_2(x)} = p_2{\partial L \over \partial F_3(x)} = p_3取负梯度，则 -{\partial L \over \partial F_1(x)} = 1 - p_1-{\partial L \over \partial F_2(x)} = -p_2 = 0 - p_2-{\partial L \over \partial F_3(x)} = -p_3 = 0 - p_3符合公式18中的$\tilde y{ik} = y{ik} - p_{k,m-1(x_i)}$ 叶子节点值的估计： \{r_{jkm}\} = argmin_{\gamma_{jk}}\sum_{i=1}^N \sum_{k=1}^K \phi \left( y_{ik}, F_{k,m-1}(x_i) + \sum_{j=1}^J\gamma_{jk}I(x_i \in {jm})\}\right)\qquad(19)可以通过一步Newton-Raphson来近似公式19，估计结果为： \gamma_{jkm} = {K-1\over K}{\sum_{x_i \in R_{jkm}}\tilde y_{ik} \over {\sum_{x_i \in R_{jkm}}}|\tilde y_{ik}|(1-|\tilde y_{ik}|)}Regression我们使用Least-squares作为损失函数：L(y, F) = {(y-F)^2\over 2} 计算残差：\tilde y_i = -[{\partial L(y,F(x_i)) \over \partial F(x_i)}]_{F(x) = F_{m-1}(x)} = {y_i - F_{m-1}(x_i)}\qquad(20) 叶子节点值的估计： \gamma_{jm} = argmin_\gamma \sum_{x_i \in R_{jm}} {1\over 2}(y_i - (F_{m-1}(x_i) + \gamma))^2\qquad (21)\gamma_{jm} = argmin_\gamma \sum_{x_i \in R_{jm}} {1\over 2}(y_i - F_{m-1}(x_i) - \gamma)^2\gamma_{jm} = argmin_\gamma \sum_{x_i \in R_{jm}} {1\over 2}(\tilde y_i - \gamma)^2容易得出以下结果：\gamma_{jm} = ave_{x_i \in R_{jm}} \tilde y_i 回归树的创建拟合残数是一个回归问题，所以在分割样本时，我们不会采用基尼指数（Gini）、信息增益（IG）等用于分类的标准。我们可以选用MSE(mean square error impurity criterion)作为分割样本的标准。也可是采用Friedman在论文中的the least-squares improvement criterion，公式如下： i_2(R_l, R_r) = {w_lw_r\over w_l + w_r}(\bar y_l - \bar y_r)^2其中$\bar y_l \, \bar y_r$分别是左右孩子的平均值，$w_l \, w_r$分别是左右孩子对应的权重和 本文是针对具体的损失函数进行的相关推导，泛化能力差，大家可以参考xgboost作者的这篇文章，作者进行了更加一般的推导，这一个抽象的形式对于实现机器学习工具也是非常有帮助的。 引用：Greedy Function Approximation: A Gradient Boosting Machine 与RF的区别实现GBDT的python实现 python配置XGB参考 gbdt http://www.itopmarketing.com/index.php/News/show/id/7951/lmid/197/utm_source/tuicool/utm_medium/referral/ 【关于点击率模型，你知道这三点就够了】 http://www.cnblogs.com/zhouxiaohui888/p/6008368.html 【xgboost原理及应用】 官网介绍的原理：https://xgboost.readthedocs.io/en/latest/model.html 原理的中文解释http://dataunion.org/15787.html https://github.com/dmlc/xgboost https://github.com/Schwimmer/xgboost/tree/master/jvm-packages/xgboost4j-example/src/main/scala/ml/dmlc/xgboost4j/scala/example/spark GBDT （Gradient Boost Decision Tree）是一种常用的非线性模型[6][7][8][9]，它基于集成学习中的boosting思想[10]，每次迭代都在减少残差的梯度方向新建立一颗决策树，迭代多少次就会生成多少颗决策树。 GBDT 的思想使其具有天然优势可以发现多种有区分性的特征以及特征组合，决策树的路径可以直接作为LR输入特征使用，省去了人工寻找特征、特征组合的步骤。这种通过 GBDT 生成LR特征的方式（ GBDT +LR），业界已有实践（Facebook，Kaggle-2014），且效果不错 来源： http://blog.csdn.net/lilyth_lilyth/article/details/48032119 XGBoost http://blog.csdn.net/dusj1993/article/details/51925387【 在集群上部署xgboost踩过的坑】 http://blog.csdn.net/u010306433/article/details/51403894 【xgboost 分布式部署教程】 mvn install:install-file -Dfile=D:\code\jar_package\xgboost4j-spark-0.5-jar-with-dependencies.jar -DgroupId=ml.dmlc -DartifactId=xgboost4j -Dversion=0.5 -Dpackaging=jar ml.dmlc xgboost4j 0.7 \1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556http://blog.csdn.net/zhangweijiqn/article/details/53214186XGBoost4J:Distributed XGBoost for Scala/Java (XFBoost 4 JVM environment)，目前看来还比较小众，文档不够多，网上xgboost4j的资料也很少，社区不够活跃。Portable Distributed XGBoost in Spark, Flink and Dataflow: http://dmlc.ml/2016/03/14/xgboost4j-portable-distributed-xgboost-in-spark-flink-and-dataflow.htmlScala和Spark等分布式的包在jvm-packages下。安装：http://xgboost.readthedocs.io/en/latest/jvm/目前安装仅支持从源码安装:$ git clone --recursive https://github.com/dmlc/xgboost$ cd xgboost/jvm-packages$ mvn package安装scala/java的jvm版xgboost:$ cd jvm-packages/xgboost4j$ mvn install:install-file -Dfile=target/xgboost4j-0.7.jar -DgroupId=ml.dmlc -DartifactId=xgboost4j -Dversion=0.7 -Dpackaging=jar安装spark版的xgboost:$ cd jvm-packages/xgboost4j-spark$ mvn install:install-file -Dfile=target/xgboost4j-spark-0.7.jar -DgroupId=ml.dmlc -DartifactId=xgboost4j-spark -Dversion=0.7 -Dpackaging=jarMaven pom.xml file:&lt;dependency&gt;&lt;groupId&gt;ml.dmlc&lt;/groupId&gt;&lt;artifactId&gt;xgboost4j-spark&lt;/artifactId&gt;&lt;version&gt;0.7&lt;/version&gt;&lt;/dependency&gt;Spark code，参考https://github.com/dmlc/xgboost/tree/master/jvm-packages，分别有RDD版本和DataFrame版本:importml.dmlc.xgboost4j.scala.spark.XGBoost\ 用maven安装xgboost4j时候的坑 执行 mvn clean -DskipTests=true package 1、checkstyle报错 xgboost4j的pom中增加了checkstyple插件，在win7下package会报错 \12345678method name must match patternno-trailing-spaces等\ 解决：在pom.xml中去掉checkstyle插件 \1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556 &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-checkstyle-plugin&lt;/artifactId&gt; &lt;version&gt;2.17&lt;/version&gt; &lt;configuration&gt; &lt;configLocation&gt;checkstyle.xml&lt;/configLocation&gt; &lt;failOnViolation&gt;true&lt;/failOnViolation&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;checkstyle&lt;/id&gt; &lt;phase&gt;validate&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;check&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;checkstyle&lt;/id&gt; &lt;phase&gt;validate&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;check&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt;\ 2、用scala的例子BasicWalkThrough.scala报错 \1234ERROR [main] (DMatrix.java:41) - java.io.FileNotFoundException: File /lib/xgboost4j.dll was not found inside JAR.\ 参考：https://github.com/dmlc/xgboost/issues/1148 https://www.ibm.com/developerworks/community/blogs/jfp/entry/Installing_XGBoost_For_Anaconda_on_Windows?lang=en 需要装MINGW，然后make jvm，但是会报错 \1234mingw32-make: *** No rule to make target &apos;jvm&apos;. Stop.\ 应该在有MakeFile的目录下执行，也就是根目录D:\gitlab\xgboost。并且，要从git上手动下载项目中的dmlc-core和rabit，再执行，继续报错 \123456789101112131415161718192021222324252627282930process_begin: CreateProcess(NULL, uname, ...) failed.mingw32-make: Makefile:44: pipe: No errorg++ -std=c++0x -Wall -Wno-unknown-pragmas -Iinclude -Idmlc-core/include -Irabit/include -O3 -funroll-loops -msse2 -fopenmp -MM -MT build/learner.o src/learner.cc &gt;build/learner.dg++ -c -std=c++0x -Wall -Wno-unknown-pragmas -Iinclude -Idmlc-core/include -Irabit/include -O3 -funroll-loops -msse2 -fopenmp src/learner.cc -o build/learner.o子目录或文件 -p 已经存在。处理: -p 时出错。子目录或文件 build 已经存在。处理: build 时出错。Makefile:113: recipe for target &apos;build/logging.o&apos; failedmingw32-make: *** [build/logging.o] Error 1\ 应该用git-bash来执行，然后接着报错 \12\ https://github.com/dmlc/xgboost/issues/1267 跟着一步步执行，再报错 \1234cc1plus.exe: sorry, unimplemented: 64-bit mode not compiled in\ windows安装失败，在ubuntu环境下安装 CARThttps://www.cnblogs.com/qwj-sysu/p/5974421.html 离散值处理 https://www.cnblogs.com/qwj-sysu/p/5981231.html 连续值处理 经典算法详解—CART分类决策树、回归树和模型树 https://blog.csdn.net/jiede1/article/details/76034328]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Numpy技巧总结]]></title>
    <url>%2F2018%2F03%2F17%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FPYTHON%2FNumpy%2F</url>
    <content type="text"><![CDATA[创建数据创建ndarrayNumPy的数组类被称作ndarray。通常被称作数组。 Numpy库中的矩阵模块为ndarray对象，有很多属性：T，data, dtype,flags,flat,imag,real,size, itemsize,nbytes,ndim,shape,strides,ctypes,base等等。 12345678import numpy as npvector = np.array([10,20,30])matrix=np.array([[1,2,3],[4,5,6],[7,8,9]])# 创建10个float32的一维数组np.random.rand(10).astype(np.float32)# 这样就是10*2的二维数组np.random.rand(10,2).astype(np.float32) 随机arraynp.random.randn可以返回一个随机数组 123456789np.random.randn(1,2)Out[11]: array([[-2.67809797, 1.49728361]])# 在0-5之间生成随机数np.random.rand(2,3)*5# 或者np.dot(5,np.random.rand(2,3))# 指定生成随机数的范围np.random.randint(0, 20, size=[2,3]) np.random.rand 随机样本位于[0,1)中 np.random.randn 从标准正态分布$N=(\mu , \sigma ^2)$中返回样本，默认的范围是$N(0,1)$，等价于np.random.standard_normal 如果要返回2*4的$N(3,6.25)$的随机分布，可知均值是3，标准差是2.5，则 12&gt; 3+2.5*np.random.randn(2,4)&gt; 指定随机数类型1x_data = np.random.rand(100).astype(np.float32) 创建空的array1np.zeros((5,1)) 注意有两层括号，因为参数是一个shape 随机数的seed1numpy.random.seed() seed( ) 用于指定随机数生成时所用算法开始的整数值，如果使用相同的seed( )值，则每次生成的随即数都相同，如果不设置这个值，则系统根据时间来自己选择这个值，此时每次生成的随机数因时间差异而不同。 123456from numpy import *num=0while(num&lt;5): random.seed(5) print(random.random()) num+=1 每次都一样 123456from numpy import *num=0random.seed(5)while(num&lt;5): print(random.random()) num+=1 每次不一样 也就是一次有效。 创建等差数列创建等差数列，默认是创建50个，一般写成 1x2 = np.linspace(1,10,10) 由函数创建1234567&gt;&gt;&gt; def func (i):... return i%4+1&gt;&gt;&gt; np. fromfunction(func, (10 ,))array([ 1., 2., 3., 4., 1., 2., 3., 4., 1., 2.]) 首先，定义一个func函数，模4加1。然后，调用np对象的fromfunction内建函数，第一个参数是我们自定义的func,第二个参数（m,n），他处理的逻辑是这个样的：第一行第一列取（0，0）带入func函数，第一行第二列取（0，1）带入func函数，第一行第三列取（0，2）带入func函数……循环往复，直到取到值（0,n-1）带入函数以后，开始取第二行。。因为我们定义的函数只有一个参数，所以m从0取到9即可。最后返回数列：array([ 1., 2., 3., 4., 1., 2., 3., 4., 1., 2.]) 。 读取csv转为numpy假设第一行是描述，第二行起是数据；第一列是标签，后面是特征项 12345import numpy as npdata = np.loadtxt(open(&apos;sample.csv&apos;,&apos;rb&apos;), delimiter=&apos;,&apos;, skiprows=1)y_train = data[:,0]x_train = data[:,1:-1] np.random.randn可以返回一个随机数组 123np.random.randn(1,2)Out[11]: array([[-2.67809797, 1.49728361]]) np.random.rand 随机样本位于[0,1)中 np.random.randn 从标准正态分布$N=(\mu , \sigma ^2)$中返回样本，默认的范围是$N(0,1)$，等价于np.random.standard_normal 如果要返回2*4的$N(3,6.25)$的随机分布，可知均值是3，标准差是2.5，则 12&gt; 3+2.5*np.random.randn(2,4)&gt; 如果在matlib模块中使用，则返回的是matrix而不是array 1234import numpy.matlibnp.matlib.randn(1,2)Out[13]: matrix([[ 0.13107513, -0.87977247]]) 矩阵变换shape12vector_shape = vector.shapematrix_shape = matrix.shape (3,) (3, 3) reshape生成新矩阵 12345678910111213141516&gt;&gt;&gt; xdataarray([2, 4, 0, 3, 5, 2, 6, 0, 1, 8])&gt;&gt;&gt; len(xdata)10&gt;&gt;&gt; xdata.reshape(5,-1)array([[2, 4], [0, 3], [5, 2], [6, 0], [1, 8]])&gt;&gt;&gt; xdata.reshape(5,2)array([[2, 4], [0, 3], [5, 2], [6, 0], [1, 8]]) 指定了第一维后，第二维可以不指定，写为-1 flatten把(a,b,c,d)的X reshape为(b*c*d, a) 1X_flatten = X.reshape(-1, X.shape[0]) 获得x的转置1print(x.T) 123[[1 4 7] [2 5 8] [3 6 9]] 返回数组内部的信息 1print(x.flags) 123456C_CONTIGUOUS : TrueF_CONTIGUOUS : FalseOWNDATA : TrueWRITEABLE : TrueALIGNED : TrueUPDATEIFCOPY : False 将数组变为1维数组，并获取其中的一部分数据 1print(x.flat[2:6]) 1[3 4 5 6] 将值赋给1维数组，再转化成有原有数组的大小形式 12x.flat=4;xprint(x) 123[[4 4 4] [4 4 4] [4 4 4]] 轴的个数（秩） 1print(x.ndim) 2 数组的维度，翻坠一个整数构成的元组。元组的长度就是秩 1print(x.shape) (3,3) 可以取任意维的shape 12# 取到第二维print(x.shape[:2]) 数组元素的总数 1print(x.size) 9 axis理解NumPy数组的维数称为轴（axes），轴的个数叫秩（rank），一维数组的秩为1，二维数组的秩为2。 Stackoverflow系列(1) -Python Pandas与Numpy中axis参数的二义性 取数组元素123x = np.array([2, 4, 0, 3, 5])# 不包括倒数第一个x[:-1] [2,4,0,3] 123x=np.array([[1,2,3],[4,5,6],[7,8,9]])# 二维数组，逗号前后表示要取的行和列，:就是全部取，0:2就是取第0列和第1列，不包括第2列print(x[:,0:2]) 123[[1 2] [4 5] [7 8]] 如果只取一列，下面这种形式就会变成一个一位数组，要加上一个[]，才可以维持原有的二维数组的形式。 1print(x[:,-1]) 1[3 6 9] 1print(x[:,[-1]]) 123[[3] [6] [9]] 排序numpy教程：排序、搜索和计数 默认是升序排序。 1234567list1 = [[1,3,2], [3,5,4]]array = numpy.array(list1)array = sort(array, axis=1) #对第1维升序排序#array = sort(array, axis=0) #对第0维print(array)[[1 2 3] [3 4 5]] 降序排序的实现: 123array = -sort(-array, axis=1) #降序[[3 2 1] [5 4 3]] 参考 【1】numpy中的ndarray方法和属性 运算、索引、切片http://blog.csdn.net/liangzuojiayi/article/details/51534164 矩阵的各类乘法dot product点积 a \cdot b = a_1b_1+a_2b_2...a_nb_n1234567x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]### VECTORIZED DOT PRODUCT OF VECTORS ###tic = time.process_time()dot = np.dot(x1,x2)toc = time.process_time()print ("dot = " + str(dot) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms") 只有两个值是普通数组的时候才可以是点积，如果是np.array，则dot会变成矩阵乘法。也就是 123x1 = np.array([[1,2,3]])x2 = np.array([[1,2,3]])np.dot(x1,x2) 会报错 1ValueError: shapes (1,3) and (1,3) not aligned: 3 (dim 1) != 1 (dim 0) outer product外积1234x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]### VECTORIZED OUTER PRODUCT ###outer = np.outer(x1,x2) element-wise multipulation按位乘1mul = np.multiply(x1,x2) general dot product矩阵乘法12W = np.random.rand(3,len(x1))dot = np.dot(W,x1) 可以看出dot既可以用作点积，也可以执行矩阵乘法 Broadcasting广播用以描述numpy中对两个形状不同的阵列进行数学计算的处理机制。较小的阵列“广播”到较大阵列相同的形状尺度上，使它们对等以可以进行数学计算。广播提供了一种向量化阵列的操作方式，因此Python不需要像C一样循环。广播操作不需要数据复制，通常执行效率非常高。然而，有时广播是个坏主意，可能会导致内存浪费以致计算减慢。 Numpy操作通常由成对的阵列完成，阵列间逐个元素对元素地执行。最简单的情形是两个阵列有一样的形状，例如： 1234&gt;&gt;&gt; a = np.array([1.0, 2.0, 3.0])&gt;&gt;&gt; b = np.array([2.0, 2.0, 2.0])&gt;&gt;&gt; a * barray([ 2., 4., 6.]) Numpy的广播机制放宽了对阵列形状的限制。最简单的情形是一个阵列和一个尺度值相乘： 1234&gt;&gt;&gt; a = np.array([1.0, 2.0, 3.0])&gt;&gt;&gt; b = 2.0&gt;&gt;&gt; a * barray([ 2., 4., 6.]) 上面两种结果是一样的，我们可以认为尺度值b在计算时被延展得和a一样的形状。延展后的b的每一个元素都是原来尺度值的复制。延展的类比只是一种概念性的。实际上，Numpy并不需要真的复制这些尺度值，所以广播运算在内存和计算效率上尽量高效。 上面的第二个例子比第一个更高效，因为广播在乘法计算时动用更少的内存。 expbroadcast运算 12x = np.array([1,2,3])np.exp(x) sumbroadcast运算。 1234def softmax(x): x_exp = np.exp(x) x_sum = np.sum(x_exp, axis=1, keepdims=True) s = x_exp/x_sum matrixarray转matrix12s = np.array([5,5,0,0,0,5])np.matrix(s) 加载数据loadtxt1numpy.loadtxt(fname, dtype=&lt;type &apos;float&apos;&gt;, comments=&apos;#&apos;, delimiter=None, converters=None, skiprows=0, usecols=None, unpack=False, ndmin=0)[source] 参数 fname : file, str, or pathlib.Path File, filename, or generator to read. If the filename extension is .gz or .bz2, the file is first decompressed. Note that generators should return byte strings for Python 3k. dtype : data-type, optional Data-type of the resulting array; default: float. If this is a structured data-type, the resulting array will be 1-dimensional, and each row will be interpreted as an element of the array. In this case, the number of columns used must match the number of fields in the data-type. comments : str or sequence, optional The characters or list of characters used to indicate the start of a comment; default: ‘#’. delimiter : str, optional The string used to separate values. By default, this is any whitespace. converters : dict, optional A dictionary mapping column number to a function that will convert that column to a float. E.g., if column 0 is a date string: converters = {0: datestr2num}. Converters can also be used to provide a default value for missing data (but see also genfromtxt):converters = {3: lambda s: float(s.strip() or 0)}. Default: None. skiprows : int, optional Skip the first skiprows lines; default: 0. usecols : int or sequence, optional Which columns to read, with 0 being the first. For example, usecols = (1,4,5) will extract the 2nd, 5th and 6th columns. The default, None, results in all columns being read. New in version 1.11.0. Also when a single column has to be read it is possible to use an integer instead of a tuple. E.g usecols = 3 reads the fourth column the same way as usecols = (3,)` would. unpack : bool, optional If True, the returned array is transposed, so that arguments may be unpacked using x, y, z = loadtxt(...). When used with a structured data-type, arrays are returned for each field. Default is False. ndmin : int, optional The returned array will have at least ndmin dimensions. Otherwise mono-dimensional axes will be squeezed. Legal values: 0 (default), 1 or 2. New in version 1.6.0. 返回 out : ndarray Data read from the text file. genfromtxt12import numpynfl = numpy.genfromtxt(&quot;data.csv&quot;, delimiter=&quot;,&quot;) 12# U75就是将每个值作为一个75 byte的unicode来读取world_alcohol = np.genfromtxt(&apos;world_alcohol.csv&apos;, dtype=&apos;U75&apos;, skip_header=1, delimiter=&apos;,&apos;) 12345data = np.genfromtxt(&apos;/Users/david/david/code/00project/carthage/scripts/adult.data&apos;, delimiter=&apos;, &apos;, dtype=str)# 取第14列labels = data[:,14]# 取除了倒数第二列之外的所有列data = data[:,:-1] matrix转数组1np.argsort(y_score, kind=&quot;mergesort&quot;)[::-1] 随机数字的矩阵12345678910import numpy as npnumpy_matrix = np.random.randint(10, size=[5,2])‘’‘array([[1, 0], [8, 4], [0, 5], [2, 9], [9, 9]])’‘’ 获取排序后数据位置的下标12345import numpy as npdd=np.mat([4,5,1]) dd1 = dd.argsort()print ddprint dd1 #matrix([[2, 0, 1]], dtype=int64) squeeze从数组的形状中删除单维条目，即把shape中为1的维度去掉 12x = np.array([[[0], [1], [2]]]) np.squeeze(x) 1array([0, 1, 2]) 如果本来就是(1,1)的矩阵，则变成常数 12cost = np.array([[1]])cost = np.squeeze(cost) 得到1，cost的shape变成() 获取符合条件的行列集合数据如 12341,1,1,0,0,00,1,1,1,1,01,0,0,1,1,00,0,0,1,1,0 第一列作为y_train，后面矩阵作为x_train，需要获取y_train中为1的x_train的行 12pos_rows = (y_train == 1)x_train[pos_rows,:] 还有个例子 12vector = numpy.array([5, 10, 15, 20])vector == 10 [False, True, False, False] 123456matrix = numpy.array([ [5, 10, 15], [20, 25, 30], [35, 40, 45] ]) matrix == 25 12345[ [False, False, False], [False, True, False], [False, False, False]] 比如要找第二列中是25的那一行 12345678matrix = np.array([ [5, 10, 15], [20, 25, 30], [35, 40, 45] ]) second_column_25 = (matrix[:,1] == 25) # 等同于print(matrix[second_column_25]) print(matrix[second_column_25, :]) 123[ [20, 25, 30]] 多个条件的比较 12vector = numpy.array([5, 10, 15, 20])equal_to_ten_and_five = (vector == 10) &amp; (vector == 5) [False, False, False, False] 12vector = numpy.array([5, 10, 15, 20])equal_to_ten_or_five = (vector == 10) | (vector == 5) [True, True, False, False] 也可以根据比较的结果改变值 1234vector = numpy.array([5, 10, 15, 20])equal_to_ten_or_five = (vector == 10) | (vector == 5)vector[equal_to_ten_or_five] = 50print(vector) true的都变成了50 [50, 50, 15, 20] 判断条件并转成0和1的输出12345678910# x是numpy的数组，如# x = np.array([-1.0, 1.0, 2.0])def step_function(x): return np.array(x &gt; 0, dtype=np.int) # 或者def step_function(x): y = x &gt; 0 # astype把bool转成int return y.astype(np.int)]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[硬币问题]]></title>
    <url>%2F2018%2F03%2F17%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E7%A1%AC%E5%B8%81%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[转自 动态规划之硬币组合问题 动态规划的本质是将原问题分解为同性质的若干相同子结构，在求解最优值的过程中将子结构的最优值记录到一个表中以避免有时会有大量的重复计算。 例如硬币组合问题，若求凑够11元的最少硬币数，可以先从凑够0元、1元、2元……的子结构开始分析。 假设d(i)为凑够i元所需最少硬币数，则 d(0) = 0 理所当然 d(1) = 1 要凑够1元，需要从面值小于等于1元的硬币中选择，目前只有面值为1元的硬币 此时d(1) = d(0) + 1 d(2) = d(2 - 1) + 1 = 2， 从面值小于等于2元的硬币中选择，符合要求的硬币面值为：1元。 此时d(2) = d(2-1) + 1 d(3) = d(3 - 3) + 1 = 1， 从面值小于等于3元的硬币中选择，符合要求的硬币面值为：1元，3元。 此时有有两种选择：是否选择含有面值3元的硬币 含有3元硬币：d(3) = d(3 - 3) + 1 = 1 不含3元硬币：d(3) = d(3 - 1) + 1 = d(2) + 1 = 3 自然是选择二者中较小值 依次类推… 就该问题总结一下，随着要凑够钱数的增加： 1、首先要知道所有不大于该钱数的面值; 2、对于每种面值的硬币，求出当选择一个该面值的硬币时所需的硬币数 当选择一个硬币后，所需硬币数+1，所要凑够的钱数=原所要凑的钱数-该硬币面值，所要凑够的钱数减少，求减少后要凑钱数最少所需硬币数，属于原问题的子结构，已求出解 3.在上述求出的结果集中，选择最小值，即为要凑够该钱数所需的最少硬币数 由此可以看出，每个问题的最优值都是借其子结构的最优值得到的。 而该算法的最小的子结构的最优解是已知的，即：当要凑钱数为0元时，最少需要0枚硬币。 利用这个最小的子结构，通过递推式便可求出所指定值凑够钱数的最优值 上面所提到的递推式，便是状态转移方程。利用已知状态，不断通过状态转移方程求解，便得到了最优值和最优解。 下面看一下硬币组合问题的数学描述： d(i)=min{ d(i-vj)+1 }，其中i-vj &gt;=0，vj表示第j个硬币的面值，i表示要凑够i元，d(i)表示凑够i元最少需要的硬币数。即： 123 0 i == 0 时min_coin_num(i) = &#123; min&#123; min_coin_num( i-coin_value(j) )+1 | i-coin_value(j)&gt;0&#125; coin_value(j)表示第j种硬币的面值 i &gt; 0 时 当总值total_value为i时， 对于所有的 coin_value(j) &lt; i的硬币j ,取min{ min_coin_num(i-coin_value(j)) } 代码在 dynamic_programming/coin.py]]></content>
      <categories>
        <category>算法与数据结构</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数学问题综合]]></title>
    <url>%2F2018%2F03%2F17%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E7%BB%BC%E5%90%88%2F</url>
    <content type="text"><![CDATA[从1-200中任意选出101个自然数,其中一个数必是另一个数的整数倍把这200个数分类如下： 以上共分为100类，即100个抽屉。显然在同一类中的数若不少于两个，那么这类中的任意两个数都有倍数关系。从中任取101个数，根据抽屉原理，一定至少有两个数取自同一类，因此其中一个数是另一个数的倍数。]]></content>
      <categories>
        <category>算法与数据结构</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[排序]]></title>
    <url>%2F2018%2F03%2F17%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[堆排序 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#!/usr/bin/env python2# -*- coding: utf-8 -*-"""Created on Tue Mar 13 09:46:42 2018@author: david"""def sift_down(array, start, end): """ 调整成大顶堆，初始堆时，从下往上；交换堆顶与堆尾后，从上往下调整 :param array: 列表的引用 :param start: 父结点 :param end: 结束的下标 :return: 无 """ while True: # 当列表第一个是以下标0开始，结点下标为i,左孩子则为2*i+1,右孩子下标则为2*i+2; # 若下标以1开始，左孩子则为2*i,右孩子则为2*i+１ left_child = 2*start + 1 # 左孩子的结点下标 # 当结点的右孩子存在，且大于结点的左孩子时 if left_child &gt; end: break if left_child+1 &lt;= end and array[left_child+1] &gt; array[left_child]: left_child += 1 if array[left_child] &gt; array[start]: # 当左右孩子的最大值大于父结点时，则交换 array[left_child], array[start] = array[start], array[left_child] start = left_child # 交换之后以交换子结点为根的堆可能不是大顶堆，需重新调整 else: # 若父结点大于左右孩子，则退出循环 break print("&gt;&gt;", array)def heap_sort(array): # 堆排序 # 先初始化大顶堆 first = len(array)//2 -1 # 最后一个有孩子的节点(//表示取整的意思) # 第一个结点的下标为０，很多博客&amp;课本教材是从下标1开始，无所谓吧，你随意 for i in range(first, -1, -1): # 从最后一个有孩子的节点开始往上调整 print(array[i]) sift_down(array, i, len(array)-1) # 初始化大顶堆 print("初始化大顶堆结果:", array) # 交换堆顶与堆尾 for head_end in range(len(array)-1, 0, -1): # start stop step array[head_end], array[0] = array[0], array[head_end] # 交换堆顶与堆尾 sift_down(array, 0, head_end-1) # 堆长度减一(head_end-1)，再从上往下调整成大顶堆if __name__ == "__main__": array = [16, 7, 3, 20, 17, 8] print(array) heap_sort(array) print("堆排序最终结果:", array)]]></content>
      <categories>
        <category>算法与数据结构</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112'''Created on Oct 27, 2010Logistic Regression Working Module@author: Peter'''from numpy import *def loadDataSet(): dataMat = []; labelMat = [] fr = open('testSet.txt') for line in fr.readlines(): lineArr = line.strip().split() dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])]) labelMat.append(int(lineArr[2])) return dataMat,labelMatdef sigmoid(inX): return 1.0/(1+exp(-inX))def gradAscent(dataMatIn, classLabels): dataMatrix = mat(dataMatIn) #convert to NumPy matrix labelMat = mat(classLabels).transpose() #convert to NumPy matrix m,n = shape(dataMatrix) alpha = 0.001 maxCycles = 500 weights = ones((n,1)) for k in range(maxCycles): #heavy on matrix operations h = sigmoid(dataMatrix*weights) #matrix mult error = (labelMat - h) #vector subtraction weights = weights + alpha * dataMatrix.transpose()* error #matrix mult return weightsdef plotBestFit(weights): import matplotlib.pyplot as plt dataMat,labelMat=loadDataSet() dataArr = array(dataMat) n = shape(dataArr)[0] xcord1 = []; ycord1 = [] xcord2 = []; ycord2 = [] for i in range(n): if int(labelMat[i])== 1: xcord1.append(dataArr[i,1]); ycord1.append(dataArr[i,2]) else: xcord2.append(dataArr[i,1]); ycord2.append(dataArr[i,2]) fig = plt.figure() ax = fig.add_subplot(111) ax.scatter(xcord1, ycord1, s=30, c='red', marker='s') ax.scatter(xcord2, ycord2, s=30, c='green') x = arange(-3.0, 3.0, 0.1) y = (-weights[0]-weights[1]*x)/weights[2] ax.plot(x, y) plt.xlabel('X1'); plt.ylabel('X2'); plt.show()def stocGradAscent0(dataMatrix, classLabels): m,n = shape(dataMatrix) alpha = 0.01 weights = ones(n) #initialize to all ones for i in range(m): h = sigmoid(sum(dataMatrix[i]*weights)) error = classLabels[i] - h weights = weights + alpha * error * dataMatrix[i] return weightsdef stocGradAscent1(dataMatrix, classLabels, numIter=150): m,n = shape(dataMatrix) weights = ones(n) #initialize to all ones for j in range(numIter): dataIndex = range(m) for i in range(m): alpha = 4/(1.0+j+i)+0.0001 #apha decreases with iteration, does not randIndex = int(random.uniform(0,len(dataIndex)))#go to 0 because of the constant h = sigmoid(sum(dataMatrix[randIndex]*weights)) error = classLabels[randIndex] - h weights = weights + alpha * error * dataMatrix[randIndex] del(dataIndex[randIndex]) return weightsdef classifyVector(inX, weights): prob = sigmoid(sum(inX*weights)) if prob &gt; 0.5: return 1.0 else: return 0.0def colicTest(): frTrain = open('horseColicTraining.txt'); frTest = open('horseColicTest.txt') trainingSet = []; trainingLabels = [] for line in frTrain.readlines(): currLine = line.strip().split('\t') lineArr =[] for i in range(21): lineArr.append(float(currLine[i])) trainingSet.append(lineArr) trainingLabels.append(float(currLine[21])) trainWeights = stocGradAscent1(array(trainingSet), trainingLabels, 1000) errorCount = 0; numTestVec = 0.0 for line in frTest.readlines(): numTestVec += 1.0 currLine = line.strip().split('\t') lineArr =[] for i in range(21): lineArr.append(float(currLine[i])) if int(classifyVector(array(lineArr), trainWeights))!= int(currLine[21]): errorCount += 1 errorRate = (float(errorCount)/numTestVec) print "the error rate of this test is: %f" % errorRate return errorRatedef multiTest(): numTests = 10; errorSum=0.0 for k in range(numTests): errorSum += colicTest() print "after %d iterations the average error rate is: %f" % (numTests, errorSum/float(numTests)) 逻辑回归详解 对随机梯度下降算法，我们做两处改进来避免上述的波动问题： 1）在每次迭代时，调整更新步长alpha的值。随着迭代的进行，alpha越来越小，这会缓解系数的高频波动（也就是每次迭代系数改变得太大，跳的跨度太大）。当然了，为了避免alpha随着迭代不断减小到接近于0（这时候，系数几乎没有调整，那么迭代也没有意义了），我们约束alpha一定大于一个稍微大点的常数项，具体见代码。 2）每次迭代，改变样本的优化顺序。也就是随机选择样本来更新回归系数。这样做可以减少周期性的波动，因为样本顺序的改变，使得每次迭代不再形成周期性。 sklearn的LRscikit-learn 逻辑回归类库使用小结 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758# coding: utf-8# pylint: disable = invalid-name, C0111import numpy as npimport matplotlib.pyplot as pltfrom itertools import cycleimport pandas as pdfrom sklearn import svm, datasetsfrom sklearn.metrics import roc_curve, aucfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import label_binarizefrom sklearn.multiclass import OneVsRestClassifierfrom scipy import interpfrom sklearn.metrics import confusion_matrixfrom sklearn.preprocessing import OneHotEncoderimport zipfilefrom sklearn.linear_model import LogisticRegressionfrom sklearn.externals import joblib# with zipfile.ZipFile('df_allfeature_train1-7.csv.zip', 'r') as z:# f = z.open('df_allfeature_train1-7.csv')# df = pd.read_csv(f, header=0)df = pd.read_csv('df_allfeature_train_lite.csv', header=0)col_imp = ['product_code', 'now_term_rate', 'label']df = pd.DataFrame(df, columns=col_imp)y_train = df.labelX_train = df.drop(['label'], axis=1)def f(col): y = abs(col) * 100000 colDict = dict() for idx, val in enumerate(set(y)): colDict[int(val)] = int(idx) print(colDict) return pd.Series([colDict[int(x)] for x in y])# 每一列的WOE处理成int的编号X_train = X_train.apply(f, axis=0)print(X_train)# 变成one-hot encoderenc = OneHotEncoder()enc.fit(X_train)print(enc.n_values_)print(enc.feature_indices_)X_train = pd.DataFrame(enc.fit_transform(X_train).todense())print(X_train.shape)print(X_train.head(2))classifier = LogisticRegression() # 使用类，参数全是默认的classifier.fit(X_train, y_train) # 训练数据来学习，不需要返回值print("Coefficients:%s, intercept %s"%(classifier.coef_,classifier.intercept_))# joblib.dump(classifier, 'lr.m') clf.coef_就是权重矩阵，classifier.intercept_是偏置 逻辑回归的结果解释https://www.jianshu.com/p/a72302fa03d7 https://blog.csdn.net/sjpljr/article/details/70169046 https://wenku.baidu.com/view/953be54268eae009581b6bd97f1922791688be6c.html 原理讲解http://blog.sina.com.cn/s/blog_44befaf60102vznn.html Logistic回归虽然名字叫”回归” ，但却是一种分类学习方法。使用场景大概有两个：第一用来预测，第二寻找因变量的影响因素。 一 从线性回归到Logistic回归 线性回归和Logistic回归都是广义线性模型的特例。 假设有一个因变量y和一组自变量x1, x2, x3, … , xn，其中y为连续变量，我们可以拟合一个线性方程： y =β0 +β1x1 +β2x2 +β3x3 +…+βnxn 并通过最小二乘法估计各个β系数的值。 如果y为二分类变量，只能取值0或1，那么线性回归方程就会遇到困难: 方程右侧是一个连续的值，取值为负无穷到正无穷，而左侧只能取值[0,1]，无法对应。为了继续使用线性回归的思想，统计学家想到了一个变换方法，就是将方程右边的取值变换为[0,1]。最后选中了Logistic函数： y = 1 / (1+e-x) 这是一个S型函数，值域为(0,1)，能将任何数值映射到(0,1)，且具有无限阶可导等优良数学性质。 机器学习sklearn19.0——Logistic回归算法https://blog.csdn.net/loveliuzz/article/details/78708359 参数讲解全面]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[评价指标]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87%2F</url>
    <content type="text"><![CDATA[准确率Accuracy分类器正确分类的样本数与总样本数之比。 ROC AUC 相关(Relevant),正类 无关(NonRelevant),负类 被检索到(Retrieved) true positives(TP 正类判定为正类,例子中就是正确的判定”这位是女生”) false positives(FP 负类判定为正类,”存伪”,例子中就是分明是男生却判断为女生,当下伪娘横行,这个错常有人犯) 未被检索到(Not Retrieved) false negatives(FN 正类判定为负类,”去真”,例子中就是,分明是女生,这哥们却判断为男生—梁山伯同学犯的错就是这个) true negatives(TN 负类判定为负类,也就是一个男生被判断为男生,像我这样的纯爷们一准儿就会在此处) 假如某个班级有男生80人,女生20人,共计100人.目标是找出所有女生.现在某人挑选出50个人,其中20人是女生,另外还错误的把30个男生也当作女生挑选出来了.作为评估者的你需要来评估(evaluation)下他的工作 TP=20FP=30FN=0TN=50 考虑ROC曲线图中的四个点和一条线。第一个点，(0,1)，即FPR=0, TPR=1，这意味着FN（false negative）=0，并且FP（false positive）=0。 ROC曲线越接近左上角，该分类器的性能越好。 曲线就是一系列FPR和TPR的结果。就是将每次分类结果的（0,1）的点作为分类的阈值。 但是，ROC的曲线——如上面几位已经说过——有数据均衡的问题。在数据极度不平衡的情况下，譬如说1万封邮件中只有1封垃圾邮件，那么如果我挑出10封，50封，100，。。封垃圾邮件（假设全部包含真正的那封垃圾邮件），Recall都是100%，但是FPR分别是9/9999, 49/9999, 99/9999（数据都比较好看：FPR越低越好），而Precision却只有1/10，1/50， 1/100 （数据很差：Precision越高越好）。所以在数据非常不均衡的情况下，看ROC的AUC可能是看不出太多好坏的，而PR curve就要敏感的多。（不过真实世界中，垃圾邮件也许与你的有用的邮件一样多——甚至比有用的还更多。。。）作者：竹间智能 Emotibot链接：https://www.zhihu.com/question/30643044/answer/161955532来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 精确率PrecisionTP/(TP+FP) 检索到的结果中是正确的比例，比如检索出50个女生只有20个是正确的 召回率RecallTP/(TP+FN) 检索到的结果占应该检索结果的比例。比如检索的结果里面有20个女生，全班一共有20个女生，所以是100% F1精确值和召回率的调和均值 F_1 = \frac{2PR}{P+R} = \frac{2TP}{2TP + FP + FN}多分类的精确率和召回率http://blog.csdn.net/lanchunhui/article/details/51221729 把每个类别单独视为”正“，所有其它类型视为”负“ 就是看每行对角线的P和R 代码见multi_recall.py 1234567891011121314151617M = [ [14371, 6500, 9, 0, 0, 2, 316], [5700, 22205, 454, 20, 0, 11, 23], [0, 445, 3115, 71, 0, 11, 0], [0, 0, 160, 112, 0, 0, 0], [0, 888, 39, 2, 0, 0, 0], [0, 486, 1196, 30, 0, 74, 0], [1139, 35, 0, 0, 0, 0, 865]]n = len(M)for i in range(n): rowsum, colsum = sum(M[i]), sum(M[r][i] for r in range(n)) try: print 'precision: %s' % (M[i][i]/float(colsum)), 'recall: %s' % (M[i][i]/float(rowsum)) except ZeroDivisionError: print 'precision: %s' % 0, 'recall: %s' %0 回归的混淆矩阵1234567y_test = df_base_pred.dpdy_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)cm1 = confusion_matrix(np.where(y_pred &gt; 0.8, 0, 1), np.where(y_test &gt; 0.8, 0, 1))cm2 = confusion_matrix(np.where(((y_test &gt; 0.5) &amp; (y_test &lt; 0.8)), 0, 1), np.where(((y_pred &gt; 0.5) &amp; (y_pred &lt; 0.8)), 0, 1))cm3 = confusion_matrix(np.where(((y_test &gt; 0.1) &amp; (y_test &lt; 0.5)), 0, 1), np.where(((y_pred &gt; 0.1) &amp; (y_pred &lt; 0.5)), 0, 1)) K-Shttp://www.sohu.com/a/211697143_793685 https://blog.csdn.net/sinat_30316741/article/details/80018932 https://www.cnblogs.com/nxld/p/6208613.html https://blog.csdn.net/pzw_0612/article/details/45280411 1234567891011121314151617181920212223242526def compute_ks(data): sorted_list = data.sort_values([&apos;predict_proba&apos;], ascending=[True])#按照样本为正样本的概率值升序排序 ，也即坏样本的概率从高到低排序 total_good=sorted_list[&apos;label&apos;].sum() total_bad = sorted_list.shape[0] - total_good max_ks = 0.0 good_count = 0.0 bad_count = 0.0 for index, row in sorted_list.iterrows(): #按照标签和每行拆开 if row[&apos;label&apos;] == 0: bad_count +=1 else: good_count +=1 val = abs(bad_count/total_bad - good_count/total_good) max_ks = max(max_ks, val) return max_ks test_pd=pd.DataFrame()y_predict_proba=est.predict_proba(X_test)[:,1]#取被分为正样本的概率那一列Y_test_1=np.array(Y_test)test_pd[&apos;label&apos;]=Y_test_1test_pd[&apos;predict_proba&apos;]=y_predict_probaprint (&quot;测试集 KS:&quot;,compute_ks(test_pd))作者：暸望塔链接：https://www.jianshu.com/p/fec4105a60d7來源：简书简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。 回归模型的评价https://blog.csdn.net/shy19890510/article/details/79375062 对于回归模型效果的判断指标经过了几个过程，从SSE到R-square再到Ajusted R-square, 是一个完善的过程： SSE(误差平方和)：The sum of squares due to error R-square(决定系数)：Coefficient of determination Adjusted R-square：Degree-of-freedom adjusted coefficient of determination 下面我对以上几个名词进行详细的解释下，相信能给大家带来一定的帮助！！ 一、SSE(误差平方和) 计算公式如下： ​ 同样的数据集的情况下，SSE越小，误差越小，模型效果越好 缺点： SSE数值大小本身没有意义，随着样本增加，SSE必然增加，也就是说，不同的数据集的情况下，SSE比较没有意义 二、R-square(决定系数) 数学理解： 分母理解为原始数据的离散程度，分子为预测数据和原始数据的误差，二者相除可以消除原始数据离散程度的影响 其实“决定系数”是通过数据的变化来表征一个拟合的好坏。 理论上取值范围（-∞，1], 正常取值范围为[0 1] ———实际操作中通常会选择拟合较好的曲线计算R²，因此很少出现-∞ 越接近1，表明方程的变量对y的解释能力越强，这个模型对数据拟合的也较好 越接近0，表明模型拟合的越差 经验值：&gt;0.4， 拟合效果好 缺点： 数据集的样本越大，R²越大，因此，不同数据集的模型结果比较会有一定的误差 三、Adjusted R-Square (校正决定系数） ​ n为样本数量，p为特征数量 消除了样本数量和特征数量的影响]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[知识点]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E7%9F%A5%E8%AF%86%E7%82%B9%2F</url>
    <content type="text"><![CDATA[特征选择经典算法LR线性回归模型的映射函数是一个线性方程。逻辑回归要解决分类问题，用logistic function代替线性方程，通过y的取值判断类别，y的取值是一个概率。 优点是，输出是0,1满足概率分布的要求。函数可微。 LR的L2正则 BFGS 贝叶斯在多项式模型中： 在多项式模型中， 设某文档d=(t1,t2,…,tk)，tk是该文档中出现过的单词，允许重复，则 先验概率P(c)= 类c下单词总数（包括重复的）/整个训练样本的单词总数 类条件概率P(tk|c)=(类c下单词tk在各个文档中出现过的次数之和+1)/(类c下单词总数+|V|) V是训练样本的单词表（即抽取单词，单词出现多次，只算一个），|V|则表示训练样本包含多少种单词。 P(tk|c)可以看作是单词tk在证明d属于类c上提供了多大的证据，而P(c)则可以认为是类别c在整体上占多大比例(有多大可能性)。 在伯努利模型中： P(c)= 类c下文件总数/整个训练样本的文件总数 P(tk|c)=(类c下包含单词tk的文件数+1)/(类c下文件总数+2) 平滑项是应对没有特征的情况。 朴素贝叶斯算法的12条建议 SVMSVM在哪个地方引入的核函数?如果用高斯核可以升到多少维? BP的推导反向传播的原理 随机森林解决决策树容易过拟合的缺点。采用多个决策树的投票机制来改善决策树。RF有多个决策树，不能用全样本去训练，要用到采样方法。 1、每棵树选择样本时，通过重采样产生n个样本 2、从m个特征中随机选择k个特征，构建决策树。 3、多数投票制预测 决策树IC3，用信息增益找分裂特征。 C4.5，用信息增益比找特征。 GBDT机器学习算法GBDT的面试要点总结-上篇 机器学习算法中GBDT和XGBOOST的区别有哪些？ 推荐系统稍微看看 k折交叉验证中k取值多少有什么关系, 和bias和variance有关系吗? 翻转二叉树 平衡二叉树 hadoop和spark的应用场景 样本倾斜的处理样本不均衡的情况下，用AUC会偏高。 分类算法的比较用于数据挖掘的分类算法有哪些，各有何优劣？ 预测的评价指标精确率、召回率、F1 值、ROC、AUC 各自的优缺点是什么？ ROC和AUC构建一个混淆矩阵 TP FP FN TN 精确率(tp+tn)/(tp+fp+fn+tn) 分类器对整个样本分类能力，正为正，负为负 准确率 TP/TP+FP 分类器判定为正例中，真正的正例样本比重。 召回率 tp/tp+fn 判定为正例在所有正例的比重 word2vec的原理 如何增量训练 是一个词转向量的工具，语料是收集的百万篇的url。 如何将词转成向量表达的？ 模型的目标函数是什么 L=\sum_{w \in C} \log p(w|Context(w)) 如何推导的？ ngram模型+对数似然函数 基于神经网络的模型是如何做的 输入层是(Context(w),w)的训练样本。输入的时候是n-1个词向量首尾拼接。（词向量还没训练呢是怎么得到的？在[0,1]之间随机取值初始化） 输入层到隐藏层用双曲正切函数作为激活函数。 隐藏层到输出层再用一个线性函数。这样输出的y只是一个普通向量，需要用softmax再做一个归一化处理。 有了目标函数后，如何构建CBOW网络结构？ 网络结构： 输入层：前后各c个词，共2c个词的词向量。 投影层：向量做sum再取平均。 输出层：输出到Huffman树。每个叶子节点是一个词典中的词。 与神经网络模型的区别： 1）前者是拼接，后者是累加。2）后者没有隐藏层。3）前者是线性结构；后者是树形结构。 如何利用Huffman树来定义目标函数？ 对于每个词，都存在一个路径，路径上存在分支，将每个分支看成一个二分类，每次分类产生一个概率。将这些概率连乘，就是目标函数。 公式见《NLP/Word2Vec原理》 权重初始化为0，词向量初始化为[0-1]的随机数。权重向量的长度就是所有词*词向量的长度。因为每层都有一个权重。 用了多线程方法加速训练。 为什么叫层次softmax？ 每层都是一个二分类问题 为什么同义词的向量也相近 我理解是上下文接近，对于CBOW，训练每个词后更新的是上下文，对于近义词往往有类似的上下文 如何计算距一个词最近的向量？ KD树搜索。（https://www.cnblogs.com/21207-iHome/p/6084670.html） 构建方法：任选一个特征（或者数据方差最大的特征，说明分散，越可能不属于同一个区间），以此为坐标轴划分，将最近的点落在坐标轴上。 KD树搜索 先从根节点往下找到叶子节点，再回溯，回溯到每层时要判断是否跟另一区间相交。 但是KD是递归查找，效率低。递归效率低是函数调用的开销导致的（函数调用需要准备资源），且有栈溢出的风险。 idistance https://en.m.wikipedia.org/wiki/IDistance 目前线上怎么做的 如何用word2vec计算句子相似度 词向量按tfidf权重求mean 也要分应用，需求是topic相关还是语义相关。比如我爱苹果，我不爱苹果，topic相似语义不相似。 如果从词的粒度比较，还要结合上下文，避免一词多义。 文本分类 可以直接用fasttext，也可以tfidf+svm 熵 随机游走算法 动态规划算法 异常检测算法 CTR预测如何预测CTR线上的效果线下的可以用log loss和AUC 这里要特别强调一下用线上的其它业务指标如点击率、营收、利润、eCPC等等是不能给出CTR预估效果评价的。这些业务指标，受到整个广告系统其它模块如bid optimization,budget pacing等和外部竞价环境的综合影响，它的变化或者AB test中观察到的不同，不是简单地由于CTR预估变化带来的。换句话说，如果上了一个新的CTR预估模型的实验，发现业务指标变好了，这不等于说CTR预估更准了。一个简单的例子：如果一个CTR预估模型给出的预估总是比上帝视角的完美预估在低CTR区域低估，在高CTR区域高估，那么假设bid是base_bid*pCTR的话，相比完美预估，这个模型会赢得更多的高CTR区域的竞价，输掉更多在低CTR区域的竞价，最后会观察到实验组的CTR反而比完美预估的试验组更高。作者：Jian Xu链接：https://www.zhihu.com/question/54009615/answer/137820154来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 为什么AUC可以评价CTR它和Wilcoxon-Mann-Witney Test是等价的[3]。而Wilcoxon-Mann-Witney Test就是测试任意给一个正类样本和一个负类样本，正类样本的score有多大的概率大于负类样本的score。 如何平衡样本？平衡后的数据不是真实分布 CTR评价CTR的评价用logloss和AUC。因为是概率输出，不方便用PR。 点击率各模型优缺点广告点击率模型中，LR, GBDT+LR, FM, DNN等模型的优点和缺点？实际效果如何? CTR预估中GBDT与LR融合方案 CTR点击率预估干货分享 Ad Click Prediction: a View from the Trenches LR 优点：实现简单 缺点：需要寻找特征。我们也使用了FTRL，但实践中它并不能非常有效的产生稀疏模型，如果模型非常大，会导致同步模型变慢，一样会严重影响效果。 点击率模型中特征如何选择广告点击率预估中的特征选择 不是看它分布均不均衡，而是看它符不符合原来的分布。如果符合原来的分布，那么训练误差最小化也就意味着整体分布误差的最小化，也就没有必要进行均衡。 如何降维，每个特征都对应的ctr，在构建模型时，根据ctr的区间将属于同一个区间的特征值作为一个特征；随着ctr区间越分越细来迭代。停止条件是不超过所有特征的一半或达到最小区间阈值。 投放中出现的问题spark数据倾斜https://www.cnblogs.com/hd-zg/p/6089220.html spark资源调优http://blog.csdn.net/u012102306/article/details/51637366 我们使用yarn作为资源管理集群。yarn集群管理器根据spark参数，在各个工作节点上，启动一定数量的Executor进程，每个进程有一定的内存和CPU Core。 在申请到了作业执行所需的资源之后，Driver进程就会开始调度和执行我们编写的作业代码了。Driver将代码拆分为多个stage，为每个stage创建一批task。将这些task分配到各个Executor执行。 SparkConf的一些参数： spark.sql.shuffle.partitions指partition的数量。SparkSQL在运行时，将一个查询任务分解成多个task，一个task就是一个partition。默认是200个partition，而如果实际集群只能并行3个task，则跑完200个partition要200/3=67次。 spark.network.timeout所有网络通信的超时时间，默认是120s mapreduce shuffle过程http://langyu.iteye.com/blog/992916 map过程，阶段，将输入文本做split，并转成格式，V的初始值是1 之后，partitioner可以根据key和value以及reduce的数量来决定map的输出放到那个reduce task。默认是key hash后对reduce取模。 接下来将output写入缓冲区，减少磁盘IO的影响。key，value，partitioner的结果都写入缓冲区。 当缓冲区内容达到一定比例，就调用单独线程溢写到磁盘。溢写前，如果设置了combiner，在这里就要做合并。当溢写启动后，需要对溢写内容的key做sort。 Merge。每次溢写生成一个文件，map task完成后内存缓冲区所有的数据也全部溢写生成一个文件。用Merge合并溢写文件。 reduce 执行之前，拉取每个job中每个map task的最终结果。从不同地方拉过来的做merge。作为reducer的输入，然后执行reducer，结果写入hdfs。 文本分类怎么优化特征 CTR预测怎么优化特征选特征，版位id，版位类型，地理位置，曝光时间，ua信息（操作系统，浏览器等），人群特征因为样本稀疏所有没有选 根据直接观察CTR，卡方检验，单特征AUC。 用互信息、卡方检验有没有用 没有CTR的新版位 word2vec新词 es java api常见操作，从hive导入，从文件导入，中文分词 user-gene，投放，tracking处理的流程图 domain黑名单 DFA算法过滤敏感词 二分查找 特征稀疏怎么做传统的CTR或推荐系统拥有高维特征和稀疏数据，转向深度学习如何 如果可用特征值太少，就丢弃；要么就设值为unknown；xgboost有稀疏感知算法 怎么想到age预测算法的 哪些媒体获取？电商，微信 audience的重合度，距离代表相关度 hive的存储，ORC格式，以前用SequenceFile pig，数据倾斜时的join用replicated，大表放左，其他放右。 动态规划 L1和L2有什么区别 乱序数组找中位数快排+二分。取partition后，去掉一半的数字。 直接拿一个素材的点击率当一个维度的特征？ 随机森林和GBDT的区别 如何提高召回率或精确率如何提高机器学习算法的召回率？（尤其在样本集不平衡时） 引擎流程用timeslot监控campaign参数变化，预测下一个slot。第一个slot内不投放。 1、计算的参数包括flowbiddingrate、利润区间。 fbr衡量擦camp能投放的量占总量的百分比。比如根据预算，剩余每个slot的预算/每个slot的最大预算；曝光就是每个slot剩余曝光量/最大曝光量。 2、然后预测ctr并计算松弛系数（实际-期望），判断是否可以投放。 3、计算竞价，计算初始值，对于CPM，根据投放和曝光速度进行调整。对于CPC，如果实际CTR较高，通过提高一些小CTR的出价来平滑。 4、通过版位历史价格计算预期利润，统计利润区间分布。根据fbr确定可投放的利润区间。 5、通过投放平衡率计算camp打分。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[知识图谱入门]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[知识图谱入门 知道RDF，OWL，SPARQL这些W3C技术堆栈，知道它们的长处和局限。会使用RDF数据库和推理机。 了解一点描述逻辑基础，知道描述逻辑和一阶逻辑的关系。知道模型论，不然完全没法理解RDF和OWL。 了解图灵机和基本的算法复杂性。知道什么是决策问题、可判定性、完备性和一致性、P、NP、NExpTime。 最好再知道一点逻辑程序（Logic Programming），涉猎一点答集程序（Answer Set Programming），知道LP和ASP的一些小工具。这些东西是规则引擎的核心。如果不满足于正则表达式和if-then-else，最好学一点这些。 从正则文法到自动机。不理解自动机很多高效的模式提取算法都理解不了。 熟悉常见的知识库，不必事事重新造轮子，如Freebase, Wikidata, Yago, DBPedia。 熟悉结构化数据建模的基本方法，如ER，面向对象，UML，脑图。 学会使用一些本体编辑器，如Protege。 熟悉任何一种关系数据库。会使用存储过程写递归查询。明白什么叫物化视图、传递闭包、推理闭包。 熟悉任何一种图数据库。明白图的局部索引和关系的全局索引的理论和实践性能差异。 熟悉词法分析的基本工具，如分词、词性标注 熟悉句法分析的基本工具，如成分分析、依存文法分析、深层文法分析 熟悉TFIDF、主题模型和分布式表示的基本概念和工具。知道怎么计算两个词的相似度、词和句子的关联度。 知道怎么做命名实体识别。知道一些常用的词表。知道怎么用规则做关系提取。 了解前人已经建好的各种Lexical数据库，如Wordnet, framenet, BabelNet, PropBank。熟悉一些常用的Corpus。 知道信息检索的基本原理。知道各种结构的索引的代价。 掌握Lucene或者Solr/Elasticsearch的使用。 知识融合把结构化数据、半结构化数据、非结构化数据的知识表达形式都统一成RDF的形式，便于存储和查询。具体的知识融合主要包括如下两种类型： 合并外部知识库： 数据层的融合、模式层的融合 开放数据集成框架：LDIF 合并关系型数据库：将关系型数据转换成RDF的格式，现有工具Triplify、 d2rServer 、OpenLink、 Virtuoso 、SparqlMap等 知识推理 jena是一个java 的API，用来支持语义网的有关应用，学习jena需要了解XML 、RDF、 Ontology、OWL等方面的知识。 RDFox是一个高度可扩展的内存RDF三重存储，支持共享内存并行数据推理。它是一个用C ++编写的跨平台软件，带有一个Java包装器，可以与任何基于Java的解决方案轻松集成]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[特征选择]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%2F</url>
    <content type="text"><![CDATA[选择哪些指标https://blog.csdn.net/fisherming/article/details/79925574 历届的Kaggle/天池比赛，天猫/京东排序和推荐业务线里模型用到的特 1.加减平均：这个用户所买商品高于所有用户购买商品平均价格的多少（权衡一个人的消费能力），用户连续登录天数超过平均多少（表明这个用户对该产品的黏性） 2.分位线：商品属于售出商品价格的多少分位线处。（比如20%，说明20%的人买东西都不会低于这个价格）。 3.次序型：排在第几位。 4.比例型：电商中，某商品在某电商平台好/中/差评的比例 转自知乎：特征工程到底是什么？ https://www.cnblogs.com/jasonfreak/p/5448385.html 代码在blogcodes/feature_selection.py 目录 1 特征工程是什么？2 数据预处理 2.1 无量纲化 2.1.1 标准化 2.1.2 区间缩放法 2.1.3 标准化与归一化的区别 2.2 对定量特征二值化 2.3 对定性特征哑编码 2.4 缺失值计算 2.5 数据变换3 特征选择 3.1 Filter 3.1.1 方差选择法 3.1.2 相关系数法 3.1.3 卡方检验 3.1.4 互信息法 3.2 Wrapper 3.2.1 递归特征消除法 3.3 Embedded 3.3.1 基于惩罚项的特征选择法 3.3.2 基于树模型的特征选择法4 降维 4.1 主成分分析法（PCA） 4.2 线性判别分析法（LDA）5 总结6 参考资料 本文中使用sklearn中的IRIS（鸢尾花）数据集来对特征处理功能进行说明。IRIS数据集由Fisher在1936年整理，包含4个特征（Sepal.Length（花萼长度）、Sepal.Width（花萼宽度）、Petal.Length（花瓣长度）、Petal.Width（花瓣宽度）），特征值都为正浮点数，单位为厘米。目标值为鸢尾花的分类（Iris Setosa（山鸢尾）、Iris Versicolour（杂色鸢尾），Iris Virginica（维吉尼亚鸢尾））。导入IRIS数据集的代码如下： 123from sklearn.datasets import load_iris# 返回的是一个Bunch，类似Dict，可以获取iris.data、iris.target等iris = load_iris() 数据预处理未处理的特征有如下问题，我们使用sklearn中的preproccessing库来进行数据预处理。 无量纲化无量纲化使不同规格的数据转换到同一规格。常见的无量纲化方法有标准化和区间缩放法。 标准化的前提是特征值服从正态分布，标准化后，其转换成标准正态分布。 区间缩放法利用了边界值信息，将特征的取值区间缩放到某个特定的范围，例如[0, 1]等。 标准化z-score。将原始数据归一化到均值为0，方差为1的数据集。 优点：当X的最大值和最小值未知，或孤立点左右了最大－最小规范化时， 该方法有用 1234from sklearn.preprocessing import StandardScaler #标准化，返回值为标准化后的数据StandardScaler().fit_transform(iris.data) 区间缩放归一化最大最小归一化。线性转换。 1234from sklearn.preprocessing import MinMaxScaler#区间缩放，返回值为缩放到[0, 1]区间的数据MinMaxScaler().fit_transform(iris.data) L2范数归一化 L2范数归一化就是向量中每个元素除以向量的L2范数 1234from sklearn.preprocessing import Normalizer#归一化，返回值为归一化后的数据Normalizer().fit_transform(iris.data) 什么情况需要主要看模型是否具有伸缩不变性。 有些模型在各个维度进行不均匀伸缩后，最优解与原来不等价，例如SVM。对于这样的模型，除非本来各维数据的分布范围就比较接近，否则必须进行标准化，以免模型参数被分布范围较大或较小的数据dominate。 标准化是依照特征矩阵的列处理数据 归一化是依照特征矩阵的行处理数据 有些模型在各个维度进行不均匀伸缩后，最优解与原来等价，例如logistic regression。对于这样的模型，是否标准化理论上不会改变最优解。但是，由于实际求解往往使用迭代算法，如果目标函数的形状太“扁”，迭代算法可能收敛得很慢甚至不收敛。所以对于具有伸缩不变性的模型，最好也进行数据标准化。 缩放的最主要优点是能够避免大数值区间的属性过分支配了小数值区间的属性。 另一个优点能避免计算过程中数值复杂度。因为关键值通常依赖特征向量的内积（inner products），例如，线性核和多项式核，属性的大数值可能会导致数值问题。我们推荐将每个属性线性缩放到区间[-1,+1]或者[0, 1]。 对定量特征二值化定量特征二值化的核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0，公式表达如下 1234from sklearn.preprocessing import Binarizer#二值化，阈值设置为3，返回值为二值化后的数据Binarizer(threshold=3).fit_transform(iris.data) 对定性特征哑编码1234from sklearn.preprocessing import OneHotEncoder#哑编码，对IRIS数据集的目标值，返回值为哑编码后的数据OneHotEncoder().fit_transform(iris.target.reshape((-1,1))) 缺失值计算默认用均值填充 1234567from numpy import vstack, array, nanfrom sklearn.preprocessing import Imputer#缺失值计算，返回值为计算缺失值后的数据#参数missing_value为缺失值的表示形式，默认为NaN#参数strategy为缺失值填充方式，默认为mean（均值）Imputer().fit_transform(vstack((array([nan, nan, nan, nan]), iris.data))) 数据变换常见的数据变换有基于多项式的、基于指数函数的、基于对数函数的。4个特征，度为2的多项式转换公式如下： 12345from sklearn.preprocessing import PolynomialFeatures#多项式转换#参数degree为度，默认值为2PolynomialFeatures().fit_transform(iris.data) 基于单变元函数的数据变换可以使用一个统一的方式完成，使用preproccessing库的FunctionTransformer对数据进行对数函数转换的代码如下： 123456from numpy import log1pfrom sklearn.preprocessing import FunctionTransformer#自定义转换函数为对数函数的数据变换#第一个参数是单变元函数FunctionTransformer(log1p).fit_transform(iris.data) 特征选择选有效的特征，一般从两个方面看： 1）特征是否发散。一个方差接近于0，也就是说样本在这个特征上基本没有差异，对于样本的区分就没什么用。 2）特征与目标的相关性。 根据特征选择的形式又可以将特征选择方法分为3种： Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。 Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。 Embedded：嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。 我们使用sklearn中的feature_selection库来进行特征选择。 Filter方差选择法选择方差大于阈值的特征。 12345from sklearn.feature_selection import VarianceThreshold#方差选择法，返回值为特征选择后的数据#参数threshold为方差的阈值VarianceThreshold(threshold=3).fit_transform(iris.data) 相关系数法缺点是对非线性关系不敏感。 计算各个特征对目标值的pearson相关系数 1234567from sklearn.feature_selection import SelectKBestfrom scipy.stats import pearsonr#选择K个最好的特征，返回选择特征后的数据#第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量，输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值。在此定义为计算相关系数#参数k为选择的特征个数SelectKBest(lambda X, Y: array(map(lambda x:pearsonr(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target) 要理解Pearson相关系数，首先要理解协方差（Covariance），协方差是一个反映两个随机变量相关程度的指标，如果一个变量跟随着另一个变量同时变大或者变小，那么这两个变量的协方差就是正值，反之相反，公式如下： Pearson相关系数公式如下： pearson是一个介于-1和1之间的值，当两个变量的线性关系增强时，相关系数趋于1或-1；当一个变量增大，另一个变量也增大时，表明它们之间是正相关的，相关系数大于0；如果一个变量增大，另一个变量却减小，表明它们之间是负相关的，相关系数小于0；如果相关系数等于0，表明它们之间不存在线性相关关系。 卡方检验方检验是检验定性自变量对定性因变量的相关性。 https://blog.csdn.net/snowdroptulip/article/details/78770088 卡方检验原理及应用 衡量实际值和理论值的差异程度。卡方值越大，相关程度越大。 比如某个特征有3个取值，构建矩阵，看这3个特征对应的点击和不点击的值分别是多少，然后计算总的点击率，求出每个特征的理论点击和不点击的数量。然后计算卡方值，再查表。 因此卡方就可以用来降维，比如找相关程度最大的特征。 12345from sklearn.feature_selection import SelectKBestfrom sklearn.feature_selection import chi2#选择K个最好的特征，返回选择特征后的数据SelectKBest(chi2, k=2).fit_transform(iris.data, iris.target) 另一种方法 12345import numpy as npfrom scipy.stats import chi2_contingencyd = np.array([[37, 49, 23], [150, 100, 57]])chi2_contingency(d) 数据如下： 杀虫效果 甲 乙 丙 死亡数 37 49 23 未死亡数 150 100 57 输出为： (7.6919413561281065, 0.021365652322337315, 2, array([[ 48.99759615, 39.04086538, 20.96153846],​ [ 138.00240385, 109.95913462, 59.03846154]])) 第一个值为卡方值，第二个值为P值，第三个值为自由度，第四个为与原数据数组同维度的对应理论值 [https://blog.csdn.net/QimaoRyan/article/details/72824766?utm_source=copy ] 互信息法图像标签hashtag的特征处理https://www.jiqizhixin.com/articles/050303 由于人们通常使用 hashtag 描述照片，所以 Facebook 研究人员认为它们可以作为模型训练数据的完美来源。这允许研究人员使用 hashtag 来完成一直以来的目标：基于人们自己标注的 hashtag 获取更多图像。 但是 hashtag 通常指非视觉概念，如 #tbt 表示「throwback Thursday」。或者它们比较模糊，如 #party 可以描述活动、设置，或者 both。对于图像识别来说，tag 的作用是弱监督数据，而模糊和／或不相关的 hashtag 是标签噪声，可能会混淆深度学习模型。 这些噪声标签是大规模训练工作的重点，因此研究人员开发了一种新方法，专为使用 hashtag 监督执行图像识别实验而准备。该方法包括处理每个图像的多个标签（加 hashtag 的用户通常会添加多个 hashtag）、整理 hashtag 同义词、平衡经常出现的 hashtag 和出现频率较低的 hashtag 的影响力。 特征选择和特征理解https://www.cnblogs.com/tonglin0325/p/6214978.html]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模型的假设条件]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%81%87%E8%AE%BE%E6%9D%A1%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[作者：李韶华链接：https://www.zhihu.com/question/46301335/answer/112354887来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 理解模型的基本假设，看自己的数据是否符合这种假设。任何模型都是有某种假设的，如果数据不符合这种假设，就不太可能学出有意义的模型并用于预测。 比如LDA（主题模型），假设是在同样一批文档中经常共现的词，语义上往往是相关的。这种特性不仅在自然语言中成立，在一些领域，比如每个人经常访问的网址集合，可能也是成立的，所以LDA也可以拿过去用。但如果数据不符合这个特性，套用LDA就是没有意义的，比如每个球队里的队员，可能并没有因为属于一个球队而具有什么相似性。 再举个例子，CNN（卷积神经网络），它的基本假设是特征的不同维度之间有局部相关性，卷积操作可以抓住这只局部相关性，形成新的特征。比如自然语言里，有重复出现的bigram，或者图像里代表性的局部像素块。不满足这种局部相关性的数据，比如收到的邮件序列，这种局部相关性很弱，那用CNN就不能抓到有用的特征。 最后，高斯copula，在量化金融里曾被广泛使用，把债券之间非高斯的相关性用copula转化成高斯然后拟合。然而这个模型隐含的假设是这种相关性符合瘦尾分布(thin tailed distribution)，即罕见事件发生的概率非常非常低。这个不合理假设导致对黑天鹅事件概率严重低估，曾被视为2008年金融危机的根源之一。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[样本倾斜]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%A0%B7%E6%9C%AC%E5%80%BE%E6%96%9C%2F</url>
    <content type="text"><![CDATA[严重数据倾斜文本分类，比如正反比1:20～100，适合什么model，查准一般要做到多少可以上线？ - 竹间智能 Emotibot的回答 - 知乎 https://www.zhihu.com/question/59236897/answer/164500508 首先要明确对于precision和recall，我们的需求是怎样的 。对于数据极其不平衡的情况，precision和recall的trade-off尤其显著。通过under-sampling/over-sampling来配平正反例是可以提升recall，但是一定会出现大量的false positive。如果我们认为错杀的成本很高，可以适当地降低对于precision的要求。反之，如果我们追求precision，那么可以采用基于规则的方式，通过对关键词的特征进行过滤，当然这样recall就会很惨。这是一个必须接受的现实。如果将这两种策略结合，最起码可以做这样的尝试：对于高precision的分类器，采取比较高信心的策略，譬如探测出来就直接报告这个用户甚至屏蔽；对于高recall的分类器，可以采取一些warning的措施，不强制做影响用户的操作。 粗暴一点，随机森林+bootstrap效果不错。 或者如果倾斜非常大，可以考虑异常检测的方法。 过采样 SMOTE算法，插值 高维度不适宜SMOTE。因为高维空间数据倾向于接近互相正交，故两两不相近，所以效果不好。 欠采样再缩放/再平衡假设训练集是真是样本总体的无偏采样，观测几率就是真实几率。只要分类器的预测几率高于观测几率就应判定为正例，即 \frac {\tilde {y}} {1- \tilde{y}} = \frac y {1-y} × \frac {m^-} {m^+}使用机器学习处理分类问题时，若训练样本比较稀疏，可否向训练语料中增加人工构造样本，以提升模型泛化能力？ 如何解决机器学习中样本不均衡问题？]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[样本不平衡带来的问题及如何解决]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%A0%B7%E6%9C%AC%E4%B8%8D%E5%B9%B3%E8%A1%A1%E5%B8%A6%E6%9D%A5%E7%9A%84%E9%97%AE%E9%A2%98%E5%8F%8A%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%2F</url>
    <content type="text"><![CDATA[不平衡数据下的机器学习方法简介 面向不平衡分类的逻辑回归算法 在样本分布及其不均匀的情况下,建议用PRC。。。可以看下这个精确率、召回率、F1 值、ROC、AUC 各自的优缺点是什么？ - 机器学习里面qian lv的回答 在分类中如何处理训练集中不平衡问题 确切的知道正样本但负样本不确定，且训练数据正负样本分布极不平衡问题求教？ 在分类中如何处理训练集中不平衡问题]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[熵、条件熵、相对熵、交叉熵]]></title>
    <url>%2F2018%2F03%2F17%2F%E7%AE%97%E6%B3%95%E8%83%8C%E5%90%8E%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%86%B5%2F</url>
    <content type="text"><![CDATA[熵1、什么是信息量？假设X是一个离散型随机变量，其取值集合为X，概率分布函数为$p(x) = P(X=x), x \in X$，定义$X=x_0$的信息量为： I(x_0) = -log(p(x_0))可以理解为，一个事件发生的概率越大，携带的信息量越小。当$p(x_0)=1$时，信息量为0。 2、什么是熵？第一，假设存在一个随机变量，可以问一下自己当我们观测到该随机变量的一个样本时，我们可以接受到多少信息量呢？毫无疑问，当我们被告知一个极不可能发生的事情发生了，那我们就接收到了更多的信息；而当我们观测到一个非常常见的事情发生了，那么我们就接收到了相对较少的信息量。因此信息的量度应该依赖于概率分布，所以说熵的定义应该是概率的单调函数。 第二，假设两个随机变量和是相互独立的，那么分别观测两个变量得到的信息量应该和同时观测两个变量的信息量是相同的，即：$H(X+Y)=H(X)+H(Y)$ 。而从概率上来讲，两个独立随机变量就意味着$p(x,y)=p(x)p(y)$，所以此处可以得出结论熵的定义应该是概率的函数。因此一个随机变量的熵可以使用如下定义： 设$X \in {x_1,x_2,…,x_n} $ 为一个离散随机变量，其概率分布为$P(X=x_i)=p_i$。则X的熵为 H(X)=-\sum_{i=1}^np_i\log p_i其中，当$p_i=0$时，熵为0。 此处的负号仅仅是用来保证熵（即信息量）是正数或者为零。而函数基的选择是任意的（信息论中基常常选择为2，因此信息的单位为比特bits；而机器学习中基常常选择为自然常数，因此单位常常被称为nats）。 用熵来评价整个随机变量x平均的信息量，而平均最好的量度就是随机变量的期望。 熵的取值范围是 0 \leq H(X) \leq \log n两个随机变量一起发生的熵就是联合熵 3、条件熵设$Y \in {y_1,y_2,…,y_m}$为随机变量，在已知X的条件下，Y的条件熵（conditional entropy）为 H(Y|X) = \sum_{i=1}^n p(x_i)H(Y|X=x_i) = -\sum_{i=1}^np(x_i) \sum_{j=1}^m p(y_j|x_i)\log p(y_j|x_i)表示在已知X的条件下，Y的条件概率分布的熵对X的数学期望。 4、交叉熵例如： 箱子里面有小球任意个，但其中1/2是橙色球，1/4是紫色球，1/8是蓝色球及1/8是青色球。我从中拿出一个球，你猜我手中的球是什么颜色的？ 知道了每种颜色小球的比例，比如橙色占比二分之一，如果我猜橙色，很有可能第一次就猜中了。所以，根据策略2，1/2的概率是橙色球，小明需要猜一次，1/4的概率是紫色球，小明需要猜两次，1/8的概率是蓝色球，小明需要猜三次，1/8的概率是青色球，小明需要猜三次，所以小明预期的猜题次数为： H = 1/2 1 + 1/4 2 + 1/8 3 + 1/8 3= 1.75 针对概率为p的小球，需要猜球的次数$=log_2 \frac 1 p$ 。例如1/8是蓝色球，次数就是3。则预期的猜题次数就是熵。 因此，每个系统都会有一个真实的概率分布（真实分布）。根据真实分布，可以找到一个最优策略，以最小的代价消除系统的不确定性，这个不确定性的值就是熵（比如猜题次数，编码长度）。 如果小明不知道真实分布，认为小球的分布为（1/4，1/4，1/4，1/4），这个分布就是非真实分布。此时，小明猜中任何一种颜色的小球都需要猜两次，即1/2 2 + 1/4 2 + 1/8 2 + 1/8 2 = 2。 当我们使用非最优策略消除系统的不确定性，所需要付出的努力的大小我们该如何去衡量呢？ 这就需要引入交叉熵，其用来衡量在给定的真实分布下，使用非真实分布所指定的策略消除系统的不确定性所需要付出的努力的大小。 交叉熵的公式为： H = \sum_{i=1}^N p_i log_2 \frac 1 {q_i} 在网上看的其他文章，包括LR等的损失函数中，都是如下的形式 H = \sum_{i=1}^N p_i log_2 q_i不清楚哪种表达是正确的。 其中，$p_i$是真实分布，$q_i$是非真实分布。 因此，交叉熵越低，这个策略就越好，最低的交叉熵也就是使用了真实分布所计算出来的信息熵，因为此时$p_i=q_i$ ，交叉熵 = 信息熵。 这也是为什么在机器学习中的分类算法中，我们总是最小化交叉熵，因为交叉熵越低，就证明由算法所产生的策略最接近最优策略，也间接证明我们算法所算出的非真实分布越接近真实分布。 来自知乎王赟 交叉熵（Cross Entropy）和negative log likelihood当ground truth只有一类为1，其它类为0时，cross entropy就是negative log likelihood，可以认为只是叫了一个比较fancy的名字。当ground truth本身也是一个分布时（比如在知识蒸馏过程中），cross entropy这个名字就比较好理解了。 5、相对熵(KL散度)最后，我们如何去衡量不同策略之间的差异呢？这就需要用到相对熵，其用来衡量两个取值为正的函数或概率分布之间的差异，即： KL(f(x) || g(x)) = \sum_{x \in X} f(x) * log_2 \frac {f(x)} {g(x)}现在，假设我们想知道某个策略和最优策略之间的差异，我们就可以用相对熵来衡量这两者之间的差异。即，相对熵 = 某个策略的交叉熵 - 信息熵（根据系统真实分布计算而得的信息熵，为最优策略），公式如下： KL(p || q) = H(p,q)-H(p) = \sum_{k=1}^N p_k log_2 \frac 1 {q_k} - \sum_{k=1}^N p_k log_2 \frac 1 {p_k} = \sum_{k=1}^N p_k log_2 \frac {p_k} {q_k}互信息是相对熵的特殊形式。如果变量不是独立的，可以通过考察联合概率分布和边缘概率分布乘积之间的相对熵，来判断它们是否接近于相对独立。此时，散度表示为 这被称为变量 x 和变量 y 之间的互信息( mutual information )。根据 Kullback-Leibler 散度的性质,我们看到 I[x, y] ≥ 0 ,当且仅当 x 和 y 相互独立时等号成立。经过推导，得互信息公式为 互信息不能归一化，对连续变量计算不方便（连续变量需要先离散化）。最大信息系数首先寻求一种最优离散化方式，然后把互信息取值转换成一种度量方式，取值区间在[0，1]。 6、总结熵：衡量不确定性的度量 联合熵：X、Y在一起时的不确定性度量 条件熵：X确定时，Y的不确定性度量。也就是在X发生的前提下，新发生Y带来的熵 交叉熵：衡量p和q的相似性，越小越相似 相对熵：p和q的不相似度量。]]></content>
      <categories>
        <category>算法背后的数学原理</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[文本分类]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习模型的比较]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%AF%94%E8%BE%83%2F</url>
    <content type="text"><![CDATA[转自 用于数据挖掘的分类算法有哪些，各有何优劣？ - Jason Gu的回答 - 知乎https://www.zhihu.com/question/24169940/answer/26952728 机器学习经典算法优缺点总结 首先看训练集多大， 贝叶斯因为模型简单，所以高偏差低方差。数据量小的时候，贝叶斯更好，因为LR容易过拟合。随着训练集增大，LR能训练出更准确的模型。 偏差指因模型太简单带来的估计不准确的部分。方差指因模型太复杂带来的不确定性。 LR相对于贝叶斯，不需要考虑特征是否相关。更容易增量训练。 LR可解释好，某个特征的权重高，则对结果的影响也大。可以在线学习。 决策树容易过拟合，使用剪树枝，或者RF。 Linear SVM和LR的比较http://www.jishux.com/plus/view-615065-1.html 从模型解决问题的方式来看Linear SVM直观上是trade-off两个量 a large margin，就是两类之间可以画多宽的gap ；不妨说是正样本应该在分界平面向左gap/2（称正分界），负样本应该在分解平面向右gap/2（称负分界） L1 error penalty，对所有不满足上述条件的点做L1 penalty 给定一个数据集，一旦完成Linear SVM的求解，所有数据点可以被归成两类 一类是落在对应分界平面外并被正确分类的点，比如落在正分界左侧的正样本或落在负分界右侧的负样本 第二类是落在gap里或被错误分类的点。 假设一个数据集已经被Linear SVM求解，那么往这个数据集里面增加或者删除更多的一类点并不会改变重新求解的Linear SVM平面。不受数据分布的影响。 求解LR模型过程中，每一个数据点对分类平面都是有影响的，它的影响力远离它到分类平面的距离指数递减。换句话说，LR的解是受数据本身分布影响的。在实际应用中，如果数据维度很高，LR模型都会配合参数的L1 regularization。 两者的区别两个模型对数据和参数的敏感程度不同，Linear SVM比较依赖penalty的系数和数据表达空间的测度，而（带正则项的）LR比较依赖对参数做L1 regularization的系数。但是由于他们或多或少都是线性分类器，所以实际上对低维度数据overfitting的能力都比较有限，相比之下对高维度数据，LR的表现会更加稳定，为什么呢？因为Linear SVM在计算margin有多“宽”的时候是依赖数据表达上的距离测度的，换句话说如果这个测度不好（badly scaled，这种情况在高维数据尤为显著），所求得的所谓Large margin就没有意义了，这个问题即使换用kernel trick（比如用Gaussian kernel）也无法完全避免。所以使用Linear SVM之前一般都需要先对数据做normalization，而求解LR（without regularization）时则不需要或者结果不敏感。 Linear SVM和LR都是线性分类器Linear SVM不直接依赖数据分布，分类平面不受一类点影响；LR则受所有数据点的影响，如果数据不同类别strongly unbalance一般需要先对数据做balancing。Linear SVM依赖数据表达的距离测度，所以需要对数据先做normalization；LR不受其影响Linear SVM依赖penalty的系数，实验中需要做validationLinear SVM和LR的performance都会收到outlier的影响，其敏感程度而言，谁更好很难下明确结论。 balance的方法 调整正、负样本在求cost时的权重，比如按比例加大正样本cost的权重。然而deep learning的训练过程是on-line的因此你需要按照batch中正、负样本的比例调整。 做训练样本选取：如hard negative mining，只用负样本中的一部分。 做训练样本选取：如通过data augmentation扩大正样本数量。 过拟合方面 LR容易欠拟合，准确度低。 SVM不太容易过拟合：松弛因子+损失函数形式 注意SVM的求解方法叫拉格朗日乘子法，而对于均方误差的优化方法是最小二乘法。 方法的选择在Andrew NG的课里讲到过： 如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM 如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel 如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况 当你的数据非常非常非常非常非常大然后完全跑不动SVM的时候，跑LR。SVM适合于小样本学习。多大算是非常非常非常非常非常非常大？ 比如几个G，几万维特征，就勉强算大吧…而实际问题上几万个参数实在完全不算个事儿，太常见了。随随便便就得上spark。读一遍数据就老半天，一天能训练出来的模型就叫高效了。所以在新时代，LR其实反而比以前用的多了=. = 应用场景方面不同拟合程度，样本量， 距离测度，数据balance 模型简单易解释 如果数据特征维度高，svm要使用核函数来求解 Note：拉格朗日对偶没有改变最优解，但改变了算法复杂度：原问题—样本维度；对偶问题–样本数量。所以 线性分类&amp;&amp;样本维度&lt;样本数量：原问题求解（liblinear默认）； 非线性–升维—一般导致 样本维度&gt;样本数量：对偶问题求解 SVM适合处理什么样的数据？高维稀疏，样本少。【参数只与支持向量有关，数量少，所以需要的样本少，由于参数跟维度没有关系，所以可以处理高维问题】 机器学习算法选择机器学习算法小结与收割offer遇到的问题 随机森林平均来说最强，但也只在9.9%的数据集上拿到了第一，优点是鲜有短板。SVM的平均水平紧随其后，在10.7%的数据集上拿到第一。神经网络（13.2%）和boosting（~9%）表现不错。数据维度越高，随机森林就比AdaBoost强越多，但是整体不及SVM2。数据量越大，神经网络就越强。 K近邻典型的例子是KNN，它的思路就是——对于待判断的点，找到离它最近的几个数据点，根据它们的类型决定待判断点的类型。 它的特点是完全跟着数据走，没有数学模型可言。 随机森林RF与传统bagging的区别（1）样本采样：RF有放回选取和整体样本数目相同的样本，一般bagging用的样本&lt;总体样本数（2）特征采样：RF对特征进行采样，BAGGING用全部特征]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[人群画像]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E4%BA%BA%E7%BE%A4%E7%94%BB%E5%83%8F%2F</url>
    <content type="text"><![CDATA[比你更了解你，浅谈用户画像 爱点击的性别预测模型 为什么用朴素贝叶斯？ 如何选择特征？ 去除覆盖率低的，去除 如何解决特征有依赖关系的问题？ 假设，对于同一个一级域名，下面的N级域名中男女分布比例在接近的合并为同一个特征。 训练集和测试集？ 线上效果 鼎盛时期，平均每个cookie有5个url]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[sklearn的一些总结]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2Fsklearn%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[转自 sklearn的一些总结]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CTR预测专题]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FCTR%E9%A2%84%E6%B5%8B%E4%B8%93%E9%A2%98%2F</url>
    <content type="text"><![CDATA[如何评价CTR预估效果？ 为什么LR可以用来做CTR预估？ 关于CTR预测的一个总结 为什么CTR预估使用AUC来评估模型？ 广告计算中的AUC和ROC曲线 常见计算广告点击率预估算法总结 关于贝叶斯平滑 http://blog.csdn.net/z363115269/article/details/78637702 http://blog.csdn.net/google19890102/article/details/50492787 http://blog.csdn.net/jinping_shi/article/details/78334362 http://blog.csdn.net/wwqwkg6e/article/details/55000216 贝叶斯平滑的思想是给CTR预设一个经验初始值，再通过当前的点击量和曝光量来修正这个初始值。如果某商品的点击量和曝光量都是0，那么该商品的CTR就是这个经验初始值；如果商品A和商品B的曝光量差别很大，那么可以通过这个经验初始值来修正，使得曝光量大的商品的权重增大。 贝叶斯平滑就是确定这个经验值的过程。贝叶斯平滑是基于贝叶斯统计推断的，因此经验值计算的过程依赖于数据的分布情况。 贝叶斯平滑的推导涉及贝叶斯参数估计 ctr能不能加入id类特征？ 这里没看懂？为什么ctr越高的分段上权重越大？ 假设一个最简单的问题，预估广告的点击率CTR。为了便于讨论，假设你只有一个特征，就是每次展现广告在过去一个时间窗内的历史点击率ctr，现在目标是预测下一次点击的ctr。简单起见，不妨假设系统中只有两条候选广告。 显而易见，预测分数是和ctr正相关的。如果你使用的是离散LR，那么在分段之后，显然ctr越高的分段上权重越大。这个模型实际跑起来就是最简单的“热门广告”的效果。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SVD]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FSVD%2F</url>
    <content type="text"><![CDATA[整理自： 奇异值分解(SVD)原理与在降维中的应用 奇异值分解(SVD) —- 线性变换几何意义 机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 特征值和特征分解 Ax=\lambda x其中A是一个n×n的矩阵，x是一个n维向量，则我们说λ是矩阵A的一个特征值，而x是矩阵A的特征值λ所对应的特征向量。 而从几何上看，A相当于对向量x进行了拉伸，λ是拉伸的尺度。 前提是A是一个对称矩阵。 对称矩阵 转置后与原矩阵相等。任意矩阵乘以它的转置也是对称矩阵。 特征分解时，A必须是方阵。 SVD之后，对于奇异值,它跟我们特征分解中的特征值类似，在奇异值矩阵中也是按照从大到小排列，而且奇异值的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上的比例。也就是说，我们也可以用最大的k个的奇异值和对应的左右奇异向量来近似描述矩阵。 A_{m \times n} = U_{m \times m}\Sigma_{m \times n} V^T_{n \times n} \approx U_{m \times k}\Sigma_{k \times k} V^T_{k \times n}这样，矩阵A就可以近似的表示为 （文本分析中） 三个矩阵有非常清楚的物理含义： 第一个矩阵X中的每一行表示意思相关的一类词，其中的每个非零元素表示这类词中每个词的重要性（或者说相关性），数值越大越相关。 第三个矩阵Y中的每一列表示同一主题一类文章，其中每个元素表示这类文章中每篇文章的相关性。 第二个矩阵B则表示类词和文章之间的相关性。因此，我们只要对关联矩阵A进行一次奇异值分解，我们就可以同时完成了近义词分类和文章的分类。（同时得到每类文章和每类词的相关性）。 SVD的性质 降维 对于奇异值,它跟我们特征分解中的特征值类似，在奇异值矩阵中也是按照从大到小排列，而且奇异值的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上的比例。也就是说，我们也可以用最大的k个的奇异值和对应的左右奇异向量来近似描述矩阵。也就是说： A_{m \times n} = U_{m \times m}\Sigma_{m \times n} V^T_{n \times n} \approx U_{m \times k}\Sigma_{k \times k} V^T_{k \times n}]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FSVM%2F</url>
    <content type="text"><![CDATA[线性可分的推导http://blog.sina.com.cn/s/blog_4298002e010144k8.html 线性不可分的时候http://blog.csdn.net/american199062/article/details/51322852 松弛系数 允许错误的分类，但要付出代价。错分的苹果是大于1，在margin当中但分类正确的在0,1之间。 对于整体的惩罚力度，要另外使用一个参数C来衡量惩罚的程度。 通过核函数可以以低的计算复杂度构造更复杂的分类器，而不用在低维映射到高维。 SMO优化 https://www.cnblogs.com/pinard/p/6111471.html]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[BP神经网络]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FBP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[https://www.cnblogs.com/wjy-lulu/p/6511616.html https://www.cnblogs.com/biaoyu/archive/2015/06/20/4591304.html]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[从内容-用户画像到如何做算法研发]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%80%BB%E7%BB%93%E4%B8%8E%E6%80%9D%E8%80%83%2F%E4%BB%8E%E5%86%85%E5%AE%B9-%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F%E5%88%B0%E5%A6%82%E4%BD%95%E5%81%9A%E7%AE%97%E6%B3%95%E7%A0%94%E5%8F%91%2F</url>
    <content type="text"><![CDATA[转自http://blog.csdn.net/bitcarmanlee/article/details/77574371 要求以Spark的Mlib为载体，尽量所有人共用一个算法平台。这样做的好处是大家信息共享会更快，同一个平台也更好维护。比如，算法工程师写了一个巨牛逼的算法原型，然后他需要先给工程师讲懂这个算法，工程师看个人水平，先不说能否将算法实现，实现所花的时间，以及是否真的有时间和精力去帮着实现，实现的是不是有问题就是一个很大的问题了。来回一折腾，两个人都会比较累。 怎么才算对算法有了真正的理解。 首先我们看什么场景用什么算法，但实际用起来，效果并不是那么好。这个时候我们至少需要了解两方面： 算法的核心是什么，有什么潜在的需求？比如是不是对数据的分布做了什么假设么? 特征和数据集的情况是如何的？ 而且很多算法做了很多很粗暴的假设，这种假设会导致算法存在一些固有的问题，如果你不了解其内部的这些假设，你会以为这些是他的一个特性，其实是一个缺点。比如Gini Importance，如果你不去了解的内部思想，你在理解数据时，就会造成误解，导致错误的认为先被选中的特征是很重要的，而其余的特征是不重要的，但实际上这些特征对响应变量的作用确实非常接近的。 我有时候觉得，引用算法工程师最流行的一个话，就是tricky。 中文我不知道怎么翻译更合适，很多时候是需要悟性和对事物本质的了解，才能了解一个算法的，绝对不是靠几个公式就能搞定的。 协同算法是我们应用的比较广泛的一个算法。 但是我觉得协同不应该算是一个算法，而是一种模式。 我们常见的很多模型，最后都是协同模式。举个例子来说，是不是个A1用户推荐文章B1,我们可能是这么做的： 把用户用向量做表征，文章也是观察大量的用户A2,A3…AN 是不是有点击该B1使用逻辑回归/SVM等分类算法训练模型把A1,B1丢进模型，得到是否推荐。但事实上这套算法，用的就是协同。为啥的？本质上还是相近的用户做的选择互相推荐。]]></content>
      <categories>
        <category>总结与思考</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Word2Vec源码解读]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FNLP%2FWord2vec-C%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[机器学习算法实现解析——word2vec源码解析 源码在/home/david/code/nlp/word2vec/word2vecC/word2vec.c 代码的主要工作包括： 预处理。变量声明，全局变量遍历； 构建词库。包括文本处理，以及是否需要有指定词库。 初始化网络结构。参数初始化，Huffman编码的生成。 多线程模型训练。 最终结果的处理。 以上的过程，可以用下图表示： 输入参数1234567891011121314151617181920212223242526-train text8 表示的是输入文件是text8-output vectors.bin 输出文件是vectors.bin-cbow 1 表示使用cbow模型，默认为Skip-Gram模型-size 200 每个单词的向量维度是200-window 8 训练的窗口大小为5就是考虑一个词前八个和后八个词语（实际代码中还有一个随机选窗口的过程，窗口大小小于等于8）-negative 0 使用ns的时候采样的样本数，默认0，通常是5-10-save-vocab 词汇表存储文件-read-vocab 词汇表加载文件-classes 输出单词类别数，默认为0，即不输出单词-hs 1不使用NEG方法，使用HS方法。--sample 亚采样拒绝概率的参数指的是采样的阈值，如果一个词语在训练样本中出现的频率越大，那么就越会被采样。-binary为1指的是结果二进制存储，为0是普通存储（普通存储的时候是可以打开看到词语和对应的向量的）-iter 15 迭代次数 全局变量int *vocab_hash 词在词库中的index，在构建词库时先初始化为-1 123456// 词的结构体struct vocab_word &#123; long long cn; // 出现的次数 int *point; // 从根结点到叶子节点的路径 char *word, *code, codelen;// 分别对应着词，Huffman编码，编码长度&#125;; vocab_word是词的结构体 vocab = (struct vocab_word *)calloc(vocab_max_size, sizeof(struct vocab_word)); vocab存储词 vocab_size ：词汇表的总量 syn0 ：上下文词 syn1 ：$\theta_{j-1}^w$ neu1 ：映射层的向量，就是输入层的向量之和 neu1e ：对应伪代码中的e 预处理 在预处理部分，对word2vec需要使用的参数进行初始化，在word2vec中是利用传入的方式对参数进行初始化的。 在预处理部分，实现了sigmoid函数值的近似计算。 如果每一次都请求计算sigmoid值，对性能将会有一定的影响，当sigmoid的值对精度的要求并不是非常严格时，可以采用近似计算。在word2vec中，将区间[−6,6]（设置的参数MAX_EXP为6）等距离划分成EXP_TABLE_SIZE等份，并将每个区间中的sigmoid值计算好存入到数组expTable中，需要使用时，直接从数组中查找。 1234567// 申请EXP_TABLE_SIZE+1个空间expTable = (real *)malloc((EXP_TABLE_SIZE + 1) * sizeof(real)); for (i = 0; i &lt; EXP_TABLE_SIZE; i++) &#123; expTable[i] = exp((i / (real)EXP_TABLE_SIZE * 2 - 1) * MAX_EXP); // 1/(1+e^6) ~ 1/(1+e^-6)即 0.01 ~ 1 的样子 expTable[i] = expTable[i] / (expTable[i] + 1); // Precompute f(x) = x / (x + 1) &#125; 注意：在上述代码中，作者使用的是小于EXP_TABLE_SIZE，实际的区间是[−6,6)。 构建词库在word2vec源码中，提供了两种构建词库的方法，分别为： 指定词库：ReadVocab()方法 从词的文本构建词库：LearnVocabFromTrainFile()方法 构建词库的过程在这里，我们以从词的文本构建词库为例。构建词库的过程如下所示： 在这部分中，最主要的工作是对文本进行处理，包括低频词的处理，hash表的处理等等。首先，会在词库中增加一个“&lt;/s&gt;”的词，同时，在读取文本的过程中，将换行符“\n”也表示成该该词 对词的哈希处理在存储词的过程中，同时保留这两个数组： 存储词的vocab 存储词的hash的vocab_hash 其中，在vocab中，存储的是词对应的结构体： 在vocab_hash中存储的是词在词库中的Index，vocab_hash的下标是词计算出的hash值。 在对词的处理过程中，主要包括： 计算词的hash值： 1234567// 取词的hash值int GetWordHash(char *word) &#123; unsigned long long a, hash = 0; for (a = 0; a &lt; strlen(word); a++) hash = hash * 257 + word[a]; hash = hash % vocab_hash_size; return hash;&#125; SearchVocab检索词是否存在。如不存在则返回-1，否则，返回该词在词库中的索引： 1234567while (1) &#123; if (vocab_hash[hash] == -1) return -1;// 不存在该词 //strcmp两个词相等，则返回0，所以要加上! if (!strcmp(word, vocab[vocab_hash[hash]].word)) return vocab_hash[hash];// 返回索引值 hash = (hash + 1) % vocab_hash_size;// 处理冲突&#125;return -1;// 不存在该词 在这个过程中，使用到了线性探测的开放定址法处理冲突，开放定址法就是一旦发生冲突，就去寻找下一个空的散列地址。 不存在，则插入新词。 对低频词的处理在循环读取每一个词的过程中，当出现“vocab_size &gt; vocab_hash_size * 0.7”时，需要对低频词进行处理。其中，vocab_size表示的是目前词库中词的个数，vocab_hash_size表示的是初始设定的hash表的大小。 ReduceVocab() 在处理低频词的过程中，通过参数“min_reduce”来控制，若词出现的次数小于等于该值时，则从词库中删除该词。 123456for (a = 0; a &lt; vocab_size; a++) if (vocab[a].cn &gt; min_reduce) &#123; vocab[b].cn = vocab[a].cn; vocab[b].word = vocab[a].word; b++; &#125; else free(vocab[a].word); vocab_size = b;// 删减后词的个数 在删除了低频词后，需要重新对词库中的词进行hash值的计算。 123456789for (a = 0; a &lt; vocab_hash_size; a++) vocab_hash[a] = -1;for (a = 0; a &lt; vocab_size; a++) &#123; // Hash will be re-computed, as it is not actual hash = GetWordHash(vocab[a].word); while (vocab_hash[hash] != -1) hash = (hash + 1) % vocab_hash_size; vocab_hash[hash] = a;&#125;fflush(stdout);min_reduce++; 根据词频对词库中的词排序基于以上的过程，程序已经将词从文件中提取出来，并存入到指定的词库中（vocab数组），接下来，需要根据每一个词的词频对词库中的词按照词频从大到小排序，其基本过程在函数SortVocab中，排序过程为 1qsort(&amp;vocab[1], vocab_size - 1, sizeof(struct vocab_word), VocabCompare); 原 型:void qsort(void base, int nelem, int width, int (fcmp)(const void ,const void )); 功 能: 使用快速排序例程进行排序 参 数： 1 待排序数组首地址 2 数组中待排序元素数量 3 各元素的占用空间大小 4 指向函数的指针，用于确定排序的顺序 说 明：qsort函数是ANSI C标准中提供的，其声明在stdlib.h文件中，是根据二分法写的，其时间复杂度为n*log(n)。 qsort要求提供的函数是需要自己定义的一个比较函数，比较函数使得qsort通用性更好。有了比较函数qsort可以实现对数组、字符串、结构体等结构进行升序或降序排序。如int cmp(const void a, const void b)中有两个元素作为参数（参数的格式不能变的。）返回一个int值，如果比较函数返回大于0，qsort就认为a &gt; b，返回小于0,qsort就认为a &lt; b。qsort知道元素的大小了，就可以把大的放前面去。如果你的比较函数返回本来应该是1的（即a &gt; b），而却返回-1（小于0的数），那么qsort认为a &lt; b，就把b放在前面去，但实际上是a &gt; b的，所以就造成了降序排序的差别了。简单来说，比较函数的作用就是给qsort指明元素的大小事怎么比较的。 保持字符“&lt; \s&gt;”在最开始的位置。排序后，根据“min_count”对低频词进行处理，与上述一样，再对剩下的词重新计算hash值。 至此，整个对词的处理过程就已经结束了。接下来，将是对网络结构的处理和词向量的训练。 初始化网络结构有了以上的对词的处理，就已经处理好了所有的训练样本，此时，便可以开始网络结构的初始化和接下来的网络训练。网络的初始化的过程在InitNet()函数中完成。 初始化网络参数在初始化的过程中，主要的参数包括词向量的初始化和映射层到输出层的权重的初始化，如下图所示： 词向量的初始化：为每个词分配空间，大小是vocab_size*layer1_size。 初始化的时候要分配所有词*词向量长度的空间？为何要这么大？12// layer1_size是词向量的长度a = posix_memalign((void **)&amp;syn0, 128, (long long)vocab_size * layer1_size * sizeof(real)); 1234&gt; int posix_memalign (void **memptr,&gt; size_t alignment,&gt; size_t size);&gt; &gt; 调用posix_memalign( )成功时会返回size字节的动态内存，并且这块内存的地址是alignment的倍数。参数alignment必须是2的幂，还是void指针的大小的倍数。返回的内存块的地址放在了memptr里面，函数返回值是0。 CBOW网络有两种可选的算法：层次Softmax和Negative Sampling。在输入参数时选择任意一种。 12345678910111213141516// 层次softmax的结构 if (hs) &#123; // 映射层到输出层之间的权重，就是Huffman树的非叶子结点的向量θ a = posix_memalign((void **)&amp;syn1, 128, (long long)vocab_size * layer1_size * sizeof(real)); if (syn1 == NULL) &#123;printf("Memory allocation failed\n"); exit(1);&#125; for (a = 0; a &lt; vocab_size; a++) for (b = 0; b &lt; layer1_size; b++) syn1[a * layer1_size + b] = 0;// 权重初始化为0 &#125; // 负采样的结构 if (negative&gt;0) &#123; a = posix_memalign((void **)&amp;syn1neg, 128, (long long)vocab_size * layer1_size * sizeof(real)); if (syn1neg == NULL) &#123;printf("Memory allocation failed\n"); exit(1);&#125; for (a = 0; a &lt; vocab_size; a++) for (b = 0; b &lt; layer1_size; b++) syn1neg[a * layer1_size + b] = 0; &#125; 在初始化的过程中，映射层到输出层的权重都初始化为0，而对于每一个词向量的初始化，作者的初始化方法如下代码所示： 12345678// 随机初始化 for (a = 0; a &lt; vocab_size; a++) for (b = 0; b &lt; layer1_size; b++) &#123; next_random = next_random * (unsigned long long)25214903917 + 11; // 1、与：相当于将数控制在一定范围内 // 2、0xFFFF：65536 // 3、/65536：[0,1]之间 syn0[a * layer1_size + b] = (((next_random &amp; 0xFFFF) / (real)65536) - 0.5) / layer1_size;// 初始化词向量 &#125; 首先，生成一个很大的next_random的数，通过与“0xFFFF”进行与运算截断，再除以65536得到[0,1]之间的数，最终，得到的初始化的向量的范围为：[−0.5/m,0.5/m]，其中，m为词向量的长度。 3.2、Huffman树的构建在层次Softmax中需要使用到Huffman树以及Huffman编码，因此，在网络结构的初始化过程中，也需要初始化Huffman树。在生成Huffman树的过程中，首先定义了3个长度为vocab_size*2+1的数组： 1234// 申请2倍的词的空间long long *count = (long long *)calloc(vocab_size * 2 + 1, sizeof(long long));long long *binary = (long long *)calloc(vocab_size * 2 + 1, sizeof(long long));long long *parent_node = (long long *)calloc(vocab_size * 2 + 1, sizeof(long long)); 其中，count数组中前vocab_size存储的是每一个词的对应的词频，词频是从高到低排序。后面的vocab_size先初始化为很大的数。 123// 分成两半进行初始化for (a = 0; a &lt; vocab_size; a++) count[a] = vocab[a].cn;// 前半部分初始化为每个词出现的次数for (a = vocab_size; a &lt; vocab_size * 2; a++) count[a] = 1e15;// 后半部分初始化为一个固定的常数 构建Huffman树的过程如下所示 首先，设置两个指针pos1和pos2，分别指向最后一个词和最后一个词的后一位 12345// 两个指针：// pos1指向前半截的尾部// pos2指向后半截的开始pos1 = vocab_size - 1;pos2 = vocab_size; 从两个指针所指的数中选择出最小的值，记为min1i， 如pos1所指的值最小，此时，将pos1左移，再比较pos1和pos2所指的数，选择出最小的值，记为min2i，将他们的和存储到pos2所指的位置。并将此时pos2所指的位置设置为min1i和min2i的父节点，同时，记min2i所指的位置的编码为1（这里令右子树的编码为1）。 如pos2所指的值小，此时，将pos2右移，再比较pos1和pos2，选出最小的值，记为min2i， 12345678910111213141516171819202122232425262728293031323334353637// Following algorithm constructs the Huffman tree by adding one node at a time // 每次增加一个节点，构建Huffman树 for (a = 0; a &lt; vocab_size - 1; a++) &#123; // First, find two smallest nodes &apos;min1, min2&apos; // 选择最小的节点min1 if (pos1 &gt;= 0) &#123; if (count[pos1] &lt; count[pos2]) &#123; min1i = pos1; pos1--; &#125; else &#123; min1i = pos2; pos2++; &#125; &#125; else &#123; min1i = pos2; pos2++; &#125; // 选择最小的节点min2 if (pos1 &gt;= 0) &#123; if (count[pos1] &lt; count[pos2]) &#123; min2i = pos1; pos1--; &#125; else &#123; min2i = pos2; pos2++; &#125; &#125; else &#123; min2i = pos2; pos2++; &#125; count[vocab_size + a] = count[min1i] + count[min2i]; // 设置父节点 parent_node[min1i] = vocab_size + a; parent_node[min2i] = vocab_size + a; binary[min2i] = 1;// 设置一个子树的编码为1 &#125; 构建好Huffman树后，此时，需要根据构建好的Huffman树生成对应节点的Huffman编码。假设，上述的数据生成的最终的Huffman树为： 此时，count数组，binary数组和parent_node数组分别为： 在生成Huffman编码的过程中，针对每一个词（词都在叶子节点上），从叶子节点开始，将编码存入到code数组中，如对于上图中的“R”节点来说，其code数组为{1,0}，再对其反转便是Huffman编码： 12345678910111213141516171819// Now assign binary code to each vocabulary word // 为每一个词分配二进制编码，即Huffman编码 for (a = 0; a &lt; vocab_size; a++) &#123;// 针对每一个词 b = a; i = 0; while (1) &#123; code[i] = binary[b];// 找到当前的节点的编码 point[i] = b;// 记录从叶子节点到根结点的序列 i++; b = parent_node[b];// 找到当前节点的父节点 if (b == vocab_size * 2 - 2) break;// 已经找到了根结点，根节点是没有编码的 &#125; vocab[a].codelen = i;// 词的编码长度 vocab[a].point[0] = vocab_size - 2;// 根结点 for (b = 0; b &lt; i; b++) &#123; vocab[a].code[i - b - 1] = code[b];// 编码的反转 vocab[a].point[i - b] = point[b] - vocab_size;// 记录的是从根结点到叶子节点的路径 &#125; &#125; 3.3、负样本选中表的初始化（自己没看） 如果是采用负采样的方法，此时还需要初始化每个词被选中的概率。在所有的词构成的词典中，每一个词出现的频率有高有低，我们希望，对于那些高频的词，被选中成为负样本的概率要大点，同时，对于那些出现频率比较低的词，我们希望其被选中成为负样本的频率低点。这个原理于“轮盘赌”的策略一致（详细可以参见“优化算法——遗传算法”）。在程序中，实现这部分功能的代码为： 12345678910111213141516171819// 生成负采样的概率表void InitUnigramTable() &#123; int a, i; double train_words_pow = 0; double d1, power = 0.75; table = (int *)malloc(table_size * sizeof(int));// int --&gt; int for (a = 0; a &lt; vocab_size; a++) train_words_pow += pow(vocab[a].cn, power); // 类似轮盘赌生成每个词的概率 i = 0; d1 = pow(vocab[i].cn, power) / train_words_pow; for (a = 0; a &lt; table_size; a++) &#123; table[a] = i; if (a / (double)table_size &gt; d1) &#123; i++; d1 += pow(vocab[i].cn, power) / train_words_pow; &#125; if (i &gt;= vocab_size) i = vocab_size - 1; &#125;&#125; 在实现的过程中，没有直接使用每一个词的频率，而是使用了词的0.75次方。 4、多线程模型训练以上的各个部分是为训练词向量做准备，即准备训练数据，构建训练模型。在上述的初始化完成后，接下来就是根据不同的方法对模型进行训练，在实现的过程中，作者使用了多线程的方法对其进行训练。 4.1、多线程的处理为了能够对文本进行加速训练，在实现的过程中，作者使用了多线程的方法，TrainModelThread() 对每一个线程上分配指定大小的文件： 12// 利用多线程对训练文件划分，每个线程训练一部分的数据fseek(fi, file_size / (long long)num_threads * (long long)id, SEEK_SET); 这个过程可以通过下图简单的描述： 在实现多线程的过程中，作者并没有加锁的操作，而是对模型参数和词向量的修改可以任意执行，这一点类似于基于随机梯度的方法，训练的过程与训练样本的训练是没有关系的，这样可以大大加快对词向量的训练。抛开多线程的部分，在每一个线程内执行的是对模型和词向量的训练。 作者在实现的过程中，主要实现了两个模型，即CBOW模型和Skip-gram模型，在每个模型中，又分别使用到了两种不同的训练方法，即层次Softmax和Negative Sampling方法。 4.2、CBOW模型4.2.1、从输入层到映射层首先找到每个词对应的词向量，并将这些词的词向量相加，程序代码如下所示： 12345678910111213141516// in -&gt; hiddencw = 0;//b是随机生成的0到window-1，相当于左右各看window-b/2个词for (a = b; a &lt; window * 2 + 1 - b; a++) if (a != window) &#123; //sentence_position 单词在句子中的位置 c = sentence_position - window + a; // 判断c是否越界 if (c &lt; 0) continue; if (c &gt;= sentence_length) continue; // 找到c对应的索引 last_word = sen[c]; if (last_word == -1) continue; // neu1就是隐藏层向量，也就是上下文对应vector的和 for (c = 0; c &lt; layer1_size; c++) neu1[c] += syn0[c + last_word * layer1_size]; cw++;&#125; 当累加完窗口内的所有的词向量的之后，存储在映射层neu1中，并取平均，程序代码如下所示： 1for (c = 0; c &lt; layer1_size; c++) neu1[c] /= cw; 当取得了映射层的结果后，此时就需要使用Hierarchical Softmax或者Negative Sampling对模型进行训练。 4.2.2、Hierarchical Softmax1234567891011121314151617181920if (hs) for (d = 0; d &lt; vocab[word].codelen; d++) &#123; f = 0; // point存储了从该词的叶子结点的编号到root的序号，这些序号可以对应到syn1的位置，也就是参数向量的位置 l2 = vocab[word].point[d] * layer1_size; // Propagate hidden -&gt; output // q=sigma(x*theta) for (c = 0; c &lt; layer1_size; c++) f += neu1[c] * syn1[c + l2]; if (f &lt;= -MAX_EXP) continue; else if (f &gt;= MAX_EXP) continue; // 查表得知sigma的值，省去计算的时间 else f = expTable[(int)((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))]; // g = eta(1-d-q) g = (1 - vocab[word].code[d] - f) * alpha; // Propagate errors output -&gt; hidden // e = e + g*theta for (c = 0; c &lt; layer1_size; c++) neu1e[c] += g * syn1[c + l2]; // Learn weights hidden -&gt; output // theta = theta + g*x for (c = 0; c &lt; layer1_size; c++) syn1[c + l2] += g * neu1[c]; &#125; 接下来更新Context(w) 123456789// hidden -&gt; in for (a = b; a &lt; window * 2 + 1 - b; a++) if (a != window) &#123; c = sentence_position - window + a; if (c &lt; 0) continue; if (c &gt;= sentence_length) continue; last_word = sen[c]; if (last_word == -1) continue; for (c = 0; c &lt; layer1_size; c++) syn0[c + last_word * layer1_size] += neu1e[c]; &#125; Word2Vec为什么快 用查表代替计算sigmoid 相对于神经网络的结构，去掉了隐藏层 增量训练从搜索引擎爬包含新词的文本，加上一个小语料，训练一个w2v模型。 对于每个新词，找出小模型中最接近的10个词，以及每个词与新词的相似度打分score。 再从大模型中找出每个词的词向量，每个维度乘以小模型中的score，最多叠加5个。再对每个维度取加权平均。 最后转成单位向量。 参考 【1】http://blog.csdn.net/google19890102/article/details/51887344]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Word2Vec原理]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FNLP%2FWord2Vec%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[参考 《word2vec数学原理.pdf》 词向量基础​ 用词向量来表示词并不是word2vec的首创，在很久之前就出现了。最早的词向量是很冗长的，它使用是词向量维度大小为整个词汇表的大小，对于每个具体的词汇表中的词，将对应的位置置为1。比如我们有下面的5个词组成的词汇表，词”Queen”的序号为2， 那么它的词向量就是(0,1,0,0,0)。同样的道理，词”Woman”的词向量就是(0,0,0,1,0)。这种词向量的编码方式我们一般叫做1-of-N representation或者one hot representation. ​ One hot representation用来表示词向量非常简单，但是却有很多问题。最大的问题是我们的词汇表一般都非常大，比如达到百万级别，这样每个词都用百万维的向量来表示简直是内存的灾难。这样的向量其实除了一个位置是1，其余的位置全部都是0，表达的效率不高，能不能把词向量的维度变小呢？ ​ Dristributed representation可以解决One hot representation的问题，它的思路是通过训练，将每个词都映射到一个较短的词向量上来。所有的这些词向量就构成了向量空间，进而可以用普通的统计学的方法来研究词与词之间的关系。这个较短的词向量维度是多大呢？这个一般需要我们在训练时自己来指定。 ​ 比如下图我们将词汇表里的词用”Royalty”,”Masculinity”, “Femininity”和”Age”4个维度来表示，King这个词对应的词向量可能是(0.99,0.99,0.05,0.7)(0.99,0.99,0.05,0.7)。当然在实际情况中，我们并不能对词向量的每个维度做一个很好的解释。 ​ 有了用Dristributed representation表示的较短的词向量，我们就可以较容易的分析词之间的关系了，比如我们将词的维度降维到2维，有一个有趣的研究表明，用下图的词向量表示我们的词时，我们可以发现： \vec {King} - \vec {Man} + \vec {Woman} = \vec {Queen} 可见我们只要得到了词汇表里所有词对应的词向量，那么我们就可以做很多有趣的事情了。不过，怎么训练得到合适的词向量呢？一个很常见的方法是使用神经网络语言模型。 2. CBOW与Skip-Gram用于神经网络语言模型​ 在word2vec出现之前，已经有用神经网络DNN来用训练词向量进而处理词与词之间的关系了。采用的方法一般是一个三层的神经网络结构（当然也可以多层），分为输入层，隐藏层和输出层(softmax层)。 ​ 这个模型是如何定义数据的输入和输出呢？一般分为CBOW（Continuous Bag-of-Words） 与Skip-Gram两种模型。 2.1 CBOW上下文相关的词对应的词向量，而输出就是这特定的一个词的词向量。比如下面这段话，我们的上下文大小取值为4，特定的这个词是”Learning”，也就是我们需要的输出词向量,上下文对应的词有8个，前后各4个，这8个词是我们模型的输入。由于CBOW使用的是词袋模型，因此这8个词都是平等的，也就是不考虑他们和我们关注的词之间的距离大小，只要在我们上下文之内即可。 在这个CBOW神经网络模型中，输入层有8个神经元（8个词向量），输出层有词汇表D大小的神经元。 目标函数通常为 L=\sum_{w \in C} \log p(w|Context(w)) 为什么是这个形式？ 根据n-gram模型和对数最大似然，得到这个目标函数。 包括四个层：输入、投影、隐藏、输出。 对于语料C中的任意一个词w，将Context(w)设为取前面的n-1个词，这样二元对(Context(w),w)就是一个训练样本。 投影层向量$x_w$的构造是，将输入层的n-1个词向量按顺序首尾相接的拼起来，长度就是m(n-1)了（每个词向量的长度是m）。 从而 z_w = tanh(W{x_w}+p) \\ y_w=Uz_w+q其中，tanh是双曲正切函数，用来做隐藏层的激活函数。 经过上面两步计算得到的$yw=(y{w,1},y{w,2},…,y{w,N})^T$是一个长度为N的向量，其分量不能代表概率。如果想要$y_{w,i}$表示当上下文为Context(w)时下一次为词典D中第i个词的概率，则还需要做一个softmax归一化，之后得到 p(w|Context(w))=\frac {e^{y_{w,i_w}}} {\sum_{i=1}^N e^{y_{w,i_w}}}其中$i_w$表示词w在词典D中的索引。 与n-gram相比，神经概率语言模型的优势是： 1、词语之间的相似性可以通过词向量来体现。 1）神经网络模型通过上下文来预测，那么相似的上下文的词的词向量也是相似的； 2）概率函数关于词向量是光滑的，即词向量的一个小变化对概率的影响也是一个小变化。 2、词向量自带平滑功能（因为$p(w|Context(w)) \in (0,1)$不会为零）。 2.2 Skip-Gram与CBOW相反，输入是一个特定向量，输出是特定词对应的上下文词向量。 3、基于Hierarchical Softmax的模型3.1 CBOW模型网络的构建 输入层是上下文的若干个词的词向量 投影层就是将这些词向量直接相加。 层次Softmax的基本思想就是： 对于词典D中的任意词w，Huffman树中必存在一条从根结点到词w对应结点的路径$p^w$，且这条路径是唯一的。路径$p^w$上存在$l^w-1$个分支，将每个分支看做是一个二分类，每一次分类就产生一个概率，将这些概率连乘起来，就是所需的$p(w|Context(w))$ 。 条件概率连乘的公示可以写为 p(w|Context(w))=\prod_{j=2}^{l^w}p(d_j^w|x_w,\theta_{j-1}^w)其中， p(d_j^w|x_w,\theta_{j-1}^w)=[\sigma(x_w^T\theta_{j-1}^w)]^{1-d_j^w} \cdot [1-\sigma(x_w^T\theta_{j-1}^w)]^{d_j^w} \tag{3-1}这里$\sigma(xw^T\theta{j-1}^w)$表示分到正类的概率。 将3-1代入对数似然函数，得到 L=\sum_{w \in C} \sum_{j=2}^{l^w} \{ (1-d_j^w)\log [\sigma(x_w^T\theta_{j-1}^w)] + d_j^w \log[1-\sigma(x_w^T\theta_{j-1}^w)] \}其中，令 L(w,j)=(1-d_j^w)\log [\sigma(x_w^T\theta_{j-1}^w)] + d_j^w \log[1-\sigma(x_w^T\theta_{j-1}^w)]至此，已经推导出对数似然函数，这就是CBOW模型的目标函数。 参数$\theta$是怎样的矩阵？ 我理解，任何用树的多个二分类问题，目标函数都可以表示成这种形式。 为了使该目标函数最大化，word2vec采用的是随机梯度上升法。每取一个样本，计算梯度再刷新所有的参数。推导出更新公式为 \theta_{j-1}^w = \theta_{j-1}^w + \eta [1-d_j^w-\sigma(x_w^T\theta_{j-1}^w)]x_w^T同样的 \frac {\partial L(w,j)} {\partial x_w}=[1-d_j^w-\sigma(x_w^T\theta_{j-1}^w)]\theta_{j-1}^w我们的最终目的是要求词典D中每个词的词向量，而这里的$x_w$表示的是Context(w)所有词向量的累加和，那么如何利用偏导对$v(\tilde w), w \in Context(\tilde w)$进行更新呢，word2vec算法中直接取 v(\tilde w) = v(\tilde w)+\eta \sum_{j=2}^{l^w} \frac {\partial L(w,j)} {\partial x_w}即把后面增量的值贡献到Context(w)的每一个词的词向量上。 伪代码如下： 1234567891011for j=huffman code:&#123; 1)q=sigma(x*theta) 2)g=eta(1-d-q) 3)e=e+g*theta 4)theta=theta+g*x&#125;for w in Context(w):&#123; v(w) = v(w) + e&#125; 注意3、 4两步不能颠倒位置，要先计算出e，再更新θ。 4、基于Negative Sampling的模型NEG不使用Huffman树，而是利用随机负采样，能大幅度提高性能。 前面的原理差不多，也是用梯度下降，关键在于，h-softmax是通过Huffman树的路径长度来进行迭代和更新参数；NEG是通过找出负采样来迭代。 4.1 负采样算法词典D中的词在语料C中出现的次数有高有低，对于那些高频词，被选为负采样的概率就应该比较大，反之较小。这是我们对采样过程的一个大致要求，本质上就是一个带权采样问题。 设词典D中的每个词w对应一条线段l(w)，则线段的长度可以表示为 len(w)=\frac {count(w)} {\sum_{u \in D} count(u)}也就是计算词频再归一化。现在将这些线段首尾相连拼接在一起，形成一个长度为1的单位线段。如果随机往这个单位线段上打点，则其中越长的线段命中概率越大。 通过这些线段得到一个非等距剖分（假设分成N个区间），再定义一个等距剖分（假设分成M个区间），$M &gt;&gt; N$ 。 建立如下的映射关系 Table(i) = w_k, \ where\ m_i \in I_k,\ i=1,2,...,M-1那么，采样就是每次生成一个[1,M-1]之间的整数r，Table(r)就是一个样本。如果负采样的时候选到自己，就跳过再选。 Word2Vec与神经网络模型的区别 NN的输入是类似n-gram，取前N-1个词，w2v取前后各n-1个词 NN多了隐层，输入到隐层用双曲正切当激活函数。 NN的输入词向量是首尾拼接，W2V是加总。这样当窗口中向量不足时，也不需要补。 NN的输出是一个长度为N的向量，就是整个词汇表的长度，然后再做一个softmax归一化，得到给定上下文时下一个词恰好为词汇表的某个词的概率。 疑问1）在这里非叶子结点对应的那些向量就可以扮演参数的角色？ 就是说可以把$\theta$当做待求的参数，这样就可以用sigmoid来求分类的概率。 参考 【1】word2vec原理(一) CBOW与Skip-Gram模型基础]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[NL工具]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FNLP%2FNLP%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[NLP工具Word2Vec-Java加载模型及词向量计算hdfs版 /pig-ext-lite/src/main/java/com/buzzinate/pig/util/WordVec.java 单机版 /persona/src/main/java/com/iclick/persona/nlp/word2vec/W2VContral.java 词向量聚类/home/david/gitlab/user-gene/domain-classify-model/src/main/java/com/buzzinate/domain/classifier/DomainClassifier.java 123456789101112131415161718192021/** * 通过keams得到中心点向量 * * @param wordVec * @return */ public ArrayList&lt;float[]&gt; getCenterVecByKeams(ArrayList&lt;float[]&gt; wordVec) &#123; ArrayList&lt;float[]&gt; centerVec = new ArrayList&lt;float[]&gt;(); List&lt;Classes&gt; cls = null; logger.info(&quot;before keams： &quot; + wordVec.size()); System.out.println(&quot;before keams： &quot; + wordVec.size()); cls = KMeansClustering.getClusteringResult(wordVec, wordVec.subList(0, Math.min(100, wordVec.size() - 1))); for (Classes cl : cls) &#123; if (!Float.isNaN(WVUtils.toFloat(cl.getCenter())[0])) centerVec.add(WVUtils.toFloat(cl.getCenter())); &#125; logger.info(&quot;after keams： &quot; + centerVec.size()); System.out.println(&quot;after keams： &quot; + centerVec.size()); return centerVec;&#125; 通过向量找最近的词Ansj-seg添加词到自定义词典user-gene中用的是某一版ansj的基础上修改的，使用方法是 12Value value = new Value(newWord, new String[] &#123; &quot;userDefine&quot;, &quot;1000&quot; &#125;); Library.insertWord(UserDefineRecognition.FOREST, value); 知乎找到的方法是https://www.zhihu.com/question/32226656 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152作者：ansj链接：https://www.zhihu.com/question/32226656/answer/113724991来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。package org.ansj;import java.util.List;import org.ansj.app.summary.SummaryComputer;import org.ansj.domain.Result;import org.ansj.domain.Term;import org.ansj.library.UserDefineLibrary;import org.ansj.splitWord.analysis.ToAnalysis;import org.nlpcn.commons.lang.tire.domain.Forest;import org.nlpcn.commons.lang.tire.domain.Value;import org.nlpcn.commons.lang.tire.library.Library;public class Test &#123; public static void main(String[] args) throws Exception &#123; // 构造一个用户词典 Forest forest = Library.makeForest(&quot;library/default.dic&quot;); forest = new Forest(); // 增加新词,中间按照&apos;\t&apos;隔开 UserDefineLibrary.insertWord(&quot;ansj中文分词&quot;, &quot;userDefine&quot;, 1000); Result terms = ToAnalysis.parse(&quot;我觉得Ansj中文分词是一个不错的系统!我是王婆!&quot;); System.out.println(&quot;增加新词例子:&quot; + terms); // 删除词语,只能删除.用户自定义的词典. UserDefineLibrary.removeWord(&quot;ansj中文分词&quot;); terms = ToAnalysis.parse(&quot;我觉得ansj中文分词是一个不错的系统!我是王婆!&quot;); System.out.println(&quot;删除用户自定义词典例子:&quot; + terms); // 歧义词 Value value = new Value(&quot;济南下车&quot;, &quot;济南&quot;, &quot;n&quot;, &quot;下车&quot;, &quot;v&quot;); System.out.println(ToAnalysis.parse(&quot;我经济南下车到广州.中国经济南下势头迅猛!&quot;)); Library.insertWord(UserDefineLibrary.ambiguityForest, value); System.out.println(ToAnalysis.parse(&quot;我经济南下车到广州.中国经济南下势头迅猛!&quot;)); // 多用户词典 String str = &quot;神探夏洛克这部电影作者.是一个dota迷&quot;; System.out.println(ToAnalysis.parse(str)); // 两个词汇 神探夏洛克 douta迷 Forest dic1 = new Forest(); Library.insertWord(dic1, new Value(&quot;神探夏洛克&quot;, &quot;define&quot;, &quot;1000&quot;)); Forest dic2 = new Forest(); Library.insertWord(dic2, new Value(&quot;dota迷&quot;, &quot;define&quot;, &quot;1000&quot;)); System.out.println(ToAnalysis.parse(str, dic1, dic2)); &#125;&#125; 如果在spark下面用要修改 Hanlp官方文档http://hanlp.linrunsoft.com/doc/_build/html/index.html https://github.com/hankcs/HanLP/releases HanLP中的数据分为词典和模型，其中词典是词法分析必需的，模型是句法分析必需的。 1234data│├─dictionary└─model 配置文件的作用是告诉HanLP数据包的位置，只需修改第一行 1root=usr/home/HanLP/ 为data的父目录即可 一开始用1.2.9版本，一直提示找不到HanLP.properties，后来换了1.3.2，默认是项目根目录下的data目录。 4.1 分词 标准分词 1list&lt;Term&gt; termlist = StandardTokenizer.segment(&quot;XXXX&quot;); HanLP中有一系列“开箱即用”的静态分词器，以Tokenizer结尾。 HanLP.segment其实是对StandardTokenizer.segment的包装。 分词结果包含词性，每个词性的意思请查阅《HanLP词性标注集》。 算法详解：《词图的生成》 词性标注（Part-of-Speech tagging 或POS tagging)：又称词类标注或者简称标注，是指为分词结果中的每个单词标注一个正确的词性的程序，也即确定每个词是名词、动词、形容词或其他词性的过程。在汉语中，词性标注比较简单，因为汉语词汇词性多变的情况比较少见，大多词语只有一个词性，或者出现频次最高的词性远远高于第二位的词性。据说，只需选取最高频词性，即可实现80%准确率的中文词性标注程序。 利用HMM即可实现更高准确率的词性标注。 去掉停用词、标点的分词 1list&lt;Term&gt; termlist = NotionalTokenizer.segment()]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[iau的ES使用场景]]></title>
    <url>%2F2018%2F03%2F17%2FElastic%20Search%2Fiau%E7%9A%84ES%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF%2F</url>
    <content type="text"><![CDATA[比较brand和competitor的交叉人群 分别算出brand和competitor的人群，然后再计算包含brand和competitor的人群，就得出交叉人群。 比较brand和competitor的距离 就是比较brand和competitor的每个产品和对方关键词的相关度。]]></content>
      <categories>
        <category>Elastic Search</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ES的analyzer]]></title>
    <url>%2F2018%2F03%2F17%2FElastic%20Search%2FES%E7%9A%84analyzer%2F</url>
    <content type="text"><![CDATA[转自ElasticSearch 解析机制常见用法库 之 analyzer常用用法 单词或文档先经过Character Filters；Character Filters的作用就是对文本进行一个预处理，例如把文本中所有“&amp;”换成“and”，把“?”去掉等等操作。 ​ 之后就进入了十分重要的tokenizers模块了，Tokenizers的作用是进行分词，例如，“tom is a good doctor .”。经过Character Filters去掉句号“.”（假设）后，分词器Tokenizers会将这个文本分出很多词来：“tom”、“is”、“a”、“good”、“doctor”。 ​ 经过分词之后的集合，最后会进入Token Filter词单元模块进行处理，此模块的作用是对已经分词后的集合(tokens)单元再进行操作，例如把“tom”再次拆分成“t”、“o”、“m”等操作。最后得出来的结果集合，就是最终的集合。 ​ 所以整个流程是：单词 ====》Character Filter 预处理 =====》tokenizer分词 ====》 token filter对分词进行再处理。 ​ 到此为止，Analyzer是什么鬼？它干什么呢？ ​ Analyzer是由一个tokenizer、零到多个Token Filter、还有零到多个CharFilters构成的，也就是说一个Analyzer就是一个完整的解析模块。 ​ 下面，着重介绍一下常用的Analyzer、Tokenizer、Token filter、Character Filter： Standard Analyzer 一个“standard”标准类型的 analyzer 就是由 标准分词 “Standard Tokenizer”和标准分词过滤器“Standard Token Filter”、小写字母转换分词过滤“Lower case Token Filter”、还有“Stop Token Filter”过滤构成的 以下是一个standard类型 设置说明stopwords 一个用于初始化stop filter的需要stop 单词的列表.默认为空 。 max_token_length 最大的token集合,即经过tokenizer过后得到的结果集的最大值。如果token的长度超过了设置的长度，将会继续分，默认255 Stop Analyzer 一个stop类型的analyzer是由 Lower case Tokenizer 和 Stop Token Filter构成的。 以下是一个stop analyzer可以设置的属性: 设置 说明 stopwords 一个用于初始化stop filter的需要stop 单词的列表.默认单词是英语 stopwords_path 一个有stopwords 配置文件的路径(一个和config文件相关的路径或者URL) 用“stopwords: _none_ ”来定义一个空的stopword列表 Simple Analyzer 一个simple类型的analyzer是由lower case Tokenizer构成的，具体信息可以查看此Tokenzier Whitespace Analyzer 一个whitespace类型的analyzer是由Whitespace Tokenizer构成，请具体查看Whitespace Tokenizer Keyword Analyzer 一个keyword类型的analyzer，它的Tokenizer将整块的数据作为一个小Token（即经过Tokenizer过滤后的数据），这对于像“邮政编码”、“id”等数据非常有用。注意：当使用并定义这种analyzer的时候，单纯的将fieled 设置为“not_analyzed”可能会更有意义。 Pattern Analyzer 一个pattern类型的analyzer可以通过正则表达式将文本分成”terms”(经过token Filter 后得到的东西 )。接受如下设置: 一个 pattern analyzer 可以做如下的属性设置: lowercase terms是否是小写. 默认为 true 小写. pattern 正则表达式的pattern, 默认是 \W+. flags 正则表达式的flags. stopwords 一个用于初始化stop filter的需要stop 单词的列表.默认单词是空的列表 Snowball Analyzer 一个snowball类型的analyzer是由standard tokenizer和standard filter、lowercase filter、stop filter、snowball filter这四个filter构成的。 snowball analyzer 在Lucene中通常是不推荐使用的。 Language Analyzers 一个用于解析特殊语言文本的analyzer集合。（ arabic,armenian, basque, brazilian, bulgarian, catalan, cjk, czech, danish, dutch, english, finnish, french,galician, german, greek, hindi, hungarian, indonesian, irish, italian, latvian, lithuanian, norwegian,persian, portuguese, romanian, russian, sorani, spanish, swedish, turkish, thai.）可惜没有中文。不予考虑 Custom Analyzer 简而言之，是自定义的analyzer。允许多个零到多个tokenizer，零到多个 Char Filters. custom analyzer 的名字不能以 “_”开头. The following are settings that can be set for a custom analyzer type: Setting Description tokenizer 通用的或者注册的tokenizer. filter 通用的或者注册的 token filters. char_filter 通用的或者注册的 character filters. position_increment_gap 距离查询时，最大允许查询的距离，默认是100 自定义的模板： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051index : analysis : analyzer : myAnalyzer2 : type : custom tokenizer : myTokenizer1 filter : [myTokenFilter1, myTokenFilter2] char_filter : [my_html] position_increment_gap: 256 tokenizer : myTokenizer1 : type : standard max_token_length : 900 filter : myTokenFilter1 : type : stop stopwords : [stop1, stop2, stop3, stop4] myTokenFilter2 : type : length min : 0 max : 2000 char_filter : my_html : type : html_strip escaped_tags : [xxx, yyy] read_ahead : 1024]]></content>
      <categories>
        <category>Elastic Search</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Scala技巧总结]]></title>
    <url>%2F2018%2F02%2F18%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FSCALA%2Fscala%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[optionOption[A] 是一个类型为 A 的可选值的容器： 如果值存在， Option[A] 就是一个 Some[A] ，如果不存在， Option[A] 就是对象 None 。 Option类型的值通常作为Scala集合类型（List,Map等）操作的返回类型。比如Map的get方法： 12345678scala&gt; val capitals = Map("France"-&gt;"Paris", "Japan"-&gt;"Tokyo", "China"-&gt;"Beijing")capitals: scala.collection.immutable.Map[String,String] = Map(France -&gt; Paris, Japan -&gt; Tokyo, China -&gt; Beijing)scala&gt; capitals get "France"res0: Option[String] = Some(Paris)scala&gt; capitals get "North Pole"res1: Option[String] = None Option有两个子类别，Some和None。当程序回传Some的时候，代表这个函式成功地给了你一个String，而你可以透过get()函数拿到那个String，如果程序返回的是None，则代表没有字符串可以给你。 在返回None，也就是没有String给你的时候，如果你还硬要调用get()来取得 String 的话，Scala一样是会抛出一个NoSuchElementException异常给你的。 我们也可以选用另外一个方法，getOrElse。这个方法在这个Option是Some的实例时返回对应的值，而在是None的实例时返回传入的参数。换句话说，传入getOrElse的参数实际上是默认返回值。 12345678910111213141516scala&gt; capitals get "North Pole" getwarning: there was one feature warning; re-run with -feature for detailsjava.util.NoSuchElementException: None.get at scala.None$.get(Option.scala:347) at scala.None$.get(Option.scala:345) ... 33 elidedscala&gt; capitals get "France" getwarning: there was one feature warning; re-run with -feature for detailsres3: String = Parisscala&gt; (capitals get "North Pole") getOrElse "Oops"res7: String = Oopsscala&gt; capitals get "France" getOrElse "Oops"res8: String = Paris 提示：Scala程序使用Option非常频繁，在Java中使用null来表示空值，代码中很多地方都要添加null关键字检测，不然很容易出现NullPointException。因此Java程序需要关心那些变量可能是null,而这些变量出现null的可能性很低，但一但出现，很难查出为什么出现NullPointerException。Scala的Option类型可以避免这种情况，因此Scala应用推荐使用Option类型来代表一些可选值。使用Option类型，读者一眼就可以看出这种类型的值可能为None。文／JasonDing（简书作者）原文链接：http://www.jianshu.com/p/95896d06a94d 详解Option[T]在Scala里Option[T]实际上是一个容器，就像数组或是List一样，你可以把他看成是一个可能有零到一个元素的List。当你的Option里面有东西的时候，这个List的长度是1（也就是 Some），而当你的Option里没有东西的时候，它的长度是0（也就是 None）。 case class例如： 12345abstract class Exprcase class Var(name:String) extends Exprcase class Number(num:Double) extends Exprcase class UnOp(operator:String, arg:Expr) extends Exprcase class BinOp(operator:String,left:Expr,right:Expr) extends Expr 首先，编译器为case class生成一个同名的对象构造器（Factory Method），也就是你可以使用 Var(“x”) 来创建一个类的实例，而无需使用new Var(“x”). 12scala&gt; val x = Var(&quot;x&quot;)x: Var = Var(x) 其次，Scala编译器为case class的构造函数的参数创建以参数名为名称的属性，比如Val的类的参数name:String 可以直接通过 .name访问，比如： 12scala&gt; x.nameres1: String = x 第三，编译器为case class 构造了更自然的toString，hashCode和equals实现，它们会递归打印，比较case class的参数属性。比如： 12345678scala&gt; val op=BinOp(&quot;+&quot;,Number(1),x)op: BinOp = BinOp(+,Number(1.0),Var(x))scala&gt; println(op)BinOp(+,Number(1.0),Var(x))scala&gt; op.right == Var(&quot;x&quot;)res3: Boolean = true 数据类型double转int 12val a=1.0a.toInt 类型强转，用asInstanceOf 12345//java中StructObjectInspector inspector = (StructObjectInspector)reader.getObjectInspector();//scala中val inspector = reader.getObjectInspector().asInstanceOf[StructObjectInspector] 文件读写读文件 1234567891011import scala.io.Sourceobject Test &#123; def main(args: Array[String]) &#123; println(&quot;Following is the content read:&quot; ) Source.fromFile(&quot;test.txt&quot; ).foreach&#123; print &#125; &#125;&#125; 写文件123val writer = new PrintWriter(new File(&quot;ctrout.dat&quot;))writer.println(p.toString() + &quot;,&quot; + log.get.predictionCtr.toString() + &quot;,&quot; + log.get.isClick)writer.close() 初始化空对象123456var qc_hk : QueueCollection = null try &#123; qc_hk = new QueueCollection(config.getProperty(&quot;data.dir&quot;, &quot;/var/lib/queues_hk&quot;)) &#125; catch &#123; case t: Throwable =&gt; t.printStackTrace() &#125; 集合数组定义数组 123456789val greetStrings = new Array[String](3) greetStrings(0) = &quot;Hello&quot; greetStrings(1) = &quot;, &quot; greetStrings(2) = &quot;world!\n&quot; for (i &lt;- 0 to 2) print(greetStrings(i)) 取数组下标greetStrings(0) = ‘Hello’ 定义2val numNames = Array(‘1’, ‘2’ , ‘3’)实际是创造并返回新数组的apply工厂方法。apply有个不定个数的参数。完整写法是val numNames = Array.apply (‘1’, ‘2’ , ‘3’) 12//新建一个固定长度的数组val data = Array.ofDim[T](max) 1234//取数组中不为空的并转为listdef list(): List[T] = &#123; data.filter(x =&gt; x != null).toList &#125; 12345// 重复多次计算的结果生成一个数组Array.fill(3)&#123; math.random &#125;res3: Array[Double] = Array(0.365461167592537, 1.550395944913685E-4, 0.7907242137333306)val queues = Array.fill(4)(new java.util.LinkedList[QueueInfo]) 列表List创建了不能改变。val list1 = List(1,2,3) 列表的叠加 123 val oneTwo = List(1,2) val threeFour = List(3,4) val oneFour = oneTwo:::threeFour 将新元素组合到现有列表的前端 12 val oneTwo = List(1,2) val zeroTwo = 1 :: oneTwo 在 1 :: oneTwo中，::是右操作数oneTwo的方法，1是方法的传入参数，因此也可写成oneTwo.::(1) list的一些方法计算长度为4的元素个数list.count(s =&gt; s.length == 4) 判断元素是否在listlist.exists(s =&gt; s == ‘until’) 返回长度为4的元素组成的新列表list.filter(s =&gt; s.length == 4) 判断是否列表所有元素都以1结尾list.forall(s =&gt; s.endsWith(‘1’)) 打印数组list.foreach(print) 列表每个元素加上字符的新列表list.map(s =&gt; s + ‘y’) 用列表元素组成字符串list.mkStirng(“, “) 按照第一个元素的小写字母排序list.sort((s,t) =&gt; s.charAt(0).toLowerCase &lt; t.charAr(0).toLowerCase) list整体转数据类型123name2NumMap: Map[String, String]val sum = name2NumMap.map(_._2.toDouble).sum 元组元组不可变，但可以包含不同类型的元素 定义val pair = (99, ‘tuple’)用下划线访问元素pair._1pair._2 列表可以用list(0)，因为列表的apply方法始终返回同样类型。另外，元组_N的索引从1开始。因为对拥有静态类型元素的其他语言，入Haskell和ML，从1开始是传统设定。 集（set）和映射（map）scala的API包含set的基本特质（trait），特质相当于接口。Scala还提供了两个子特质，可变set和不可变set。 比如图中的HashSet，各有一个可变或不可变的类型。 定义 12var jetset = Set(&quot;1&quot;, &quot;2&quot;) //不可变setjetset += &quot;3&quot; //成为可变set 不可变set用+=，会产生一个新的可变set，+=在这里的完整写法是jetset = jetset + “3” 这里jetset默认是不可变set，因此+=操作会需要对jetset重新赋值，要用var 12345import scala.collection.mutable.Setval movieSet = Set(&quot;Hi&quot;, &quot;df&quot;)movieSet += &quot;Shet&quot; 这里显式指定了是可变set，所以新增元素就不需要重新赋值，用val就可以 定义 123456import scala.collection.mutable.Mapval treaMap = Map[Int, String]()treaMap += (1-&gt; &quot;Go&quot;)println(treaMap(1)) //这里的1是key map的遍历注意这个case 12var a:Map[String,Int]=Map("k1"-&gt;1,"k2"-&gt;2)a.foreach&#123;case (e,i) =&gt; println(e,i)&#125; List转Map1list的toMap方法，但list需要是类似(String, String)的结构，这样会自动转为map 元组创建map12val allDistinctFeatures: Map[Int, mutable.HashSet[Double]] = Map((startCol until endCol).map(col =&gt; (col, mutable.HashSet.empty[Double])): _*) map插入元素1allDistinctFeatures(col) += feature 创建一个map的list123var testMapList = ListBuffer[Map[String, String]]()testMapList += Map(&quot;gpstime&quot;-&gt;&quot;2018-1-06 00:03:44&quot;, &quot;lon&quot;-&gt;&quot;118.22065000&quot;, &quot;lat&quot;-&gt;&quot;26.19307700&quot;, &quot;unittype&quot;-&gt;&quot;1&quot;)testMapList += Map(&quot;gpstime&quot;-&gt;&quot;2018-1-06 00:12:52&quot;, &quot;lon&quot;-&gt;&quot;118.22065000&quot;, &quot;lat&quot;-&gt;&quot;26.19307700&quot;, &quot;unittype&quot;-&gt;&quot;1&quot;) 零碎Unit：在scala任何的函数、表达式、方法都有返回值（有些情况类似与java的void，所以scala创立了unit这个标识符来表示特殊的返回值） 12val (codeMap, codeLength) = featureCodeMap.get(featureFlag).getfeatureCodeMap.get(featureFlag).get出来是一个tuple (Map[String, Int], Int) 定义空的scala对象 123var buff: BufferedWriter = nullbuff = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(existedSlotTagsPath)))buff = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(featureCodeMapPath))) SeqSeq 是列表，适合存有序重复数据，进行快速插入/删除元素等场景Set 是集合，适合存无序非重复数据，进行快速查找海量元素的等场景 本地文件读取123import scala.io.Sourceval s = Source.fromFile(&quot;D:\\code\\Data\\rtb.BJ2.2016052513_7.log&quot;).getLines().foreach &#123; x =&gt; println(x.toString()) &#125; 找出文本中最长的一行 123val longestLine = lines.reduceLeft ( (a,b) =&gt; if (a.length &gt; b.length ) then a else b) Array数组的初始化 1234567891011121314//长度为10的整数数组，所有元素初始化为0 val numArr = new Array[Int](10) //长度为10的字符串数组，所有元素初始化为null val numArr = new Array[String](10) //长度为2的数组，数据类型自动推断出来，已经提供初始值就不需要new关键字 val s = Array(&quot;cai&quot;,&quot;yong&quot;) //通过ArrayName(index)访问数组元素和更改数组元素 val s = Array(&quot;cai&quot;,&quot;yong&quot;) println(s(0)) s(0) = &quot;haha&quot; println(s(0)) 1234//n.length是0val n = Array[Double](100)//n.length是100val n = new Array[Double](100) Listlist元素用字符拼接1234val keyList = new ListBuffer[String]keyList += &quot;a&quot;keyList += &quot;b&quot;println(keyList.addString(new mutable.StringBuilder, &quot;|&quot;).toString) ListBuffer123val instance: ListBuffer[Int]=ListBuffer()ListBuffer.result = ListBuffer.toList ListBuffer添加元素12val chiResult = new ListBuffer[(Int, Double)]chiResult += ((0, left)) // 两层括号 list转listBuffer1234scala&gt; val l = List(1,2,3)l: List[Int] = List(1, 2, 3)scala&gt; l.to[ListBuffer]res1: scala.collection.mutable.ListBuffer[Int] = ListBuffer(1, 2, 3) 转java.util.List首先需要scala.collection.JavaConversions._ Java和Scala容器的转换 https://blog.csdn.net/high2011/article/details/52204625 1234import scala.collection.JavaConverters._val list: java.util.List[Int] = Seq(1,2,3,4).asJavaval buffer: scala.collection.mutable.Buffer[Int] = list.asScala map新建一个map12// 如果后面没有括号，会被认为是一个seqvar map = mutable.Map[String, String]() map根据key排序123456789101112val aMap = new scala.collection.mutable.HashMap[String, Double] val a = Array(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;) val b = Array(4, 5, 8, 9) for(i &lt;- a.indices)&#123; aMap += (a(i) -&gt; b(i)) &#125; // 从小到大(默认) val mapSortSmall = aMap.toList.sortBy(_._2) mapSortSmall.foreach(line =&gt; println(line._1 +&quot;\t&quot;+ line._2)) 12345678val arr = new ArrayBuffer[String] arr += &quot;aaa&quot; arr += &quot;bbb&quot; arr += &quot;bbb&quot; val aa = arr.map(f =&gt; &#123; (f, 1) &#125;).groupBy(_._1).mapValues(_.size).toList.sortBy(_._2) println(aa.last._1) 取所有的key1234567val scores=Map(&quot;Alice&quot;-&gt;10,&quot;Bob&quot;-&gt;3,&quot;Cindy&quot;-&gt;8)// 获取所有的key val nameList=scores.map(_._1)// map 函数返回List println(nameList.getClass)遍历list中的元素 nameList.foreach((x:String)=&gt;print(x+&quot; &quot;)) mutable和immutable互转If you just want a mutable HashMap, you can just use x.toMap in 2.8 or collection.immutable.Map(x.toList: _*) in 2.7. But if you want the whole structure to be immutable—including the underlying set!—then you have to do more: you need to convert the sets along the way. In 2.8: 1x.map(kv =&gt; (kv._1,kv._2.toSet)).toMap In 2.7: 123collection.immutable.Map( x.map(kv =&gt; (kv._1,collection.immutable.Set(kv._2.toList: _*))).toList: _*) 可变容器listbuffer12345678import scala.collection.mutable.ListBufferval fieldListBuffer = new ListBuffer[String]// orval list = ListBuffer.empty[String] list += &quot;ds&quot; list += &quot;sd&quot; list.foreach(println) HashSet定义1val distinctLabels = mutable.HashSet.empty[Double] for循环1234567891011object Test &#123; def main(args: Array[String]) &#123; var a = 0; var b = 0; // for 循环 for( a &lt;- 1 to 3; b &lt;- 1 to 3)&#123; println( &quot;Value of a: &quot; + a ); println( &quot;Value of b: &quot; + b ); &#125; &#125;&#125; 以上语法中，Range 可以是一个数字区间表示 i to j ，或者 i until j。左箭头 &lt;- 用于为变量 x 赋值，循环范围是[i, j]。 有过滤器的循环 123456789101112object Demo &#123; def main(args: Array[String]) &#123; var a = 0; val numList = List(1,2,3,4,5,6,7,8,9,10); // for loop execution with multiple filters for( a &lt;- numList if a != 3; if a &lt; 8 )&#123; println( &quot;Value of a: &quot; + a ); &#125; &#125;&#125; 跳出循环http://www.cnblogs.com/r0n9/p/6733909.html 我是定义一个flag来实现break match12345678910def main(args: Array[String]) &#123; //通过模式匹配进行条件判断 val test1: String = "1" val result1 = test1 match &#123; case "1" =&gt; &#123; "one" &#125; case "2" =&gt; "two" case _ =&gt; "other" &#125; 1234567val test2: Int = 1; val result2 = test2 match &#123; case i if i + 1 == 2 =&gt; "one" case i if i + 1 == 3 =&gt; "tow" case _ =&gt; "error" &#125; println(result2) 线上执行1234567891011#!/bin/shexport JAVA_HOME=/opt/jdk1.7.0_67export PATH=$PATH:$JAVA_HOME/binP_HOME="/home/xmo/buzzads-bidding-model-test"CLASSPATH=$&#123;P_HOME&#125;/conf:$&#123;P_HOME&#125;/buzzads-bidding-model-local.jarfor f in $&#123;P_HOME&#125;/lib/*.jar; do CLASSPATH=$&#123;CLASSPATH&#125;:$fdonejava -server -Dfile.encoding=UTF-8 -Xms1G -Xmx1G -XX:+UseConcMarkSweepGC -XX:+CMSIncrementalMode -XX:+CMSIncrementalPacing -XX:CMSIncrementalDutyCycleMin=0 -XX:CMSIncrementalDutyCycle=10 -XX:MaxNewSize=1024M -XX:MaxPermSize=256M -XX:+DisableExplicitGC -cp $CLASSPATH com.buzzinate.bidding.main.Main &gt; $&#123;P_HOME&#125;/train.log 随机数12345import java.util.Randomval random = new Random(System.currentTimeMillis)if (random.nextDouble() &lt; 0.00001) &#123; trait类似于接口。使用的时候 12//with后面的就是traitval titleCnt = new HashMap[String, Int] with HashMapUtil.IntHashMap[String] HashTable一个应用是定义一个可以插入或累加的方法 1234567891011121314151617181920212223242526272829303132//定义是/home/david/gitlab/user-gene/nlp/src/main/scala/com/buzzinate/keywords/util/HashMapUtil.scalaobject HashMapUtil &#123; trait IntHashMap[A] extends HashTable[A, DefaultEntry[A, Int]] &#123; def adjustOrPut(key: A, incr: Int, value: Int): Int = &#123; val e = findEntry(key) if (e == null) &#123; addEntry(new DefaultEntry[A, Int](key, value)) 0 &#125; else &#123; val old = e.value e.value += incr old &#125; &#125; def putMax(key: A, value: Int): Int = &#123; val e = findEntry(key) if (e == null) &#123; addEntry(new DefaultEntry[A, Int](key, value)) 0 &#125; else &#123; val old = e.value if (value &gt; e.value) e.value = value old &#125; &#125; &#125;&#125;//使用时val titleCnt = new HashMap[String, Int] with HashMapUtil.IntHashMap[String]titleCnt.adjustOrPut(te.extract(rawTitle).trim, 1, 1) case class让编译器可以自动生成一些方法，如apply、copy、equals等，当希望设计一个类只是用来作为数据载体时，case class是一个ERR Client sent AUTH, but no password is set很好的选择。 RDD操作配置123val conf = new SparkConf().setAppName("wordSegname").setMaster("local[4]"). set("spark.sql.shuffle.partitions","10").set("spark.network.timeout","30s") val sc = new SparkContext(conf) local[4]是指在本地运行，用4核CPU。 spark.sql.shuffle.partitions是指partition的数量。SparkSQL在运行时，将一个查询任务分解成多个task，一个task就是一个partition。默认是200个partition，而如果实际集群只能并行3个task，则跑完200个partition要200/3=67次。 spark.network.timeout是指所有网络通信的超时时间，默认是120s 读文件12345678//读取本地文件val path = &quot;file:///home/david/get_keyword_hash.txt&quot;val word1 = sc.textFile(path).map &#123; x =&gt; val x_filter = x.replaceAll(&quot;\\p&#123;Punct&#125;&quot;, &quot; &quot;).replaceAll(&quot;\\pP&quot;, &quot; &quot;).replaceAll(&quot;[&quot; + AtomsUitl.stopwords + &quot;]&quot;, &quot; &quot;) .replaceAll(&quot; &quot;, &quot; &quot;).replaceAll(&quot;\\p&#123;Blank&#125;&quot;, &quot; &quot;).replaceAll(&quot;\\p&#123;Space&#125;&quot;, &quot; &quot;).replaceAll(&quot;\\p&#123;Cntrl&#125;&quot;, &quot; &quot;) x_filter &#125; 上文中，是将所有的特殊字符都用空格代替 AtomsUitl.stopwords停用词是”的很了么呢是嘛个都也比还这于不与才上用就好在和对挺去后没说” sc.textFile读取文件后，生成一个RDD，以行为单位，所以后面的map是对每行的操作 打印出所有的元素，用 word1.foreach { x =&gt; println(x) } 12345678val word_document = word1.zipWithIndex.filter &#123; x =&gt; !StringUtils.isBlank(x._1) &#125;.flatMap &#123; x =&gt; val arr = ArrayBuffer[(String, Int)]() val line = x._1.split(&quot; &quot;) for (i &lt;- line) &#123; arr += ((i, x._2.toInt)) &#125; arr &#125;.map &#123; x =&gt; (x._1.trim, x._2) &#125;.filter(x =&gt; !StringUtils.isBlank(x._1)) zipWithIndex该函数将RDD中的元素和这个元素在RDD中的ID（索引号）组合成键/值对。 例如： 1234567&gt; scala&gt; var rdd2 = sc.makeRDD(Seq(&quot;A&quot;,&quot;B&quot;,&quot;R&quot;,&quot;D&quot;,&quot;F&quot;),2)&gt; rdd2: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[34] at makeRDD at :21&gt; &gt; scala&gt; rdd2.zipWithIndex().collect&gt; res27: Array[(String, Long)] = Array((A,0), (B,1), (R,2), (D,3), (F,4))&gt;&gt; filter 过滤的结果还是RDD zipWithIndex反过来12 flatMap和Map Spark 中 map函数会对每一条输入进行指定的操作，然后为每一条输入返回一个对象； 而flatMap函数则是两个操作的集合——正是“先映射后扁平化”： 操作1：同map函数一样：对每一条输入进行指定的操作，然后为每一条输入返回一个对象 操作2：最后将所有对象合并为一个对象 获取目录下所有文件路径123456789101112import java.io.PrintWriterimport java.io.Fileimport scala.reflect.io.Directoryobject Test &#123; def main(args: Array[String]): Unit = &#123; val dir = new File(&quot;F:\\joke\\DCIM\\299MEDIA&quot;) val children = dir.listFiles() for ( d &lt;- children) println(d) &#125;&#125; 结果为123456F:\joke\DCIM\299MEDIA\YDXJ0580.THMF:\joke\DCIM\299MEDIA\YDXJ0580_thm.mp4F:\joke\DCIM\299MEDIA\YDXJ0581.THMF:\joke\DCIM\299MEDIA\YDXJ0581_thm.mp4F:\joke\DCIM\299MEDIA\YDXJ0582.THMF:\joke\DCIM\299MEDIA\YDXJ0582_thm.mp4 位运算除以212// 二进制向右移动一位10 &gt;&gt;&gt; 1 得到5 翻倍12// 二进制向左移动一位10 &lt;&lt; 1 得到20 scala实现wordcount几种方法https://blog.csdn.net/qq_31780525/article/details/79036728 123456789 val lines=List(&quot;hello tom hello jerry&quot;,&quot;hello tom hello kitty hello china&quot;)方法一: val wc=lines.flatMap(_.split(&quot; &quot;)).map((_,1)).groupBy(_._1).map(t=&gt;(t._1,t._2.size)).toList.sortBy(_._2).reverse方法二： val wc2=lines.flatMap(_.split(&quot; &quot;)).map((_,1)).groupBy(_._1).mapValues(_.size)方法三： val wc3=lines.flatMap(_.split(&quot; &quot;)).map((_,1)).groupBy(_._1).mapValues(_.foldLeft(0)(_+_._2))如果是在spark上： val wc4=lines.flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).sortBy(_._2,false).collect 对于以下情况 123(1.0,2)(2.0,1)(3.0,3) 用groupBy(_._1).mapValues(_.foldLeft(0)(_+_._2))是可以的， mapValues的用途是直接将Array的内容进行按照key相同的进行统计计算。.foldLeft(0)(+._2) 第一个下划线表示的是的是数组中的key，意思是分别取出其中的Array集合，.foldLeft(0)(+_.2)表示将Array进行求和，后边括号中的第一个下划线表示的是处事值0，第二个下划线是表示的是元组，.2表示的是元组中的第二个值，即单词出现的次数。 immutable.Map转mutable.Maphttps://stackoverflow.com/questions/5042878/how-can-i-convert-immutable-map-to-mutable-map-in-scala The cleanest way would be to use the mutable.Map varargs factory. Unlike the ++ approach, this uses the CanBuildFrom mechanism, and so has the potential to be more efficient if library code was written to take advantage of this: 12val m = collection.immutable.Map(1-&gt;&quot;one&quot;,2-&gt;&quot;Two&quot;)val n = collection.mutable.Map(m.toSeq: _*) This works because a Map can also be viewed as a sequence of Pairs. import的类改名1import breeze.linalg.&#123;DenseMatrix =&gt; BDM&#125; 数组抽样12345678import scala.util.RandomRandom.shuffle(list).take(n)Random.shuffle(array.toList).take(n)// Seeded versionval r = new Random(seed)r.shuffle(...) 保留小数12val df = new DecimalFormat(&quot;0.0000&quot;)println(df.format(-1.0)) 解析python导出的jsonpython导出的一个dict，形如 1&#123;&quot;product_code&quot;:&#123;&quot;0.3661996&quot;:&quot;45&quot;&#125;,&quot;now_term_rate&quot;:&#123;&quot;0.0811413&quot;:&quot;90&quot;&#125;&#125; scala解析报错 12Exception in thread &quot;main&quot; org.json4s.package$MappingException: No usable value for colDiscretizedPointsExpected object but got JNothing 如果用json4s来解析，需要解析成一个case class，并且JSON中要包含case class中的一个对象名，如colWoe2ID 1case class OneHotModel(colWoe2ID: Map[String, Map[String, Int]]) 所以在python导出的JSON中手动增加一个对象 1&#123;&quot;colWoe2ID&quot;: &#123;&quot;product_code&quot;: &#123;&quot;-0.3235455&quot;: 45&#125;, &quot;now_term_rate&quot;: &#123;&quot;0.1132489&quot;: 90, &quot;-0.0115938&quot;: 91, &quot;0.0294193&quot;: 92, &quot;-0.6089315&quot;: 93, &quot;0.0811413&quot;: 94, &quot;1.3411297&quot;: 95, &quot;-0.0237762&quot;: 96, &quot;0.0890516&quot;: 97, &quot;0.1255305&quot;: 98&#125;&#125;&#125; 这样就不会报错了 123val dict = Source.fromFile(&quot;/Users/david/david/code/00project/carthage/dictjson.txt&quot;).getLines().toList(0) val dict2 = parse(dict).extract[OneHotModel].colWoe2ID println(dict2.toList.toString()) 正则判断12val regex=&quot;&quot;&quot;^\d+$&quot;&quot;&quot;.r println(regex.pattern.matcher(&quot;321239&quot;).matches()) 方法返回多个参数12// 返回的变量可以是之前定义过的val (is_more_than_3_cars_meet_in_100m, more_than_3_cars_meet_in_100m_app_code) = GPSWXMetricInfoHandler.isCarsMeetInDistance(wxLon, wxLat, app_code, 3, 0.1, geoHash2appCodeLonlatMap) 泛型classOf如何用于类型参数参考https://cloud.tencent.com/developer/ask/117470 1234567891011val clazz = classOf[T]gdav.app_code = clazz.getDeclaredMethod("getAppCode").invoke(x).asInstanceOf[String]// 或者一个java方法体是// public static &lt;T&gt; ArrayList&lt;T&gt; getAnomalyData(String sql, Class&lt;T&gt; obj)// scala调用时def commonFunc[T](sc: SparkContext, sqlName: String): RDD[Row] = &#123; val anomalyList = DefaultAnomalyDataJdbc.getAnomalyData( HiveHandler.getExecuteSql(sqlName, List(currentDateStr)), classOf[T] ).asScala 就会报错class type required but T found，显示classOf[T]这里报错 1class type required but T found 解决方案是 123456def commonFunc[T](sc: SparkContext, sqlName: String)(implicit tag: ClassTag[T]): RDD[Row] = &#123; val anomalyList = DefaultAnomalyDataJdbc.getAnomalyData( HiveHandler.getExecuteSql(sqlName, List(currentDateStr)), // 这里 tag.runtimeClass ).asScala 泛型调用方法12345sc.parallelize(anomalyList, 5).map(&#123; x =&gt; val gdav = new GpsDefaultAnomalyVals val clazz = tag.runtimeClass gdav.app_code = clazz.getDeclaredMethod("getAppCode").invoke(x).asInstanceOf[String] 最小堆123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263object TopK &#123; val K = 3 val ord = Ordering.by[(String, Int), Int](_._2).reverse def main(args: Array[String]) &#123; // 执行 wordcount val conf = new SparkConf().setAppName("TopK") val spark = new SparkContext(conf) val textRDD = spark.textFile("hdfs://10.0.8.162:9000/home/yuzx/input/wordcount.txt") val countRes = textRDD.flatMap(line =&gt; line.split(" ")).map(word =&gt; (word, 1)).reduceByKey(_ + _) // debug mapreduce 的结果 countRes.foreach(println) /* 每个 RDD 分区内进行 TOP K 计算 需要每个分区内有自己的桶，如果整个程序使用一个 heap（将 heap 设定为成员变量） 会不正确 为什么呢？ */ val topk = countRes.mapPartitions(iter =&gt; &#123; val heap = new mutable.PriorityQueue[(String, Int)]()(ord) while (iter.hasNext) &#123; val n = iter.next println("分区计算：" + n) putToHeap(heap, n) &#125; heap.iterator &#125;).collect() println("分区结果：") topk.foreach(println) // 每个分区的 TOP K 合并，计算总的 TopK val heap = new mutable.PriorityQueue[(String, Int)]()(ord) val iter = topk.iterator while (iter.hasNext) &#123; putToHeap(heap, iter.next) &#125; println("最终结果：") while (heap.nonEmpty) &#123; println(heap.dequeue()) &#125; spark.stop() &#125; def putToHeap(heap: mutable.PriorityQueue[(String, Int)], iter: (String, Int)): Unit = &#123; if (heap.nonEmpty &amp;&amp; heap.size &gt;= K) &#123; if (heap.head._2 &lt; iter._2) &#123; heap += iter heap.dequeue() &#125; &#125; else &#123; heap += iter &#125; &#125;&#125; PriorityQueue的使用1234567891011121314151617181920212223242526272829// 定义// 表示队列中是(String,Double),按Double排序，从小到大排序val ord = Ordering.by[(String, Double), Double](_._2).reverseval heap = new mutable.PriorityQueue[(String, Double)]()(ord)// 插入 if (heap.nonEmpty &amp;&amp; heap.size &gt;= topK) &#123; if (heap.head._2 &lt; similarity) &#123; heap += ((x._1, similarity)) heap.dequeue() &#125; &#125; else &#123; heap += ((x._1, similarity)) &#125;// 取元素if (heap.nonEmpty) &#123; val res = heap.dequeue() msv.app_code_top_1 = res._1 msv.similariry_top_1 = res._2&#125;// 取元素另一种方法val iter = heap.iteratorif (iter.hasNext) &#123; val res = iter.next() msv.app_code_top_1 = res._1 msv.similariry_top_1 = res._2&#125; mapValues的用法1val cluster2Cnt = dbscan.points.asScala.groupBy(_.getCluster).mapValues(_.size) 这里的points是一个List&lt;Object&gt;，对Object中的某个属性做group，然后再mapValues，就返回每个属性的count的一个Map对象 1cluster2Cnt: Map[Int, Int]]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac使用技巧]]></title>
    <url>%2F2018%2F02%2F18%2F%E5%B7%A5%E5%85%B7%E5%92%8C%E7%8E%AF%E5%A2%83%2FMac%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[卸载jdk9安装jdk8brew默认安装的jdk9带来了一些麻烦， 比如mvn java和scala混编项目会报错 1error: error while loading package, Missing dependency &apos;object java.lang.Object in compiler mirror&apos;, required by /Users/david/david/.m2/repository/org/scala-lang/scala-library/2.10.5/scala-library-2.10.5.jar(scala/package.class) 于是打算卸载并换回jdk8 安装Java： 1brew cask install java //命令安装的是最新的Java9，我需要的是Java8…… 卸载Java9: 12345678ls /Library/Java/JavaVirtualMachines/ //查看jdk版本//卸载sudo rm -rf /Library/Java/JavaVirtualMachines/jdk-9.0.4.jdk //此处9.0.1即是上一步查看到的版本号sudo rm -fr /Library/Internet\ Plug-Ins/JavaAppletPlugin.plugin sudo rm -fr /Library/PreferencePanes/JavaControlPanel.prefPane sudo rm -fr ~/Library/Application\ Support/Java 安装Java8: 12brew tap caskroom/versionsbrew cask install java8 安装eclipse： 1brew cask install eclipse-ide 配置Java环境变量： 12345678910111213141516/usr/libexec/java_home //定位JAVA_HOME位置 Matching Java Virtual Machines (1): 1.8.0_121, x86_64: &quot;Java SE 8&quot; /Library/Java/JavaVirtualMachines/jdk1.8.0_152.jdk/Contents/Homevi ~/.bash_profile //编辑profile文件//按i键，输入以下代码JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_152.jdk/Contents/HomePATH=$JAVA_HOME/bin:$PATHCLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar//然后按esc键，输入 :wq 保存并退出profile文件。 之后执行： 123source ~/.base_profile echo ~/.base_profile 查看环境变量是否配置成功： 123$ echo $JAVA_HOME/Library/Java/JavaVirtualMachines/jdk1.8.0_152.jdk/Contents/Home Finder复制路径https://www.jianshu.com/p/757f9ffc5acf 拷贝文件内容到剪切板1pbcopy &lt; id_rsa.pub 支持LUIT 下载地址http://invisible-island.net/luit/luit.html#download 解压tar zxvf luit.tar.gzcd luit-20141204 可能版本会不一样 配置./configure 编译make 安装make install 卸载make uninstall 使用luit -encoding gbk ssh user@host 刷新dns1sudo dscacheutil -flushcache windows的文本文件乱码https://bbs.feng.com/read-htm-tid-11194986.html 大家可能都碰到过从Windows复制了一些文本文件到mac上，打开以后发现乱码的问题。楼主经常mac和win两个系统之间互相传一些程序文件比如.cpp .h，由于编码不一样，用win的人把他的程序传给我，我在mac里打开的中文注释总是乱码，令我大为恼火原来win上很多文本编辑器用的是GB18030编码来保存汉字，mac上都是用的utf-8编码，导致了乱码 下面附下这两个服务的下载 https://pan.baidu.com/s/1kU50qxh GB18030转换为utf-8.zip utf-8转为GB18030.zip 下载解压后复制到~/资源库/Services 目录下即可使用 如果不想用了在系统偏好设置的如图位置可以取消 nginx查看nginx安装目录： 1open /usr/local/Cellar/nginx //其实这个才是nginx被安装到的目录 nginx的配置文件（nginx.conf）: 1vim /usr/local/etc/nginx/nginx.conf nginx重启 1nginx -s reload 安装lisp1、安装sbcl 1brew install sbcl 2、安装emacs 12 在Drscheme使用stream的问题https://blog.csdn.net/interhanchi/article/details/83190473 安装MySQL1brew install mysql 1234567To connect run: mysql -urootTo have launchd start mysql now and restart at login: brew services start mysqlOr, if you don&apos;t want/need a background service you can just run: mysql.server start redis12# 带密码启动src/redis-server redis.conf mac下载破解软件https://www.waitsun.com/ Mac安装软件时提示已损坏的解决方法今天要说的内容是重中之重。一直有朋友同事反映，从网上下载的Sketch、Principle等设计软件，以及输入法等常用软件，安装时总是提示“已损坏，移至废纸篓”这类信息，根本无法打开。如下图： 其实，这是新系统（macOS Sierra 10.12.X）惹的祸。新系统加强了安全机制，默认不允许用户自行下载安装应用程序，只能从Mac App Store里安装应用。 解决方法步骤一：打开终端（按F4启动Launchpad，终端默认在“其他”中） 步骤二：输入代码：sudo spctl —master-disable（master前面为两个短横线，看下面的截图） 注意红框处应有空格步骤三：按回车输入自己电脑密码，再次回车（密码不会显示出来，放心输就好） 不显示密码，输完按回车 步骤四：打开系统偏好设置 » 安全性与隐私，若显示任何来源，大功告成；若没有此选项，一定是你前面的步骤不对 回到桌面双击安装文件，发现都可以打开啦，尽情享受Mac带给你的乐趣吧！ MongoDB1brew install mongodb 启动服务 1brew services start mongodb 可视化工具 Robomongo `` brew更换镜像https://mirror.tuna.tsinghua.edu.cn/help/homebrew/ docker1brew cask install docker Pytorch安装1conda create -n pytorch python=3.6 1pip3 install torch torchvision]]></content>
      <categories>
        <category>工具和环境</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[推荐系统]]></title>
    <url>%2F2018%2F02%2F11%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[推荐算法概述根据数据源的不同分为 1）基于人口的统计学推荐 2）基于内容的推荐Content-based 比如说电影推荐中，基于电影的内容，推荐相似的 3）基于协同过滤的推荐Collaborative Filtering-based 然后CF再分User-Based、Item-Based、Model-Based User-Based、Item-Based都是将用户所有输入载入内存进行运算，又合称为Memory-Based Model-Based包括Aspect Model，pLSA，LDA，聚类，SVD，Matrix Factorization等。 SVD和矩阵分解算法SVD是矩阵分解的一种，不过两种方法在推荐系统中的用法不一样。 基于SVD的推荐系统对A做奇异值分解后，取奇异值矩阵的前N个对角值，相应的取U和V的行和列，再重新拼成A2。 此时的A2相当于A的有损压缩。 分析得知，U矩阵和V矩阵可以近似来代表A矩阵，换据话说就是将A矩阵压缩成U矩阵和V矩阵，至于压缩比例得看当时对S矩阵取前k个数的k值是多少。 SVD在推荐系统中的应用 代码见svd_recommendation.py 假设行是user，列是item，SVD分解后， U的行和user的行数一致，代表user的主题分布 V的列和item的列数一致，代表item的主题分布 S是奇异值，从中选择k 该计算的含义不明白 基于矩阵分解的推荐系统koren获得netflix grand prize时关于矩阵分解的的论文 Matrix Factorization Techniques For Recommender Systems https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf 矩阵分解在推荐系统中的应用：NMF和经典SVD实战 论文Factorization meets the neighborhood a multifaceted collaborative filtering model http://vdisk.weibo.com/s/khQ1v Collaborative Filtering with Temporal Dynamics http://vdisk.weibo.com/s/khQ9o 其他推荐系统的论文 作者：严林链接：https://www.zhihu.com/question/25566638/answer/37455091来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 The Wisdom of The Few 豆瓣阿稳在介绍豆瓣猜的时候极力推荐过这篇论文，豆瓣猜也充分应用了这篇论文中提出的算法； Restricted Boltzmann Machines for Collaborative Filtering 目前Netflix使用的主要推荐算法之一； Factorization Meets the Neighborhood: a Multifaceted Collaborative Filtering Model 这个无需强调重要性，LFM几乎应用到了每一个商业推荐系统中； Collaborative Filtering with Temporal Dynamics 加入时间因素的SVD++模型，曾在Netflix Prize中大放溢彩的算法模型； Context-Aware Recommender Systems 基于上下文的推荐模型，现在不论是工业界还是学术界都非常火的一个topic； Toward the next generation of recommender systems 对下一代推荐系统的一个综述； Item-Based Collaborative Filtering Recommendation Algorithms 基于物品的协同过滤，Amazon等电商网站的主力模型算法之一； Information Seeking-Convergence of Search, Recommendations and Advertising 搜索、推荐和广告的大融合也是未来推荐系统的发展趋势之一； Ad Click Prediction: a View from the Trenches 可以对推荐结果做CTR预测排序； Performance of Recommender Algorithm on top-n Recommendation Task TopN预测的一个综合评测，TopN现在是推荐系统的主流话题，可以全部实现这篇文章中提到的算法大概对TopN有个体会； http://dsec.pku.edu.cn/~jinlong/publication/wjlthesis.pdf 北大一博士对Netflix Prize算法的研究做的毕业论文，这篇论文本身对业界影响不大，但是Netflix Prize中运用到的算法极大地推动了推荐系统的发展 Hulu视频推荐算法干货：从相关性到 RNN，一家线上“租碟店”的视频推荐算法演进 http://blog.csdn.net/dzjx2eotaa24adr/article/details/79060659 推荐系统的模型： 利用（Exploitation）： 计算要给用户推荐什么，比较确定的兴趣。 货架：从协同滤波到深度神经网络 自动播放：RNN 探索（Exploration）： 探索用户的新兴趣，防止出现一样的推荐。 Adaptive：在线学习 Diversity：行列式点过程 推荐系统的优化目标，如果从用户的偏好来讲，我们是希望提供给用户一个非常健康的组合，既符合他们的口味，又有足够的多样性 。 最后从算法或者数学抽象来讲，我们就把这个问题抽象成为“怎样最大化用户的观看时长”。包括 点击进去看一个视频之后，你看了多长时间？这个跟视频本身的长度有关，跟你的完成度有关，我们叫playback_duration。 有效点击eCTR。（推荐系统优劣的评价） 整个内容的曝光量。（推荐系统的作用！） 把这三项乘在一起的话，就变成了每个用户的观看时长。 我们最终想要最大化的是eCTR和曝光量的乘积。 视频推荐系统的框架 1）Data。最底下一层是基础数据。 2）Feature。第二层是特征，特征可以分为用户的特征和内容的特征。 3）Model，分为Exploitation、Exploration。 Exploitation包括： a、Relevant相关度， b、Transparent透明度（对产品经理还是对最终用户是不是可解释） c、Contextual上下文（时间、设备、地点这些上下文信息） Exploration包括 a、Coverage范围，对新内容我们有没有给足够的展现 b、Serendipity惊喜，有没有给用户惊喜、发现用户隐藏的一些兴趣。 c、Adapive适应性，当用户兴趣发生改变的时候，我们是不是非常快地适应了用户这种兴趣改变。 d、Diverse多样性，我们给出的这个套餐是不是组合了很多不同的类别，而不是非常单调、单一的。 4）Application。这些应用主要是服务用户在四个阶段的行为。 a、Onboarding：新用户，刚刚订阅我们的服务的时候，还在一个初始的、给我们一些信号的阶段。我们让用户选择他最喜欢的内容频道，然后用户会告诉我们他喜欢体育、喜剧或者动作片。基于这些大类，我们可以给他做一个冷启动，给他第一屏的推荐结果。 b、Convert-to-Pay：在一个新用户进来之后，我们大概有七天的时间把他转化为一个付费用户。在这个阶段，我们需要快速地探索用户的各种需求，让他体会到我们的服务非常有价值，那么他才愿意买单。 c、Retention留存：到了第三个阶段，就是用户已经是一个付费用户，那我们就需要留住他，所以就是不断的去给他更多的、他之前可能没有看过，但是和之前看过的很相关的内容。 d、Monetization变现：广告变现。一个已经付费的用户，已经留下来的用户，我们怎么样用广告把他的流量变现。 视频推荐算法Exploitation基于用户行为和side information做货架场景的排序和自动播放的这种持续预测。 相似性算法1）Relevant的角度，还是user-based+item-based 基于存储的Memory-based：item-based CF 基于模型的Model-based：矩阵分解和神经网络。神经网络现在也有基于RBM的，还有Embedding-based Neural Network。 Hulu经历了三代相关性算法的演进，第一代是item-based CF，第二代是基于矩阵分解，现在我们正在开发的第三代是基于Embedding-based Neural Network。从Netflix公开的文件来看，它主要使用的是SVD和RBM的方法。 a、第一代，协同滤波 创建一个相关度矩阵。这个矩阵是很稀疏的，大约有百分之七八十的数据是实际上是零。 b、第二代，矩阵分解 矩阵分解为了解决这个稀疏性的问题，使用了线性代数里面的一个特性，就是一个低秩矩阵，可以用两个相对低维度矩阵的乘积来表示。 相关度矩阵（评分矩阵）是低秩的，可以用一个P和Q的乘积来表示，P就是所有用户的特征。 SVD待看 c、第三代，深度学习 把原来矩阵分解里面代数运算的步骤，用一个前向神经网络来替换。好处是， 一方面非线性前向网络允许一些非线性映射，可以有更好的表达能力去model一个更复杂的分布。 另外可以直接把关于用户的处行为外的所有信息，用一个矢量feed到神经网络中去。 对内容我们也可以做相应的处理，就是把元数据，比如说导演、演员信息用一个向量来表示，然后把它feed到神经网络里面去。 相似性算法应用场景 对应这两种不同的场景，其实需要不同的相关运算。 一个是所谓的货架场景，就是给一个网格里面按照相关性做了排序，然后希望用户点越靠上越靠左的这些内容。CF比较适合货架场景的召回和排序。 一个是自动播放的场景，就是播完一个内容之后，我们会自动地开始下一个我们觉得用户最可能看的内容。用RNN。 用反向传播的方法去训练一个RNN模型，来预测用户在网络上的下一个行为。 如何评估： 仿真测试的方法就是当用户看完剧A/B/C之后，我们假装不知道这个用户接下来看了哪一个，然后基于时间序列的建模，来算出一个最可能看的剧，它可能是当前这一剧的下一集，或者是跳到另外一个剧D或者是另外一个剧E。根据RNN模型，我们找到最有可能的下一个剧，然后和用户实际看的下一个剧之间做比对，这是离线的一种评估方案。 Exploration自适应为了解决用户兴趣的时变以及新内容的冷启动，我们采用一种叫做多臂老虎机（MAB）的模型。 原理 一个赌徒可以在不同的时间选择不同的摇臂，每个摇臂会给这个赌徒不同的赢率。如果赌徒每次都选择摇臂1的话，有可能不是最优的，因为可能另外一个摇臂的反馈更好。 在推荐系统中，每个摇臂就是推荐用户的一个剧，算法就是赌徒，根据一些策略选择推荐哪个剧。每个摇臂的奖励就是这个用户是否点击和观看了。 LinUCB算法。会根据当前推的结果，来实时更新对每个摇臂的点击率的预测。 在线上部署LinUCB的算法，有一个线上更新提取特征以及模型运算的过程，以及一个线下根据之前模型采集到的信号去更新模型参数的过程。 在我们这个实验里面，可能对大家比较有参考意义的就是我们发现的LinUCB的一些特征，其中包括用户当前看剧的完成度，就是他看到了第几集、是不是看到了高潮部分还是快要结束的部分。完成度是一个很重要的特征维度。然后就是上次给用户曝光这个剧的时间和现在之间的时间间隔，以及它历史上的点击率，还有这个剧的一些元数据信息，它在外面的流行程度，以及根据刚才讲的协同滤波的方法，得到的用户和这个剧之间的相关性。 多样性 为什么要关注多样性？ 因为用户的兴趣爱好可能不是单峰分布的。有可能用户有多个兴趣爱好，其中有非常突出的一个，就是这个比较高的右边的峰。但是还有一个比较低的，就是右边这张图里面的靠左的这个峰值。用多样性来考虑用户的多个爱好。不是只推荐他最喜欢的。 传统上使用启发式的方法，它会在多样性和相关性之间用一个加权平均的方法来获得一个总体的优化目标，然后两两之间比较当前推荐的差异性，然后试图最大化这个总的平衡了之后的优化目标，用穷举的方法。 我们在现有的启发式的搜索基础上，采用了一些不同的代数模型，就是把两两之间比较不相似性改变成为用一个多边形的体积来量化我们给出的不同物品之间的差异性。把每个物品看作一个多维空间里的向量，然后用这些向量总体张成的一个多边形的体积来度量这个集合的差异性。 推荐的应用场景生成推荐的理由比如说，当我们推荐《终结者2》，我们说是由于你历史上看过《终结者1》，这时候就比完全没有任何原因的推荐显得更加顺理成章。如何构建一个推荐的理由？我们可以用刚才很简单的模板，就是因为你历史上看过和它相关的一个剧。 但是如果我们想做得更加人性化、更加自然，我们要用一种知识图谱的方法。在知识图谱里面构建内容，用户的群组，相关性的信息，以及一些统计信息，包括这个剧的流行程度，它在外面的排名。我们用一种N元组的方法来记录这个知识图谱。 推理规则基于这个知识图谱，我们可以设定一些推理规则，手工建立规则。 每一条规则其实对应某一种经典算法，比如第一条规则，就是如果用户喜欢电视剧，一是由于他曾经看过电视剧，二是电视剧2和1非常相近，这就是item-based CF逻辑的一种表达方式。类似的话，我们还可以把user-based CF也用一条规则来表达。比如说这个地方列出的第二条规则，就是如果用户属于某一个群组，而这个群组里面60%的人都看过剧1，那就说明当前这个用户也可能会喜欢看剧1。 语音对话推荐然后当用户对当前的推荐不满的时候，他也可以用自然语言来告诉我们，为什么他不喜欢这个剧，以及他想要换另外一个什么样的剧。大家知道PC时代，鼠标和键盘是最流行的交互方式，到了移动时代，触屏变成了手机上的最流行的交互方式。随着物联网的发展，我们认为语音会成为下一代的交互方式。 怎么识别用户的兴趣是否改变呢？ 其实对于多臂老虎机的问题模型来讲，认为用户的兴趣是一个可以实时更新的参数。 知识图谱是怎么建立和生成的？ 一方面是从第三方采买的，有专门的构建知识图谱的厂商，他们会做数据清洗爬取。另外一部分是我们从内容提供商那里获得的一些元数据信息。 延伸：bandit问题]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Anaconda安装配置]]></title>
    <url>%2F2018%2F02%2F11%2F%E5%B7%A5%E5%85%B7%E5%92%8C%E7%8E%AF%E5%A2%83%2FAnaconda%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[安装环境使用清华大学的镜像 https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/ 切换版本目前我装的是anaconda2的版本，若要使用py3，先安装python3 1conda create -n py3 python=3 在py3下安装spyder 1conda install -n py3 spyder 要启动py3的spyder，在切换到py3环境后，直接执行spyder即可 再安装jupyter 1conda install -n py3 jupyter 当切换至py3的环境 1source activate py3 打开spyder报错1234567[warn] kq_init: detected broken kqueue; not using.: Undefined error: 0[warn] kq_init: detected broken kqueue; not using.: Undefined error: 0[warn] kq_init: detected broken kqueue; not using.: Undefined error: 0[warn] kq_init: detected broken kqueue; not using.: Undefined error: 0[warn] kq_init: detected broken kqueue; not using.: Undefined error: 0[warn] kq_init: detected broken kqueue; not using.: Undefined error: 0[warn] kq_init: detected broken kqueue; not using.: Undefined error: 0 https://stackoverflow.com/questions/40047607/problems-with-spyder-3-in-macos-sierra The problem is not with Spyder itself but with Qt, the graphical library on top of which Spyder is built on. The current Qt version in Anaconda (5.6.0) does not support macOS Sierra. According to this Github comment, the first versions that do it are 5.6.2 and 5.7.1. As soon as Continuum (the company behind Anaconda) updates Qt to one of these versions, I’m pretty sure those strange problems you’re seeing will be solved.]]></content>
      <categories>
        <category>工具和环境</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MCMC]]></title>
    <url>%2F2018%2F02%2F11%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FEM%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>MCMC</tag>
        <tag>随机游走</tag>
        <tag>PyMC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MCMC]]></title>
    <url>%2F2018%2F02%2F11%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FMCMC%2F</url>
    <content type="text"><![CDATA[MCMC马尔可夫链蒙特卡洛(MCMC)采样 Metropolis-Hasting 算法 &amp; 图上的Metropolis-Hasting Random Walk (MHRW) Markov Chain Monte Carlo，用于模拟采样的算法。 像Word2Vec中的负采样算法（Negative Sampling），适合离散的概率分布，知道每个值的概率。 而连续分布（概率密度函数表示的）就不行了。 知乎问题：为什么要使用MCMC方法？ 为何不用等距采样替换MCMC MCMC的应用是和”维数灾难”有关的。MCMC应用的概率模型，其参数维数往往巨大，但每个参数的支撑集非常小。比如一些NLP问题的参数只取{0,1}，但维数往往达到几千甚至上万左右 参考 马尔可夫链蒙特卡洛(MCMC)采样 延伸：word2vec里面也涉及到采样：。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>MCMC</tag>
        <tag>随机游走</tag>
        <tag>PyMC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo + Github 搭建博客入门]]></title>
    <url>%2F2018%2F02%2F01%2F%E5%B7%A5%E5%85%B7%E5%92%8C%E7%8E%AF%E5%A2%83%2FHexo%20%2B%20Github%20%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[1、安装node.js12345$ sudo add-apt-repository ppa:chris-lea/node.js$ sudo apt-get update$ sudo apt-get install nodejs#原方法没有这一步，但是后面的操作会提示npm command not found$ sudo apt-get install npm 2、安装hexo1$ sudo npm install hexo -g 如果是mac，要用 1sudo npm install hexo --no-optional 不然会报错Error: Cannot find module ‘./build/Release/DTraceProviderBindings’] code: ‘MODULE_NOT_FOUND’这个错误。 如果报权限错误，需要用 1sudo npm install --unsafe-perm --verbose -g hexo 3、初始博客的根目录12$ cd ~/myblog$ hexo init mac的根目录在 1/Users/david/david/myblog 4、在github上新建仓库名称必须是 1gitusername.github.io 我的就是 1Schwimmer.github.io 并将本地的SSH KEY添加到git上（略） 5、让博客可以发布到git1）安装hexo-deployer-Git（不然会出现ERROR Deployer not found: git） 1npm install hexo-deployer-git --save 2） 配置你hexo博客根目录下的_config.yml文件(应该是最下面一行，修改成你的github) 1234deploy: type: git repo: git@github.com:Schwimmer/Schwimmer.github.io.git branch: master tips 冒号后面一定要跟空格 6、hexo常用命令12345hexo clean #清除缓存hexo new &quot;title&quot; #新建文章hexo g #生成html，或hexo generatehexo s #在本地启动服务，启动后访问localhost:4000就可以打开，或hexo serverhexo d #发布到git，发布后访问https://schwimmer.github.io/就可以打开，或hexo deploy tips 我目前用的新建文章的方法，就是直接在source/_posts/下面新建md文件 可以偷懒写成 cd ~/myblog hexo clean;hexo g;hexo s 或 hexo clean;hexo g;hexo d 支持数学公式Next 7的版本中，数学公式可以直接开启配置 Settings123456789101112131415161718192021222324252627282930313233343536# Math Equations Render Supportmath: enable: true # Default(true) will load mathjax/katex script on demand # That is it only render those page who has `mathjax: true` in Front-matter. # If you set it to false, it will load mathjax/katex srcipt EVERY PAGE. per_page: true engine: mathjax #engine: katex # hexo-renderer-pandoc (or hexo-renderer-kramed) needed to full MathJax support. mathjax: # Use 2.7.5 as default, jsdelivr as default CDN, works everywhere even in China cdn: //cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML # For direct link to MathJax.js with CloudFlare CDN (cdnjs.cloudflare.com) #cdn: //cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML # See: https://mhchem.github.io/MathJax-mhchem/ #mhchem: //cdn.jsdelivr.net/npm/mathjax-mhchem@3 #mhchem: //cdnjs.cloudflare.com/ajax/libs/mathjax-mhchem/3.3.0 # hexo-renderer-markdown-it-plus (or hexo-renderer-markdown-it with markdown-it-katex plugin) needed to full Katex support. katex: # Use 0.7.1 as default, jsdelivr as default CDN, works everywhere even in China cdn: //cdn.jsdelivr.net/npm/katex@0.7.1/dist/katex.min.css # CDNJS, provided by cloudflare, maybe the best CDN, but not works in China #cdn: //cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css copy_tex: # See: https://github.com/KaTeX/KaTeX/tree/master/contrib/copy-tex enable: false copy_tex_js: //cdn.jsdelivr.net/npm/katex@0/dist/contrib/copy-tex.min.js copy_tex_css: //cdn.jsdelivr.net/npm/katex@0/dist/contrib/copy-tex.min.css 8、安装主题转自：NexT主题安装教程 9、文章阅读计数转自：Hexo添加不蒜子和LeanCloud统计无标题文章 找到站点的themes/next/layout/_partials目录下的footer.swig文件。插入代码如下。 123456789101112131415161718192021&#123;% if theme.copyright %&#125;&lt;div class=&quot;powered-by&quot;&gt; &#123;&#123; __(&apos;footer.powered&apos;, &apos;&lt;a class=&quot;theme-link&quot; href=&quot;https://hexo.io&quot;&gt;Hexo&lt;/a&gt;&apos;) &#125;&#125;&lt;/div&gt;&lt;div class=&quot;theme-info&quot;&gt; &#123;&#123; __(&apos;footer.theme&apos;) &#125;&#125; - &lt;a class=&quot;theme-link&quot; href=&quot;https://github.com/iissnan/hexo-theme-next&quot;&gt; NexT.&#123;&#123; theme.scheme &#125;&#125; &lt;/a&gt;&lt;/div&gt;# 此位置插入以下代码&lt;div&gt;&lt;script async src=&quot;https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js&quot;&gt;&lt;/script&gt;本站总访问量 &lt;span id=&quot;busuanzi_value_site_pv&quot;&gt;&lt;/span&gt; 次&amp;nbsp&amp;nbsp&amp;nbsp本站访客数&lt;span id=&quot;busuanzi_value_site_uv&quot;&gt;&lt;/span&gt;人次&lt;/div&gt;&#123;% endif %&#125; 10、增加图片原来记录过一个只安装插件的方法，现在已经失效了。目前参考的是 hexo引用本地图片无法显示的方法，感谢。 1、安装插件 1npm install https://github.com/CodeFalling/hexo-asset-image --save 2、修改文件 打开/node_modules/hexo-asset-image/index.js，将内容更换为下面的代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061'use strict';var cheerio = require('cheerio');// http://stackoverflow.com/questions/14480345/how-to-get-the-nth-occurrence-in-a-stringfunction getPosition(str, m, i) &#123; return str.split(m, i).join(m).length;&#125;var version = String(hexo.version).split('.');hexo.extend.filter.register('after_post_render', function(data)&#123; var config = hexo.config; if(config.post_asset_folder)&#123; var link = data.permalink; if(version.length &gt; 0 &amp;&amp; Number(version[0]) == 3) var beginPos = getPosition(link, '/', 1) + 1; else var beginPos = getPosition(link, '/', 3) + 1; // In hexo 3.1.1, the permalink of "about" page is like ".../about/index.html". var endPos = link.lastIndexOf('/') + 1; link = link.substring(beginPos, endPos); var toprocess = ['excerpt', 'more', 'content']; for(var i = 0; i &lt; toprocess.length; i++)&#123; var key = toprocess[i]; var $ = cheerio.load(data[key], &#123; ignoreWhitespace: false, xmlMode: false, lowerCaseTags: false, decodeEntities: false &#125;); $('img').each(function()&#123; if ($(this).attr('src'))&#123; // For windows style path, we replace '\' to '/'. var src = $(this).attr('src').replace('\\', '/'); if(!/http[s]*.*|\/\/.*/.test(src) &amp;&amp; !/^\s*\//.test(src)) &#123; // For "about" page, the first part of "src" can't be removed. // In addition, to support multi-level local directory. var linkArray = link.split('/').filter(function(elem)&#123; return elem != ''; &#125;); var srcArray = src.split('/').filter(function(elem)&#123; return elem != '' &amp;&amp; elem != '.'; &#125;); if(srcArray.length &gt; 1) srcArray.shift(); src = srcArray.join('/'); $(this).attr('src', config.root + link + src); console.info&amp;&amp;console.info("update link as:--&gt;"+config.root + link + src); &#125; &#125;else&#123; console.info&amp;&amp;console.info("no src attr, skipped..."); console.info&amp;&amp;console.info($(this)); &#125; &#125;); data[key] = $.html(); &#125; &#125;&#125;); 3、打开_config.yml文件，修改下述内容 1post_asset_folder: true 4、要显式的图片需要放在和md同名的文件夹下面。例如，有一篇机器学习算法推导（一）逻辑回归.md，图片就要放在 1![2](机器学习算法推导（一）逻辑回归/SouthEast-20190616095710926.png) 这里可以通过Typora的设置来方便实现。 打开Typora-&gt;偏好设置-&gt;编辑器，配置为如下的格式 再重新发布hexo就可以了。 11、sitemap 插件Hexo Seo优化让你的博客在google搜索排名第一 12&lt;meta name=&quot;google-site-verification&quot; content=&quot;Mx7Ikp0IpBtTbSpHDTBV0_CMJA-E8CLn8NRIrwyq5m4&quot; /&gt;&lt;meta name=&quot;baidu-site-verification&quot; content=&quot;ZBTsWx4NdC&quot; /&gt; 12、首页显示文章摘要 进入hexo博客项目的themes/next目录 用文本编辑器打开_config.yml文件 搜索”auto_excerpt”,找到如下部分： 12345# Automatically Excerpt. Not recommand.# Please use &lt;!-- more --&gt; in the post to control excerpt accurately.auto_excerpt: enable: false length: 150 把enable改为对应的false改为true，然后hexo d -g，再进主页，问题就解决了！ 13、启用分类和标签1、在博客的开头要加上 123456title: Hexo + Github 搭建博客入门date: 2017-07-12 11:49:53categories: &quot;工具和环境&quot;tags: - hexodescription: 2、修改主题配置文档 123456menu: home: / categories: /categories #about: /about archives: /archives tags: /tags 3、hexo加上page 如分类 1hexo new page categories 然后打开source/categories/index.md，增加一行 1type: &quot;categories&quot; 增加标签也是一样 1hexo new page tags 然后打开source/tags/index.md，增加一行 1type: &quot;tags&quot; 谷歌与百度的站点地图，前者适用于其他搜索引擎，用来手动提交以增加收录 安装： 12npm install hexo-generator-sitemap@1 --savenpm install hexo-generator-baidu-sitemap@0.1.1 --save _config.yml添加代码： 12baidusitemap: path: baidusitemap.xml 谷歌的sitemap.xml不需要写到配置文件中，自动生效。 在主页后面加/baidusitemap.xml可以看到baidusitemap（谷歌同理），将该网址它提交给百度搜索：百度站长平台，贴吧账号无法在这里使用。 不过由于Github禁止了百度爬虫，百度无法抓取其中的URL： 添加搜索Local search no need any external 3rd-party services and can be extra indexed by search engines. That search method recommended for most users. 安装Install hexo-generator-searchdb by run following command in site root dir: 1$ npm install hexo-generator-searchdb --save hexo 配置Edit site config file and add following content: 1234567hexo/_config.ymlsearch: path: search.xml field: post format: html limit: 10000 themes 配置Edit theme config file to enable Local Search: 12345678910111213next/_config.yml# Local search# Dependencies: https://github.com/theme-next/hexo-generator-searchdblocal_search: enable: true # if auto, trigger search by changing input # if manual, trigger search by pressing enter key or search button trigger: auto # show top n results per article, show all results by setting to -1 top_n_per_article: 1 # unescape html strings to the readable one unescape: false 添加评论DisqusDisqus is a global comment system that improves discussion on websites and connects conversations across the web. Create an account and log into Disqus. Once logged in, click the GET STARTED button on the homepage, then select I want to install Disqus on my site option and you will see the Create a new site interface. Enter your Website Name, which will serve as your Disqus shortname, and select a Category from the drop-down menu. Then click Create Site button. Choose I don&#39;t see my platform listed, install manually with Universal Code, configure Disqus for your site, and click Complete Setup button. Set the valueenable to true, add the obtained Disqus shortname (shortname), and edit other configurations in disqus section in the theme config file as following: 12345next/_config.ymldisqus: enable: false shortname: your-short-disqus-name count: true 不蒜子统计Edit busuanzi_count option in theme config file.When enable: true, global setting is enabled. If site_uv, site_pv, page_pv are all false, Busuanzi only counts but never shows. 绑定代码到Coding创建coding仓库 上传公钥 https://coding.net/user/account/setting/keys 测试SSH Key 是否配置成功 1ssh -T git@git.coding.net 用来部署Hexo博客的Coding项目地址为：git@git.coding.net:ddxy1986/DavidXu-Blog deploy的配置改为 123456deploy: type: git repo: github: git@github.com:Schwimmer/Schwimmer.github.io.git coding: git@git.coding.net:ddxy1986/DavidXu-Blog branch: master 配置Coding项目的Pages服务开启Coding项目的Pages服务 踩过的坑启动时报错1234 Error: Warning: Permanently added &apos;github.com,192.30.253.112&apos; (RSA) to the list of known hosts.sign_and_send_pubkey: signing failed: agent refused operationPermission denied (publickey).fatal: Could not read from remote repository. 处理是 12$ eval &quot;$(ssh-agent -s)&quot;$ ssh-add 生成html时报错1end of the stream or a document separator is expected 更新next主题后，菜单不是中文也不是英文next主题的所有语言配置文件都在\themes\next\languages文件夹下，原来中文对应的是zh-Hans.yml，更新之后变成了zh-CN.yml。 而语言的配置信息再主目录的_config.yml文件，于是找到这里 将其改为zh-CN，解决问题。 安装npm报错1Error: EACCES: permission denied, access &apos;/Users/david/david/myblog/node_modules/babel-polyfill/node_modules/core-js&apos; 用了sudu命令也依旧报错。 解决方案是 you can fix that error by allowing unsafe perms 1sudo npm config set unsafe-perm=true 参考https://github.com/Microsoft/WSL/issues/14 next安装搜索后一直转圈本来以为是npm插件的问题，反复卸载安装几次后还是不行，后面看到这篇文章的做法解决了问题，摘录如下： 因为搜索插件的原理是生成search.xml文件，先找到/public/search.xml，将其拖到浏览器中打开，如果有问题就会提示错误和错误的行号。 一般的问题都是出现了非法字符，定位到md文件，删去非法字符后再打包就OK啦。 参考：ubuntu下使用hexo搭建博客]]></content>
      <categories>
        <category>工具和环境</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习随笔]]></title>
    <url>%2F2018%2F01%2F30%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9A%8F%E7%AC%94%2F</url>
    <content type="text"><![CDATA[根据数据采样来估计概率分布，往往可以用极大似然估计法。这种做法需要假定参数符合一个先验分布。贝叶斯分类用的就是这个思路。 机器学习实践中学到的最重要的内容]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[vim命令]]></title>
    <url>%2F2018%2F01%2F30%2F%E5%B7%A5%E5%85%B7%E5%92%8C%E7%8E%AF%E5%A2%83%2Fvim%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[查找并删除:g/要删除的内容/d 查找 /要查找的 按n就是下一个 删除一行 dd 查找匹配的个数 :%s/refering_site/&amp;/gn 或 :%s/17779//gn :%s/“opxcreativeid”:16650//gn 查找并替换%s/源字符串/目的字符串/g 如 :%s/\/home\/weinan/\/opt\/pig_home\/bshare_etl/g :%s/gpadmin/gpxmo/g :%s/\t-1/\t1/g :%s/“//g :%s/16-06-28/16-06-29/g 多行变1行大写V选中行+shift J 替换每行的行首、行尾12:1,$ s/^/HELLO/g:1,$ s/$/WORLD/g sh文件的编码转成unix查看用:set fileformat 修改用:set fileformat=unix 编码从latin1转成utf8:e ++enc=cp936:set fileencoding=utf-8]]></content>
      <categories>
        <category>工具和环境</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[scp免密码登录]]></title>
    <url>%2F2018%2F01%2F30%2F%E5%B7%A5%E5%85%B7%E5%92%8C%E7%8E%AF%E5%A2%83%2Fscp%E5%85%8D%E5%AF%86%E7%A0%81%E7%99%BB%E5%BD%95%2F</url>
    <content type="text"><![CDATA[假设A要免密码传输文件到B 在A上创建秘钥 1ssh-keygen -t rsa 拷贝id_rsa.pub到B的.ssh，并改名为authorized_keys 注意要修改权限 A机器 .ssh目录，以及/home/当前用户 需要700权限，参考以下操作调整 sudo chmod 700 ~/.ssh sudo chmod 700 /home/当前用户 B机器 .ssh目录下的authorized_keys文件需要600或644权限，参考以下操作调整 sudo chmod 600 ~/.ssh/authorized_keys]]></content>
      <categories>
        <category>工具和环境</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux各种命令]]></title>
    <url>%2F2018%2F01%2F30%2F%E5%B7%A5%E5%85%B7%E5%92%8C%E7%8E%AF%E5%A2%83%2Flinux%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[各种压缩解压缩rar 解压缩 unrar e XXX.rar 7z 7za x word2vec_c_from_weixin.7z -r -o./ 注意-o和后面的路径之间没有空格 查看gz压缩文件行数 zcat *.gz | wc -l tar 压缩 tar -czvf tracking_ingest_BJIDCnopig.tar.gz tracking_ingest_BJIDCnopig 解压缩 tar -zxvf greenplum-db-4.2.3.1.tar -C /usr/local/ gz gunzip -c gp_dump_1_1_20160806093243.gz &gt; gp_dump_1_1_20160806093243 zip zip mydata.zip mydata 文件夹就是 zip -r unzip archive_name.zip rar 安装：sudo apt install unrar卸载：sudo apt-get remove unrar rar常用命令主要有:e 将文件解压到当前目录 例:rar e test.rar​ 注:用e解压的话，不仅原来的file1.txt和file2.txt被解压到当前目录，就连dir1里面的所有文件​ 也被解压到当前目录下，不能保持压缩前的目录结构，如果想保持压缩前的目录结构，用x解压x 带路径解压文档中内容到当前目录 例:rar x test.rar​ 这样解压的话，dir1就会保持原来的目录结构 查看OS版本lsb_release -a 看某个端口是否在使用netstat -tunlp |grep 9527 统计指定文件的大小du -c -h adgroup.BJ1.20160623* 看linux某个软件的版本rpm -qa | grep mapr 看文件的指定行 sed -n ‘5,10p’ filename 这样你就可以只查看文件的第5行到第10行。 文件链接ln -s greenplum-db-4.2.3.1/ greenplum-db 停止crontab服务这个命令在red hat当中常用,有的linux发行版本中没有这个命令.$ service crond start //启动服务$ service crond stop //关闭服务$ service crond restart //重启服务 2.linux发行版本没有service这个命令时：/etc/init.d/cron stop/etc/init.d/cron start cronjob路径 /var/spool/cron/ 查看已安装版本号sudo apt-get install apt-show-versions 用apt-show-versions查看 若查看单个软件包的版本 apt-show-versions –p 查看可升级的软件包 apt-show-versions –u centos中查看已安装的包 1yum list installed |grep mysql 查看可以安装的包 yum install mysql mysql-server mysql-devel 指定文件大小总和du -m 201604 | awk ‘{sum += $1}; END{print sum}’ 查看目录结构tree -a 文件夹搜索文件find . -name “*.py” dpkgdpkg命令常用格式如下：sudo dpkg -I iptux.deb#查看iptux.deb软件包的详细信息，包括软件名称、版本以及大小等（其中-I等价于—info）sudo dpkg -c iptux.deb#查看iptux.deb软件包中包含的文件结构（其中-c等价于—contents）sudo dpkg -i iptux.deb#安装iptux.deb软件包（其中-i等价于—install）sudo dpkg -l iptux#查看iptux软件包的信息（软件名称可通过dpkg -I命令查看，其中-l等价于—list）sudo dpkg -L iptux#查看iptux软件包安装的所有文件（软件名称可通过dpkg -I命令查看，其中-L等价于—listfiles）sudo dpkg -s iptux#查看iptux软件包的详细信息（软件名称可通过dpkg -I命令查看，其中-s等价于—status）sudo dpkg -r iptux#卸载iptux软件包（软件名称可通过dpkg -I命令查看，其中-r等价于—remove） #清空文件 > filename #linux 匹配tab ctrl+M+tab #在行首添加字符 sed ‘s/^/HEAD&amp;/g’ test.file #在行尾添加字符 sed ‘s/$/&amp;TAIL/g’ test.file 查找文件夹最近修改的文件查找最近30分钟修改的当前目录下的.php文件 1find . -name &apos;*.php&apos; -mmin -30 查找最近24小时修改的当前目录下的.php文件 1find . -name &apos;*.php&apos; -mtime 0 查找最近24小时修改的当前目录下的.php文件，并列出详细信息 1find . -name &apos;*.inc&apos; -mtime 0 -ls 查找当前目录下，最近24-48小时修改过的常规文件。 1find . -type f -mtime 1 查找当前目录下，最近1天前修改过的常规文件。 1find . -type f -mtime +1 删除 find . -name “*.pyc” | xargs rm ping端口telnet 1.1.1.1 8080 文件去重sort -k2n file | uniq &gt; a.out 当file中的重复行不再一起的时候，uniq没法删除所有的重复行。经过排序后，所有相同的行都在相邻，因此uniq可以正常删除重复行。 统计文件夹大小并排序du -sh * | sort -rn | head -5 清空文件1&gt; filename 批量杀进程1ps -ef | grep monitor_saige_with_realtime.py | awk &apos;&#123; print $2; &#125;&apos; | xargs -i kill -9 &#123;&#125; 杀掉某个端口的进程12lsof -i :5136kill -9 39607]]></content>
      <categories>
        <category>工具和环境</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ubuntu java反编译]]></title>
    <url>%2F2018%2F01%2F30%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FJAVA%2Fubuntu-java%E5%8F%8D%E7%BC%96%E8%AF%91%2F</url>
    <content type="text"><![CDATA[从https://sourceforge.net/projects/jadx/下载 假设安装路径为~/opt/jadx 12cd ~/opt/jadx/jadx/build/jadx/bin./jadx-gui 打开gui，拖入jar包或者class文件]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python编码问题]]></title>
    <url>%2F2018%2F01%2F29%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FPYTHON%2FPython%E7%BC%96%E7%A0%81%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[困扰了很长时间的问题，找到一篇解释不错的文章，转载并整理之。 ‘ascii’ codec can’t decode byte 0xe4 in position 0: ordinal not in range(128) python的编码是unicode -&gt; str，解码是str -&gt; unicode 关于文件开头的”编码指示”，也就是-*- coding: -*-这个语句。Python 默认脚本文件都是 UTF-8 编码的，当文件中有非 UTF-8 编码范围内的字符的时候就要使用”编码指示”来修正. 关于 sys.defaultencoding，这个在解码没有明确指明解码方式的时候使用。比如我有如下代码： 1234#! /usr/bin/env python # -*- coding: utf-8 -*- s = &apos;中文&apos; # 注意这里的 str 是 str 类型的，而不是 unicode s.encode(&apos;gb18030&apos;) 这句代码将 s 重新编码为gb18030的格式，即进行unicode -&gt; str的转换。因为 s 本身就是 str类型的，因此 Python 会自动的先将 s 解码为unicode，然后再编码成 gb18030。因为解码是python自动进行的，我们没有指明解码方式，python 就会使用sys.defaultencoding指明的方式来解码。很多情况下 sys.defaultencoding 是ANSCII，如果 s 不是这个类型就会出错。 拿上面的情况来说，我的 sys.defaultencoding是anscii，而 s 的编码方式和文件的编码方式一致，是 utf8 的，所以出错了:UnicodeDecodeError: &#39;ascii&#39; codec can&#39;t decode byte 0xe4 in position 0: ordinal not in range(128) Unicode和UTF8的区别 unicode指的是万国码，是一种“字码表”。而utf-8是这种字码表储存的编码方法。unicode不一定要由utf-8这种方式编成bytecode储存，也可以使用utf-16,utf-7等其他方式。目前大多都以utf-8的方式来变成bytecode。 python中字符串类型分为byte string 和unicode string两种。 如果在python文件中指定编码方式为utf-8(#coding=utf-8)，那么所有带中文的字符串都会被认为是utf-8编码的byte string（例如：mystr=”你好”），但是在函数中所产生的字符串则被认为是unicode string问题就出在这边，unicode string 和byte string是不可以混合使用的，一旦混合使用了，就会产生这样的错误。例如： 1self.response.out.write(&quot;你好&quot;+self.request.get(&quot;argu&quot;)) 以下有两个解决方法： 第一种,是明确的指示出 s 的编码方式 1234#! /usr/bin/env python # -*- coding: utf-8 -*- s = &apos;中文&apos; s.decode(&apos;utf-8&apos;).encode(&apos;gb18030&apos;) 第二种,更改sys.defaultencoding为文件的编码方式 12345#! /usr/bin/env python # -*- coding: utf-8 -*- import sys #要重新载入sys。因为 Python 初始化后会删除 sys.setdefaultencoding 这个方 法reload(sys) sys.setdefaultencoding(&apos;utf-8&apos;) unicode转url编码https://blog.csdn.net/xyw_blog/article/details/41854635 今天修改一个天气预报的东西，但输入城市不能得到天气预报，感觉是编码不对，因为你输入一个城市（比如‘杭州’），url的地址编码却是’%E4%B8%BD%E6%B1%9F’，因此需 要做一个转换。这里我们就用到了模块urllib。 那我们想转回去呢？ >&gt;&gt; urllib.unquote(‘%E6%9D%AD%E5%B7%9E’)‘\xe6\x9d\xad\xe5\xb7\x9e’>&gt;&gt; print urllib.unquote(‘%E6%9D%AD%E5%B7%9E’)杭州 细心的同学会发现贴吧url中出现的是%C0%F6%BD%AD，而非’%E4%B8%BD%E6%B1%9F’，其实是编码问题。百度的是gbk，其他的一般网站比如google就是utf8的。所以可以用下列语句实现。 >&gt;&gt; import sys,urllib >&gt;&gt; s = ‘杭州’ >&gt;&gt; urllib.quote(s.decode(sys.stdin.encoding).encode(‘gbk’)) %BA%BC%D6%DD >&gt;&gt; urllib.quote(s.decode(sys.stdin.encoding).encode(‘utf8’)) ‘%E6%9D%AD%E5%B7%9E’ 2018年10月25日 这个问题今天又遇到了，在服务器上用nohup就无法打印包含中文字符的内容，直接跑python就没问题]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JPA连接池问题]]></title>
    <url>%2F2018%2F01%2F29%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FJAVA%2FJPA%E8%BF%9E%E6%8E%A5%E6%B1%A0%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[用hibernate的JPA框架连接MySql并提供API接口，往往过一夜就会报错 1org.hibernate.exception.JDBCConnectionException: Unable to acquire JDBC Connection 根据stackflow上的解释，因为有太多connection，导致无法建立新的connection。因此需要设置连接池 连接池的作用 1.JDBC数据库连接池的必要性 在使用开发基于数据库的web程序时，传统的模式基本是按以下步骤： 在主程序（如servlet、beans）中建立数据库连接。 进行sql操作 断开数据库连接。 这种模式开发，存在的问题: 普通的JDBC数据库连接使用 DriverManager 来获取，每次向数据库建立连接的时候都要将 Connection 加载到内存中，再验证用户名和密码(得花费0.05s～1s的时间)。需要数据库连接的时候，就向数据库要求一个，执行完成后再断开连接。这样的方式将会消耗大量的资源和时间。数据库的连接资源并没有得到很好的重复利用.若同时有几百人甚至几千人在线，频繁的进行数据库连接操作将占用很多的系统资源，严重的甚至会造成服务器的崩溃。 对于每一次数据库连接，使用完后都得断开。否则，如果程序出现异常而未能关闭，将会导致数据库系统中的内存泄漏，最终将导致重启数据库。 这种开发不能控制被创建的连接对象数，系统资源会被毫无顾及的分配出去，如连接过多，也可能导致内存泄漏，服务器崩溃. 2.数据库连接池（connection pool） 为解决传统开发中的数据库连接问题，可以采用数据库连接池技术。 数据库连接池的基本思想就是为数据库连接建立一个“缓冲池”。预先在缓冲池中放入一定数量的连接，当需要建立数据库连接时，只需从“缓冲池”中取出一个，使用完毕之后再放回去。 数据库连接池负责分配、管理和释放数据库连接，它允许应用程序重复使用一个现有的数据库连接，而不是重新建立一个。 数据库连接池在初始化时将创建一定数量的数据库连接放到连接池中，这些数据库连接的数量是由最小数据库连接数来设定的。无论这些数据库连接是否被使用，连接池都将一直保证至少拥有这么多的连接数量。连接池的最大数据库连接数量限定了这个连接池能占有的最大连接数，当应用程序向连接池请求的连接数超过最大连接数量时，这些请求将被加入到等待队列中。 3.数据库连接池技术的优点 (1)资源重用：由于数据库连接得以重用，避免了频繁创建，释放连接引起的大量性能开销。在减少系统消耗的基础上，另一方面也增加了系统运行环境的平稳性。 (2)更快的系统反应速度:数据库连接池在初始化过程中，往往已经创建了若干数据库连接置于连接池中备用。此时连接的初始化工作均已完成。对于业务请求处理而言，直接利用现有可用连接避免了数据库连接初始化和释放过程的时间开销，从而减少了系统的响应时间 (3)新的资源分配手段对于多应用共享同一数据库的系统而言，可在应用层通过数据库连接池的配置实现某一应用最大可用数据库连接数的限制避免某一应用独占所有的数据库资源. (4)统一的连接管理，避免数据库连接泄露在较为完善的数据库连接池实现中，可根据预先的占用超时设定，强制回收被占用连接，从而避免了常规数据库连接操作中可能出现的资源泄露。 4.c3p0数据库连接池 设置的方法参见 How to configure the C3P0 connection pool in Hibernate 配置后的persistence.xml 12345678910111213141516171819202122232425&lt;properties&gt; &lt;property name="hibernate.dialect" value="org.hibernate.dialect.MySQL5Dialect" /&gt; &lt;property name="hibernate.connection.driver_class" value="com.mysql.jdbc.Driver" /&gt; &lt;property name="hibernate.connection.username" value="usr_dba" /&gt; &lt;property name="hibernate.connection.password" value="4rfv%TGB^YHN" /&gt; &lt;property name="hibernate.connection.url" value="jdbc:mysql://10.11.10.44:3306/symphony" /&gt; &lt;property name="hibernate.max_fetch_depth" value="3" /&gt; &lt;property name="hibernate.hbm2ddl.auto" value="update" /&gt; &lt;!-- 下面开始c3p0的配置 --&gt; &lt;property name="hibernate.connection.provider_class" value="org.hibernate.service.jdbc.connections.internal.C3P0ConnectionProvider"/&gt; &lt;!-- 最小连接数 --&gt; &lt;property name="hibernate.c3p0.min_size" value="1"/&gt; &lt;!-- 最大连接数 --&gt; &lt;property name="hibernate.c3p0.max_size" value="100"/&gt; &lt;!-- 获得连接的超时时间,如果超过这个时间,会抛出异常，单位（毫秒） --&gt; &lt;property name="hibernate.c3p0.timeout" value="10"/&gt; &lt;!-- 指定连接池里最大缓存多少个Statement对象 --&gt; &lt;property name="hibernate.c3p0.max_statements" value="100"/&gt; &lt;!-- 每隔3000秒检查连接池里的空闲连接 ，单位是（秒）--&gt; &lt;property name="hibernate.c3p0.idle_test_period" value="3000"/&gt; &lt;!-- 当连接池里面的连接用完的时候，C3P0自动一次性获取多少个新的连接 --&gt; &lt;property name="hibernate.c3p0.acquire_increment" value="5"/&gt; &lt;!-- 每次都验证连接是否可用 --&gt; &lt;property name="hibernate.c3p0.validate" value="true"/&gt;&lt;/properties&gt;]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Docker配置和使用]]></title>
    <url>%2F2018%2F01%2F29%2F%E5%B7%A5%E5%85%B7%E5%92%8C%E7%8E%AF%E5%A2%83%2FDocker-%E9%85%8D%E7%BD%AE%E5%92%8C%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[0、基本概念镜像 Docker镜像通过镜像ID进行识别。镜像ID是一个64字符的十六进制的字符串。但是当我们运行镜像时，通常我们不会使用镜像ID来引用镜像，而是使用镜像名来引用。要列出本地所有有效的镜像，可以使用命令 $ docker images 镜像可以发布为不同版本，这种机制我们称之为标签（Tag）。 容器 Docker容器可以使用命令创建： $ docker run imagename 它会在所有的镜像层之上增加一个可写层。这个可写层有运行在CPU上的进程，而且有两个不同的状态：运行态（Running）和退出态（Exited）。这就是Docker容器。当我们使用docker run启动容器，Docker容器就进入运行态，当我们停止Docker容器时，它就进入退出态。 对容器的变更是写入到容器的文件系统的，而不是写入到Docker镜像中的。 用同一个镜像启动多个Docker容器，这些容器启动后都是活动的，彼此还是相互隔离的。我们对其中一个容器所做的变更只会局限于那个容器本身。 如果对容器的底层镜像进行修改，那么当前正在运行的容器是不受影响的，不会发生自动更新现象。 1、配置1.1 安装12$ sudo apt-get update$ sudo apt-get install docker 编辑/etc/default/docker增加如下配置，自定义docker镜像存放路径 1DOCKER_OPTS=&quot;--graph=/home/david/opt/docker&quot; sudo service docker start #启动sudo service docker stop #关闭sudo service docker restart #重启 1.2 pull一个镜像查看镜像：此时应该没有镜像docker images 下载镜像 sudo docker pull ubuntu 1.3 创建容器docker ps命令 123$ sudo docker ps #列出当前所有正在运行的container$ sudo docker ps -l #列出最近一次启动的，且正在运行的container$ sudo docker ps -a #列出所有的container 启动容器，并且进入到Ubuntu容器的bash命令 1$ sudo docker run -itv /home/david/docker:/home/davida/gitlab ubuntu /bin/bash 其中， 1.4 提交容器退出容器，使用 docker commit 命令来提交更新后的副本。 1$ sudo docker commit -m 'xmo run evn' -a 'david' bde4f2f2db5f ubuntu-ruby:v1 其中，-m 来指定提交的说明信息，跟我们使用的版本控制工具一样；-a 可以指定更新的作者信息；之后是用来创建镜像的容器的ID；最后指定目标镜像的仓库名和 tag 信息。创建成功后会返回这个镜像的 ID 信息。v1是tag（版本号） 1$ sudo docker inspect ubuntu-ruby 查看详细信息 1.5 Dockerfile提交容器Dockerfile的指令是忽略大小写的，建议使用大写，使用 # 作为注释，每一行只支持一条指令，每条指令可以携带多个参数。Dockerfile的指令根据作用可以分为两种，构建指令和设置指令。构建指令用于构建image，其指定的操作不会在运行image的容器上执行；设置指令用于设置image的属性，其指定的操作将在运行image的容器中执行。 1）FROM 基础image 基础image可以是官方远程仓库中的，也可以位于本地仓库。 1FROM &lt;image&gt; 或者 1FROM &lt;image&gt;:&lt;tag&gt; 制定某个tag版本 1.4 用SSH访问容器先安装ssh 12apt-get updateapt-get install openssh-server 需要修改/etc/ssh/sshd_config文件中内容 1234RSAAuthentication yes #启用 RSA 认证PubkeyAuthentication yes #启用公钥私钥配对认证方式AuthorizedKeysFile .ssh/authorized_keys #公钥文件路径（和上面生成的文件同）PermitRootLogin yes #root能使用ssh登录 重启sshd服务 12/etc/init.d/ssh stop/etc/init.d/ssh start 2、常用命令docker start和docker run 从图片可以看出，docker run 命令先是利用镜像创建了一个容器，然后运行这个容器。 docker run命令类似于git pull命令。git pull命令就是git fetch 和 git merge两个命令的组合，同样的，docker run就是docker create和docker start两个命令的组合。 docker psdocker ps 命令会列出所有运行中的容器。这隐藏了非运行态容器的存在，如果想要找出这些容器，我们需要使用docker ps -a docker imagesdocker images命令会列出了所有顶层（top-level）镜像。实际上，在这里我们没有办法区分一个镜像和一个只读层，所以我们提出了top-level 镜像。只有创建容器时使用的镜像或者是直接pull下来的镜像能被称为顶层（top-level）镜像，并且每一个顶层镜像下面都隐藏了多个镜像层。 docker images -a docker images –a命令列出了所有的镜像，也可以说是列出了所有的可读层。如果你想要查看某一个image-id下的所有层，可以使用docker history来查看。 docker stop docker stop命令会向运行中的容器发送一个SIGTERM的信号，然后停止所有的进程。 docker commit docker commit命令将容器的可读写层转换为一个只读层，这样就把一个容器转换成了不可变的镜像。 dockesr exec docker exec 命令会在运行中的容器执行一个新进程。 docker save docker save命令会创建一个镜像的压缩文件，这个文件能够在另外一个主机的Docker上使用。和export命令不同，这个命令为每一个层都保存了它们的元数据。这个命令只能对镜像生效。 docker export docker export命令创建一个tar文件，并且移除了元数据和不必要的层，将多个层整合成了一个层，只保存了当前统一视角看到的内容（expoxt后 的容器再import到Docker中，通过docker images –tree命令只能看到一个镜像；而save后的镜像则不同，它能够看到这个镜像的历史镜像）。 参考Ubuntu 15.04下安装Docker Docker的镜像和容器的区别 Docker容器和镜像区别 Docker学习笔记（3）— 如何使用Dockerfile构建镜像]]></content>
      <categories>
        <category>工具和环境</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[符号约定]]></title>
    <url>%2F2018%2F01%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E7%AC%A6%E5%8F%B7%E7%BA%A6%E5%AE%9A%2F</url>
    <content type="text"><![CDATA[大写字母表示随机变量，小写字母表示某个随机变量具体的取值，如$X=x$ 用$P(X)$表示随机变量X的概率分布，用$P(X,Y)$表示X和Y的联合概率分布，用$P(Y|X)$表示已知X时Y的条件概率分布。 用$p(X=x)$表示X某个取值的概率，在不引起混淆的情况下，$p(X=x)=p(x)$。 用$p(x,y)$表示联合概率，用$p(y|x)$表示条件概率。 m组训练样本的表示 \{(x^{(1)},y^{(1)}) ,(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)}) \}其中，$x^{(i)} \in \mathbb{R}^{n} $，n表示特征向量x的维度，$y^{(i)} \in {0,1}$。 令$X={ x^{(1)},x^{(2)},…,x^{(m)} }$，$Y={ y^{(1)},y^{(2)},…,y^{(m)} }$]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[xgboost]]></title>
    <url>%2F2018%2F01%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2Fxgboost%2F</url>
    <content type="text"><![CDATA[XGBoost 中文文档: http://xgboost.apachecn.org/cn/latest/ xgboost入门与实战（实战调参篇） 标签： xgboostpythonkaggle机器学习 Python XGBoost算法代码实现和筛选特征应用 xgboost+LR做ctr预测 手写识别示例代码 https://www.cnblogs.com/harvey888/p/7203256.html mac安装 12git clone --recursive https://github.com/dmlc/xgboostcd xgboost; cp make/minimum.mk ./config.mk; make -j4 xgboost 特征评分的计算原理xgboost实战http://www.360doc.com/content/17/1123/17/35874779_706497143.shtml# Xgboost叶子特征作为特征组合https://blog.csdn.net/zhangf666/article/details/70183788 xgboost调参、stackinghttps://github.com/lytforgood/MachineLearningTrick 利用GBDT模型构造新特征https://blog.csdn.net/bryan__/article/details/51769118 XGBoost Plotting API以及GBDT组合特征实践https://blog.csdn.net/zhangf666/article/details/70183788 画图https://blog.csdn.net/pipixiu/article/details/79057885 1234567891011121314151617def ceate_feature_map(features): outfile = open(&apos;xgb.fmap&apos;, &apos;w&apos;) i = 0 for feat in features: outfile.write(&apos;&#123;0&#125;\t&#123;1&#125;\tq\n&apos;.format(i, feat)) i = i + 1 outfile.close()ceate_feature_map(X_train.columns)#特征名列表# plot_tree(model_sklearn,fmap=&apos;xgb.fmap&apos;)fig,ax = plt.subplots()fig.set_size_inches(40,30)plot_tree(model_sklearn,ax = ax,fmap=&apos;xgb.fmap&apos;)# plot_tree(model_sklearn)plt.show() 问题记录123456789101112Traceback (most recent call last): File &quot;/Users/david/david/code/00project/carthage/xgboost_example.py&quot;, line 43, in &lt;module&gt; model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=100) File &quot;/Users/david/anaconda3/lib/python3.6/site-packages/xgboost/training.py&quot;, line 204, in train xgb_model=xgb_model, callbacks=callbacks) File &quot;/Users/david/anaconda3/lib/python3.6/site-packages/xgboost/training.py&quot;, line 74, in _train_internal bst.update(dtrain, i, obj) File &quot;/Users/david/anaconda3/lib/python3.6/site-packages/xgboost/core.py&quot;, line 894, in update dtrain.handle)) File &quot;/Users/david/anaconda3/lib/python3.6/site-packages/xgboost/core.py&quot;, line 130, in _check_call raise XGBoostError(_LIB.XGBGetLastError())xgboost.core.XGBoostError: b&apos;[09:28:32] src/objective/regression_obj.cc:43: Check failed: info.labels_.size() != 0U (0 vs. 0) label set cannot be empty\n\nStack trace returned 2 entries:\n[bt] (0) 0 libxgboost.dylib 0x000000010b91f3c3 dmlc::StackTrace[abi:cxx11]() + 67\n[bt] (1) 1 libstdc++.6.dylib 0x000000010bd9bce0 vtable for std::__cxx11::basic_stringbuf&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; + 16\n\n&apos; 没有加上label 1xgboost.core.XGBoostError: b&apos;[14:00:02] src/objective/regression_obj.cc:44: Check failed: preds-&gt;Size() == info.labels_.size() (220 vs. 110) labels are not correctly providedpreds.size=220, label.size=110\n\nStack trace returned 2 entries:\n[bt] (0) 0 libxgboost.dylib 0x000000011539b3c3 dmlc::StackTrace[abi:cxx11]() + 67\n[bt] (1) 1 libstdc++.6.dylib 0x0000000115817ce0 vtable for std::__cxx11::basic_stringbuf&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; + 16\n\n&apos; 画图失败1graphviz.backend.ExecutableNotFound: failed to execute [&apos;dot&apos;, &apos;-Tpng&apos;], make sure the Graphviz executables are on your systems&apos; PATH 调参https://zhuanlan.zhihu.com/p/25308120 https://blog.csdn.net/han_xiaoyang/article/details/52665396 以上特征，都表明我们需要降低模型复杂程度，有哪些参数可以调整呢： - 直接降低模型复杂度 max_depth、min_child_weight、gamma - 随机化 subsample、colsample_bytree - 正则化 lambda、alpha 还有，先粗调，再微调 — 降低learning_rate，当然同时，提高n_estimators \2. 非平衡数据集怎么办 — 想办法弄到更多的数据 — 想办法把数据弄平衡 — 利用smote等算法来过采样/欠采样 — 设置weight（初始化DMatrix时） — 使用更好的metrics：auc、f1 — min_child_weight 设的小一点 — scale_pos_weight = 0值的样本数量/1值的样本数量 — max_delta_step — 自定义评价函数]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>xgboost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kaggle经典问题]]></title>
    <url>%2F2018%2F01%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FKaggle%E7%BB%8F%E5%85%B8%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[Digit Recognizerhttps://www.kaggle.com/c/digit-recognizer]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Kaggle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[吴恩达深度学习笔记]]></title>
    <url>%2F2018%2F01%2F12%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[参考资料 代码位于blogcodes/deeplearning_ai Numpy用法sigmoid12def sigmoid(x): s = 1/(1+ np.exp(-x)) img2Vec-reshape123def image2vector(image): v = image.reshape((image.shape[0]*image.shape[1]*image.shape[2]),1) return v normalize行归一化：一行的每个列的值，除以该列的L2范数。 1234def normalizeRows(x): x_norm = np.linalg.norm(x,axis=1,keepdims=True) x = x/x_norm return x softmax normalize 1234def softmax(x): x_exp = np.exp(x) x_sum = np.sum(x_exp, axis=1, keepdims=True) s = x_exp/x_sum 实现损失函数L1损失函数 L_1(\hat y,y)=\sum_{i=0}^m |\hat y^{(i)}-y^{(i)}|123def L1(yhat, y): loss = np.sum(abs(yhat-y)) return loss L2损失函数 L_2(\hat y, y)=\sum_{i=0}^m (\hat y^{(i)}-y^{(i)})^2123def L2(yhat, y): loss = np.dot(y-yhat,y-yhat) return loss LR图像分类代码见 blogcodes/deeplearning_ai/lr_image.py 吴恩达课程的数据集在blogcodes/deeplearning_ai/datasets和blogcodes/deeplearning_ai/lr_utils.py 1234567import numpy as npimport matplotlib.pyplot as pltimport h5pyimport scipyfrom PIL import Imagefrom scipy import ndimagefrom lr_utils import load_dataset 数据预处理包括加载数据、flatten到二维数组（原来是四维数组），归一化 加载数据 1train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset() train_set_x_orig是shape为(m_train, num_px, num_px, 3)的numpy-array，m_train是训练样本数。 ​ 为了便于训练，将其reshapre为(num_px*num_px*3, 1)，这样每列就代表一个图像，一共有m_train列。 123# 写成train_set_x_orig.reshape(-1,train_set_x_orig.shape[0])就可以了，不知为何要反过来再加上Ttrain_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0],-1).Ttest_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0],-1).T 再将矩阵的元素归一化到[0-1]的范围，每个元素/255即可 12train_set_x = train_set_x_flatten/255test_set_x = test_set_x_flatten/255 LR的主要公式为 Building the Neural Network构建一个神经网络的主要步骤是： 1、Define the model structure ( usually such as number of input features) 2、Initialize the model’s parameters 3、Loop: - Calculate current loss ( forward propagation) Calculate current gradient ( backward porpagation) Update parameters ( gradient descent) Initializing Parameterswill creates a vector of zeros of shape (dim, 1) for w and initializes b to 0. 1234567def initialize_with_zeros(dim): w = np.zeros((dim, 1)) b = 0 assert(w.shape == (dim ,1)) assert(isinstance(b, float) or isinstance(b, int)) return w,b the shape of w should be (num_px*num_px*3, 1) Forward and backward propagation 之前的LR没有考虑b，只有对w的梯度下降优化 123456789101112131415161718192021222324# 3个样本，每个样本2个特征，X的每行是一个特征，每列是一个样本# w=(2,1), X=(2,3), Y=(1,3)w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]),np.array([[1,0,1]])#my codedef forward_propagation(w, b, X, Y): A = sigmoid(np.dot(w.T, X) + b) # 错误的写法 J = -1/m * np.sum( np.dot(Y, np.log(A)) + np.dot((1-Y),np.log(1-A)) ) dw = 1/m * np.dot(X, np.dot(A-Y).T) db = 1/m * np.sum(A-Y) # correct codedef forward_propagation(w, b, X, Y): # m是样本个数，3 m = X.shape[1] A = sigmoid(np.dot(w.T, X) + b) cost = - ( np.dot(Y, np.log(A.T)) + np.dot(np.log(1-A),(1-Y).T) )/m dw = np.dot(X, (A-Y).T) / m db = np.sum(A-Y) / m cost = np.squeeze(cost) grads = &#123;"dw": dw, "db", db&#125; return grads, cost 1、损失函数写成矩阵运算的形式，否则dot会报错。 2、漏定义了m，m为样本个数 3、将cost变为数字而不是矩阵 4、dw和db放到dict中，方便读取 Optimizationupdate parameters using gradient descent. The goal of optimization function is to learn w and b by minimizing J. For a parameter $\theta$ , the update rule is \theta = \theta - \alpha \ d\theta1234567891011121314151617181920def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False): costs = [] for i in range(num_iterations): grads, cost = forward_propagation(w, b, X, Y) dw = grads[&quot;dw&quot;] db = grads[&quot;db&quot;] w = w - learning_rate * dw b = b - learning_rate * db if i % 100 == 0: costs.append(cost) print(cost) params = &#123; &quot;w&quot;:w, &quot;b&quot;:b &#125; return params 例子中给的方法，是批量梯度下降，每次循环都要计算所有样本 PredictWe will use w and b to predict the labels for a dataset X. 1234567891011121314151617181920# my codedef predict(w, b, X): Y = sigmoid(np.dot(w.T, X) + b) return Y # correct codedef predict(w, b, X): m = X.shape[1] Y_prediction = np.zeros((1,m)) A = sigmoid(np.dot(w.T, X) + b) for i in range(A.shape[1]): if A[0][i] &lt;= 0.5: A[0][i] = 0 else: A[0][i] = 1 Y_prediction = A return Y_prediction 1、我求出来的只是sigmoid的输出，还要转化成分类。 2、先定义一个空白的Y，将sigmoid的输出根据值转换为0和1，然后赋值给Y。 Merge All Functions into a Model]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数据预处理]]></title>
    <url>%2F2018%2F01%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[MNIST使用和处理1234567891011121314151617181920import numpy as npimport pandas as pdimport xgboost as xgbfrom sklearn.cross_validation import train_test_split#记录程序运行时间import time start_time = time.time()#读入数据train = pd.read_csv("Digit_Recognizer/train.csv")tests = pd.read_csv("Digit_Recognizer/test.csv") #用sklearn.cross_validation进行训练数据集划分，这里训练集和交叉验证集比例为7：3，可以自己根据需要设置train_xy,val = train_test_split(train, test_size = 0.3,random_state=1)y = train_xy.labelX = train_xy.drop(['label'],axis=1)val_y = val.labelval_X = val.drop(['label'],axis=1)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>预处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树]]></title>
    <url>%2F2017%2F09%2F11%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[1 决策树公式符号 \begin{align} &Ent(X)\ \ \ 熵 \\ &Gain(X)\ \ \ 信息增益\\ &Gini(X)\ \ \ 基尼指数\\ &D\ \ \ 训练集\\ &A\ \ \ 训练集的某个特征\\ &N\ \ \ 特征A的类别总数\\ &K\ \ \ 标签分类的数量\\ \end{align}1.1 关键步骤-python实现创建决策树分支的createBranch()伪代码函数 123456789检查数据集中每个子项是否属于同一个分类： IF YES return 类标签； ELSE 寻找划分数据集的最好特征； 划分数据集； 创建分支节点； for 每个划分的子集 递归调用createBranch()并增加返回结果到分支节点中 return 分支节点 对label的分类计算熵 12345678910111213141516def calcEnt(dataSet): labelNum = len(dataSet) ent = 0.0 #定义字典存放每个类别的count统计 labelCounts = &#123;&#125; #统计每个label的个数 for featureVec in dataSet: #最后一列是label label = featureVec[-1] if label not in labelCounts.keys(): labelCounts[label] = 0 labelCounts[label] += 1 #计算概率以及熵 for key in labelCounts: prob = float(labelCounts[key]) / labelNum ent -= prob * log(2, prob) return ent 对数据集进行划分 12345678def splitDataSet(dataSet, axis, value): subDataSet = [] for featureVec in dataSet: if featureVec[axis] == value: reducedFeatVec = featureVec[:axis] reducedFeatVec.extend(featureVec[axis+1:]) resDataSet.append(reducedFeatVec) return subDataSet 选出最好的数据集划分方式 信息增益 熵的定义是 Ent(X) = -\sum_{i=1}^{n}p(x_i)log_2p(x_i)n是类别总数。 条件熵$Ent(Y|X)$表示在已知X的条件下Y的不确定性，定义为给定X时Y的条件概率分布的熵对X的期望 Ent(Y|X)=\sum_{i=1}^np_iEnt(Y|X=x_i)对于训练集D以及其中的特征A，熵就是 Ent(D) = -\sum_{k=1}^K \frac {|C_k|}{|D|} log_2\frac{|C_k|}{|D|}其中，K是标签分类的数量，$C_k$是每个分类的样本数 条件熵就是 \begin{aligned} Ent(D|A) &=\sum_{i=1}^N\frac{|D_i|}{|D|}Ent(D_i) \\ &=\sum_{i=1}^N\frac{|D_i|}{|D|}(-\sum_{k=1}^K \frac {|D_{ik}|}{|D_i|} log_2\frac{|D_{ik}|}{|D_i|}) \end{aligned}其中，N是特征A的类别总数，$D_i$是特征A的每种类别的数量。 信息增益就是两者之差 Gain(D,A)=Ent(D)-Ent(D|A)信息增益也称为互信息。 找出信息增益最大的来划分数据集 12345678910111213141516171819202122def chooseBestFeature(dataSet): #feature数量，最后一列是label numFeature = len(dataSet[0]-1) bestInfoGain = 0.0 bestFeature = -1 #先计算熵 baseEntropy = calcEnt(dataSet) for i in range(numFeature): #首先需要知道该特征有几个值 uniqueValue = set([sample[i] for sample in dataSet]) #用set去重是最快方法 newEntropy = 0.0 #对于每个特征，计算条件熵 for value in uniqueValue: #用这个特征划分数据集 subDataSet = splitDataSet(dataSet, i, value) newEntropy += calcEnt(subDataSet) #计算信息增益 infoGain = baseEntropy-newEntropy if infoGain &gt; bestInfoGain: bestInfoGain = infoGain bestFeature = ireturn bestFeature 如果所有特征都处理过了，但是类标签依然不是唯一的，用投票决定 1234567def majorityCnt(classList): classCount=&#123;&#125; for vote in classList: if vote not in classCount.keys() classCount[vote] = 0 classCount[vote] += 1 sortedClassCount = sorted(classCount, key = operator.itemgetter(1), reverse = True) return sortedClassCount[0][0] 1.2 ID3算法与上面的步骤类似。但是ID3只有树的生成，容易过拟合。 1.3 C4.5算法与ID3相比，C4.5用信息增益比来选择特征。 信息增益比 在面对类别比较少的离散数据时，两者差不多。但如果面对连续的数据（如体重、身高、年龄、距离等），或者每列数据没有明显的类别之分（最极端的例子的该列所有数据都独一无二）。 那么根据信息增益公式，$Ent(D)$不变，当数据独一无二时， Ent(D|A)=\sum_{i=1}^n \frac {1}{n}Ent(D_i)这样$Ent(D|A)$最小，程序会倾向于这种划分，导致划分效果差。 信息增益比的公式为 Gain_R(D,A)=\frac {Gain(D,A)}{Ent(D)}可以理解成对分支数目的惩罚项。 1.5 CART算法CART是分类与回归树，由特征选择、树的生成和剪枝组成。 CART是在给定输入变量X条件下输出随机变量Y的条件概率分布的方法。CART假设决策树是二叉树，内部结点特征的取值为是和否，约定左是右否。 决策树等价于递归的二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定的条件下输出的条件概率分布。 1.5.1 CART的生成 递归构建二叉树的过程。回归树用最小二乘，分类树用基尼指数。 1）回归树 2）分类树 假设有K个类，样本点属于第k类的概率是$p_k$，则基尼指数定义为 Gini(p) = \sum_{k=1}^K p_k(1-p_k) = 1-\sum_{k=1}^K p_k^2=1-\sum_{k=1}^K(\frac{|C_k|}{|D|})^2如果是两分类问题，则概率分布的基尼指数为 Gini(P)=2p(1-p)若样本集合D根据特征A是否取某一值a被划分为$D_1$和$D_2$两部分，即 D_1=\{(x,y)\in D | A(x)=a\}, D_2=D-D_1则在特征A的条件下，集合D的基尼指数为 Gini(D,A)=\frac {|D_1|}{|D|}Gini(D_1)+\frac {|D_2|}{|D|}Gini(D_2)Gini越大，样本集合的不确定性越大，与熵相似。 算法过程 12345678输入：训练集D，停止条件输出：CART决策树从根结点递归对每个结点进行以下操作，构建二叉树：1）对每个特征和可能的取值a，根据A=a的为是或否，将D分割成D1和D2，计算基尼指数2）选出基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。3）对两个子结点递归调用#1、#2，直至满足停止条件4）生成CART决策树 3）CART剪枝 1.6 决策树的剪枝 如何判断剪枝后泛华性能提升？ 用留出法。将一部分训练集作为验证集。 剪枝是为了解决过拟合。通过极小化决策树整体的损失函数来实现。设树T的叶结点个数为$|T|$，t是T的叶结点，该叶结点有$Nt$个样本点，其中k类的样本点有$N{tk}$个，则损失函数定义为 C_{\alpha}(T) = \sum_{t=1}^T N_tEnt_t(T) + \alpha|T|由于 Ent_t(T) = - \sum_{k=1}^K \frac {N_{tk}}{N_t} log_2\frac {N_{tk}}{N_t}则令 C(T) = - \sum_{t=1}^T\sum_{k=1}^KN_{tk}log_2\frac {N_{tk}}{N_t}于是 C_\alpha(T) = C(T) +\alpha|T|这里，$C(T)$表示训练数据的预测误差，$|T|$表示模型复杂度，$\alpha$控制两者影响，较大时选择较简单的树，反之亦然，等于0时就不考虑模型复杂度。 两种剪枝思路 预剪枝（Pre-Pruning） 构造的同时剪枝。比如设一个阈值，熵减小的数量小于这个阈值，即使还可以继续降低熵，也停止继续创建分支。 有些分支虽然当前划分时性能下降，但后续划分有可能又会提高，仅根据当前验证集来判断是否要继续划分，往往会导致欠拟合。 后剪枝（Post-Pruning） 三种主要方法 1）REP错误率降低剪枝 简单粗暴，对每个非叶结点的子树，用其替换一个叶结点，类别用子树覆盖训练样本中类最多的代替。这样产生的简化树再跟原树比较在测试数据集中的效果。若错误更少就替换。算法以Bottom-up的方式遍历所有的子树，直到没有任何改进时，终止。 2）PEP悲观剪枝 1.7 连续和缺失值处理1）连续值离散化 jueceshu最简单的策略是二分法，也是C4.5采用的机制。 对于连续属性a，可以考察包含n-1个元素的候选划分点集合 T_a=\left \{ \frac {a^i+a^{i+1}} {2} | 1 \leqslant i \leqslant n-1 \right \}即把区间的中位点作为候选划分点，然后像离散值那样考察划分点，再选出最优的划分点。 2）缺失值 考虑：①如何在属性值缺失的情况下进行划分属性选择？②给定划分属性，若样本在该属性的值缺失，如何划分？ 靠权重。在判定划分时，权重相等，用已知的样本来划分属性。对于每个划分属性，若属性缺失，将缺失的记录根据属性的每个划分所占比例作为权重，分到属性的每个子结点中。 1.8 多变量决策树非子结点不再针对某个属性，而是多个属性的线性组合。即，每个非子结点都是一个线性分类器。 2、随机森林随机森林如何随机的，特征也随机，样本也随机 3、GBDT决策树是否应该用one-hot编码 参考 统计学习方法 决策树的剪枝问题]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>决策树</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[贝叶斯分类器]]></title>
    <url>%2F2017%2F09%2F11%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8%2F</url>
    <content type="text"><![CDATA[半朴素贝叶斯分类器AODE 比如西瓜书提到的TAN。每个属性依赖的另外的属性由最大带权生成树来确定。 （1）先求每个属性之间的互信息来作为他们之间的权值。 （2）构件完全图。权重是刚才求得的互信息。然后用最大带权生成树算法求得此图的最大带权的生成树。 （3）找一个根变量，然后依次将图变为有向图。 （4）添加类别y到每个属性的的有向边。 贝叶斯判定准则（Bayes decision rule）：为最小化总体风险，只需在每个样本上选择那个能使条件风险$R(c|x)$最小的类别标记。 我定义了一个条件风险和一个判断分类的准则，如果这个准则能使条件风险最小，也就能使总体风险最小。 朴素贝叶斯假设数据是独立分布的，即属性条件独立性假设 （attribute conditional independence assumption）。对已知类别，假定所有属性相互独立。这一假设使朴素贝叶斯变得简单，但会牺牲一定的分类准确度。 然后，如果属性中有连续值的属性，在计算概率时又假定数据符合正态分布。 贝叶斯分类器有两种实现方式， 基于伯努利模型，不考虑样本中特征出现的次数，只考虑出不出现，相当于假设每个特征是同等权重的。 基于多项式模型，也考虑出现次数 机器学习实战的代码 从文档中创建词典 12345def createVocabList(dataSet): vocabSet = set([]) #create empty set for document in dataSet: vocabSet = vocabSet | set(document) #union of the two sets return list(vocabSet) 给定一个词典和输入文档，如果某个词出现，就给词典的下标置1，就是创建词袋 1234567def setOfWords2Vec(vocabList, inputSet): returnVec = [0]*len(vocabList) for word in inputSet: if word in vocabList: returnVec[vocabList.index(word)] = 1 else: print "the word: %s is not in my Vocabulary!" % word return returnVec 训练朴素贝叶斯分类器 书中这里出现了错误！ http://blog.csdn.net/lming_08/article/details/37542331 1234567891011121314151617181920def trainNB0(trainMatrix,trainCategory): numTrainDocs = len(trainMatrix) numWords = len(trainMatrix[0]) # 计算所有样本为第一类的概率p(c)，分类是0和1所以就直接相加 pAbusive = sum(trainCategory)/float(numTrainDocs) # 防止最后某一个的概率是0 p0Num = ones(numWords); p1Num = ones(numWords) #change to ones() p0Denom = 2.0; p1Denom = 2.0 #change to 2.0，这里代表类别数 for i in range(numTrainDocs): if trainCategory[i] == 1: # 矩阵粒度的加法，对应位置相加 p1Num += trainMatrix[i] # 这里为何是所有特征相加，回去再看看 p1Denom += sum(trainMatrix[i]) else: p0Num += trainMatrix[i] p0Denom += sum(trainMatrix[i]) p1Vect = log(p1Num/p1Denom) #change to log() p0Vect = log(p0Num/p0Denom) #change to log() return p0Vect,p1Vect,pAbusive 预测 12345678def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1): # 之前用了log，于是概率相乘就转化为相加，最后再加上p(c) p1 = sum(vec2Classify * p1Vec) + log(pClass1) #element-wise mult p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1) if p1 &gt; p0: return 1 else: return 0 高斯、伯努利、多项式高斯模型假设连续变量符合高斯分布， 多项式模型常用于文本分类，特征是单词，值是单词的出现次数。 多项式模型在训练一个数据集结束后可以继续训练其他数据集而无需将两个数据集放在一起进行训练。在sklearn中，MultinomialNB()类的partial_fit()方法可以进行这种训练。这种方式特别适合于训练集大到内存无法一次性放入的情况。 伯努利模型每个特征的取值是bool型，即true或false]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>贝叶斯</tag>
        <tag>西瓜书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（转）LDA]]></title>
    <url>%2F2017%2F08%2F13%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FNLP%2FGensim-LDA%2F</url>
    <content type="text"><![CDATA[原理gensim-LDAhttp://blog.csdn.net/whzhcahzxh/article/details/17528261 引用gensim包，gensim包中引用corpora,models, similarities，分别做语料库建立，模型库和相似度比较库 from gensim import corpora, models, similarities import jieba 1、分词123456sentences = [&quot;我喜欢吃土豆&quot;,&quot;土豆是个百搭的东西&quot;,&quot;我不喜欢今天雾霾的北京&quot;]words=[]for doc in sentences:# 结巴分词返回的是一个generator，要用list()转成list words.append(list(jieba.cut(doc)))print words 输出： [[u’\u6211’, u’\u559c\u6b22’, u’\u5403’, u’\u571f\u8c46’], [u’\u571f\u8c46’, u’\u662f’, u’\u4e2a’, u’\u767e’, u’\u642d’, u’\u7684’, u’\u4e1c\u897f’], [u’\u6211’, u’\u4e0d’, u’\u559c\u6b22’, u’\u4eca\u5929’, u’\u96fe’, u’\u973e’, u’\u7684’, u’\u5317\u4eac’]] 此时输出的格式为unicode，不影响后期运算，因此我保留不变，如果想看分词结果可以用循环输出jieba分词结果 2、分词结果构造词典12345dic = corpora.Dictionary(words)# 词袋中的所有词print dic# 每个词和编号print dic.token2id 输出： Dictionary(15 unique tokens: [u’\u973e’, u’\u5403’, u’\u5317\u4eac’, u’\u7684’, u’\u4e1c\u897f’]…) {u’\u973e’: 14, u’\u5403’: 0, u’\u5317\u4eac’: 12, u’\u7684’: 9, u’\u4e1c\u897f’: 4, u’\u4e2a’: 5, u’\u642d’: 6, u’\u662f’: 7, u’\u6211’: 3, u’\u559c\u6b22’: 1, u’\u4eca\u5929’: 11, u’\u571f\u8c46’: 2, u’\u4e0d’: 10, u’\u96fe’: 13, u’\u767e’: 8} 为方便看数据： 12for word,index in dic.token2id.iteritems(): print word +&quot; 编号为:&quot;+ str(index) 输出： 北京 编号为:12搭 编号为:6的 编号为:9喜欢 编号为:1不 编号为:10东西 编号为:4土豆 编号为:2霾 编号为:14是 编号为:7个 编号为:5雾 编号为:13百 编号为:8今天 编号为:11我 编号为:3吃 编号为:0 3、生成语料库词袋模型 12corpus = [dic.doc2bow(text) for text in words]print corpus 输出： [[(0, 1), (1, 1), (2, 1), (3, 1)], [(2, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1)], [(1, 1), (3, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1)]] 4、TFIDF变换通过语料库得到tfidf的值，由tfidf来描述句子 123456789#通过语料库得到tfidf模型tfidf = models.TfidfModel(corpus)vec = [(0, 1), (4, 1)]print tfidf[vec]#对corpus的每个文档中的每个词计算tfidf，得到的结果是，每个文档的每个词都是一个元组，包括id和tfidf值corpus_tfidf = tfidf[corpus]for doc in corpus_tfidf: print doc 输出： [(0, 0.7071067811865475), (4, 0.7071067811865475)][(0, 0.8425587958192721), (1, 0.3109633824035548), (2, 0.3109633824035548), (3, 0.3109633824035548)][(2, 0.16073253746956623), (4, 0.4355066251613605), (5, 0.4355066251613605), (6, 0.4355066251613605), (7, 0.4355066251613605), (8, 0.4355066251613605), (9, 0.16073253746956623)][(1, 0.1586956620869655), (3, 0.1586956620869655), (9, 0.1586956620869655), (10, 0.42998768831312806), (11, 0.42998768831312806), (12, 0.42998768831312806), (13, 0.42998768831312806), (14, 0.42998768831312806)] vec是查询文本向量，比较vec和训练中的三句话相似度 123index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features=14)sims = index[tfidf[vec]]print list(enumerate(sims)) 输出： [(0, 0.59577906), (1, 0.30794966), (2, 0.0)] 表示和第1句话相似度为59.578%，和第二句话的相似度位30.79%，第三句没有相似度， 我们看看vec这句话是什么：0为吃，4为东西，所以vec这句话可以是[“吃东西”]或者[“东西吃”] 而第一句话”我喜欢吃土豆”,”土豆是个百搭的东西”明显有相似度，而第三句话”我不喜欢今天雾霾的北京”，相似度几乎为0，至于为什么第一句比第二句更相似，就需要考虑TfIdf document representation和cosine similarity measure了 回到tfidf转换，接着训练LSI模型，假定三句话属于2个主题， 1234lsi = models.LsiModel(corpus_tfidf, id2word=dic, num_topics=2)lsiout=lsi.print_topics(2)print lsiout[0]print lsiout[1] 输出： 0.532“吃” + 0.290“喜欢” + 0.290“我” + 0.258“土豆” + 0.253“霾” + 0.253“雾” + 0.253“北京” + 0.253“今天” + 0.253“不” + 0.166“东西”0.393“百” + 0.393“搭” + 0.393“东西” + 0.393“是” + 0.393“个” + -0.184“霾” + -0.184“雾” + -0.184“北京” + -0.184“今天” + -0.184“不” 这就是基于SVD建立的两个主题模型内容 将文章投影到主题空间中 123corpus_lsi = lsi[corpus_tfidf]for doc in corpus_lsi: print doc 输出： [(0, -0.70861576320682107), (1, 0.1431958007198823)][(0, -0.42764142348481798), (1, -0.88527674470703799)][(0, -0.66124862582594512), (1, 0.4190711252114323)] 因此第一三两句和主题一相似，第二句和主题二相似 同理做个LDA 1234567lda = models.LdaModel(corpus_tfidf, id2word=dic, num_topics=2)ldaOut=lda.print_topics(2)print ldaOut[0]print ldaOut[1]corpus_lda = lda[corpus_tfidf]for doc in corpus_lda: print doc 得到的结果每次都变，给一次的输出： 0.077吃 + 0.075北京 + 0.075雾 + 0.074今天 + 0.073不 + 0.072霾 + 0.070喜欢 + 0.068我 + 0.062的 + 0.061土豆0.091吃 + 0.073搭 + 0.073土豆 + 0.073个 + 0.073是 + 0.072百 + 0.071东西 + 0.066我 + 0.065喜欢 + 0.059霾[(0, 0.31271095988105352), (1, 0.68728904011894654)][(0, 0.19957991735916861), (1, 0.80042008264083142)][(0, 0.80940337254233863), (1, 0.19059662745766134)] 第一二句和主题二相似，第三句和主题一相似 结论和LSI不一样，我估计这和样本数目太少，区别度不高有关，毕竟让我来区分把第一句和哪一句分在一个主题，我也不确定 输入一句话，查询属于LSI得到的哪个主题类型，先变成词袋模型，然后查询LSI： 12345query = "雾霾"query_bow = dic.doc2bow(list(jieba.cut(query)))print query_bowquery_lsi = lsi[query_bow]print query_lsi 输出: [(13, 1), (14, 1)][(0, 0.50670602027401368), (1, -0.3678056037187441)] 与第一个主题相似 比较和第几句话相似，用LSI得到的索引接着做，并排序输出 12345index = similarities.MatrixSimilarity(lsi[corpus])sims = index[query_lsi]print list(enumerate(sims))sort_sims = sorted(enumerate(sims), key=lambda item: -item[1])print sort_sims 输出： [(0, 0.90161765), (1, -0.10271341), (2, 0.99058259)][(2, 0.99058259), (0, 0.90161765), (1, -0.10271341)] 可见和第二句话相似度很高，因为只有第二句话出现了雾霾两个词，可是惊讶的是和第一句话的相似度也很高，这得益于LSI模型的算法：在A和C共现，B和C共现的同时，可以找到A和B的相似度 代码位于blogcodes/gensim_lda.py]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>gensim</tag>
        <tag>LDA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Gensim-Tutorials]]></title>
    <url>%2F2017%2F08%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FNLP%2FGensim-Tutorials%2F</url>
    <content type="text"><![CDATA[http://radimrehurek.com/gensim/tutorial.html Gensim 使用Python标准logging模块来记录log，使用方法是 12import logginglogging.basicConfig(format=&apos;%(asctime)s : %(levelname)s : %(message)s&apos;, level=logging.INFO)]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>gensim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLP代码片段]]></title>
    <url>%2F2017%2F08%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FNLP%2FNLP%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5%2F</url>
    <content type="text"><![CDATA[根据标点拆分句子[AtomSplit.java](../../../../gitlab/user-gene/nlp/src/main/java/com/buzzinate/nlp/segment/AtomSplit.java) 12345678910111213141516171819202122 public static List&lt;String&gt; splitSentences(String text) throws IOException &#123; List&lt;String&gt; result = new ArrayList&lt;String&gt;(); int last = 0; for (Term term: Segment.split(text, ToAnalysis.USE_USER_DEFINE)) &#123; if (sentenceNatures.contains(term.getNatrue().natureStr) &amp;&amp; !isWhiteSpace(term.getName())) &#123; if (term.getOffe() &gt; last) &#123; String snippet = text.substring(last, term.getOffe()).trim(); if (snippet.length() &gt; 0) result.add(snippet); &#125; last = term.getOffe() + term.getName().length(); &#125; &#125; if (text.length() &gt; last) &#123; String snippet = text.substring(last, text.length()).trim(); if (snippet.length() &gt; 0) result.add(snippet); &#125; return result; &#125;private static boolean isWhiteSpace(String term) &#123; return term.length() == 1 &amp;&amp; (Character.isWhitespace(term.charAt(0)) || term.charAt(0) == '-'); &#125; 给每个字符标记类型[AtomSplit.java](../../../../gitlab/user-gene/nlp/src/main/java/com/buzzinate/nlp/segment/AtomSplit.java) 123456789101112131415161718192021222324252627282930/** * 给每个字符标记类型，如 * [2011(AT_NUM), -(AT_PUNC), 34(AT_NUM), -(AT_PUNC), 43(AT_NUM), (AT_PUNC), 为(AT_CHINESE), 中(AT_CHINESE), 国(AT_CHINESE)] * @param text * @return */public static List&lt;Atom&gt; split(String text) &#123; List&lt;Atom&gt; result = new ArrayList&lt;Atom&gt;(); int last = 0; AtomType t = AtomType.AT_LETTER; for (int i = 0; i &lt; text.length(); i++) &#123; char ch = text.charAt(i); if (TextUtil.isAlphaOrDigit(ch) || ch == '\'' || ch == '.') &#123; if (i == last) &#123; t = AtomType.AT_LETTER; if (Character.isDigit(ch)) t = AtomType.AT_NUM; &#125; &#125; else if (Character.isLetter(ch)) &#123; if (i &gt; last) result.add(new Atom(text.substring(last, i), t)); result.add(new Atom(text.substring(i, i+1), AtomType.AT_CHINESE)); last = i + 1; &#125; else &#123; if (i &gt; last) result.add(new Atom(text.substring(last, i), t)); if (t != AtomType.AT_LETTER || !isConnectChar(ch)) result.add(new Atom(text.substring(i, i+1), AtomType.AT_PUNC)); last = i + 1; &#125; &#125; if (text.length() &gt; last) result.add(new Atom(text.substring(last, text.length()), t)); return result;&#125; 判断字符串的语言有两个开源的项目可以使用。一个是Apache Tika，一个是language-detection。language-detection是google Code上开源的一个语言检测软件包，不折不扣的日货，但使用起来非常方便，其project链接如下：http://code.google.com/p/language-detection。基本上，你只需要引用langdetect.jar和其依赖的jsonic-1.3.0.jar（也是日货）即可 /rocket-iaudience-api/src/main/java/com/iclick/rocket/iaudience/api/common/LanguageDetectUtil.java 判断中文字符1Character.isLetter() 判断是否为英文或数字123456789public static boolean isAlphaOrDigit(char ch) &#123; if (ch &gt;= 'a' &amp;&amp; ch &lt;= 'z') return true; if (ch &gt;= 'A' &amp;&amp; ch &lt;= 'Z') return true; if (ch &gt;= '0' &amp;&amp; ch &lt;= '9') return true; return false;&#125; 去除停用词java 12345ArrayList&lt;String&gt; wordSet = new ArrayList&lt;String&gt;();// 自动去除停用词 for (Term term : NotionalTokenizer.segment(simplePhrase)) &#123; wordSet.add(term.word); &#125; python参考gensim 123456789101112131415161718192021documents = [&quot;Human machine interface for lab abc computer applications&quot;, &quot;A survey of user opinion of computer system response time&quot;, &quot;The EPS user interface management system&quot;, &quot;System and human system engineering testing of EPS&quot;, &quot;Relation of user perceived response time to error measurement&quot;, &quot;The generation of random binary unordered trees&quot;, &quot;The intersection graph of paths in trees&quot;, &quot;Graph minors IV Widths of trees and well quasi ordering&quot;, &quot;Graph minors A survey&quot;]#停用词stoplist = set(&apos;for a of the and to in&apos;.split())texts = [ [word for word in document.lower().split() if word not in stoplist ] for document in documents ] #删除仅出现一次的词from collections import defaultdictfrequency = defaultdict(int)for text in texts: for token in text: frequency[token] += 1texts = [[token for token in text if frequency[token] &gt; 1 ] for text in texts] 分词python用jieba 1234567import jiebasentences = ["我喜欢吃土豆","土豆是个百搭的东西","我不喜欢今天雾霾的北京"]words=[]for doc in sentences:# 结巴分词返回的是一个generator，要用list()转成list words.append(list(jieba.cut(doc)))print words java用hanlp 12345ArrayList&lt;String&gt; wordSet = new ArrayList&lt;String&gt;();// 自动去除停用词 for (Term term : NotionalTokenizer.segment(simplePhrase)) &#123; wordSet.add(term.word); &#125; 英文词干化]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CNN]]></title>
    <url>%2F2017%2F07%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%93%E9%A2%98%2FCNN%2FCNN%2F</url>
    <content type="text"><![CDATA[卷积核也称为滤波器。 权重共享：卷积核的权重（矩阵的值）对于不同位置的所有输入都是相同的。 卷积操作的意义例如，有整体边缘滤波器Ke，横向边缘滤波器Kh，纵向边缘滤波器Kv。 K_e=\begin{bmatrix} 0 & -4 & 0\\ -4 & 16 & -4\\ 0 & -4 & 0 \end{bmatrix}\ \ K_h=\begin{bmatrix} 1 & 2 & 1\\ 0 & 0 & 0\\ -1 & -2 & -1 \end{bmatrix}\ \ K_v=\begin{bmatrix} 1 & 0 & -1\\ 2 & 0 & -2\\ 1 & 0 & -1 \end{bmatrix}若某像素位于物体边缘，则周边像素与该像素会有明显差异，用Ke可以放大边缘和周边的差异，起到边缘检测的作用。同理，Kh、Kv可以保留横向、纵向的边缘信息。 池化层也叫汇合层。通常操作有平均值池化（average-pooling）和最大值汇合（max-pooling）。与卷积核操作不同，池化层不包含需要学习的参数。仅指定汇合类型，核大小（kernel size）和步长（stride）。 汇合的结果相对输入降小了，是一种降采样（down-sampling）操作。也可以看成是一个用p范数（p-norm）作为非线性映射的卷积操作。当p趋于正无穷时就是最大值汇合。 汇合层的引入是仿照人的视觉系统对视觉输入对象进行降维和抽象。作用有： 1）特征不变性（feature invariant）。使模型更关注是否存在某些特征而不是特征具体的位置。 2）特征降维。 3）一定程度上防止过拟合。 全连接层fully connected layers 参考： 【1】解析卷积神经网络.pdf]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark-sql笔记]]></title>
    <url>%2F2017%2F07%2F12%2Fhadoop-spark%2Fspark%2Fspark-sql%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[spark sql可以用基本的SQL语法，或者hiveQL。Spark SQL也能从hive中读数据。 Data Frame的来源可以是：结构化数据、hive表、外部数据库或者RDD Starting Point: SQLContextThe entry point into all functionality in Spark SQL is the SQLContext class, or one of its descendants. To create a basic SQLContext, all you need is a SparkContext. 12345val sc: SparkContext // An existing SparkContext.val sqlContext = new org.apache.spark.sql.SQLContext(sc)// this is used to implicitly convert an RDD to a DataFrame.import sqlContext.implicits._ 在SQLContext的基础上，也要创建一个HiveContext，可以用Hive的udf，也能读hive表。HiveContext是一个独立的包，不需要安装hive Creating DataFramesWith a SQLContext, applications can create DataFrames from an existing RDD, from a Hive table, or from data sources. As an example, the following creates a DataFrame based on the content of a JSON file: 1234567val sc: SparkContext // An existing SparkContext.val sqlContext = new org.apache.spark.sql.SQLContext(sc)val df = sqlContext.read.json("examples/src/main/resources/people.json")// Displays the content of the DataFrame to stdoutdf.show() Creating Datasets123456789101112131415161718import org.apache.spark.sql.SQLContext......// Encoders for most common types are automatically provided by importing sqlContext.implicits._val sqlContext = SQLContext.getOrCreate(sc)import sqlContext.implicits._val ds = Seq(1, 2, 3).toDS()ds.map(_ + 1).collect() // Returns: Array(2, 3, 4)// Encoders are also created for case classes.case class Person(name: String, age: Long)val ds = Seq(Person("Andy", 32)).toDS()// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name.val path = "examples/src/main/resources/people.json"val people = sqlContext.read.json(path).as[Person] 在DataFrame中创建表并查询12345678val sqlContext = SQLContext.getOrCreate(sc)import sqlContext.implicits._//把andy和32匹配到name和age列。//先创建一个DataFrame，再注册为tableval ds = sc.parallelize(Seq(("Andy", 32))).toDF("name","age") ds.registerTempTable("ds1")sqlContext.sql("select * from ds1") sparkSQL链接GP1、maven中增加包一开始试过8.2的包就不行12345&lt;dependency&gt; &lt;groupId&gt;postgresql&lt;/groupId&gt; &lt;artifactId&gt;postgresql&lt;/artifactId&gt; &lt;version&gt;8.3-606.jdbc3&lt;/version&gt;&lt;/dependency&gt; 2、连接jdbc12345val jdbcDF = sqlContext.read.format("jdbc").options( Map("url" -&gt; "jdbc:postgresql://10.1.1.230:5432/xmo_dw", "user"-&gt;"david_xu", "password"-&gt;"w7dtfxHD", "dbtable" -&gt; "(select * from xmo_dw.bshare_blacklist_tagid) as aa")).load().show() 在spark sql命令行中测试连接/usr/lib/spark/bin/spark-sql —jars /home/david/jars/postgresql-8.3-606.jdbc4.jar 12345678910CREATE TEMPORARY TABLE temp_imageviewsUSING org.apache.spark.sql.jdbcOPTIONS ( driver "org.postgresql.Driver", url "jdbc:postgresql://10.1.1.230:5432/xmo_dw", user "david_xu", password "w7dtfxHD", dbtable "(select date_i,opxpid::text from xmo_dw.imageviews where date_i=20160512 limit 20000000) as aa", numPartitions "6"); 1234567891011121314cd /usr/lib/spark/bin//usr/lib/spark/bin/spark-sql --executor-memory 100gadd jar /home/wilson/sparksql/postgresql-8.3-606.jdbc3.jar;set spark.sql.shuffle.partitions=20; CREATE TEMPORARY TABLE temp_imageviewsUSING org.apache.spark.sql.jdbcOPTIONS ( driver "org.postgresql.Driver", url "jdbc:postgresql://10.1.1.230:5432/xmo_dw", user "david_xu", password "w7dtfxHD", dbtable "(select timeslot,record_server,referring_site ,opxsid from xmo_dw.imageviews where date_i=20160515 limit 5000000) as aa", numPartitions "6"); spark sql读取HDFS建表12345678910111213/usr/lib/spark/bin/spark-sql -e "CREATE TEMPORARY TABLE rtbreq_hourUSING org.apache.spark.sql.bytesOPTIONS ( paths '$&#123;rtbreq_path&#125;');create TEMPORARY TABLE rtbreq_tanx_hourUSING org.apache.spark.sql.bytesOPTIONS( paths '$&#123;rtbreq_tanx_path&#125;');select ip,count(1) cnt from (select ip from rtbreq_hour union all select ip from rtbreq_tanx_hour) a group by ip having cnt &gt; 30 order by cnt desc;select bxid,count(1) cnt from (select bxid from rtbreq_hour union all select bxid from rtbreq_tanx_hour) a group by bxid having cnt &gt; 1000 order by cnt desc;" &gt; rtbreq/$day/$hour.txt]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
      <tags>
        <tag>spark-sql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试刷题]]></title>
    <url>%2F2017%2F07%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E9%9D%A2%E8%AF%95%E5%88%B7%E9%A2%98%2F</url>
    <content type="text"><![CDATA[那些深度学习《面试》你可能需要知道的 如何准备机器学习工程师的面试 ？ 七月在线实验室—-BAT机器学习面试题 如何准备机器学习工程师的面试 ？ 读完这21个机器学习面试问题和答案，入职率提升99% 国内互联网公司机器学习数据挖掘类的职位面试主要考察什么方面的东西？ ….等等 如何判断一个而链表中是否有环？给定一棵二叉查找树中的两个元素，求它们的最近公共祖先。给一个栈排序基于比较的排序算法的时间复杂度是什么？证明？如何求一个带权图中两个结点直接按的最短路径？如果有些权值是负的怎么办？求一个字符串中所有的回文子串。对这些问题你都要能够推导你的解法的时间和空间复杂度（大 O 表示法），并且尽量用最低的复杂度解决。只有通过大量的练习才能将这些不同类型的问题烂熟于胸，从而在面试中迅速地给出一个高效的解法。常用的算法面试准备平台有 InterviewBit、LeetCode、Interview Cake、Pramp、http://interviewing.io 等。 概率论和统计典型问题 给出一个群体中男性和女性各自的平均身高，求整个群体的平均身高。一次调查表明意大利三分之一的汽车都是法拉利，并且在那之中一半的车都是红色的。如果你在意大利的街头看到一辆红色的汽车驶来，请问它是法拉利的可能性有多大？你试图找出在自己的网站上放置版头的最佳方案。变量包括版头的尺寸（大、中、小）以及放置的位置（顶部、中间、底部）。假定需要 95% 的置信水平，请问你至少需要多少次访问和点击来确定某个方案比其他的组合都要好？很多机器学习算法都以概率论和统计作为理论基础。对于这些基础知识有清晰的概念是极为重要的。当然同时你也要能够将这些抽象的概念与现实联系起来。数据建模和评估典型问题 一位农民想搞明白是什么因素影响了他的牛奶产量。他记录了每天的气温（30 - 40 度）、湿度（60 - 90%）、饲料消耗（2000 - 2500 千克）以及牛奶产量（500 - 1000 升）。假设问题是要预测每天的牛奶产量，你会如何处理数据并建立模型？这是一个什么类型的机器学习问题？你的公司在开发一个面部表情识别系统。这个系统接受 1920 x 1080 的图片作为输入，并告诉用户图片中的人脸处于以下哪种情绪状态：平常、高兴、悲伤、愤怒和恐惧。当图片中没有人脸时系统要能够分辨这种情况。这是一个什么类型的机器学习问题？如果每个像素点由 3 个值来表示（RGB），那么输入数据的原始维度有多大？有办法降维吗？如何对系统的输出进行编码？为什么？过去几个世纪的气象数据展现出一种循环的气温模式：一会升高一会下降。对于这样的数据（一个年平均气温的序列），你会如何建模并预测未来 5 年的平均气温？你在一家在线新闻网站工作，需要从各处收集文本，并将不同来源的内容聚集成一篇报道。你会如何设计这样一个系统？会用到哪些机器学习技术？应用机器学习算法和库 你用一个给定的数据集训练一个单隐层的神经网络，发现网络的权值在训练中强烈地震荡（有时在负值和正值之间变化）。为了解决这个问题你需要调整哪个参数？支持向量机的训练在本质上是在最优化哪个值？LASSO 回归用 L1-norm 作为惩罚项，而岭回归（Ridge Regression）则使用 L2-norm 作为惩罚项。这两者哪个更有可能得到一个稀疏（某些项的系数为 0）的模型？在用反向传播法训练一个 10 层的神经网络时，你发现前 3 层的权值完全没有变化，而 4 ~ 6 层的权值则变化得非常慢。这是为什么？如何解决？你手上有一个关于小麦产出的数据集，包括年降雨量 R、平均海拔 A 以及小麦产量 O。你经过初步分析认为产量跟年降雨量的平方以及平均海报的对数之间存在关系，即：O = β_0 + β_1 x R^2 + β_2 x log(A)。能用线性回归求出系数 β 吗？你可以通过像 Kaggle 比赛那样的数据科学和机器学习挑战来了解各种各样的问题和它们之间的细微差别。多多参加这些比赛，并尝试应用不同的机器学习模型。软件工程和系统设计典型问题 你有一个电商网站，当用户点击一个商品打开详情页面时，你想基于商品特征和用户的购买历史为用户推荐 5 个其他的商品显示在页面的底部。你需要哪些服务和数据表来实现这个功能？请写一个查询语句或一段过程式代码来返回所要推荐的 5 个商品。对于 YouTube 那样的在线视频网站，你会收集哪些数据来衡量用户的参与度和视频的人气度？一个简单的垃圾邮件检测系统是这样的：它每次处理一封邮件，统计不同单词的出现频率（Term frequency），并将这些频率与之前已经被标注为垃圾 / 正常邮件的那些频率进行比较。现在需要对这系统进行拓展来处理海量的邮件流量，请设计一个 Map-Reduce 方案在一个集群上部署这个系统。你要生成一个实时的热力图，来展示用户正在浏览和点击一个网页的哪些部分。在客户端和服务端分别需要哪些组件 / 服务 / API 来实现这个功能？ 机器学习岗位面试问题汇总 之 集成学习]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利率计算]]></title>
    <url>%2F2017%2F07%2F12%2F%E7%AE%97%E6%B3%95%E8%83%8C%E5%90%8E%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86%2F%E5%88%A9%E7%8E%87%E8%AE%A1%E7%AE%97%2F</url>
    <content type="text"><![CDATA[等额本息 1234每月月供额=〔贷款本金×月利率×(1＋月利率)＾还款月数〕÷〔(1＋月利率)＾还款月数-1〕每月应还利息=贷款本金×月利率×〔(1+月利率)^还款月数-(1+月利率)^(还款月序号-1)〕÷〔(1+月利率)^还款月数-1〕每月应还本金=贷款本金×月利率×(1+月利率)^(还款月序号-1)÷〔(1+月利率)^还款月数-1〕总利息=还款月数×每月月供额-贷款本金 等额本金 12345每月月供额=(贷款本金÷还款月数)+(贷款本金-已归还本金累计额)×月利率每月应还本金=贷款本金÷还款月数每月应还利息=剩余本金×月利率=(贷款本金-已归还本金累计额)×月利率每月月供递减额=每月应还本金×月利率=贷款本金÷还款月数×月利率总利息=还款月数×(总贷款额×月利率-月利率×(总贷款额÷还款月数)*(还款月数-1)÷2+总贷款额÷还款月数) 信用卡账单分期真实年化利率 年利率利息率=利息量÷本金÷时间×100% IRR内部收益率 (IRR) 的定义是：净现值 (NPV) 为零时的折现率。 综合考虑了每期的流入流出现金的量和时间，加权出来的结果。IRR实质上是一个折现率，用IRR折现时会达到该项目的净现值NPV为0的状态。也可以理解为一个项目的预期收益率。举例来说，如IRR为8%，可以简单解释为以8%的利率借钱投资于此项目，刚好可以不赚不赔。]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>利率</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elastic Search 配置和使用]]></title>
    <url>%2F2017%2F07%2F12%2FElastic%20Search%2FElastic%20Search%E9%85%8D%E7%BD%AE%E5%92%8C%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[官网：https://www.elastic.co/products/elasticsearch 最好的教程：https://es.xiaoleilu.com/ docker的ELK环境：https://hub.docker.com/r/sebp/elk/ ES 5.4中文文档 http://cwiki.apachecn.org/pages/viewpage.action?pageId=4260364 0、基本概念接近实时（NRT） Elasticsearch是一个接近实时的搜索平台。这意味着，从索引一个文档直到这个文档能够被搜索到有一个轻微的延迟（通常是1秒）。 集群（cluster） 一个集群就是由一个或多个节点组织在一起，它们共同持有你整个的数据，并一起提供索引和搜索功能。一个集群一个唯一的名字标识，这个名字默认就是 “elasticsearch”。这个名字是重要的，因为一个节点只能通过指定某个集群的名字，来加入这个集群。在产品环境中显式地设定这个名字是一个好 习惯，但是使用默认值来进行测试/开发也是不错的。 节点（node） 一个节点是你集群中的一个服务器，作为集群的一部分，它存储你的数据，参与集群的索引和搜索功能。和集群类似，一个节点也是由一个名字来标识的，默认情况 下，这个名字是一个随机的漫威漫画角色的名字，这个名字会在启动的时候赋予节点。这个名字对于管理工作来说挺重要的，因为在这个管理过程中，你会去确定网 络中的哪些服务器对应于Elasticsearch集群中的哪些节点。 一个节点可以通过配置集群名称的方式来加入一个指定的集群。默认情况下，每个节点都会被安排加入到一个叫做“elasticsearch”的集群中，这意 味着，如果你在你的网络中启动了若干个节点，并假定它们能够相互发现彼此，它们将会自动地形成并加入到一个叫做“elasticsearch”的集群中。 在一个集群里，只要你想，可以拥有任意多个节点。而且，如果当前你的网络中没有运行任何Elasticsearch节点，这时启动一个节点，会默认创建并加入一个叫做“elasticsearch”的集群。 12Relational DB -&gt; Databases -&gt; Tables -&gt; Rows -&gt; ColumnsElasticsearch -&gt; Indices -&gt; Types -&gt; Documents -&gt; Fields 索引（index） 一个索引就是一个拥有几分相似特征的文档的集合。比如说，你可以有一个客户数据的索引，另一个产品目录的索引，还有一个订单数据的索引。一个索引由一个名 字来标识（必须全部是小写字母的），并且当我们要对对应于这个索引中的文档进行索引、搜索、更新和删除的时候，都要使用到这个名字。 在一个集群中，如果你想，可以定义任意多的索引。 类型（type） 在一个索引中，你可以定义一种或多种类型。一个类型是你的索引的一个逻辑上的分类/分区，其语义完全由你来定。通常，会为具有一组共同字段的文档定义一个 类型。比如说，我们假设你运营一个博客平台并且将你所有的数据存储到一个索引中。在这个索引中，你可以为用户数据定义一个类型，为博客数据定义另一个类 型，当然，也可以为评论数据定义另一个类型。 文档（document） 一个文档是一个可被索引的基础信息单元。比如，你可以拥有某一个客户的文档，某一个产品的一个文档，当然，也可以拥有某个订单的一个文档。文档以 JSON（Javascript Object Notation）格式来表示，而JSON是一个到处存在的互联网数据交互格式。 在一个index/type里面，只要你想，你可以存储任意多的文档。注意，尽管一个文档，物理上存在于一个索引之中，文档必须被索引/赋予一个索引的type。 分片和复制（shards &amp; replicas） 一个索引可以存储超出单个结点硬件限制的大量数据。比如，一个具有10亿文档的索引占据1TB的磁盘空间，而任一节点都没有这样大的磁盘空间；或者单个节点处理搜索请求，响应太慢。 为了解决这个问题，Elasticsearch提供了将索引划分成多份的能力，这些份就叫做分片。当你创建一个索引的时候，你可以指定你想要的分片的数量。每个分片本身也是一个功能完善并且独立的“索引”，这个“索引”可以被放置到集群中的任何节点上。 分片之所以重要，主要有两方面的原因： - 允许你水平分割/扩展你的内容容量 - 允许你在分片（潜在地，位于多个节点上）之上进行分布式的、并行的操作，进而提高性能/吞吐量 至于一个分片怎样分布，它的文档怎样聚合回搜索请求，是完全由Elasticsearch管理的，对于作为用户的你来说，这些都是透明的。 在一个网络/云的环境里，失败随时都可能发生，在某个分片/节点不知怎么的就处于离线状态，或者由于任何原因消失了，这种情况下，有一个故障转移机制是非 常有用并且是强烈推荐的。为此目的，Elasticsearch允许你创建分片的一份或多份拷贝，这些拷贝叫做复制分片，或者直接叫复制。 复制之所以重要，有两个主要原因： - 在分片/节点失败的情况下，提供了高可用性。因为这个原因，注意到复制分片从不与原/主要（original/primary）分片置于同一节点上是非常重要的。 - 扩展你的搜索量/吞吐量，因为搜索可以在所有的复制上并行运行 总之，每个索引可以被分成多个分片。一个索引也可以被复制0次（意思是没有复制）或多次。一旦复制了，每个索引就有了主分片（作为复制源的原来的分片）和 复制分片（主分片的拷贝）之别。分片和复制的数量可以在索引创建的时候指定。在索引创建之后，你可以在任何时候动态地改变复制的数量，但是你事后不能改变 分片的数量。 默认情况下，Elasticsearch中的每个索引被分片5个主分片和1个复制，这意味着，如果你的集群中至少有两个节点，你的索引将会有5个主分片和另外5个复制分片（1个完全拷贝），这样的话每个索引总共就有10个分片。 1、安装-5.5.01.1 ElasticSearch下载的 elasticsearch-5.5.0.tar.gz kibana-5.5.0-linux-x86_64.tar.gz 解压到/home/david/opt，在主目录直接运行 1$ bin/elasticsearch 启动服务，启动后，访问localhost:9200，若出现 12345678910111213&#123; &quot;name&quot; : &quot;Jr1It8C&quot;, &quot;cluster_name&quot; : &quot;elasticsearch&quot;, &quot;cluster_uuid&quot; : &quot;JMo_h3-USdegKS1yZ0WCnA&quot;, &quot;version&quot; : &#123; &quot;number&quot; : &quot;5.5.0&quot;, &quot;build_hash&quot; : &quot;260387d&quot;, &quot;build_date&quot; : &quot;2017-06-30T23:16:05.735Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;6.6.0&quot; &#125;, &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125; 则说明安装成功。 1.2 Marvel5.0后集成到了x-pack中 1）安装X-pack到elasticsearch 1$ bin/elasticsearch-plugin install x-pack 2）安装到kibana 1bin/kibana-plugin install x-pack 用户名elastic 密码changeme 1.3 关闭服务关闭Elastic search 1ps -ef | grep elastic 关闭kibana 1fuser -n tcp 5601 2、第一个例子摘自教程 假设我们刚好在Megacorp工作，这时人力资源部门出于某种目的需要让我们创建一个员工目录，这个目录用于促进人文关怀和用于实时协同工作，所以它有以下不同的需求： 数据能够包含多个值的标签、数字和纯文本。 检索任何员工的所有信息。 支持结构化搜索，例如查找30岁以上的员工。 支持简单的全文搜索和更复杂的短语(phrase)搜索 高亮搜索结果中的关键字 能够利用图表管理分析这些数据 2.1 索引员工文档索引含义的区分 你可能已经注意到索引(index)这个词在Elasticsearch中有着不同的含义，所以有必要在此做一下区分: 索引（名词） 如上文所述，一个索引(index)就像是传统关系数据库中的数据库，它是相关文档存储的地方，index的复数是indices 或indexes。 索引（动词） 「索引一个文档」表示把一个文档存储到索引（名词）里，以便它可以被检索或者查询。这很像SQL中的INSERT关键字，差别是，如果文档已经存在，新的文档将覆盖旧的文档。 倒排索引 传统数据库为特定列增加一个索引，例如B-Tree索引来加速检索。Elasticsearch和Lucene使用一种叫做倒排索引(inverted index)的数据结构来达到相同目的。 创建一个员工目录 每个文档的类型为employee。 employee类型归属于索引megacorp。 megacorp索引存储在Elasticsearch集群中。 1234567891011121314151617181920212223242526PUT /megacorp/employee/1&#123; &quot;first_name&quot; : &quot;John&quot;, &quot;last_name&quot; : &quot;Smith&quot;, &quot;age&quot; : 25, &quot;about&quot; : &quot;I love to go rock climbing&quot;, &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ]&#125;PUT /megacorp/employee/2&#123; &quot;first_name&quot; : &quot;Jane&quot;, &quot;last_name&quot; : &quot;Smith&quot;, &quot;age&quot; : 32, &quot;about&quot; : &quot;I like to collect rock albums&quot;, &quot;interests&quot;: [ &quot;music&quot; ]&#125;PUT /megacorp/employee/3&#123; &quot;first_name&quot; : &quot;Douglas&quot;, &quot;last_name&quot; : &quot;Fir&quot;, &quot;age&quot; : 35, &quot;about&quot;: &quot;I like to build cabinets&quot;, &quot;interests&quot;: [ &quot;forestry&quot; ]&#125; 2.2 检索文档1GET /megacorp/employee/1 响应的内容中包含一些文档的元信息，John Smith的原始JSON文档包含在_source字段中。 1234567891011121314151617&#123; &quot;_index&quot;: &quot;megacorp&quot;, &quot;_type&quot;: &quot;employee&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;found&quot;: true, &quot;_source&quot;: &#123; &quot;first_name&quot;: &quot;John&quot;, &quot;last_name&quot;: &quot;Smith&quot;, &quot;age&quot;: 25, &quot;about&quot;: &quot;I love to go rock climbing&quot;, &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ] &#125;&#125; 简单搜索 1GET /megacorp/employee/_search 使用关键字_search来取代原来的文档ID。响应内容的hits数组中包含了我们所有的三个文档。默认情况下搜索会返回前10个结果。 12345678910111213141516171819202122232425262728293031&#123; "took": 6, "timed_out": false, "_shards": &#123; "total": 5, "successful": 5, "failed": 0 &#125;, "hits": &#123; "total": 3, "max_score": 1, "hits": [ &#123; "_index": "megacorp", "_type": "employee", "_id": "2", "_score": 1, "_source": &#123; "first_name": "Jane", "last_name": "Smith", "age": 32, "about": "I like to collect rock albums", "interests": [ "music" ] &#125; &#125;, ...... ] &#125;&#125; 接下来，让我们搜索姓氏中包含“Smith”的员工。要做到这一点，我们将在命令行中使用轻量级的搜索方法。这种方法常被称作查询字符串(query string)搜索，因为我们像传递URL参数一样去传递查询语句： 1GET /megacorp/employee/_search?q=last_name:Smith 2.3 使用DSL语句查询查询字符串搜索便于通过命令行完成特定(ad hoc)的搜索，但是它也有局限性（参阅简单搜索章节）。Elasticsearch提供丰富且灵活的查询语言叫做DSL查询(Query DSL),它允许你构建更加复杂、强大的查询。 DSL(Domain Specific Language特定领域语言)以JSON请求体的形式出现。我们可以这样表示之前关于“Smith”的查询: 12345678GET /megacorp/employee/_search&#123; &quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;last_name&quot; : &quot;Smith&quot; &#125; &#125;&#125; 2.4 更复杂的搜索我们让搜索稍微再变的复杂一些。我们依旧想要找到姓氏为“Smith”的员工，但是我们只想得到年龄大于30岁的员工。我们的语句将添加过滤器(filter),它使得我们高效率的执行一个结构化搜索： 1234567891011121314151617GET /megacorp/employee/_search&#123; &quot;query&quot; : &#123; &quot;filtered&quot; : &#123; &quot;filter&quot; : &#123; &quot;range&quot; : &#123; &quot;age&quot; : &#123; &quot;gt&quot; : 30 &#125; &lt;1&gt; &#125; &#125;, &quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;last_name&quot; : &quot;smith&quot; &lt;2&gt; &#125; &#125; &#125; &#125;&#125; 5.0以后的DSL语法变了 3、数据3.1 文档一个文档不只有数据。它还包含了元数据(metadata)——关于文档的信息。三个必须的元数据节点是： 节点 说明 _index 文档存储的地方 _type 文档代表的对象的类 _id 文档的唯一标识 _index索引(index)类似于关系型数据库里的“数据库”——它是我们存储和索引关联数据的地方。 提示： 事实上，我们的数据被存储和索引在分片(shards)中，索引只是一个把一个或多个分片分组在一起的逻辑空间。然而，这只是一些内部细节——我们的程序完全不用关心分片。对于我们的程序而言，文档存储在索引(index)中。剩下的细节由Elasticsearch关心既可。 我们将会在《索引管理》章节中探讨如何创建并管理索引，但现在，我们将让Elasticsearch为我们创建索引。我们唯一需要做的仅仅是选择一个索引名。这个名字必须是全部小写，不能以下划线开头，不能包含逗号。让我们使用website做为索引名。 _type在应用中，我们使用对象表示一些“事物”，例如一个用户、一篇博客、一个评论，或者一封邮件。每个对象都属于一个类(class)，这个类定义了属性或与对象关联的数据。user类的对象可能包含姓名、性别、年龄和Email地址。 在关系型数据库中，我们经常将相同类的对象存储在一个表里，因为它们有着相同的结构。同理，在Elasticsearch中，我们使用相同类型(type)的文档表示相同的“事物”，因为他们的数据结构也是相同的。 每个类型(type)都有自己的映射(mapping)或者结构定义，就像传统数据库表中的列一样。所有类型下的文档被存储在同一个索引下，但是类型的映射(mapping)会告诉Elasticsearch不同的文档如何被索引。 我们将会在《映射》章节探讨如何定义和管理映射，但是现在我们将依赖Elasticsearch去自动处理数据结构。 _type的名字可以是大写或小写，不能包含下划线或逗号。我们将使用blog做为类型名。 _idid仅仅是一个字符串，它与_index和_type组合时，就可以在Elasticsearch中唯一标识一个文档。当创建一个文档，你可以自定义_id，也可以让Elasticsearch帮你自动生成。 3.2 索引一个文档自定义ID123456PUT /website/blog/123&#123; &quot;title&quot;: &quot;My first blog entry&quot;, &quot;text&quot;: &quot;Just trying this out...&quot;, &quot;date&quot;: &quot;2014/01/01&quot;&#125; Elasticsearch的响应： 12345678910111213&#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;blog&quot;, &quot;_id&quot;: &quot;123&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;created&quot;: true&#125; _version ：Elasticsearch中每个文档都有版本号，每当文档变化（包括删除）都会使_version增加。 自增ID123456POST /website/blog/&#123; &quot;title&quot;: &quot;My second blog entry&quot;, &quot;text&quot;: &quot;Still trying this out...&quot;, &quot;date&quot;: &quot;2014/01/01&quot;&#125; 自动生成的ID有22个字符长，URL-safe, Base64-encoded string universally unique identifiers, 或者叫 UUIDs。 3.3 检索想要从Elasticsearch中获取文档，我们使用同样的_index、_type、_id，但是HTTP方法改为GET： 1GET /website/blog/123?pretty pretty在任意的查询字符串中增加pretty参数，类似于上面的例子。会让Elasticsearch美化输出(pretty-print)JSON响应以便更加容易阅读。_source字段不会被美化，它的样子与我们输入的一致。 {&quot;found&quot;: true}。这意味着文档已经找到。 如果我们请求一个不存在的文档，依旧会得到一个JSON，不过found值变成了false。 此外，HTTP响应状态码也会变成&#39;404 Not Found&#39;代替&#39;200 OK&#39;。我们可以在curl后加-i参数得到响应头： 1curl -i -XGET http://localhost:9200/website/blog/124?pretty 检索文档的一部分通常，GET请求将返回文档的全部，存储在_source参数中。但是可能你感兴趣的字段只是title。请求个别字段可以使用_source参数。多个字段可以使用逗号分隔： 1GET /website/blog/123?_source=title,text 或者你只想得到_source字段而不要其他的元数据，你可以这样请求： 1GET /website/blog/123/_source 3.4 更新整个文档文档在Elasticsearch中是不可变的——我们不能修改他们。如果需要更新已存在的文档，我们可以使用《索引文档》章节提到的index API 重建索引(reindex) 或者替换掉它。 123456PUT /website/blog/123&#123; &quot;title&quot;: &quot;My first blog entry&quot;, &quot;text&quot;: &quot;I am starting to get the hang of this...&quot;, &quot;date&quot;: &quot;2014/01/02&quot;&#125; 在响应中，我们可以看到Elasticsearch把_version增加了，且result是updated update API。这个API 似乎 允许你修改文档的局部，但事实上Elasticsearch遵循与之前所说完全相同的过程，这个过程如下： 从旧文档中检索JSON 修改它 删除旧文档 索引新文档 唯一的不同是update API完成这一过程只需要一个客户端请求既可，不再需要get和index请求了。 3.5 创建新文档请记住_index、_type、_id三者唯一确定一个文档。所以要想保证文档是新加入的，最简单的方式是使用POST方法让Elasticsearch自动生成唯一_id： 12POST /website/blog/&#123; ... &#125; 如果要确保是create操作 1）使用op_type查询参数： 12PUT /website/blog/123?op_type=create&#123; ... &#125; 2）在URL后加/_create做为端点： 12PUT /website/blog/123/_create&#123; ... &#125; 如果包含相同的_index、_type和_id的文档已经存在，Elasticsearch将返回409 Conflict响应状态码 3.6 删除文档使用DELETE方法： 1DELETE /website/blog/123 删除一个文档也不会立即从磁盘上移除，它只是被标记成已删除。Elasticsearch将会在你之后添加更多索引的时候才会在后台进行删除内容的清理。 3.7 MappingElasticSearch的Mapping之字段类型 （一）核心数据类型： （1）string： 默认会被分词，一个完整示例如下 12345678910111213141516171819&quot;status&quot;: &#123; &quot;type&quot;: &quot;string&quot;, //字符串类型 &quot;index&quot;: &quot;analyzed&quot;//分词，不分词是：not_analyzed ，设置成no，字段将不会被索引 &quot;analyzer&quot;:&quot;ik&quot;//指定分词器 &quot;boost&quot;:1.23//字段级别的分数加权 &quot;doc_values&quot;:false//对not_analyzed字段，默认都是开启，分词字段不能使用，对排序和聚合能提升较大性能，节约内存 &quot;fielddata&quot;:&#123;&quot;format&quot;:&quot;disabled&quot;&#125;//针对分词字段，参与排序或聚合时能提高性能，不分词字段统一建议使用doc_value &quot;fields&quot;:&#123;&quot;raw&quot;:&#123;&quot;type&quot;:&quot;string&quot;,&quot;index&quot;:&quot;not_analyzed&quot;&#125;&#125; //可以对一个字段提供多种索引模式，同一个字段的值，一个分词，一个不分词 &quot;ignore_above&quot;:100 //超过100个字符的文本，将会被忽略，不被索引 &quot;include_in_all&quot;:ture//设置是否此字段包含在_all字段中，默认是true，除非index设置成no选项 &quot;index_options&quot;:&quot;docs&quot;//4个可选参数docs（索引文档号） ,freqs（文档号+词频），positions（文档号+词频+位置，通常用来距离查询），offsets（文档号+词频+位置+偏移量，通常被使用在高亮字段）分词字段默认是position，其他的默认是docs &quot;norms&quot;:&#123;&quot;enable&quot;:true,&quot;loading&quot;:&quot;lazy&quot;&#125;//分词字段默认配置，不分词字段：默认&#123;&quot;enable&quot;:false&#125;，存储长度因子和索引时boost，建议对需要参与评分字段使用 ，会额外增加内存消耗量 &quot;null_value&quot;:&quot;NULL&quot;//设置一些缺失字段的初始化值，只有string可以使用，分词字段的null值也会被分词 &quot;position_increament_gap&quot;:0//影响距离查询或近似查询，可以设置在多值字段的数据上火分词字段上，查询时可指定slop间隔，默认值是100 &quot;store&quot;:false//是否单独设置此字段的是否存储而从_source字段中分离，默认是false，只能搜索，不能获取值 &quot;search_analyzer&quot;:&quot;ik&quot;//设置搜索时的分词器，默认跟ananlyzer是一致的，比如index时用standard+ngram，搜索时用standard用来完成自动提示功能 &quot;similarity&quot;:&quot;BM25&quot;//默认是TF/IDF算法，指定一个字段评分策略，仅仅对字符串型和分词类型有效 &quot;term_vector&quot;:&quot;no&quot;//默认不存储向量信息，支持参数yes（term存储），with_positions（term+位置）,with_offsets（term+偏移量），with_positions_offsets(term+位置+偏移量) 对快速高亮fast vector highlighter能提升性能，但开启又会加大索引体积，不适合大数据量用 &#125; 4、结构化查询DSLmatch 相当于and should 相当于or must_not 相当于not Query精确查询 Matchhttps://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl.html match查询 12345678GET /_search&#123; &quot;query&quot;: &#123; &quot;match&quot; : &#123; &quot;message&quot; : &quot;this is a test&quot; &#125; &#125;&#125; Filter判断某个字段不为空 123&quot;filter&quot;: [ &#123; &quot;script&quot;:&#123;&quot;script&quot;:&quot;doc[&apos;interests&apos;].values.length==60&quot;&#125; &#125; ] should5、聚合统计对查询的结果聚合12345678910111213141516171819GET iclick_persona/iclick/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;term&quot;: &#123; &quot;articles.domains&quot;: &quot;www.baby-kingdom.com&quot; &#125;&#125;, &#123; &quot;match&quot;: &#123; &quot;province&quot;: &quot;HK&quot; &#125;&#125;, &#123;&quot;match&quot;: &#123;&quot;interests&quot;: &quot;20&quot;&#125;&#125; ] &#125; &#125;, &quot;aggs&quot;: &#123; &quot;interests&quot;: &#123; &quot;terms&quot;: &#123;&quot;field&quot;: &quot;interests&quot;, &quot;size&quot;: 50 &#125; &#125; &#125;&#125; 统计月活跃度 123456789101112131415161718192021222324GET iclick_persona/iclick/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;term&quot;: &#123; &quot;articles.domains&quot;: &quot;www.baby-kingdom.com&quot; &#125;&#125;, &#123; &quot;term&quot;: &#123; &quot;province&quot;: &quot;HK&quot; &#125;&#125; ], &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;create_time&quot;: &#123;&quot;gte&quot; : &quot;2017-08-10&quot;&#125; &#125; &#125; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;create_time&quot;: &#123; &quot;terms&quot;: &#123;&quot;field&quot;: &quot;create_time&quot;,&quot;size&quot;: 50,&quot;order&quot;: &#123; &quot;_term&quot;: &quot;asc&quot; &#125; &#125; &#125; &#125;&#125; 看香港的人群每天有多少 123456789101112131415161718192021GET iclick_persona/iclick/_search?size=0&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;province&quot;: &quot;HK&quot; &#125;&#125; ] &#125; &#125;, &quot;aggs&quot;: &#123; &quot;dates&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;create_time&quot;, &quot;size&quot;: 100, &quot;order&quot;: &#123; &quot;_term&quot;: &quot;asc&quot; &#125; &#125; &#125; &#125;&#125; DELETE按条件删除 1234567891011121314151617181920212223POST iclick_persona/iclick/_delete_by_query&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;term&quot;: &#123; &quot;create_time&quot;: &#123; &quot;value&quot;: &quot;2017-07-15&quot; &#125; &#125; &#125;, &#123; &quot;term&quot;: &#123; &quot;province&quot;: &#123; &quot;value&quot;: &quot;HK&quot; &#125; &#125; &#125; ] &#125; &#125;&#125; keyword和text区别[ElasticSearch]数据类型keyword和text的区别 在 ES2.x 版本字符串数据是没有 keyword 和 text 类型的，只有string类型，ES更新到5版本后，取消了 string 数据类型，代替它的是 keyword 和 text 数据类型。 Text 数据类型被用来索引长文本，比如说电子邮件的主体部分或者一款产品的介绍。这些文本会被分析，在建立索引前会将这些文本进行分词，转化为词的组合，建立索引。允许 ES来检索这些词语。text 数据类型不能用来排序和聚合。 Keyword不需要进行分词。可以被用来检索过滤、排序和聚合。keyword 类型字段只能用本身来进行检索。 默认是text类型。 match最简单的一个match例子： 查询和”我的宝马多少马力”这个查询语句匹配的文档。 123456789&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;content&quot; : &#123; &quot;query&quot; : &quot;我的宝马多少马力&quot; &#125; &#125; &#125;&#125; 上面的查询匹配就会进行分词，比如”宝马多少马力”会被分词为”宝马 多少 马力”, 所有有关”宝马 多少 马力”, 那么所有包含这三个词中的一个或多个的文档就会被搜索出来。并且根据lucene的评分机制(TF/IDF)来进行评分。 match_phrase比如上面一个例子，一个文档”我的保时捷马力不错”也会被搜索出来，那么想要精确匹配所有同时包含”宝马 多少 马力”的文档怎么做？就要使用 match_phrase 了 123456789&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;content&quot; : &#123; &quot;query&quot; : &quot;我的宝马多少马力&quot; &#125; &#125; &#125;&#125; 完全匹配可能比较严，我们会希望有个可调节因子，少匹配一个也满足，那就需要使用到slop。 12345678910&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;content&quot; : &#123; &quot;query&quot; : &quot;我的宝马多少马力&quot;, &quot;slop&quot; : 1 &#125; &#125; &#125;&#125; multi_match如果我们希望两个字段进行匹配，其中一个字段有这个文档就满足的话，使用multi_match 12345678&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot; : &quot;我的宝马多少马力&quot;, &quot;fields&quot; : [&quot;title&quot;, &quot;content&quot;] &#125; &#125;&#125; 但是multi_match就涉及到匹配评分的问题了。 best_fields我们希望完全匹配的文档占的评分比较高，则需要使用best_fields 12345678910111213&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;我的宝马发动机多少&quot;, &quot;type&quot;: &quot;best_fields&quot;, &quot;fields&quot;: [ &quot;tag&quot;, &quot;content&quot; ], &quot;tie_breaker&quot;: 0.3 &#125; &#125;&#125; 意思就是完全匹配”宝马 发动机”的文档评分会比较靠前，如果只匹配宝马的文档评分乘以0.3的系数 most_fields我们希望越多字段匹配的文档评分越高，就要使用most_fields 123456789101112&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;我的宝马发动机多少&quot;, &quot;type&quot;: &quot;most_fields&quot;, &quot;fields&quot;: [ &quot;tag&quot;, &quot;content&quot; ] &#125; &#125;&#125; cross_fields我们会希望这个词条的分词词汇是分配到不同字段中的，那么就使用cross_fields 123456789101112&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;我的宝马发动机多少&quot;, &quot;type&quot;: &quot;cross_fields&quot;, &quot;fields&quot;: [ &quot;tag&quot;, &quot;content&quot; ] &#125; &#125;&#125; termterm是代表完全匹配，即不进行分词器分析，文档中必须包含整个搜索的词汇 1234567&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;content&quot;: &quot;汽车保养&quot; &#125; &#125;&#125; 查出的所有文档都包含”汽车保养”这个词组的词汇。 使用term要确定的是这个字段是否“被分析”(analyzed)，默认的字符串是被分析的。 拿官网上的例子举例： mapping是这样的： 12345678910111213141516171819202122PUT my_index&#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;full_text&quot;: &#123; &quot;type&quot;: &quot;string&quot; &#125;, &quot;exact_value&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;index&quot;: &quot;not_analyzed&quot; &#125; &#125; &#125; &#125;&#125;PUT my_index/my_type/1&#123; &quot;full_text&quot;: &quot;Quick Foxes!&quot;, &quot;exact_value&quot;: &quot;Quick Foxes!&quot; &#125; 其中的full_text是被分析过的，所以full_text的索引中存的就是[quick, foxes]，而extra_value中存的是[Quick Foxes!]。 那下面的几个请求： 12345678GET my_index/my_type/_search&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;exact_value&quot;: &quot;Quick Foxes!&quot; &#125; &#125;&#125; 请求的出数据，因为完全匹配 12345678GET my_index/my_type/_search&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;full_text&quot;: &quot;Quick Foxes!&quot; &#125; &#125;&#125; 请求不出数据的，因为full_text分词后的结果中没有[Quick Foxes!]这个分词。 对查询的结果排序1234567891011121314151617181920GET iclick_persona/iclick/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123;&quot;term&quot;: &#123; &quot;articles.domains&quot;: &#123; &quot;value&quot;: &quot;play.google.com&quot; &#125; &#125;&#125; ] &#125; &#125;, &quot;sort&quot;: [ &#123; &quot;create_time&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125; ]&#125; bool联合查询: must,should,must_not 如果我们想要请求”content中带宝马，但是tag中不带宝马”这样类似的需求，就需要用到bool联合查询。联合查询就会使用到must,should,must_not三种关键词。 这三个可以这么理解 must: 文档必须完全匹配条件 should: should下面会带一个以上的条件，至少满足一个条件，这个文档就符合should must_not: 文档必须不匹配条件 比如上面那个需求： 12345678910111213141516&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: &#123; &quot;term&quot;: &#123; &quot;content&quot;: &quot;宝马&quot; &#125; &#125;, &quot;must_not&quot;: &#123; &quot;term&quot;: &#123; &quot;tags&quot;: &quot;宝马&quot; &#125; &#125; &#125; &#125;&#125; 多条件查询比如要实现 a &amp;&amp; (b=0 || b=1)这样的需求，则通过嵌套bool来实现，例如 12345678910111213141516171819202122232425262728293031323334353637383940GET news_v1/_search&#123; &quot;size&quot;: 20, &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;美白&quot;, &quot;fields&quot;: [ &quot;meta.description&quot;, &quot;title&quot; ] &#125; &#125;, &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;term&quot;: &#123; &quot;lang&quot;: &#123; &quot;value&quot;: &quot;zh-hk&quot; &#125; &#125; &#125;, &#123; &quot;term&quot;: &#123; &quot;lang&quot;: &#123; &quot;value&quot;: &quot;en&quot; &#125; &#125; &#125; ] &#125; &#125; ] &#125; &#125;, &quot;ext&quot;: &#123;&#125;&#125; 自定义排序ES自带的排序默认只是可以对数值字段，日期字段或者是字符串字段进行排序，那么，如果我们就是要人为的让包含字段A的排在包含字段B的前面，当前的方式无法满足。 于是需要寻求另一种方式来解决，将给定的A和B转换成数值1和2 从而就能够达到要求的排序。而且是在得分相同的情况才会进行的排序方式！通过脚本实现。 12345678910111213141516171819202122232425262728GET _search &#123; &quot;_source&quot;: &#123; &quot;include&quot;: [&quot;title.Value&quot;,&quot;dataType&quot;,&quot;_score&quot;] &#125;, &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;query_string&quot;: &#123; &quot;default_field&quot;: &quot;title.Value&quot;, &quot;query&quot;: &quot;盆地^10 Unconformity&quot; &#125; &#125; ] &#125; &#125;, &quot;sort&quot; : &#123; &quot;_score&quot;:&#123; &quot;order&quot; : &quot;dese&quot; &#125;, &quot;_script&quot; : &#123; &quot;script&quot; : &quot;&apos;区带资源量数据&apos; in doc[&apos;dataType&apos;].values?2 :(&apos;其它相关资料5&apos; in doc[&apos;dataType&apos;].values? 1 :3)&quot;, &quot;type&quot; : &quot;string&quot;, &quot;order&quot; : &quot;asc&quot; &#125; &#125; &#125; 按条件删除1POST iclick_persona/iclick/_delete_by_query ES的java api连接到ES创建一个客户端连接 12345678import org.elasticsearch.client.transport.TransportClient;import org.elasticsearch.common.settings.Settings;import org.elasticsearch.common.transport.InetSocketTransportAddress;import org.elasticsearch.transport.client.PreBuiltTransportClient;TransportClient client = new PreBuiltTransportClient(Settings.EMPTY) .addTransportAddress(new InetSocketTransportAddress(InetAddress .getByName(&quot;10.1.1.111&quot;), 9300)); 创建索引并写入数据12import org.datanucleus.store.rdbms.request.BulkRequest;import org.elasticsearch.action.index.IndexResponse; 若是单个插入索引 123// 创建es索引IndexResponse response = client.prepareIndex(&quot;movie&quot;, &quot;bt&quot;).setSource(JSON.toJSONString(obj)).get(); 若是批量插入索引 12345678910111213141516171819202122232425262728293031String line = null; JSONObject obj = null; TransportClient client = new PreBuiltTransportClient(Settings.EMPTY) .addTransportAddress(new InetSocketTransportAddress(InetAddress .getByName(&quot;10.1.1.111&quot;), 9300)); //批量插入索引 BulkRequestBuilder brq = client.prepareBulk(); File file = new File(&quot;f:\\data_utf8.json&quot;); int cnt = 0; if (file.exists() &amp;&amp; file.isFile()) &#123; InputStreamReader isr = new InputStreamReader(new FileInputStream( file)); BufferedReader br = new BufferedReader(isr); while ((line = br.readLine()) != null) &#123; obj = JSON.parseObject(line); brq.add(client.prepareIndex(&quot;btmovie&quot;, &quot;bt&quot;).setSource(JSON.toJSONString(obj))); cnt ++; if (cnt%1000 == 0) System.out.println(cnt); &#125; &#125; brq.execute().actionGet(); System.out.println(&quot;done&quot;); disable_coordtrue：多个关键词命中，打分会累加 if coord factor is enabled (by default “disable_coord”: false) then it means: if we have more search keywords in text then this result would be more relevant and will get higher score. if coord factor is disabled(“disable_coord”: true) then it means: no matter how many keywords we have in search text it will be counted just once. minimum_should_match在multi_match中，minimum_should_match 相关度控制原理http://blog.csdn.net/xyh930929/article/details/72378690?utm_source=itdadao&amp;utm_medium=referral analyzerenglish_custom 1234567891011121314151617181920212223242526272829303132333435363738&quot;analysis&quot;: &#123; &quot;filter&quot;: &#123; &quot;english_stemmer&quot;: &#123; &quot;type&quot;: &quot;stemmer&quot;, &quot;language&quot;: &quot;english&quot; &#125;, &quot;english_stop&quot;: &#123; &quot;type&quot;: &quot;stop&quot;, &quot;stopwords&quot;: &quot;_english_&quot; &#125;, &quot;english_possessive_stemmer&quot;: &#123; &quot;type&quot;: &quot;stemmer&quot;, &quot;language&quot;: &quot;possessive_english&quot; &#125; &#125;, &quot;analyzer&quot;: &#123; &quot;cjk_custom&quot;: &#123; &quot;filter&quot;: [ &quot;cjk_width&quot;, &quot;lowercase&quot;, &quot;cjk_bigram&quot;, &quot;english_stop&quot;, &quot;asciifolding&quot; ], &quot;tokenizer&quot;: &quot;standard&quot; &#125;, &quot;english_custom&quot;: &#123; &quot;filter&quot;: [ &quot;english_possessive_stemmer&quot;, &quot;lowercase&quot;, &quot;english_stop&quot;, &quot;english_stemmer&quot;, &quot;asciifolding&quot; ], &quot;tokenizer&quot;: &quot;standard&quot; &#125; &#125; &#125;]]></content>
      <categories>
        <category>Elastic Search</category>
      </categories>
      <tags>
        <tag>Elastic Search</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模型集成]]></title>
    <url>%2F2017%2F07%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%A8%A1%E5%9E%8B%E9%9B%86%E6%88%90%2F</url>
    <content type="text"><![CDATA[每个Kaggle冠军的获胜法门：揭秘Python中的模型集成 .png) Example Schematics of an ensemble. An input array X is fed through two proprocessing pipelines and then to a set of base learners f(i). The ensemble combines all base learner predictions into a final prediction array P. By the end of the post, you will: understand the fundamentals of ensembles know how to code them understand the main pitfalls and drawbacks of ensembles Predicting Republican and Democratic donationswe’ll use a data set on U.S. political contributions. The original data set was prepared by Ben Wieder at FiveThirtyEight, who dug around the U.S. government’s political contribution registry and found that when scientists donate to politician, it’s usually to Democrats. 12345678910111213141516171819202122232425262728293031323334import numpy as npimport pandas as pdimport matplotlib.pyplot as plt%matplotlib inline### Import data# Always good to set a seed for reproducibilitySEED = 222np.random.seed(SEED)df = pd.read_csv('input.csv')### Training and test setfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import roc_auc_scoredef get_train_test(test_size=0.95): """Split Data into train and test sets.""" y = 1 * (df.cand_pty_affiliation == "REP") X = df.drop(["cand_pty_affiliation"], axis=1) X = pd.get_dummies(X, sparse=True) X.drop(X.columns[X.std() == 0], axis=1, inplace=True) return train_test_split(X, y, test_size=test_size, random_state=SEED)xtrain, xtest, ytrain, ytest = get_train_test()# A look at the dataprint("\nExample data:")df.head()df.cand_pty_affiliation.value_counts(normalize=True).plot( kind="bar", title="Share of No. donations")plt.show() This claim is based on the observation on the share of donations being made to Republicans and Democrats. However, there’s plenty more that can be said: for instance, which scientific discipline is most likely to make a Republican donation, and which state is most likely to make Democratic donations? We will go one step further and predict whether a donation is most likely to be a to a Republican or Democrat. What is an ensemble?Combining predictions from several models averages out idiosyncratic errors and yield better overall predictions. How to combine predictions? Machine learning is remarkably similar in classification problems: taking the most common class label prediction is equivalent to a majority voting rule. But there are many other ways to combine predictions, and more generally we can use a model to learn how to best combine predictions. Understanding ensembles by combining decision treesThe deeper the tree, the more complex the patterns it can capture, but the more prone to overfitting it will be. Because of this, we will need an alternative way of building complex models of decision trees, and an ensemble of different decision trees is one such way. We’ll use the below helper function to visualize our decision rules: 1234567891011121314151617181920import pydotplus # you can install pydotplus with: pip install pydotplus from IPython.display import Imagefrom sklearn.metrics import roc_auc_scorefrom sklearn.tree import DecisionTreeClassifier, export_graphvizdef print_graph(clf, feature_names): """Print decision tree.""" graph = export_graphviz( clf, label="root", proportion=True, impurity=False, out_file=None, feature_names=feature_names, class_names=&#123;0: "D", 1: "R"&#125;, filled=True, rounded=True ) graph = pydotplus.graph_from_dot_data(graph) return Image(graph.create_png())]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器之心</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark笔记]]></title>
    <url>%2F2017%2F07%2F12%2Fhadoop-spark%2Fspark%2Fspark%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[算圆周率pi1234567891011121314151617import org.apache.spark.SparkConf;import org.apache.spark.SparkContext;case class PerTypeson[T,S](var name:T,var age:S) &#123;&#125;object SparkTest&#123; def main(args: Array[String]) &#123; val conf = new SparkConf().setAppName("Spark Pi").setMaster("local") //关键 val sc = new SparkContext(conf) val NUM_SAMPLES=100000 val count=sc.parallelize(1 to NUM_SAMPLES).map&#123;i=&gt; val x=Math.random() val y=Math.random() if(x*x+y*y&lt;1)1 else 0 &#125;.reduce(_+_) println("pi is rougly"+4.0*count/NUM_SAMPLES) &#125;&#125; 读取外部文件和链接数据库（用spark 1.6的版本） 12345678910111213141516171819202122232425262728import org.apache.spark.SparkConf;import org.apache.spark.SparkContextimport org.apache.spark.ml.classification.LogisticRegressionimport org.apache.spark.sql.SQLContextimport scala.collection.mutable.Mapcase class Person(var name:String)object SparkTest&#123; def main(args: Array[String]) &#123; // val conf = new SparkConf().setAppName("Spark Pi").setMaster("spark://hadoop:7070") //关键 val conf = new SparkConf().setAppName("Spark Pi").setMaster("local") //关键 val sc = new SparkContext(conf)// val textfile=sc.textFile("C:\\Users\\Administrator\\Desktop\\分词.txt")// textfile.collect().foreach(println) val sqlContext=new SQLContext(sc) val df=sqlContext.read.json("F:\\people.json") df.cache() println(df.select("age").show()) df.registerTempTable("df1") println(sqlContext.sql("select * from df1 where age=19")) val map=Map("url" -&gt; "jdbc:mysql://localhost:3306/test", "user"-&gt;"root","password"-&gt;"") map+=("dbtable" -&gt;"class") "dbtable" -&gt; "SELECT * FROM iteblog" val jdbc=sqlContext.read.format("jdbc").options(map).load() println(jdbc.show(1))// val lr = new LogisticRegression().setMaxIter(10) &#125;&#125; 创建DataFrame并简单操作DataFramespark2.0就可以直接用RDD.toDF spark1.6需要sqlContext.createDataFrame(sc.parallelize(data)).toDF(“id”, “features”, “clicked”) 123456789101112131415161718192021222324252627282930313233343536import org.apache.spark.SparkConf;import org.apache.spark.SparkContextimport org.apache.spark.ml.classification.LogisticRegressionimport org.apache.spark.sql.SQLContextimport scala.collection.mutable.Mapcase class Person(var name:String)case class Employee(age: Int, name: String)object SparkTest&#123; def main(args: Array[String]) &#123; // val conf = new SparkConf().setAppName("Spark Pi").setMaster("spark://hadoop:7070") //关键 val conf = new SparkConf().setAppName("Spark Pi").setMaster("local") //关键 val sc = new SparkContext(conf) val sqlContext=new SQLContext(sc) //第一种方式就创建DataFrame，读取外部文件 println("第一种方式就创建DataFrame，读取外部文件") val textfile=sc.textFile("C:\\Users\\Administrator\\Desktop\\分词.txt") val df_person=textfile.map(x=&gt;Person(x)) val df_test=sqlContext.createDataFrame(df_person).withColumnRenamed("name","anmoyi") println(df_test.filter(df_test("anmoyi").contains("使用")).count()) //第二种方式创建DataFrame println("第二种方式创建DataFrame，通过List和case类的方式创建") val listOfEmployee=List(Employee(1,"zhou"),Employee(1,"zhou"),Employee(2,"mei"),Employee(3,"xu")) val emFrame=sqlContext.createDataFrame(listOfEmployee) println(emFrame.show()) emFrame.registerTempTable("employeeTable") val sortedByNameEmployees = sqlContext.sql("select * from employeeTable order by name desc") println(sortedByNameEmployees.show()) println(emFrame.groupBy("age").count().show()) println(emFrame.select(emFrame("name"),emFrame("age"),(emFrame("age")+1).as("age1")).show()) println(sortedByNameEmployees.show()) //第三种方式通过TupleN来创建DataFrame println("第三种方式通过TupleN，元祖的方式来创建DataFrame") val mobiles=sqlContext.createDataFrame(Seq((1,"Android"), (2, "iPhone"))).toDF("age","mobile") println(mobiles.show()) &#125;&#125; DataFrame转列的数据类型https://blog.csdn.net/dkl12/article/details/80256585 转所有列 1234import org.apache.spark.sql.functions._val cols = colNames.map(f =&gt; col(f).cast(DoubleType))df.select(cols: _*).show() 转指定列 123val name = &quot;col1,col3,col5&quot;df.select(name.split(&quot;,&quot;).map(name =&gt; col(name)): _*).show()df.select(name.split(&quot;,&quot;).map(name =&gt; col(name).cast(DoubleType)): _*).show() Spark中统计相关的东西spark shell中增加依赖包 bin/spark-shell --packages com.databricks:spark-csv_2.10:1.0.3 12345678910111213141516171819import org.apache.spark.SparkConfimport org.apache.spark.SparkContextimport org.apache.spark.sql.SQLContextimport org.apache.spark.sql.functions._ //包含了常见的统计函数和数学函数import org.apache.log4j.&#123;Level, Logger&#125;//import com.databricks.spark.csv._object Test &#123; def main(args: Array[String]):Unit=&#123;// 屏蔽不必要的日志显示在终端上Logger.getLogger("org.apache.spark").setLevel(Level.WARN)Logger.getLogger("org.eclipse.jetty.server").setLevel(Level.OFF) val conf = new SparkConf().setAppName("stastic").setMaster("local") //关键 val sc = new SparkContext(conf) val sqlContext=new SQLContext(sc) import sqlContext.implicits._ //用于隐式转化，可以由RDD直接转换为DataFrame val df = sc.parallelize(0 until 10).toDF("id").withColumn("rand1", rand(10)) .withColumn("rand2", rand(seed=27)).withColumn("rand3",rand(20)) println(df.columns) println(df.describe().show()) Spark中的多对多JOIN如果存在多对多的情况下，则是以乘法得到最后结果，并不是以某列多的情况 12345678910111213141516171819202122val df2=sc.parallelize(0 until 6).toDF("id").withColumn("age",rand(10))println(df.join(df2,df("id")===df2("id"),"left").show()) //左链接println(df.join(df2,df("id")===df2("id"),"right").show()) //右链接println(df.join(df2,df("id")===df2("id"),"outer").show()) //全链接println(df.join(df2,df("id")===df2("id"),"inner").show()) //inner 链接df.join(df2, $"df1Key" === $"df2Key")df.join(df2).where($"df1Key" === $"df2Key")df.join(df2, Seq("user_id", "user_name")) println("统计函数开始") println(df.groupBy($"id").agg(Map( "rand1" -&gt; "avg", "rand2" -&gt; "max", "rand3" -&gt; "min" )).show()) println(df.drop("rand1").show()) println(df.stat.corr("rand1","rand2")) println(df.stat.cov("rand1", "rand2")) val df1=sqlContext.createDataFrame(Seq((1, 1), (1, 2), (2, 1), (2, 1), (2, 3), (3, 2), (3, 3))).toDF("key", "value") println(df1.stat.crosstab("key","value").show()) &#125;&#125; pyspark12345678from pyspark import SparkContextsc = SparkContext("local", "Simple App")rdd = sc.parallelize([1,2,3])rdd.collect()#[1, 2, 3]rdd1 = rdd.map(lambda x : x+1)rdd1.collect()#[2, 3, 4] spark作业提交以WordCount为例说明RDD从转换到作业提交的过程 1sc.textFile("/User/david/key.txt").flatMap(line=&gt;line.split(" ")).map(word=&gt;(word,1)).reduceByKey(_+_) 步骤1：val rawFile = sc.textFile(&quot;/User/david/key.txt&quot;) textFile先生成HadoopRDD，然后再通过map操作生成MappedRDD。在spark-shell中可以看到 123scala&gt; val rawFile = sc.textFile(&quot;/User/david/key.txt&quot;)rawFile: org.apache.spark.rdd.RDD[String] = /User/david/key.txt MapPartitionsRDD[3] at textFile at &lt;console&gt;:271.6.3版本变成了MapPartitionsRDD 步骤2： 1val splittedText = rawFile.flatMap(line=&gt;line.split(&quot; &quot;)) flatMap将原来的MappedRDD转换为FlatMappedRDD。 步骤3 1val wordCount = splittedText.map(word=&gt;(word,1)) 步骤4：reduceByKey 作业执行spark执行中相关概念 Spark中Task，Partition，RDD、节点数、Executor数、core数目的关系和Application，Driver，Job，Task，Stage理解 若干个block合并成一个输入分片InputSplit，一个InputSplit对应一个Task，一个Task生成一个Partition。 随后这些具体的Task每个都会被分配到集群上的某个节点的某个Executor去执行。 每个节点可以启一个或多个Executor。 每个Executor由若干core组成，每个Executor的每个core一次只能执行一个Task。 每个Task执行的结果就是生成了目标RDD的一个partiton。每个partition再下一步又由一个task来执行。 注意: 这里的core是虚拟的core而不是机器的物理CPU核，可以理解为就是Executor的一个工作线程。 而 Task被执行的并发度 = Executor数目 * 每个Executor核数。 所以，如果一共要执行8个task，但只有一个Executor，2个core，则并发度是2。那么需要分成4个批次，每次并发执行两个Task。 至于partition的数目： 对于数据读入阶段，例如sc.textFile，输入文件被划分为多少InputSplit就会需要多少初始Task。 在Map阶段partition数目保持不变。 在Reduce阶段，RDD的聚合会触发shuffle操作，聚合后的RDD的partition数目跟具体操作有关，例如repartition操作会聚合成指定分区数，还有一些算子是可配置的。 在任务提交中主要涉及Driver和Executor两个节点。 Driver可以理解为我们自己编写的程序。主要解决 RDD依赖性分析，以生成DAG 根据RDD DAG将Job分割为多个stage Stage确认后，生成相应的task，分发到Executor执行。 Executor：在每个WorkerNode上为某应用启动的一个进程，是一个执行task的容器。一个Executor执行多个Task。 另外 Job：包含很多task的并行计算，可以认为是Spark RDD 里面的action,每个action的计算会生成一个job。 用户提交的Job会提交给DAGScheduler，Job会被分解成Stage和Task。 Spark中的Job和MR中Job不一样。MR中Job主要是Map或者Reduce Job。而Spark的Job其实很好区别，一个action算子就算一个Job，比方说count，first等。 Stage： 一个Job会被拆分为多组Task，每组任务被称为一个Stage就像Map Stage， Reduce Stage。 Stage的划分在RDD的论文中有详细的介绍，简单的说是以shuffle和result这两种类型来划分。在Spark中有两类task，一类是shuffleMapTask，一类是resultTask，第一类task的输出是shuffle所需数据，第二类task的输出是result，stage的划分也以此为依据，shuffle之前的所有变换是一个stage，shuffle之后的操作是另一个stage。比如 rdd.parallize(1 to 10).foreach(println) 这个操作没有shuffle，直接就输出了，那么只有它的task是resultTask，stage也只有一个；如果是rdd.map(x =&gt; (x, 1)).reduceByKey( + ).foreach(println), 这个job因为有reduce，所以有一个shuffle过程，那么reduceByKey之前的是一个stage，执行shuffleMapTask，输出shuffle所需的数据，reduceByKey到最后是一个stage，直接就输出结果了。如果job中有多次shuffle，那么每个shuffle之前都是一个stage。 Task 即 stage 下的一个任务执行单元，一般来说，一个 rdd 有多少个 partition，就会有多少个 task，因为每一个 task 只是处理一个 partition 上的数据. 依赖性分析和stage划分RDD之间的依赖分为窄依赖和宽依赖。 窄依赖是指父RDD所有输出都会被执行的子RDD消费，也就是输出路径固定。例如如下的Transformation： map、flatMap、filter、sample 宽依赖是指父RDD输出会由不同子RDD消费，输出路径不固定。例如： sortByKey、reduceByKey、groupByKey、cogroupByKey、join、cartensian 调度器（Scheduler）会计算RDD之间的依赖关系，将窄依赖的RDD归并到同一个stage，而宽依赖则作为划分不同Stage的判断标准。宽依赖和窄依赖的边界就是stage的划分点 任务的创建和分发由Executor执行的Task分为ShuffleMapTask和ResultTask两种，相当于Map和Reduce。 RDD API合集Spark JAVA RDD API 最全合集整理 Spark API 详解/大白话解释 之 map、mapPartitions、mapValues、mapWith、flatMap、flatMapWith、flatMapValues Spark API 详解/大白话解释 之 RDD、partition、count、collect flatMap和mapSpark之中map与flatMap的区别 map的作用就是对rdd之中的元素进行逐一进行函数操作映射为另外一个rdd。 flatMap的操作是将函数应用于rdd之中的每一个元素，将返回的迭代器的所有内容构成新的rdd。通常用来切分单词。 传递给flatMap的函数返回的类型是一个可迭代的类型（例如list）。 map会返回多个数组对象，flatmap返回一个 map函数会对每一条输入进行指定的操作，然后为每一条输入返回一个对象；而flatMap函数则是两个操作的集合——正是“先映射后扁平化”： 操作1：同map函数一样：对每一条输入进行指定的操作，然后为每一条输入返回一个对象 操作2：最后将所有对象合并为一个对象 reduce和reduceByKey转自https://blog.csdn.net/guotong1988/article/details/50555671 reduce reduce将RDD中元素前两个传给输入函数，产生一个新的return值，新产生的return值与RDD中下一个元素（第三个元素）组成两个元素，再被传给输入函数，直到最后只有一个值为止。 12val c = sc.parallelize(1 to 10)c.reduce((x, y) =&gt; x + y)//结果55 具体过程，RDD有1 2 3 4 5 6 7 8 9 10个元素，1+2=33+3=66+4=1010+5=1515+6=2121+7=2828+8=3636+9=4545+10=55 reduceByKey reduceByKey就是对元素为KV对的RDD中Key相同的元素的Value进行binary_function的reduce操作，因此，Key相同的多个元素的值被reduce为一个值，然后与原RDD中的Key组成一个新的KV对。 12val a = sc.parallelize(List((1,2),(1,3),(3,4),(3,6)))a.reduceByKey((x,y) =&gt; x + y).collect 结果 Array((1,5), (3,10)) 设置打印日志级别如果是log4j日志 1234import org.apache.log4j.&#123; Level, Logger &#125;Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.WARN)Logger.getLogger(&quot;org.eclipse.jetty.server&quot;).setLevel(Level.OFF) 如果是console日志 12val sc = new SparkContext(conf)sc.setLogLevel(&quot;WARN&quot;) ml和mllibhttps://www.cnblogs.com/itboys/p/6860953.html ml主要操作的是DataFrame, 而mllib操作的是RDD，也就是说二者面向的数据集不一样。相比于mllib在RDD提供的基础操作，ml在DataFrame上的抽象级别更高，数据和操作耦合度更低。 ml中的操作可以使用pipeline, 跟sklearn一样，可以把很多操作(算法/特征提取/特征转换)以管道的形式串起来，然后让数据在这个管道中流动。 ml中无论是什么模型，都提供了统一的算法操作接口，比如模型训练都是fit；不像mllib中不同模型会有各种各样的trainXXX。 mllib在spark2.0之后进入维护状态, 这个状态通常只修复BUG不增加新功能。 cache的作用https://blog.csdn.net/Allenalex/article/details/79431047 如果要缓存的RDD太大的话，即使调用cache()，Spark也可能会丢掉和重新计算RDD的部分。所以在大的程序中，最后是使用RDD.filter(x=&gt;x&gt;0).persist(StorageLevel.MEMORY_AND_DISK)。 性能调优Spark性能调优之合理设置并行度 Spark性能优化：开发调优篇 spark内核揭秘-14-Spark性能优化的10大问题及其解决方案 Spark 重分区函数：coalesce和repartition区别与实现，可以优化Spark程序性能 Hive小文件合并调研 数据倾斜方案-全面 shuffle解析Spark源码系列（六）Shuffle的过程解析 1java.math.BigDecimal cannot be cast to java.lang.String 但是并没有bigDecimal类型的数据 如何避免spark dataframe的JOIN操作之后产生重复列https://blog.csdn.net/sparkexpert/article/details/52837269 1df.join(df2, Seq(&quot;key1&quot;, &quot;key2&quot;), &quot;left_outer&quot;).show() DataFrame Join123val baseinfoContactDF = baseinfoDF.join(gpsDF, Seq(&quot;app_no&quot;), &quot;left_outer&quot;).na.fill(0.0)personDataFrame.join(orderDataFrame, personDataFrame(&quot;id_person&quot;) === orderDataFrame(&quot;id_person&quot;), &quot;inner&quot;).show() spark dataframe新增一列的四种方法https://blog.csdn.net/li3xiao3jie2/article/details/81317249 spark序列化问题https://blog.csdn.net/HFUTLXM/article/details/78621406 （一）理解spark闭包 什么叫闭包： 跨作用域访问函数变量。又指的一个拥有许多变量和绑定了这些变量的环境的表达式（通常是一个函数），因而这些变量也是该表达式的一部分。 Spark闭包的问题引出：在spark中实现统计List(1,2,3)的和。如果使用下面的代码，程序打印的结果不是6，而是0。这个和我们编写单机程序的认识有很大不同。 12345678910111213object Test &#123; def main(args:Array[String]):Unit = &#123; val conf = new SparkConf().setAppName(&quot;test&quot;); val sc = new SparkContext(conf) val rdd = sc.parallelize(List(1,2,3)) var counter = 0 //warn: don&apos;t do this rdd.foreach(x =&gt; counter += x) println(&quot;Counter value: &quot;+counter) sc.stop() &#125;&#125; 我也遇到类似情况，在RDD中实例化一个类并赋值，最后出来的结果会有问题 问题分析：counter是在foreach函数外部定义的，也就是在driver程序中定义，而foreach函数是属于rdd对象的，rdd函数的执行位置是各个worker节点（或者说worker进程），main函数是在driver节点上（或者说driver进程上）执行的，所以当counter变量在driver中定义，被在rdd中使用的时候，出现了变量的“跨域”问题，也就是闭包问题。 spark输出的part文件数量1new SparkConf().setAppName(&quot;InstallAndPickup&quot;).set(&quot;spark.sql.shuffle.partitions&quot;, &quot;5&quot;) 通过这个参数控制]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记-最大熵]]></title>
    <url>%2F2017%2F07%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%9C%80%E5%A4%A7%E7%86%B5%2F</url>
    <content type="text"><![CDATA[1、最大熵原理日常生活中，很多事情的发生表现出一定的随机性，试验的结果往往是不确定的，也不知道这个随机现象所服从的概率分布。最大熵的实质就是，在已知部分知识的前提下，关于未知分布最合理的推断就是符合已知知识最不确定或者最随机的推断。任何其他的选择都意味着我们增加了其他的约束和假设。 将最大熵应用到分类，就是最大熵模型。给定一个训练集： T = \{ (x_1,y_1), (x_2,y_2),..., (x_N,y_N)\}其中$x_i \in X$是输入，$y_i \in Y$是输出，X和Y表示输入和输出空间。N为样本数。目标是，利用最大熵原理选出一个最好的分类模型，即对于任意给定的输入$x \in X$，可以以概率$p(y|x)$输出$y \in Y$ 。 按照最大熵原理，应该优先保证模型满足已知的所有约束。思路是，从训练数据T中抽取若干有用的特征，要求这些特征在T上关于经验分布$\tilde{p}(x,y)$的数学期望与它们在模型中关于$p(x,y)$的数学期望相等。这样，一个特征就是一个约束了。 这里就涉及到，特征如何刻画？经验分布如何表示？ 2、特征函数假设通过特征选择，抽取若干特征。特征通常由特征函数来表示。例如 f(x,y) =\left\{\begin{matrix} \begin{aligned} & 1，若x,y满足某个事实 \\ & 0，否则 \end{aligned} \end{matrix}\right.这里的特征不是指输入的某个特征，而是指输入和输出共同的特征。 例如，假设我们需要判断“打”是动词还是量词，已知的训练数据有 (x1,y1)=(一打火柴，量词); (x2,y2)=(三打啤酒，量词); (x3,y3)=(打电话，动词); (x4,y4)=(打篮球，动词); 通过观察，发现“打”前面是数字时，是量词，“打”后面是名词时，是动词。这就是从训练数据中提取的两个特征，可分别用特征函数表示为 3、经验分布经验（概率）分布就是通过对训练集T进行统计得到的分布，用$\tilde p$表示。这里列举两个经验分布 \tilde p(x,y) = \frac {count(x,y)} {N} , \tilde p(x)=\frac {count(x)} {N}其中，count表示出现的次数。 4、约束条件对于任意一个特征函数f，$E{\tilde p}f$ 表示f在训练数据T上关于$\tilde p(x,y)$的数学期望， $E{p}f$ 表示f在训练数据T上关于$p(x,y)$的数学期望。按照期望的定义，我们有 E_{\tilde p}f=\sum_{x,y}\tilde p(x,y)f(x,y) E_{ p}f=\sum_{x,y} p(x,y)f(x,y)其中，p(x,y)是未知的，而建模的目标是生成$p(y|x)$，因此，根据Bayes定理，$p(x,y)=p(x)p(y|x)$。在样本数量足够的条件下，$p(x)$可以用$\tilde p(x)$近似表示。这样 E_{ p}f=\sum_{x,y} \tilde p(x)p(y|x)f(x,y)对于概率分布$p(y|x)$，我们希望特征f的期望值应该和从训练集中得到的特征期望值是一致的，因此，增加约束 E_{ p}f=E_{\tilde p}f假设我们从训练集中抽取了n个特征，相应的，便有n个特征函数$f_i(i=1,2,…,n)$以及n个约束条件 C_i:E_{ p}(f_i)=E_{\tilde p}(f_i) \tag {3-1} 关于约束条件的几何解释 （a）：P是所有可能的概率空间，此时没有约束条件，所有的概率模型$p(y|x)$都是允许的； （b）：增加了一个线性约束条件$C_1$，此时，目标分布$p(y|x)$只能落在由$C_1$定义的线段上； （c）：在（b）的基础上增加了另一个约束条件$C_2$ ，且$C_1 \cap C_2 \neq \varnothing$。此时，目标分布只能落在交点上，即被唯一确定； （d）：在（b）基础上增加了另一个约束$C_3$，且$C_1 \cap C_2 = \varnothing$，此时不存在能够同时满足$C_1$和$C_3$的$p(y|x)$。 利用（3-1）定义的约束条件，我们定义P的一个子空间 C=\{p \in P | E_p(f_i)={\tau}_i, i=1,2,...,n\}5、最大熵模型由于我们的目标是获得一个条件分布，因此这里也采用相应的条件熵 H(p(y|x))=-\sum_{x,y} \tilde p(x)p(y|x)\log p(y|x)可以看出这里也是用$\tilde p(x)$来近似$p(x)$。以下将$H(p(y|x))$简记为$H(p)$。至此，可以给出最大熵模型的完整描述。 对于给定的训练集T，特征函数$f_i(x,y), i=1,2,…n$，最大熵模型就是求解 \underset {p \in C} {max} \ \ H(p) = \begin{pmatrix} -\sum_{x,y} \tilde p(x)p(y|x)\log p(y|x) \end{pmatrix}, \\ s.t. \sum_y p(y|x)=1 \tag {5-1} \\ s.t. \ C=\{p \in P | E_p(f_i)={\tau}_i, i=1,2,...,n\}其中的s.t.是为了保证$p(y|x)$是一个（合法的）条件概率分布。 等价于一个求极小值问题 \underset {p \in C} {min} \ \ -H(p) = \begin{pmatrix} \sum_{x,y} \tilde p(x)p(y|x)\log p(y|x) \end{pmatrix}, \\ s.t. \sum_y p(y|x)=1 \tag {5-2} \\ s.t. \ C=\{p \in P | E_p(f_i)={\tau}_i, i=1,2,...,n\}6、模型求解对于5-1的求解，主要思路和步骤如下： 利用Lagrange乘子将最大熵模型由一个带约束的最优化问题转为无约束的最优化问题，这是一个极小极大问题（min max）。 利用对偶问题等价性，转化为求解上一步得到的极大/极小问题的对偶问题，也是一个极大极小问题。 6.1 原始问题和对偶问题根据（5-2），引入拉格朗日乘子$\lambda=(\lambda_0,\lambda_1,…,\lambda_n)^T$，定义拉格朗日函数 L(p,\lambda) = -H(p) + \lambda_0(1-\sum_y p(y|x))+\sum_{i=1}^n\lambda_i(\tau_i-E_p(f_i)) \tag{6-1}利用对偶性，求解（6-1）的原始问题表示为： \underset {p \in C} {min}\ \underset {\lambda} {max}\ L(p,\lambda) \tag{6-2}对偶问题为： \underset {\lambda} {max}\ \underset {p \in C} {min}\ L(p,\lambda) \tag{6-3}由于$H(p)$是关于p的凸函数，因此要求解最大熵模型，只需求解对偶问题（6-3）即可。 6.1.1 指数形式的解首先求解内部的极小问题。由于$\underset {p \in C} {min}\ L(p,\lambda)$是关于$\lambda$的函数，将其记做： \Psi (\lambda) =\underset {p \in C} {min}\ L(p,\lambda) = L(p_{\lambda}, \lambda) \tag {6-4}其中 p_{\lambda}=\underset {p \in C} {argmin}\ L(p,\lambda)=p_{\lambda}(y|x) \tag {6-5}根据拉格朗日乘子法，求$L(p,\lambda)$对$p(y|x)$的偏导，得（求解过程略）： p_{\lambda}=\frac {1} {Z_{\lambda}(x)} \ \exp(\sum_{i=1}^n \lambda_i f_i(x,y)) \tag{6-6}其中， Z_{\lambda}(x)=\sum_y \exp(\sum_{i=1}^n \lambda_i f_i(x,y)) \tag{6-7}称为规范化因子（normalizing factor）。注意，此时已经没有$\lambda_0$了。 由（6-6）定义的$p_{\lambda}$就是最大熵模型的解，它具有指数形式。其中，$\lambda_i$就是特征$f_i$的权重，越大表示特征越重要。 6.1.2 最大似然估计得到对偶问题的内层极小值问题的解之后，接着求解外层的极大值问题$\underset {\lambda} {max} \ \Psi(\lambda)$。 设其解为 \lambda^* = \underset {\lambda} {argmax} \ \Psi(\lambda) \tag{6-8}则最大熵模型的解为 p^*=p_{\lambda^*} \tag{6-9}根据推导，最大化$\Psi(\lambda)$与最大似然估计是等价的！ 7、最优化方法通用的方法有梯度下降，拟牛顿法等，最大熵模型有两个量身定做的方法：通用迭代尺度法（Generalized Iterative Scaling，GIS）和改进的迭代尺度法（Impoved Iterative Scaling，IIS）。 7.1 GIS算法 算法1： S1：初始化参数，令$\lambda=0$ S2：计算$E_{\tilde p}(f_i),\ i=1,2,…,n$ S3：执行一次迭代，对参数做一次刷新。 ​ 计算$E{p{\lambda}}(f_i)$ ​ FOR i=1,2,…,n DO { ​ $\lambdai\ += \ \eta \log\frac {E{\tilde p}(fi)} {E{p_{\lambda}}(f_i)}$ ​ } S4：检查是否收敛，若未收敛则继续S3 其中，$\eta$是学习率，在实际中取$\frac {1} {C}$，$$，表示训练数据中包含特征最多的那个样本所包含的特征个数。 \Delta\lambda_i=\eta \log\frac {E_{\tilde p}(f_i)} {E_{p_{\lambda}}(f_i)}是校正量。 每次迭代，先用当前的权重估算每个特征$fi$在训练数据中的概率分布的期望，然后逐个与相应的经验分布的期望比较，其偏差程度通过$\log\frac {E{\tilde p}(fi)} {E{p_{\lambda}}(f_i)}$来进行刻画。 收敛条件就是当两次迭代的$\lambda$在一个较小的范围。 GIS每次迭代时间很长，不太稳定，容易溢出，一般不会使用。 7.2 IIS算法与GIS的不同主要在$\Delta\lambda_i$的计算上。IIS通过求解方程 \sum_{x,y} \tilde p(x)p(y|x)f_i(x,y)\exp(\Delta\lambda_i\sum_{i=1}^nf_i(x,y))=\tilde p(f_i)1）若$\sum{i=1}^nf_i(x,y)$为常数，即对任意样本(x,y)，都有$\sum{i=1}^nf_i(x,y)=C$，则 \Delta\lambda_i=\frac {1} {C} \log\frac {E_{\tilde p}(f_i)} {E_{p_{\lambda}}(f_i)}此时，IIS可以看做是GIS的一种推广。 2）若$\sum_{i=1}^nf_i(x,y)$不是常数，则需要通过数值方式来求解$\Delta\lambda_i$，如牛顿法。 8、优缺点优点是：在建模时，只需要集中精力选取特征，不需要花费精力考虑如何使用这些特征，可以灵活使用不同类型的特征。 缺点是计算量大。 参考 【1】 最大熵学习笔记 【2】统计学习方法]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>最大熵</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark笔记-新词发现]]></title>
    <url>%2F2017%2F07%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FNLP%2F%E6%96%B0%E8%AF%8D%E5%8F%91%E7%8E%B0spark%2F</url>
    <content type="text"><![CDATA[12val conf = new SparkConf().setAppName("wordSegname").setMaster("local[4]").set("spark.sql.shuffle.partitions","10").set("spark.network.timeout","30s") local[4]是指在本地运行，用4核CPU。 spark.sql.shuffle.partitions是指partition的数量。SparkSQL在运行时，将一个查询任务分解成多个task，一个task就是一个partition。默认是200个partition，而如果实际集群只能并行3个task，则跑完200个partition要200/3=67次。 spark.network.timeout是指所有网络通信的超时时间，默认是120s 123456789val word1=sc.textFile(path).map&#123;x=&gt; val x_filter=x.replaceAll("\p&#123;Punct&#125;", " ").replaceAll("\pP", " ") .replaceAll(" ", " ").replaceAll("[" + AtomsUitl.stopwords + "]", " ").replaceAll("\p&#123;Blank&#125;", " ").replaceAll("\p&#123;Space&#125;", " ").replaceAll("\p&#123;Cntrl&#125;", " ") x_filter &#125; replaceAll中是正则表达式。上文中，是将所有的特殊字符都用空格代替 AtomsUitl.stopwords停用词是”的很了么呢是嘛个都也比还这于不与才上用就好在和对挺去后没说” sc.textFile读取文件后，生成一个RDD，以行为单位，所以后面的map是对每行的操作 打印出所有的元素，用 word1.foreach { x =&gt; println(x) } 12345678val word_document = word1.zipWithIndex.filter &#123; x =&gt; !StringUtils.isBlank(x._1) &#125;.flatMap &#123; x =&gt; val arr = ArrayBuffer(String, Int) val line = x._1.split(" ") //对于每一行，都用空格分割 for (i &lt;- line) &#123; arr += ((i, x._2.toInt)) //分割后，每一个tuple加到数组中 &#125; arr &#125;.map &#123; x =&gt; (x.1.trim, x.2) &#125;.filter(x =&gt; !StringUtils.isBlank(x._1)) zipWithIndex用带有index的来压缩RDD，索引从0开始 word1.zipWithIndex.foreach { x =&gt; println(x) } (ab ef,0) (cd,1) 上述代码得到的结果是 (ab,0) (ef,0) (cd,1) 12345678val wordleft = word.map(x =&gt; AtomsUitl.reverse(x)).map &#123; x =&gt; "" + x + "" &#125;.flatMap &#123; x =&gt; var arr = ArrayBufferString for (y &lt;- 1 to AtomsUitl.len(x) - 2) &#123; arr += AtomsUitl.substring(x, y, Math.min(maxLen + y, AtomsUitl.len(x))) &#125; arr&#125;.sortBy(x =&gt; x) 将每个句子倒序排列，提取每个子集 今$ 四期星天今 处言语然自 天今$ 星天今$ 期星天今$ 然自$ 理处言语然 自$ 言语然自 语然自$ $ 12345678val wordleft_caculate = wordleft.map &#123; s =&gt; val first = AtomsUitl.substring(s, 0, 1).toString (first, s) &#125;.groupBy(f =&gt; f._1).map &#123; x =&gt; x._2 &#125;wordleft_caculate.foreach&#123;x=&gt; println(x.iterator.next())&#125; groupBy之后得到 (期, CompactBuffer((期,期星天今$))等 这个是Iterable，可迭代的。可以转换为一个迭代器x.iterator.next()。迭代出来就是 (期,期星天今$) 等]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>新词发现</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模型评估与选择]]></title>
    <url>%2F2017%2F07%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9%2F</url>
    <content type="text"><![CDATA[1、模型评估方法 可重复采样：在训练集小，难以划分训练/测试集是有用。此外，能产生多个不同训练集，对集成学习等方法有很大的好处。但是会改变初始数据集分布。 在初始数据量足够时，用留出法或者交叉验证法。 1.1 留出法Hold-out将数据集D分成两个互斥的集合。 训练/测试集尽量保证数据一致性，用分层采样，正负样本同比例。 由于单次估计结果往往不可靠，使用留出法时，一般要采用若干次随机划分，重复进行实验后取平均值作为评估值。 ‘ 1.2 交叉验证法将D分成k个大小相似的互斥子集，每个子集用分层采样得到。 每次用k-1个子集的并集作为训练集，余下的子集作为测试集。这样获得k组训练/测试集。最终返回是k个测试结果的均值。 常用10折交叉验证。 1.3 可重复采样bootstrapping sampling：给定包含m个样本的数据集D，我们进行采样产生数据集$D’$，每次随机从D中挑选一个样本，将其拷贝放入$D’$，再将样本放回D。重复m次，得到包含m个样本的$D’$。 样本在m次采样中始终不被采到的概率是$(1-\frac 1 m)^m$，取极限得到 {\lim_{m \mapsto \infty }}(1-\frac 1 m)^m \mapsto \frac 1 e\approx0.368即通过bootstrapping，D中有36.8%的样本未出现在$D’$中，于是可以将$D’$作为训练集，$D-D’$作为测试集，这样可以有1/3个未出现在训练集的样本用于测试。测试结果称为“包外估计”（out-of-bag estimate）。 1.4 调参 我们在模型评估时往往用来确定算法和参数。当这些确定后，要用所有的D再训练一次，才是最终的模型。 2、性能度量回归最常用的是“均方误差”（mean squared error） E(f;D) = \frac 1 m \sum_{i=1}^m(f(x_i)-y_i )^2更一般的，对于数据分布D和概率密度函数$p(\cdot )$，均方误差可描述为 E(f;D) = \int_{x \in D}(f(x)-y)^2p(x)dx分类的性能度量更复杂 2.1 错误率和精度错误率：分类错误的样本占总样本的比例 精度：分类正确的样本占总样本的比例]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>西瓜书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[优先级队列]]></title>
    <url>%2F2017%2F07%2F12%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E4%BC%98%E5%85%88%E7%BA%A7%E9%98%9F%E5%88%97%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122class PriorityQueue[E](top: Int) &#123; import PriorityQueue.Entry private var nsize = 0 private val heap = Array.ofDim[Entry[E]](top+1) /** * Adds an Object to a PriorityQueue in log(size) time. It returns the * object (if any) that was dropped off the heap because it was full. This * can be the given parameter (in case it is smaller than the full heap's * minimum, and couldn't be added), or another object that was previously * the smallest value in the heap and now has been replaced by a larger one, * or null if the queue wasn't yet full with maxSize elements. */ def add(key: Double, value: E): Option[Entry[E]] = &#123; if (nsize &lt; top) &#123; nsize += 1 heap(nsize) = Entry(key, value) upHeap None &#125; else if (nsize &gt; 0 &amp;&amp; key &gt;= heap(1).key) &#123; val old = heap(1) heap(1) = Entry(key, value) updateTop Some(old) &#125; else Some(Entry(key, value)) &#125; def values(): List[E] = &#123; val entries = Array.ofDim[Entry[E]](nsize) System.arraycopy(heap, 1, entries, 0, nsize) entries sortBy(-_.key) map(_.value) toList &#125; /** * Removes and returns the least element of the PriorityQueue in log(size) * time. */ def pop(): Option[Entry[E]] = &#123; if (nsize &gt; 0) &#123; val result = heap(1) heap(1) = heap(nsize) heap(nsize) = null nsize -= 1 downHeap Some(result) &#125; else None &#125; /** Returns the number of elements currently stored in the PriorityQueue. */ def size(): Int = nsize /** * Should be called when the Object at top changes values. Still log(n) * worst case, but it's at least twice as fast to * * &lt;pre&gt; * pq.top().change(); * pq.updateTop(); * &lt;/pre&gt; * * instead of * * &lt;pre&gt; * o = pq.pop(); * o.change(); * pq.push(o); * &lt;/pre&gt; * * @return the new 'top' element. */ def updateTop(): Entry[E] = &#123; downHeap heap(1) &#125; private def upHeap(): Unit = &#123; var i = nsize val node = heap(i) var j = i &gt;&gt;&gt; 1 while (j &gt; 0 &amp;&amp; node.key &lt; heap(j).key) &#123; heap(i) = heap(j) i = j j = j &gt;&gt;&gt; 1 &#125; heap(i) = node &#125; private def downHeap(): Unit = &#123; var i = 1 val node = heap(i) var j = i &lt;&lt; 1 var k = j + 1 if (k &lt;= nsize &amp;&amp; heap(k).key &lt; heap(j).key) &#123; j = k &#125; while (j &lt;= nsize &amp;&amp; heap(j).key &lt; node.key) &#123; heap(i) = heap(j) i = j j = i &lt;&lt; 1 k = j + 1 if (k &lt;= nsize &amp;&amp; heap(k).key &lt; heap(j).key) &#123; j = k &#125; &#125; heap(i) = node &#125;&#125;object PriorityQueue &#123; case class Entry[E](key: Double, value: E) def main(args: Array[String]) &#123; val pq = new PriorityQueue[(Int, Double)](4) val ds = Array[Double](0.1, 0.2, 0.7, 0.2, 0.2, 0.3, 0.3, 0.5, 0.5, 0.5) for (i &lt;- 0 until ds.length) &#123; val e = pq.add(ds(i), (i, ds(i))) println((i, ds(i)) + " =&gt; " + e) &#125; println(pq.values) &#125;&#125;]]></content>
      <categories>
        <category>算法与数据结构</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[浅谈在线最优化求解算法-以CTR预测模型为例]]></title>
    <url>%2F2017%2F07%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%B5%85%E8%B0%88%E5%9C%A8%E7%BA%BF%E6%9C%80%E4%BC%98%E5%8C%96%E6%B1%82%E8%A7%A3%E7%AE%97%E6%B3%95-%E4%BB%A5CTR%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B%E4%B8%BA%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[[TOC] 1、最优化求解问题通常，我们需要求解的最优化问题有如下三类： （1）无约束优化问题： X=\arg \underset{X}{min}f(X)含义是求解X，令目标函数$f(X)$最小。 对于这类问题，在$f(X)$ 是凸函数的前提下，通常做法就是对$f(X)$ 求导，并令$\frac {\partial} {\partial X} f(X) =0$ ，求解可以得到最优值。 凸函数 如果$f(x)$是定义在N维向量空间上的实变量函数，对于在$f(x)$的定义域C上的任意两个点$x_1$和$x_2$，以及任意[0,1]之间的值t都有： f(tX_1 + (1-t)X_2) \leq tf(X_1)+(1-t)f(X_2)\\ \forall X_1,X_2 \in C,\ \ 0 \leq t \leq 1 则称$f(x)$是凸函数。一个函数是凸函数是其存在最优解的充要条件。 此外，如果$f(x)$满足 f(tX_1 + (1-t)X_2)< tf(X_1)+(1-t)f(X_2)\\ \forall X_1,X_2 \in C,\ \ 0 \leq t \leq 1 则$f(x)$为严格凸函数。如下图所示，左边是严格凸函数，右边是凸函数 （2）有等式约束的最优化问题： X=\arg \underset{X}{min}f(X)\\ s.t. h_k(X)=0;k=1,2,...,n含义是在n个等式约束$h_k(X)$ 的条件下求解X，另目标函数$f(X)$最小。 针对有等式的最优化问题，采用拉格朗日乘数法进行求解，通过拉格朗日系数$A=[a_1,a_2,…,a_n]^T$ 把等式约束和目标函数组合成一个式子 X=\arg \underset{X}{min}[f(X)+ A^TH(X)]相当于转化成无约束最优化求解问题，解决方法是分别对X，A求偏导并令其等于0。 （3）不等式约束的优化问题求解 ： X=\arg \underset{X}{min}f(X)\\ s.t. h_k(X)=0;k=1,2,...,n\\ g_l(X)\leq 0;l=1,2,...,m对于不等式约束，通过KKT条件求解。将所有的约束和目标函数写为一个式子 L(X,A,B)=f(X)+A^TH(X)+B^TG(X)KKT条件是说最优值必须满足以下条件： \frac \partial {\partial X} L(X,A,B)=0\\ H(X)=0\\ B^TG(X)=0KKT条件是求解最优值的必要条件，要使其成为充要条件，还需要f(x)为凸函数。 2、批量最优化求解算法一些定义： $i=1,2,…,N$表示向量维度 $j=1,2,…,M$表示样本个数 $t=1,2,…$表示迭代次数 2.1 批量和随机求解我们面对的最优化问题都是无约束的最优化问题（有约束的也可以转成无约束的），因此通常可以将其描述为 W=\arg \underset{W}{min}\ l(W,Z)\\ Z=\{ (X_j,y_j) | j=1,2,...,M \}\\ y_j=h(W,X_j) \tag {2-1-1}就是在已知训练集的情况下，求使得目标函数最小的权重矩阵。其中，$Z$是训练集，$\mathbf{X}$是特征向量，$X_j$是其中一个样本，$Y$是预测值，$y_j$是其中一个样本对应的预测值。一共有M个样本。$h(W,X_j)$ 是特征向量到预测值的映射函数，$ l(W,Z)$ 最优化求解的目标函数，也称为损失函数，$W$ 为特征权重，也就是在损失函数中需要求解的参数。 损失函数一般包括损失项和正则项 常用的损失函数有： （1）平方损失函数（线性回归） 最小二乘法（Ordinary Least Squares）是常用的一种平方损失函数，最小二乘的基本原理是：最优拟合直线应该是使各点到回归直线的距离和最小的直线，即平方和最小。 线性回归的映射函数为： h(W,X_j)=W^TX_j损失函数可以表示为 l(W,Z)=\sum_{j=1}^M (y_j-W^TX_j)^2（2）Logistics损失函数（逻辑回归） 逻辑回归的映射函数为： h(W,X_j)=\frac 1 {1+e^{-W^TX_j}} logistic函数的优点是： 1、他的输入范围是$-\infty \rightarrow + \infty $ ，输出范围是(0,1)，正好满足概率分布为（0，1）的要求。我们用概率去描述分类器，自然比单纯的某个阈值要方便很多； 2、是一个单调上升的函数，具有良好的连续性，不存在不连续点。 由于该函数服从伯努利分布（0-1分布），通过最大似然估计，对于每一维的权重W，损失函数可以表示为 l(W,Z)=(Y-h_W(\mathbf X))X 推导过程 令 h_W(X) = \frac 1 {1+e^{-W^T\mathbf X}}该函数服从伯努利分布（一次点击要么成功，要么失败，通过训练集可以知道不同特征组合下成功和失败的概率） P(Y=1 | \mathbf X;W) = h_W(\mathbf X)\\ P(Y=0 | \mathbf X;W) = 1-h_W(\mathbf X)则概率分布函数为 P(Y|\mathbf X;W) = (h_W(\mathbf X))^Y*(1-h_W(\mathbf X))^{1-Y}（也就是说，我们有样本，通过样本能知道概率分布，那么我们需要知道得到这个概率分布的最有可能的参数W。即我们通过样本知道一些特征组合下的点击率，现在需要求概率函数中的系数。） 我们假设样本数据相互独立，所以它们的联合分布可以表示为各边际分布的乘积，用似然函数表示为： \begin{aligned} L(W)=P(Y|\mathbf X;W) &= (h_W(\mathbf X))^Y(1-h_W(\mathbf X))^{1-Y}\\ &=\prod_{j=1}^M(h_W(X_j))^{y_j}(1-h_W(X_j))^{1-y_j} \end{aligned} \tag {2-1-2}从而，损失函数的求解，可以转化为求最有可能导致这样概率分布的W，也就是求L(W)的最大值。最简单的方法就是对W求偏导，并令导数为零。 在多数情况下，直接对变量进行求导反而会使得计算式子更加的复杂，此时可以借用对数函数。由于对数函数是单调增函数，因此与（2-1-2）具有相同的最大值，上式变为 \begin{aligned} l(W) &= Log\ L(W)\\ &=\sum_{j=1}^M(y_jln\ h(X_j)+(1-y_j)ln\ (1-h(X_j))) \end{aligned}对其求关于W的偏导 首先求logistic函数的导数，得（最后一个X是对$W^TX$的求导） h_W^{'}(\mathbf X) = h_W(\mathbf X)(1-h_W(\mathbf X)) 推导过程如下 为了求解方便，将l(W)转为（其实1/M没用，完全可以去掉，不懂为何要加上） J(W) = -\frac {1}{M} l(W)则就变成求J(W)的最小值。求偏导的过程如下： 最后得到目标函数（损失函数）为： \frac {\partial }{\partial W}J(W) =-\frac{1}{M} (Y-h_W(\mathbf X))X 对于损失函数的求解，一个典型的方法就是梯度下降法，由于损失函数是凸函数，因此沿着梯度下降的方向找到最小点。 假设样本总数为m，批量梯度下降是： Repeat\ until\ convergence \{ \\ W^{(t+1)} := W^t - \eta^t\triangledown _{W}l(W^{t},Z) \\ \}\\ \tag{1-2}而随机梯度下降（SGD）是： Repeat\ until\ convergence \{ \\ for\ j=1\ to\ M, \{ \\ W^{(t+1)} := W^t - \eta^t\triangledown _{W}l(W^{t},Z_j) \\ \}两者的区别是： 前者每次更新$W$都需要遍历一次整个样本集合；而后者在遍历样本集合的时候，每个样本都能改变$W$ ，有更快的收敛速度 。由于SGD针对观测到的随机一条数据进行权重的更新，很适合进行增量计算，实现梯度下降的online模式。 2.2 正则化正则化的主要目的是防止过拟合。对于损失函数构成的模型，可能会出现有些权重很大，有些权重很小的情况，导致过拟合，使得模型的复杂度提高，泛化能力较差（对未知数据的预测能力）。 而正则化就是对损失函数中权重的限制，限制其模不要太大： W=\arg \underset{W}{min}\ l(W,Z)\\ s.t. \Psi(W)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>最优化问题</tag>
        <tag>CTR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最长公共子串]]></title>
    <url>%2F2017%2F07%2F12%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E4%B8%B2%2F</url>
    <content type="text"><![CDATA[找出最长子串，需要知道子串的起始位置和子串的长度。 因此，维护一个二维数组，存放两个字符串的字符关系，再创建两个变量存放index和maxLen。 123456789101112131415161718192021222324252627282930313233public static String findLcsNew(String str1, String str2) &#123; int len1, len2; len1 = str1.length(); len2 = str2.length(); int maxLen = 0, index = 0; int[][] arr = new int[len1 + 1][len2 + 1]; for (int i = 0; i &lt; len1; i++) &#123; for (int j = 0; j &lt; len2; j++) &#123; if ( i == 0 || j == 0 ) &#123; // 第一行和第一列都是0 arr[i][j] = 0; &#125; else &#123; // 该字符和上一个字符均相等时 if (str1.charAt(i) == str2.charAt(j) &amp;&amp; str1.charAt(i-1) == str2.charAt(j-1)) &#123; arr[i][j] = arr[i-1][j-1] + 1; &#125; else &#123; arr[i][j] = 0; &#125; &#125; if (arr[i][j] &gt; maxLen) &#123; maxLen = arr[i][j]; index = i; &#125; &#125; &#125; String newStr = str1.substring(index - maxLen, index + 1); return newStr; &#125; 也可以用一维数组实现，同时可以记录多个相同长度的最长子串 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public static String findLcs(String str1, String str2) &#123; StringBuffer buff = new StringBuffer(); int i, j; int len1, len2; len1 = str1.length(); len2 = str2.length(); int maxLen = len1 &gt; len2 ? len1 : len2; int[] max = new int[maxLen]; int[] maxIndex = new int[maxLen]; int[] c = new int[maxLen]; for (i = 0; i &lt; len2; i++) &#123; for (j = len1 - 1; j &gt;= 0; j--) &#123; if (str2.charAt(i) == str1.charAt(j)) &#123; if ((i == 0) || (j == 0)) c[j] = 1; else c[j] = c[j - 1] + 1; &#125; else &#123; c[j] = 0; &#125; if (c[j] &gt; max[0]) &#123; // 如果是大于那暂时只有一个是最长的,而且要把后面的清0; max[0] = c[j]; maxIndex[0] = j; for (int k = 1; k &lt; maxLen; k++) &#123; max[k] = 0; maxIndex[k] = 0; &#125; &#125; else if (c[j] == max[0]) &#123; // 有多个是相同长度的子串 for (int k = 1; k &lt; maxLen; k++) &#123; if (max[k] == 0) &#123; max[k] = c[j]; maxIndex[k] = j; break; // 在后面加一个就要退出循环了 &#125; &#125; &#125; &#125; &#125; for (j = 0; j &lt; maxLen; j++) &#123; if (max[j] &gt; 0) &#123; for (i = maxIndex[j] - max[j] + 1; i &lt;= maxIndex[j]; i++) buff.append(str1.charAt(i)); &#125; &#125; return buff.toString();&#125;]]></content>
      <categories>
        <category>算法与数据结构</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[查找树]]></title>
    <url>%2F2017%2F07%2F12%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E6%9F%A5%E6%89%BE%E6%A0%91%2F</url>
    <content type="text"><![CDATA[动态查找树主要有：二叉查找树（Binary Search Tree），平衡二叉查找树（Balanced Binary Search Tree），红黑树 (Red-Black Tree )，B-tree/B+-tree/ B*-tree (B~Tree)。前三者是典型的二叉查找树结构，其查找的时间复杂度*O(log2N)*与树的深度相关，那么降低树的深度自然对查找效率是有所提高的；还有一个实际问题：就是大规模数据存储中，实现索引查询这样一个实际背景下，树节点存储的元素数量是有限的（如果元素数量非常多的话，查找就退化成节点内部的线性查找了），这样导致二叉查找树结构由于树的深度过大而造成磁盘I/O读写过于频繁，进而导致查询效率低下（为什么会出现这种情况，待会在外部存储器-磁盘中有所解释），那么如何减少树的深度（当然是不能减少查询的数据量），一个基本的想法就是：采用多叉树结构（由于树节点元素数量是有限的，自然该节点的子树数量也就是有限的）。 这样我们就提出了一个新的查找树结构——多路查找树。根据平衡二叉树的启发，自然就想到平衡多路查找树结构，也就是B树 1、B树1.1 原理二叉搜索树： 所有非叶子结点至多拥有两个儿子（ Left 和 Right ）； 所有结点存储一个关键字； 非叶子结点的左指针指向小于其关键字的子树，右指针指向大于其关键字的子树； 1.2 搜索方法 B 树的搜索，从根结点开始，如果查询的关键字与结点的关键字相等，那么就命中；否则，如果查询关键字比 结点关键字小，就进入左儿子；如果比结点关键字大，就进入右儿子；如果左儿子或右儿子的指针为空，则报告找不到相应的关键字； 如果 B 树的所有非叶子结点的左右子树的 结点数目均保持差不多（平衡），那么 B 树的搜索性能逼近二分查找；但它比连续内存空间的二分查找的优点是，改变 B 树结构（插入与删除结点）不需要移动大段的内存数据，甚至通常是常数开销；如 但 B 树在经过多次插入与删除后，有可能导致不同的结构： 右边也是一个 B 树，但 它的搜索性能已经是线性的了；同样的关键字集合有可能导致不同的树结构索引；所以，使用 B 树还 要考虑尽可能让 B 树保持左图的结构，和避免右图的结构，也就是所谓的“平衡”问题； 实际使用的 B 树都是在原 B 树的基 础上加上平衡算法，即“平衡二叉树”；如何保持 B 树结点分布均匀的平衡算法是平衡二叉树的 关键；平衡算法是一种在 B 树中插入和删除结点的策略； 2、B-树1.1 原理是一种多路搜索树（并不是二叉的）： ​ 1. 定义任意非叶子结点最多只有 M 个儿 子；且 M&gt;2 ； ​ 2. 根结点的儿子数为 [2, M] ； ​ 3. 除根结点以外的非叶子结点的儿子数为 [M/2, M] ； ​ 4. 每个结点存放至少 M/2-1 （取 上整）和至多 M-1 个关键字；（至少 2 个关键 字） ​ 5. 非叶子结点的关键字个数 = 指向儿 子的指针个数 -1 ； ​ 6. 非叶子结点的关键字： K[1], K[2], …, K[M-1] ；且 K[i] &lt; K[i+1] ； ​ 7. 非叶子结点的指针： P[1], P[2], …, P[M] ；其中 P[1] 指向关键字小于 K[1] 的子树， P[M] 指向关键字大于K[M-1] 的子树，其它 P[i] 指 向关键字属于 (K[i-1], K[i]) 的子树； ​ 8. 所有叶子结点位于同一层； 如：（ M=3 ） B- 树的特性： ​ 1. 关键字集合分布在整颗树中； ​ 2. 任何一个关键字出现且只出现在一个结点中； ​ 3. 搜索有可能在非叶子结点结束； ​ 4. 其搜索性能等价于在关键字全集内做一次二分查找； ​ 5. 自动层次控制； ​ 由于限制了除根结点以外的非叶子结点，至少含有 M/2 个儿子，确保了结点的至少利用率，其最底搜索性能为： 其中， M 为设定的非叶子结点最多子树个 数， N 为关键字总数； ​ 所以 B- 树的性能总是等价于二分查找 （与 M 值无关），也就没有 B 树平衡 的问题； ​ 由于 M/2 的限制，在插入结点时，如果 结点已满，需要将结点分裂为两个各占 M/2 的结点；删除结点时，需将两个不足 M/2 的 兄弟结点合并； 1.2 方法参考 BTree,B-Tree,B+Tree,B*Tree都是什么 B-tree/B+tree/B*tree]]></content>
      <categories>
        <category>算法与数据结构</category>
      </categories>
      <tags>
        <tag>树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用scikit-learn生成测试数据集]]></title>
    <url>%2F2017%2F07%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E7%94%A8scikit-learn%E7%94%9F%E6%88%90%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE%E9%9B%86%2F</url>
    <content type="text"><![CDATA[转自https://www.jiqizhixin.com/articles/2018-02-05-2 分类测试问题将看三个分类问题：blobs、moons 和 circles。 线性分类 make_blobs() 函数可被用于生成具有高斯分布的 blobs 点。你可以控制生成 blobs 的数量，生成样本的数量以及一系列其他属性。考虑到 blobs 的线性可分性质，该问题也适用于线性分类问题。 下面的例子是一个多类分类预测问题，它生成了一个具有三个 blobs 的 2D 样本数据集。每个数据有两个输入和 0、1 或 2 个类的值。 1234567891011121314from sklearn.datasets.samples_generator import make_blobsfrom matplotlib import pyplotfrom pandas import DataFrame# generate 2d classification datasetX, y = make_blobs(n_samples=100, centers=3, n_features=2)# dict中定义三个key，分别是坐标和label，再通过dict创建DataFramedf = DataFrame(dict(x=X[:,0], y=X[:,1], label=y))colors = &#123;0:'red', 1:'blue', 2:'green'&#125;fig, ax = pyplot.subplots()#groupby可以通过传入需要分组的参数实现对数据的分组grouped = df.groupby('label')for key, group in grouped: group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])pyplot.show() 非线性分类make_moons() 函数用于二进制分类并且将生成一个漩涡模式，或者两个 moons。你可以控制 moon 形状中的噪声量，以及要生产的样本数量。 这个测试问题适用于能够学习非线性类边界的算法。下面的例子生成了一个中等噪音的 moon 数据集。 12345678910111213from sklearn.datasets import make_moonsfrom matplotlib import pyplotfrom pandas import DataFrame# generate 2d classification datasetX, y = make_moons(n_samples=100, noise=0.1)# scatter plot, dots colored by class valuedf = DataFrame(dict(x=X[:,0], y=X[:,1], label=y))colors = &#123;0:'red', 1:'blue'&#125;fig, ax = pyplot.subplots()grouped = df.groupby('label')for key, group in grouped: group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])pyplot.show() make_circles() 函数生成一个数据集落入同心圆的二进制分类问题。再一次地，与 moons 测试问题一样，你可以控制形状中的噪声量。该测试问题适用于可以学习复杂的非线性流行的算法。下面的例子中生成了一个具有一定噪音的 circles 数据集。 12345678910111213from sklearn.datasets import make_circlesfrom matplotlib import pyplotfrom pandas import DataFrame# generate 2d classification datasetX, y = make_circles(n_samples=100, noise=0.05)# scatter plot, dots colored by class valuedf = DataFrame(dict(x=X[:,0], y=X[:,1], label=y))colors = &#123;0:'red', 1:'blue'&#125;fig, ax = pyplot.subplots()grouped = df.groupby('label')for key, group in grouped: group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])pyplot.show() 回归测试问题线性回归make_regression() 函数将创建一个输入和输出具有线性关系的数据集。你可以配置样本数量，输入特征数量，噪声级别等等。该数据集适用于可以学习线性回归函数的算法。 下面的例子将生成 100 个示例，他们具有适度的噪声，都有一个输入特征和一个输出特征。 1234567from sklearn.datasets import make_regressionfrom matplotlib import pyplot# generate regression datasetX, y = make_regression(n_samples=100, n_features=1, noise=0.1)# plot regression datasetpyplot.scatter(X,y)pyplot.show() 通过这些测试集可以： 比较算法。选择一个测试问题，并比较该问题的一系列算法并汇报性能。 放大问题。选择一个测试问题并探索将其放大，用级数法来可视化结果，也可以探索一个特定算法模型技能和问题规模。 代码见blogcodes\sclearn_testDataset.py]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
        <tag>机器之心</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网页关键词提取]]></title>
    <url>%2F2017%2F07%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FNLP%2F%E7%BD%91%E9%A1%B5%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96%2F</url>
    <content type="text"><![CDATA[代码位于nlp 项目 jsoup解析html的DOM1Document doc =Jsoup.connect(url).userAgent(&quot;Mozilla&quot;).get(); 提取网页getPageDetail获取网页提取的结果，返回WebPageInfo类，该类包括 12345678910public String domain;public String url;public String rawTitle;public String title;public String content;public String summary;public HashMap&lt;String, String&gt; meta;public HashMap&lt;String, List&lt;String&gt;&gt; calculation;public long freq = 1;public String createTime; meta_desc，来自网页meta的description元素 1meta.put(&quot;description&quot;, meta_desc); meta_keywords，来自网页meta的keywords 1meta.put(&quot;keywords&quot;, meta_keywords); 标题rawTitle=doc.title() title，通过ExtractUtil.extractTitle(doc.body(), rawTitle)进一步抽取。目的是去掉标题中的无关信息，如网站信息等 1234567891011121314151617181920212223def extractTitle(root: Element, rawTitle:String): String = &#123; if(StringUtils.isBlank(rawTitle)) return ""; val titleCnt = new HashMap[String, Int] with HashMapUtil.IntHashMap[String] titleCnt.adjustOrPut(te.extract(rawTitle).trim, 1, 1) titleCnt.adjustOrPut(te.extractFirst(rawTitle).trim, 1, 1) val tq = new PriorityQueue[String](2) extractTitle0(root, rawTitle, 1, tq) for (candidate &lt;- tq.values) titleCnt.adjustOrPut(candidate.trim, 1, 1) var maxCnt = 0 var title = rawTitle titleCnt.foreach &#123; case (candidate, cnt) =&gt; if (maxCnt &lt; cnt) &#123; maxCnt = cnt title = candidate &#125; if (maxCnt == cnt &amp;&amp; candidate.length &gt; title.length) title = candidate &#125; title &#125; te.extract， 首先要split。通过判断unicode字符的类别（Unicode字符类）来分割标题 1234567891011121314151617181920for (int i = 0; i &lt; title.length(); i++) &#123; char ch = title.charAt(i); int type = Character.getType(ch); // 标点，前引号 if (type == Character.INITIAL_QUOTE_PUNCTUATION) quoteCnt++; // 标点，开始 if (type == Character.START_PUNCTUATION) quoteCnt++; if (quoteCnt == 0 &amp;&amp; !lastLetter &amp;&amp; !lastDigit &amp;&amp; splitChars.contains(ch)) &#123; parts.add(title.substring(last, i)); last = i + 1; &#125; // 标点，后引号 if (type == Character.FINAL_QUOTE_PUNCTUATION) quoteCnt--; // 标点，结束 if (type == Character.END_PUNCTUATION) quoteCnt--; if (ch &gt;= 'A' &amp;&amp; ch &lt;= 'Z' || ch &gt;= 'a' &amp;&amp; ch &lt;= 'z') lastLetter = true; else lastLetter = false; if (Character.isDigit(ch)) lastDigit = true; else if (!splitChars.contains(ch)) lastDigit = false; &#125; 然后提取最长的part。提取的原则是， 1、如果出现不重要的字符前缀后缀ignoreSuffixes、ignorePrefixes，降低part的长度 2、第一个part的长度翻倍，可能是考虑到真的标题往往出现在第一块，如 清润饮食“熄灭”冬季之火 - 素食 - 大渡网-佛教资讯，生活，人文，心灵感悟，佛艺时尚杂志，佛教音乐，佛教常识，佛教视频 从草根到精英——大陆网络民族主义流变-观点评论-时事评论-四月网-青年思想门户-M4.CN 1234567891011121314151617181920212223242526272829private static List&lt;String&gt; ignoreSuffixes = Arrays.asList("频道", "站", "网", "报", "集", "公司", ".com", ".cn", "平台", "门户", "博客", "精选", "博客精选"); private static List&lt;String&gt; ignorePrefixes = Arrays.asList("Powered by"); private static HashSet&lt;Character&gt; splitChars = new HashSet&lt;Character&gt;(Arrays.asList('|', '_', '-', '—', '－', '&lt;', '&gt;', '«', '»'));private String getLongestPart(List&lt;String&gt; parts) &#123; double longestNumWords = 0; String longestPart = ""; for (int i = 0; i &lt; parts.size(); i++) &#123; String p = parts.get(i).trim(); int ignoreCount = 0; for (String is: ignoreSuffixes) if (p.toLowerCase().endsWith(is)) ignoreCount++; for (String ip: ignorePrefixes) if (p.toLowerCase().startsWith(ip)) ignoreCount++; int colonCnt = StringUtils.countMatches(p, ","); if (colonCnt &gt; 0) ignoreCount += colonCnt - 1; colonCnt = StringUtils.countMatches(p, "，"); if (colonCnt &gt; 0) ignoreCount += colonCnt - 1; double numWords = TextUtil.countNumWords(p); numWords = numWords / (1 + 2 * ignoreCount); if (i == 0) numWords = numWords * 2; if (numWords &gt; longestNumWords) &#123; longestNumWords = numWords; longestPart = p; &#125; &#125; if (longestPart.length() == 0) return ""; else return longestPart.trim(); &#125; extractTitle0(root, rawTitle, 1, tq) 传入root和刚才提取的rawTitle，递归遍历root的各个head元素，h，title，每种赋值不同权重。再寻找与rawTitle的最长公共子串。 12345678910111213141516171819202122232425private def extractTitle0(node: Node, title: String, weight: Double, tq: PriorityQueue[String], depth: Int = 0): Unit = &#123; node match &#123; case textNode: TextNode =&gt; &#123; val text = textNode.text.trim if (text.length &gt; 0) &#123; val lcs = TextUtil.findLcs(title, text) val nwords = TextUtil.countNumWords(lcs) val pos = title.indexOf(lcs) if (pos != -1 &amp;&amp; nwords &gt; 0) &#123; tq.add(nwords * weight / (1 + math.log(2 + pos)), lcs) &#125; &#125; &#125; case e: Element =&gt; &#123; var w = weight if (e.tagName.startsWith("h")) w = w * 1.2 if (e.tagName == "a") w = w / 2 if (e.className.contains("title")) w = w * 1.5 if (e.tagName != "title" &amp;&amp; !isNegativeBlock(e.className + " " + e.id) &amp;&amp; depth &lt; Extract_STOP_DEPTH) &#123; for (n &lt;- e.childNodes.asScala) extractTitle0(n, title, w, tq, depth + 1) &#125; &#125; case _ =&gt; &#123;&#125; &#125; &#125; 通过以上方法提取出各种title后，选出出现频率最高的作为最终的title。 content123456789101112131415161718def extractContent(url: String, doc: Document): List[String] = &#123; val rawTitle = doc.title if(doc.body == null) return null ExtractUtil.cleanup(doc.body) val title = ExtractUtil.extractTitle(doc.body, rawTitle) val metaKeywords = ExtractUtil.extractMeta(doc, "keywords") val blocks = ExtractUtil.extractBlocks(doc, title) map &#123; block =&gt; SnippetBlock(block.snippets map &#123; snippet =&gt; TextProcess.normalize(urlReg.matcher(snippet).replaceAll("")) &#125;, block.score, block.isArticle, block.imgs) &#125; val normalTitle = TextUtil.fillText(title) val normalRawTitle = TextUtil.fillText(doc.title) val allsnippets = blocks.filter(_.isArticle).flatMap &#123; b =&gt; b.snippets &#125; return allsnippets; &#125; 清洗docExtractUtil.cleanup(doc.body) 1234567def cleanup(root: Element): Unit = &#123; val cleanNodes = for &#123; e &lt;- root.getAllElements.asScala if INVALID_TAGS.contains(e.tagName) || e.attr("style").contains("display:none") &#125; yield e for (cn &lt;- cleanNodes) cn.remove &#125; 提取title、metakeywords提取blocks1234567def extractBlocks(doc: Document, title: String): List[SnippetBlock] = &#123; val blocks = new ListBuffer[BlockDetail] val bd = new BlockDetailBuffer extractBlocks(doc.body, blocks, bd) if (bd.isDefined) blocks += bd.result calcScore(title, blocks.result filterNot(b =&gt; hasICP(b))) ++ List(SnippetBlock(List(extractMeta(doc, "keywords")), 1d, true, List()), SnippetBlock(List(extractMeta(doc, "description")), 0d, false, List())) &#125; 1234567891011121314151617181920212223private def extractBlocks(root: Node, blocks: ListBuffer[BlockDetail], bd: BlockDetailBuffer, inLink: Boolean = false, depth: Int = 0): Unit = &#123; root match &#123; case tn: TextNode =&gt; &#123; val text = StringUtils.replace(tn.text, "\u00a0", " ").trim if (text.length &gt; 0) bd.add(text, inLink) &#125; case e: Element =&gt; &#123; val isLink = inLink || (e.tagName == "a") if(depth &lt; Extract_STOP_DEPTH)&#123; e.childNodes.asScala foreach &#123; c =&gt; extractBlocks(c, blocks, bd, isLink, depth + 1) &#125; &#125; if (e.tagName == "img" || e.tagName == "embed") &#123; bd.addImg(e) &#125; if (e.isBlock) &#123; if (bd.isDefined) blocks += bd.result bd.clear &#125; &#125; case _ =&gt; &#123;&#125; &#125;&#125; 提取每个TextNode的文本，放到BlockDetailBuffer中。将每个BlockDetailBuffer的内容放到BlockDetail的list blocks中。 过滤掉包含icp备或icp证的文本，再对所有的blocks计算打分calcScore 最后提取所有是文本的snippet，作为content 提取keywords同样是先clean，提取title、metaKeyword， 再提取blocks 12345678910111213val blocks:List[SnippetBlock] = ExtractUtil.extractBlocks(doc, title).map &#123; block =&gt; &#123; SnippetBlock(block.snippets map &#123; snippet =&gt; TextProcess.normalize(urlReg.matcher(snippet).replaceAll("")) &#125;, block.score, block.isArticle, block.imgs) val temp = block.snippets map &#123; snippet =&gt; TextProcess.normalize(urlReg.matcher(snippet).replaceAll("")) &#125; maxLen += temp.map(AtomSplit.count(_)).sum val retVal:SnippetBlock = if(maxLen &lt; MAX_CONTENT_LENGTH || maxflag)&#123;SnippetBlock(temp, block.score, block.isArticle, block.imgs)&#125; else null if(maxLen &gt; MAX_CONTENT_LENGTH)&#123; maxflag = false &#125; retVal &#125; &#125;.filter( _ != null) dlg再提取dlg 1val dlg = DlgExtractor.extract(normalTitle, TextUtil.fillText(doc.title), blocks, 6)/*.filter(_._2 &gt; 1.0)*/ 分词后，计算每个词的权重， First of all, for any web page, we can use jsoup to obtain the Document Object Model (DOM) , which can access all the HTML elements of it. After that，we clean up HTML elements by drop some invalid or useless tags, such as the tags with “display:none” property. At last, We extract the Content and Keywords of HTML. For extracting Content, we iterate through the DOM tree to find all TextNode elements, extract the text and take them as the Snippets. Then we calculate the scores of all Snippets, and get the available Snippets as Content. For extracing Keywords, besides the Snippets from TextNode elements, we also collect the title, keywords and description from tag, store them as Blocks. For every Block, we segment words to generate the corpus by ansj_seg, and calculate the weight of every word using TFIDF. Finally, we get the TOP 10 words as Keywords of web page. 我们解析了10万左右的网页，根据解析的网页content打上safe和unsafe的label，后期我们会对safe和unsafe进一步细分。 训练过程：我们载入所有含标签的训练样本，由于fasttext提供了适用于各种语言的Word2Vec预向量集，将网页内容转为词向量，通过fasttext训练出模型并保存到本地。 预测过程：载入模型到内存，当输入一个网页的content后，转为词向量，根据模型给出safe或unsafe的分类结果。 We have analyzed some 100 thousand web pages, classified text in categories, such as safe and unsafe by content of these web pages, and we will extent more categories in future. In order to train the text classifier model, we load all samples containing a training sentence per line along with the labels, and transfer all words to vectors using pre-trained word vectors model published by fastText. Then we use the code from Github to run the training program. Once the model was trained, we save it on disk as a file. When input a content of web page, we transfer it to word vectors and run the prediction program, as a result we get the category of this web page.]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[（转）HDFS基本文件常用命令]]></title>
    <url>%2F2017%2F07%2F12%2Fhadoop-spark%2F%EF%BC%88%E8%BD%AC%EF%BC%89HDFS%E5%9F%BA%E6%9C%AC%E6%96%87%E4%BB%B6%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[-ls path列出path目录下的内容，包括文件名，权限，所有者，大小和修改时间。 -lsr path与ls相似，但递归地显示子目录下的内容。 -du path显示path下所有文件磁盘使用情况下，用字节大小表示，文件名用完整的HDFS协议前缀表示。 -dus path与-du相似，但它还显示全部文件或目录磁盘使用情况 -mv src dest在HDFS中，将文件或目录从HDFS的源路径移动到目标路径。 -cp src dest在HDFS中，将src文件或目录复制到dest。 –rm path删除一个文件或目录 –rmr path删除一个文件或递归删除目录注意：这里的mv cp操作的源路径和目的路径都是在HDFS中的路径文件 –put localSrc dest将本地文件或目录localSrc上传到HDFS中的dest路径。 –copyFromLocal localSrc dest与-put命令相同 –moveFromLocal localSrc dest将文件或目录从localSrc上传到HDFS中的dest目录，再删除本地文件或目录localSrc。12 –get [-crc] src localDest将文件或目录从HDFS中的src拷贝到本地文件系统localDest。13 –getmerge src localDest [addnl]将在HDFS中满足路径src的文件合并到本地文件系统的一个文件localDest中。14 –cat filename显示文件内容到标准输出上。 -copyToLocal [-crc] src localDest与-get命令相同。16 -moveToLocal [-crc] src localDest与-get命令相似，但拷贝结束后，删除HDFS上原文件。17 -mkdir path在HDFS中创建一个名为path的目录，如果它的上级目录不存在，也会被创建，如同linux中的mkidr –p。18 -setrep [-R] [-w] rep path设置目标文件的复制数。19 -touchz path创建一个文件。时间戳为当前时间，如果文件本就存在就失败，除非原文件长充为0。20 -test –[ezd] path如果路径(path)存在，返回1，长度为0(zero)，或是一个目录(directory)。21 –stat [format] path显示文件所占块数(%b)，文件名(%n)，块大小(%n)，复制数(%r)，修改时间(%y%Y)。22 –tail [-f] file显示文件最后的1KB内容到标准输出。23 –chmod [-R] [owner][:[group]] path…递归修改时带上-R参数，mode是一个3位的8进制数，或是[augo]+/-{rwxX}。24 –chgrp [-R] group设置文件或目录的所有组，递归修改目录时用-R参数。25 –help cmd显示cmd命令的使用信息，你需要把命令的“-”去掉复制到本地hadoop fs -copyToLocal /tmp/admaster_xid_15-05-08/part-r-00298 /home/david/查看文件夹大小hadoop fs -du /shortdata/xmo_info | awk ‘{ sum=$1 ;dir2=$3 ; hum[10243]=”Gb”;hum[10242]=”Mb”;hum[1024]=”Kb”; for (x=1024**3; x&gt;=1024; x/=1024){ if (sum&gt;=x) { printf “%.2f %s \t %s\n”,sum/x,hum[x],dir2;break } }}’]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
      <tags>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[动态规划理论]]></title>
    <url>%2F2017%2F07%2F12%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E8%AE%B2%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[动态规划理论讲解[TOC] 基础理论什么是动态规划？什么时候要用动态规划？怎么使用动态规划？ 1、什么是动态规划？ 求解决策过程最优化的数学方法。把多阶段过程转化为一系列单阶段问题，利用各阶段之间的关系，逐个求解，创立了解决这类过程优化问题的新方法——动态规划。 2、什么时候要用动态规划？ 如果要求一个问题的最优解（通常是最大值或者最小值），而且该问题能够分解成若干个子问题，并且小问题之间也存在重叠的子问题，则考虑采用动态规划。 3、怎么使用动态规划？ 判题题意是否为找出一个问题的最优解 从上往下分析问题，大问题可以分解为子问题，子问题中还有更小的子问题 从下往上分析问题 ，找出这些问题之间的关联（状态转移方程） 讨论边界的初始条件 解决问题（通常使用数组进行迭代求出最优解） 代表算法-硬币问题问题描述假设有 1 元，3 元，5 元的硬币若干（无限），现在需要凑出 11 元，问如何组合才能使硬币的数量最少？ 问题分析乍看之下，我们简单的运用一下心算就能解出需要 2 个 5 元和 1 个 1 元的解。当然这里只是列出了这个问题比较简单的情况。当硬币的币制或者种类变化，并且需要凑出的总价值变大时，就很难靠简单的计算得出结论了。贪心算法可以在一定的程度上得出较优解，但不是每次都能得出最优解。 这里运用动态规划的思路解决该问题。按照一般思路，我们先从最基本的情况来一步一步地推导。 我们先假设一个函数 d(i) 来表示需要凑出 i 的总价值需要的最少硬币数量。 当 i = 0 时，很显然我们可以知道 d(0) = 0。因为不要凑钱了嘛，当然也不需要任何硬币了。注意这是很重要的一步，其后所有的结果都从这一步延伸开来。 当 i = 1 时，因为我们有 1 元的硬币，所以直接在第 1 步的基础上，加上 1 个 1 元硬币，得出 d(1) = 1。 当 i = 2 时，因为我们并没有 2 元的硬币，所以只能拿 1 元的硬币来凑。在第 2 步的基础上，加上 1 个 1 元硬币，得出 d(2) = 2。 当 i = 3 时，我们可以在第 3 步的基础上加上 1 个 1 元硬币，得到 3 这个结果。但其实我们有 3 元硬币，所以这一步的最优结果不是建立在第 3 步的结果上得来的，而是应该建立在第 1 步上，加上 1 个 3 元硬币，得到 d(3) = 1。 接着就不再举例了，我们来分析一下。可以看出，除了第 1 步这个看似基本的公理外，其他往后的结果都是建立在它之前得到的某一步的最优解上，加上 1 个硬币得到。得出： d(i) = d(j) + 1 这里 j &lt; i。通俗地讲，我们需要凑出 i 元，就在凑出 j 的结果上再加上某一个硬币就行了。 那这里我们加上的是哪个硬币呢。嗯，其实很简单，把每个硬币试一下就行了： 假设最后加上的是 1 元硬币，那 d(i) = d(j) + 1 = d(i - 1) + 1。 假设最后加上的是 3 元硬币，那 d(i) = d(j) + 1 = d(i - 3) + 1。 假设最后加上的是 5 元硬币，那 d(i) = d(j) + 1 = d(i - 5) + 1。 我们分别计算出 d(i - 1) + 1，d(i - 3) + 1，d(i - 5) + 1 的值，取其中的最小值，即为最优解，也就是 d(i)。 最后公式： 123456789101112131415161718192021222324public class MinCoins &#123; public static void main(String[] args) &#123; int[] coins = &#123; 1, 3, 5 &#125;; int value = 11; CoinDp(value, coins); &#125; public static void CoinDp(int n, int[] coinValue) &#123; int count = 0;// 记录执行次数 int[] min = new int[n + 1]; // 用来存储得到n块钱需要的硬币数的最小值 min[0] = 0; for (int i = 1; i &lt;= n; i++) &#123; min[i] = Integer.MAX_VALUE;// 初始化数组中的每个值都是最大的整数 for (int j = 0; j &lt; coinValue.length; j++) &#123; count++; if (i &gt;= coinValue[j] &amp;&amp; min[i] &gt; min[i - coinValue[j]] + 1) &#123; min[i] = min[i - coinValue[j]] + 1; &#125; &#125; System.out.println("获取" + i + "块钱，最少需要的硬币数：" + min[i] + ",执行的次数：" + count); &#125; System.out.println(min[n]); &#125;&#125; 参考： https://blog.csdn.net/weixin_38278878/article/details/80037455 https://www.cnblogs.com/snowInPluto/p/5992846.html https://www.cnblogs.com/wuyuegb2312/p/3281264.html]]></content>
      <categories>
        <category>算法与数据结构</category>
      </categories>
      <tags>
        <tag>树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集成学习]]></title>
    <url>%2F2017%2F07%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[个体学习器一般是弱学习器。 弱学习器是指泛华性能略优于随机猜测的学习器，例如二分上略高于50%的学习器。 要获得好的集成，个体学习器应“好而不同”，即个体学习器要有一定的准确性和多样性（学习器之间有差异）。 理论上，假设个体学习器的误差是相互独立，那么随着学习器数量增大，集成的错误率将指数下降，最终趋向于零。 但实际上不可能相互独立。且准确性和多样性本身就是矛盾的，追求准确性就要牺牲多样性。所以如何产生并结合“好而不同”的学习器，是集成学习研究的核心。 根据集成的方式不同， 1）个体学习器存在强依赖性，必须串行生成，如Boosting； 2）个体学习器间不存在强依赖关系，可同时并行生成，如Bagging和随机森林。 mic或stacking方法https://blog.csdn.net/sb19931201/article/details/56315689?locationNum=1&amp;fps=1 从这篇帖子来 https://blog.csdn.net/a358463121/article/details/53054686#t18 https://zhuanlan.zhihu.com/jlbookworm https://blog.csdn.net/wstcjf/article/details/77989963 文章的思路有点问题？ https://blog.csdn.net/xiaoliuzz/article/details/79298841 https://blog.csdn.net/yc1203968305/article/details/73526615 https://www.cnblogs.com/zhizhan/p/5051881.html https://mlwave.com/kaggle-ensembling-guide/ StackingKaggle机器学习之模型融合（stacking）心得 Introduction to Ensembling/Stacking in Python Stacking Models for Improved Predictions 使用sklearn进行集成学习——理论1 前言2 集成学习是什么？3 偏差和方差 3.1 模型的偏差和方差是什么？ 3.2 bagging的偏差和方差 3.3 boosting的偏差和方差 3.4 模型的独立性 3.5 小结4 Gradient Boosting 4.1 拟合残差 4.2 拟合反向梯度 4.2.1 契机：引入损失函数 4.2.2 难题一：任意损失函数的最优化 4.2.3 难题二：无法对测试样本计算反向梯度 4.3 常见的损失函数 4.4 步子太大容易扯着蛋：缩减 4.5 初始模型 4.5 Gradient Tree Boosting 4.6 小结5 总结6 参考资料 1 前言 很多人在竞赛（Kaggle，天池等）或工程实践中使用了集成学习（例如，RF、GTB等），确实也取得了不错的效果，在保证准确度的同时也提升了模型防止过拟合的能力。但是，我们真的用对了集成学习吗？ sklearn提供了sklearn.ensemble库，支持众多集成学习算法和模型。恐怕大多数人使用这些工具时，要么使用默认参数，要么根据模型在测试集上的性能试探性地进行调参（当然，完全不懂的参数还是不动算了），要么将调参的工作丢给调参算法（网格搜索等）。这样并不能真正地称为“会”用sklearn进行集成学习。 我认为，学会调参是进行集成学习工作的前提。然而，第一次遇到这些算法和模型时，肯定会被其丰富的参数所吓到，要知道，教材上教的伪代码可没这么多参数啊！！！没关系，暂时，我们只要记住一句话：参数可分为两种，一种是影响模型在训练集上的准确度或影响防止过拟合能力的参数；另一种不影响这两者的其他参数。模型在样本总体上的准确度（后简称准确度）由其在训练集上的准确度及其防止过拟合的能力所共同决定，所以在调参时，我们主要对第一种参数进行调整，最终达到的效果是：模型在训练集上的准确度和防止过拟合能力的大和谐！ 本篇博文将详细阐述模型参数背后的理论知识，在下篇博文中，我们将对最热门的两个模型Random Forrest和Gradient Tree Boosting（含分类和回归，所以共4个模型）进行具体的参数讲解。如果你实在无法静下心来学习理论，你也可以在下篇博文中找到最直接的调参指导，虽然我不赞同这么做。 2 集成学习是什么？ 我们还是花一点时间来说明一下集成学习是什么，如果对此有一定基础的同学可以跳过本节。简单来说，集成学习是一种技术框架，其按照不同的思路来组合基础模型，从而达到其利断金的目的。 目前，有三种常见的集成学习框架：bagging，boosting和stacking。国内，南京大学的周志华教授对集成学习有很深入的研究，其在09年发表的一篇概述性论文《Ensemble Learning》对这三种集成学习框架有了明确的定义，概括如下： bagging：从训练集从进行子抽样组成每个基模型所需要的子训练集，对所有基模型预测的结果进行综合产生最终的预测结果： boosting：训练过程为阶梯状，基模型按次序一一进行训练（实现上可以做到并行），基模型的训练集按照某种策略每次都进行一定的转化。对所有基模型预测的结果进行线性综合产生最终的预测结果： stacking：将训练好的所有基模型对训练基进行预测，第j个基模型对第i个训练样本的预测值将作为新的训练集中第i个样本的第j个特征值，最后基于新的训练集进行训练。同理，预测的过程也要先经过所有基模型的预测形成新的测试集，最后再对测试集进行预测： 有了这些基本概念之后，直觉将告诉我们，由于不再是单一的模型进行预测，所以模型有了“集思广益”的能力，也就不容易产生过拟合现象。但是，直觉是不可靠的，接下来我们将从模型的偏差和方差入手，彻底搞清楚这一问题。 3 偏差和方差 广义的偏差（bias）描述的是预测值和真实值之间的差异，方差（variance）描述距的是预测值作为随机变量的离散程度。《Understanding the Bias-Variance Tradeoff》当中有一副图形象地向我们展示了偏差和方差的关系： 3.1 模型的偏差和方差是什么？ 模型的偏差是一个相对来说简单的概念：训练出来的模型在训练集上的准确度。 要解释模型的方差，首先需要重新审视模型：模型是随机变量。设样本容量为n的训练集为随机变量的集合(X1, X2, …, Xn)，那么模型是以这些随机变量为输入的随机变量函数（其本身仍然是随机变量）：F(X1, X2, …, Xn)。抽样的随机性带来了模型的随机性。 定义随机变量的值的差异是计算方差的前提条件，通常来说，我们遇到的都是数值型的随机变量，数值之间的差异再明显不过（减法运算）。但是，模型的差异性呢？我们可以理解模型的差异性为模型的结构差异，例如：线性模型中权值向量的差异，树模型中树的结构差异等。在研究模型方差的问题上，我们并不需要对方差进行定量计算，只需要知道其概念即可。 研究模型的方差有什么现实的意义呢？我们认为方差越大的模型越容易过拟合：假设有两个训练集A和B，经过A训练的模型Fa与经过B训练的模型Fb差异很大，这意味着Fa在类A的样本集合上有更好的性能，而Fb反之，这便是我们所说的过拟合现象。 我们常说集成学习框架中的基模型是弱模型，通常来说弱模型是偏差高（在训练集上准确度低）方差小（防止过拟合能力强）的模型。但是，并不是所有集成学习框架中的基模型都是弱模型。bagging和stacking中的基模型为强模型（偏差低方差高），boosting中的基模型为弱模型。 在bagging和boosting框架中，通过计算基模型的期望和方差，我们可以得到模型整体的期望和方差。为了简化模型，我们假设基模型的权重、方差及两两间的相关系数相等。由于bagging和boosting的基模型都是线性组成的，那么有： 3.2 bagging的偏差和方差 对于bagging来说，每个基模型的权重等于1/m且期望近似相等（子训练集都是从原训练集中进行子抽样），故我们可以进一步化简得到： 根据上式我们可以看到，整体模型的期望近似于基模型的期望，这也就意味着整体模型的偏差和基模型的偏差近似。同时，整体模型的方差小于等于基模型的方差（当相关性为1时取等号），随着基模型数（m）的增多，整体模型的方差减少，从而防止过拟合的能力增强，模型的准确度得到提高。但是，模型的准确度一定会无限逼近于1吗？并不一定，当基模型数增加到一定程度时，方差公式第二项的改变对整体方差的作用很小，防止过拟合的能力达到极限，这便是准确度的极限了。另外，在此我们还知道了为什么bagging中的基模型一定要为强模型，否则就会导致整体模型的偏差度低，即准确度低。 Random Forest是典型的基于bagging框架的模型，其在bagging的基础上，进一步降低了模型的方差。Random Fores中基模型是树模型，在树的内部节点分裂过程中，不再是将所有特征，而是随机抽样一部分特征纳入分裂的候选项。这样一来，基模型之间的相关性降低，从而在方差公式中，第一项显著减少，第二项稍微增加，整体方差仍是减少。 3.3 boosting的偏差和方差 对于boosting来说，基模型的训练集抽样是强相关的，那么模型的相关系数近似等于1，故我们也可以针对boosting化简公式为： 通过观察整体方差的表达式，我们容易发现，若基模型不是弱模型，其方差相对较大，这将导致整体模型的方差很大，即无法达到防止过拟合的效果。因此，boosting框架中的基模型必须为弱模型。 因为基模型为弱模型，导致了每个基模型的准确度都不是很高（因为其在训练集上的准确度不高）。随着基模型数的增多，整体模型的期望值增加，更接近真实值，因此，整体模型的准确度提高。但是准确度一定会无限逼近于1吗？仍然并不一定，因为训练过程中准确度的提高的主要功臣是整体模型在训练集上的准确度提高，而随着训练的进行，整体模型的方差变大，导致防止过拟合的能力变弱，最终导致了准确度反而有所下降。 基于boosting框架的Gradient Tree Boosting模型中基模型也为树模型，同Random Forrest，我们也可以对特征进行随机抽样来使基模型间的相关性降低，从而达到减少方差的效果。 3.4 模型的独立性 聪明的读者这时肯定要问了，如何衡量基模型的独立性？我们说过，抽样的随机性决定了模型的随机性，如果两个模型的训练集抽样过程不独立，则两个模型则不独立。这时便有一个天大的陷阱在等着我们：bagging中基模型的训练样本都是独立的随机抽样，但是基模型却不独立呢？ 我们讨论模型的随机性时，抽样是针对于样本的整体。而bagging中的抽样是针对于训练集（整体的子集），所以并不能称其为对整体的独立随机抽样。那么到底bagging中基模型的相关性体现在哪呢？在知乎问答《为什么说bagging是减少variance，而boosting是减少bias?》中请教用户“过拟合”后，我总结bagging的抽样为两个过程： 样本抽样：整体模型F(X1, X2, …, Xn)中各输入随机变量（X1, X2, …, Xn）对样本的抽样 子抽样：从整体模型F(X1, X2, …, Xn)中随机抽取若干输入随机变量成为基模型的输入随机变量 假若在子抽样的过程中，两个基模型抽取的输入随机变量有一定的重合，那么这两个基模型对整体样本的抽样将不再独立，这时基模型之间便具有了相关性。 3.5 小结 还记得调参的目标吗：模型在训练集上的准确度和防止过拟合能力的大和谐！为此，我们目前做了一些什么工作呢？ 使用模型的偏差和方差来描述其在训练集上的准确度和防止过拟合的能力 对于bagging来说，整体模型的偏差和基模型近似，随着训练的进行，整体模型的方差降低 对于boosting来说，整体模型的初始偏差较高，方差较低，随着训练的进行，整体模型的偏差降低（虽然也不幸地伴随着方差增高），当训练过度时，因方差增高，整体模型的准确度反而降低 整体模型的偏差和方差与基模型的偏差和方差息息相关 这下总算有点开朗了，那些让我们抓狂的参数，现在可以粗略地分为两类了：控制整体训练过程的参数和基模型的参数，这两类参数都在影响着模型在训练集上的准确度以及防止过拟合的能力。 4 Gradient Boosting 对基于Gradient Boosting框架的模型的进行调试时，我们会遇到一个重要的概念：损失函数。在本节中，我们将把损失函数的“今生来世”讲个清楚！ 基于boosting框架的整体模型可以用线性组成式来描述，其中hi为基模型与其权值的乘积： 根据上式，整体模型的训练目标是使预测值F(x)逼近真实值y，也就是说要让每一个基模型的预测值逼近各自要预测的部分真实值。由于要同时考虑所有基模型，导致了整体模型的训练变成了一个非常复杂的问题。所以，研究者们想到了一个贪心的解决手段：每次只训练一个基模型。那么，现在改写整体模型为迭代式： 这样一来，每一轮迭代中，只要集中解决一个基模型的训练问题：使Fi逼近真实值y。 4.1 拟合残差 使Fi逼近真实值，其实就是使hi逼近真实值和上一轮迭代的预测值Fi-1之差，即残差（y-Fi-1）。最直接的做法是构建基模型来拟合残差，在博文《GBDT（MART） 迭代决策树入门教程 | 简介》中，作者举了一个生动的例子来说明通过基模型拟合残差，最终达到整体模型F(x)逼近真实值。 研究者发现，残差其实是最小均方损失函数的关于预测值的反向梯度： 也就是说，若Fi-1加上拟合了反向梯度的hi得到Fi，该值可能将导致平方差损失函数降低，预测的准确度提高！这显然不是巧合，但是研究者们野心更大，希望能够创造出一种对任意损失函数都可行的训练方法，那么仅仅拟合残差是不恰当的了。 4.2 拟合反向梯度4.2.1 契机：引入任意损失函数 引入任意损失函数后，我们可以定义整体模型的迭代式如下： 在这里，损失函数被定义为泛函。 4.2.2 难题一：任意损失函数的最优化 对任意损失函数（且是泛函）的最优化是困难的。我们需要打破思维的枷锁，将整体损失函数L’定义为n元普通函数（n为样本容量），损失函数L定义为2元普通函数（记住！！！这里的损失函数不再是泛函！！！）： 我们不妨使用梯度最速下降法来解决整体损失函数L’最小化的问题，先求整体损失函数的反向梯度： 假设已知样本x的当前预测值为Fi-1，下一步将预测值按照反向梯度，依照步长为r[i]，进行更新： 步长r[i]不是固定值，而是设计为： 4.2.3 难题二：无法对测试样本计算反向梯度 问题又来了，由于测试样本中y是未知的，所以无法求反向梯度。这正是Gradient Boosting框架中的基模型闪亮登场的时刻！在第i轮迭代中，我们创建训练集如下： 也就是说，让基模型拟合反向梯度函数，这样我们就可以做到只输入x这一个参数，就可求出其对应的反向梯度了（当然，通过基模型预测出来的反向梯度并不是准确的，这也提供了泛化整体模型的机会）。 综上，假设第i轮迭代中，根据新训练集训练出来的基模型为fi，那么最终的迭代公式为： 4.3 常见的损失函数 ls：最小均方回归中用到的损失函数。在之前我们已经谈到，从拟合残差的角度来说，残差即是该损失函数的反向梯度值（所以又称反向梯度为伪残差）。不同的是，从拟合残差的角度来说，步长是无意义的。该损失函数是sklearn中Gradient Tree Boosting回归模型默认的损失函数。 deviance：逻辑回归中用到的损失函数。熟悉逻辑回归的读者肯定还记得，逻辑回归本质是求极大似然解，其认为样本服从几何分布，样本属于某类别的概率可以logistic函数表达。所以，如果该损失函数可用在多类别的分类问题上，故其是sklearn中Gradient Tree Boosting分类模型默认的损失函数。 exponential：指数损失函数，表达式为： 对该损失函数求反向梯度得： 这时，在第i轮迭代中，新训练集如下： 脑袋里有什么东西浮出水面了吧？让我们看看Adaboost算法中，第i轮迭代中第j个样本权值的更新公式： 样本的权值什么时候会用到呢？计算第i轮损失函数的时候会用到： 让我们再回过头来，看看使用指数损失函数的Gradient Boosting计算第i轮损失函数： 天呐，两个公式就差了一个对权值的归一项。这并不是巧合，当损失函数是指数损失时，Gradient Boosting相当于二分类的Adaboost算法。是的，指数损失仅能用于二分类的情况。 4.4 步子太大容易扯着蛋：缩减 缩减也是一个相对显见的概念，也就是说使用Gradient Boosting时，每次学习的步长缩减一点。这有什么好处呢？缩减思想认为每次走一小步，多走几次，更容易逼近真实值。如果步子迈大了，使用最速下降法时，容易迈过最优点。将缩减代入迭代公式： 缩减需要配合基模型数一起使用，当缩减率v降低时，基模型数要配合增大，这样才能提高模型的准确度。 4.5 初始模型 还有一个不那么起眼的问题，初始模型F0是什么呢？如果没有定义初始模型，整体模型的迭代式一刻都无法进行！所以，我们定义初始模型为： 根据上式可知，对于不同的损失函数来说，初始模型也是不一样的。对所有的样本来说，根据初始模型预测出来的值都一样。 4.5 Gradient Tree Boosting 终于到了备受欢迎的Gradient Tree Boosting模型了！但是，可讲的却已经不多了。我们已经知道了该模型的基模型是树模型，并且可以通过对特征的随机抽样进一步减少整体模型的方差。我们可以在维基百科的Gradient Boosting词条中找到其伪代码实现。 4.6 小结 到此，读者应当很清楚Gradient Boosting中的损失函数有什么意义了。要说偏差描述了模型在训练集准确度，则损失函数则是描述该准确度的间接量纲。也就是说，模型采用不同的损失函数，其训练过程会朝着不同的方向进行！ 5 总结 磨刀不误砍柴功，我们花了这么多时间来学习必要的理论，我强调一次：必要的理论！集成学习模型的调参工作的核心就是找到合适的参数，能够使整体模型在训练集上的准确度和防止过拟合的能力达到协调，从而达到在样本总体上的最佳准确度。有了本文的理论知识铺垫，在下篇中，我们将对Random Forest和Gradient Tree Boosting中的每个参数进行详细阐述，同时也有一些小试验证明我们的结论。 6 参考资料 《Ensemble Learning》 《Understanding the Bias-Variance Tradeoff》 《为什么说bagging是减少variance，而boosting是减少bias?》 《GBDT（MART） 迭代决策树入门教程 | 简介》 泛函 梯度最速下降法 《logistic regression(二分类、多分类)》 《Adaboost与指数损失》 使用sklearn进行集成学习——实践目录1 Random Forest和Gradient Tree Boosting参数详解2 如何调参？ 2.1 调参的目标：偏差和方差的协调 2.2 参数对整体模型性能的影响 2.3 一个朴实的方案：贪心的坐标下降法 2.3.1 Random Forest调参案例：Digit Recognizer 2.3.1.1 调整过程影响类参数 2.3.1.2 调整子模型影响类参数 2.3.2 Gradient Tree Boosting调参案例：Hackathon3.x 2.3.2.1 调整过程影响类参数 2.3.2.2 调整子模型影响类参数 2.3.2.3 杀一记回马枪 2.4 “局部最优解”（温馨提示：看到这里有彩蛋！） 2.5 类别不均衡的陷阱3 总结4 参考资料 1 Random Forest和Gradient Tree Boosting参数详解 在sklearn.ensemble库中，我们可以找到Random Forest分类和回归的实现：RandomForestClassifier和RandomForestRegression，Gradient Tree Boosting分类和回归的实现：GradientBoostingClassifier和GradientBoostingRegression。有了这些模型后，立马上手操练起来？少侠请留步！且听我说一说，使用这些模型时常遇到的问题： 明明模型调教得很好了，可是效果离我的想象总有些偏差？——模型训练的第一步就是要定好目标，往错误的方向走太多也是后退。 凭直觉调了某个参数，可是居然没有任何作用，有时甚至起到反作用？——定好目标后，接下来就是要确定哪些参数是影响目标的，其对目标是正影响还是负影响，影响的大小。 感觉训练结束遥遥无期，sklearn只是个在小数据上的玩具？——虽然sklearn并不是基于分布式计算环境而设计的，但我们还是可以通过某些策略提高训练的效率。 模型开始训练了，但是训练到哪一步了呢？——饱暖思淫欲啊，目标，性能和效率都得了满足后，我们有时还需要有别的追求，例如训练过程的输出，袋外得分计算等等。 通过总结这些常见的问题，我们可以把模型的参数分为4类：目标类、性能类、效率类和附加类。下表详细地展示了4个模型参数的意义： 参数 类型 RandomForestClassifier RandomForestRegressor GradientBoostingClassifier GradientBoostingRegressor loss 目标 损失函数● exponential：模型等同AdaBoost★ deviance：和Logistic Regression的损失函数一致 损失函数● exponential：模型等同AdaBoost★ deviance：和Logistic Regression的损失函数一致 alpha 目标 损失函数为huber或quantile的时，alpha为损失函数中的参数 损失函数为huber或quantile的时，alpha为损失函数中的参数 class_weight 目标 类别的权值 n_estimators 性能 子模型的数量● int：个数★ 10：默认值 子模型的数量● int：个数★ 10：默认值 子模型的数量● int：个数★ 100：默认值 子模型的数量● int：个数★ 100：默认值 learning_rate 性能 学习率（缩减） 学习率（缩减） criterion 性能 判断节点是否继续分裂采用的计算方法● entropy★ gini 判断节点是否继续分裂采用的计算方法★ mse max_features 性能 节点分裂时参与判断的最大特征数● int：个数● float：占所有特征的百分比★ auto：所有特征数的开方● sqrt：所有特征数的开方● log2：所有特征数的log2值● None：等于所有特征数 节点分裂时参与判断的最大特征数● int：个数● float：占所有特征的百分比★ auto：所有特征数的开方● sqrt：所有特征数的开方● log2：所有特征数的log2值● None：等于所有特征数 节点分裂时参与判断的最大特征数● int：个数● float：占所有特征的百分比● auto：所有特征数的开方● sqrt：所有特征数的开方● log2：所有特征数的log2值★ None：等于所有特征数 节点分裂时参与判断的最大特征数● int：个数● float：占所有特征的百分比● auto：所有特征数的开方● sqrt：所有特征数的开方● log2：所有特征数的log2值★ None：等于所有特征数 max_depth 性能 最大深度，如果max_leaf_nodes参数指定，则忽略● int：深度★ None：树会生长到所有叶子都分到一个类，或者某节点所代表的样本数已小于min_samples_split 最大深度，如果max_leaf_nodes参数指定，则忽略● int：深度★ None：树会生长到所有叶子都分到一个类，或者某节点所代表的样本数已小于min_samples_split 最大深度，如果max_leaf_nodes参数指定，则忽略● int：深度★ 3：默认值 最大深度，如果max_leaf_nodes参数指定，则忽略● int：深度★ 3：默认值 min_samples_split 性能 分裂所需的最小样本数● int：样本数★ 2：默认值 分裂所需的最小样本数● int：样本数★ 2：默认值 分裂所需的最小样本数● int：样本数★ 2：默认值 分裂所需的最小样本数● int：样本数★ 2：默认值 min_samples_leaf 性能 叶节点最小样本数● int：样本数★ 1：默认值 叶节点最小样本数● int：样本数★ 1：默认值 叶节点最小样本数● int：样本数★ 1：默认值 叶节点最小样本数● int：样本数★ 1：默认值 min_weight_fraction_leaf 性能 叶节点最小样本权重总值● float：权重总值★ 0：默认值 叶节点最小样本权重总值● float：权重总值★ 0：默认值 叶节点最小样本权重总值● float：权重总值★ 0：默认值 叶节点最小样本权重总值● float：权重总值★ 0：默认值 max_leaf_nodes 性能 最大叶节点数● int：个数★ None：不限制叶节点数 最大叶节点数● int：个数★ None：不限制叶节点数 最大叶节点数● int：个数★ None：不限制叶节点数 最大叶节点数● int：个数★ None：不限制叶节点数 bootstrap 性能 是否bootstrap对样本抽样● False：子模型的样本一致，子模型间强相关★ True：默认值 是否bootstrap对样本抽样● False：子模型的样本一致，子模型间强相关★ True：默认值 subsample 性能 子采样率● float：采样率★ 1.0：默认值 子采样率● float：采样率★ 1.0：默认值 init 性能 初始子模型 初始子模型 n_jobs 效率 并行数● int：个数● -1：跟CPU核数一致★ 1:默认值 并行数● int：个数● -1：跟CPU核数一致★ 1:默认值 warm_start 效率 是否热启动，如果是，则下一次训练是以追加树的形式进行● bool：热启动★ False：默认值 是否热启动，如果是，则下一次训练是以追加树的形式进行● bool：热启动★ False：默认值 是否热启动，如果是，则下一次训练是以追加树的形式进行● bool：热启动★ False：默认值 是否热启动，如果是，则下一次训练是以追加树的形式进行● bool：热启动★ False：默认值 presort 效率 是否预排序,预排序可以加速查找最佳分裂点，对于稀疏数据不管用● Bool★ auto：非稀疏数据则预排序，若稀疏数据则不预排序 是否预排序,预排序可以加速查找最佳分裂点，对于稀疏数据不管用● Bool★ auto：非稀疏数据则预排序，若稀疏数据则不预排序 oob_score 附加 是否计算袋外得分★ False：默认值 是否计算袋外得分★ False：默认值 random_state 附加 随机器对象 随机器对象 随机器对象 随机器对象 verbose 附加 日志冗长度● int：冗长度★ 0：不输出训练过程● 1：偶尔输出● &gt;1：对每个子模型都输出 日志冗长度● int：冗长度★ 0：不输出训练过程● 1：偶尔输出● &gt;1：对每个子模型都输出 日志冗长度● int：冗长度★ 0：不输出训练过程● 1：偶尔输出● &gt;1：对每个子模型都输出 日志冗长度● int：冗长度★ 0：不输出训练过程● 1：偶尔输出● &gt;1：对每个子模型都输出 # ★：默认值 不难发现，基于bagging的Random Forest模型和基于boosting的Gradient Tree Boosting模型有不少共同的参数，然而某些参数的默认值又相差甚远。在《使用sklearn进行集成学习——理论》一文中，我们对bagging和boosting两种集成学习技术有了初步的了解。Random Forest的子模型都拥有较低的偏差，整体模型的训练过程旨在降低方差，故其需要较少的子模型（n_estimators默认值为10）且子模型不为弱模型（max_depth的默认值为None），同时，降低子模型间的相关度可以起到减少整体模型的方差的效果（max_features的默认值为auto）。另一方面，Gradient Tree Boosting的子模型都拥有较低的方差，整体模型的训练过程旨在降低偏差，故其需要较多的子模型（n_estimators默认值为100）且子模型为弱模型（max_depth的默认值为3），但是降低子模型间的相关度不能显著减少整体模型的方差（max_features的默认值为None）。 2 如何调参？ 聪明的读者应当要发问了：”博主，就算你列出来每个参数的意义，然并卵啊！我还是不知道无从下手啊！” 参数分类的目的在于缩小调参的范围，首先我们要明确训练的目标，把目标类的参数定下来。接下来，我们需要根据数据集的大小，考虑是否采用一些提高训练效率的策略，否则一次训练就三天三夜，法国人孩子都生出来了。然后，我们终于进入到了重中之重的环节：调整那些影响整体模型性能的参数。 2.1 调参的目标：偏差和方差的协调 同样在《使用sklearn进行集成学习——理论》中，我们已讨论过偏差和方差是怎样影响着模型的性能——准确度。调参的目标就是为了达到整体模型的偏差和方差的大和谐！进一步，这些参数又可分为两类：过程影响类及子模型影响类。在子模型不变的前提下，某些参数可以通过改变训练的过程，从而影响模型的性能，诸如：“子模型数”（n_estimators）、“学习率”（learning_rate）等。另外，我们还可以通过改变子模型性能来影响整体模型的性能，诸如：“最大树深度”（max_depth）、“分裂条件”（criterion）等。正由于bagging的训练过程旨在降低方差，而boosting的训练过程旨在降低偏差，过程影响类的参数能够引起整体模型性能的大幅度变化。一般来说，在此前提下，我们继续微调子模型影响类的参数，从而进一步提高模型的性能。 2.2 参数对整体模型性能的影响 假设模型是一个多元函数F，其输出值为模型的准确度。我们可以固定其他参数，从而对某个参数对整体模型性能的影响进行分析：是正影响还是负影响，影响的单调性？ 对Random Forest来说，增加“子模型数”（n_estimators）可以明显降低整体模型的方差，且不会对子模型的偏差和方差有任何影响。模型的准确度会随着“子模型数”的增加而提高。由于减少的是整体模型方差公式的第二项，故准确度的提高有一个上限。在不同的场景下，“分裂条件”（criterion）对模型的准确度的影响也不一样，该参数需要在实际运用时灵活调整。调整“最大叶节点数”（max_leaf_nodes）以及“最大树深度”（max_depth）之一，可以粗粒度地调整树的结构：叶节点越多或者树越深，意味着子模型的偏差越低，方差越高；同时，调整“分裂所需最小样本数”（min_samples_split）、“叶节点最小样本数”（min_samples_leaf）及“叶节点最小权重总值”（min_weight_fraction_leaf），可以更细粒度地调整树的结构：分裂所需样本数越少或者叶节点所需样本越少，也意味着子模型越复杂。一般来说，我们总采用bootstrap对样本进行子采样来降低子模型之间的关联度，从而降低整体模型的方差。适当地减少“分裂时考虑的最大特征数”（max_features），给子模型注入了另外的随机性，同样也达到了降低子模型之间关联度的效果。但是一味地降低该参数也是不行的，因为分裂时可选特征变少，模型的偏差会越来越大。在下图中，我们可以看到这些参数对Random Forest整体模型性能的影响： 对Gradient Tree Boosting来说，“子模型数”（n_estimators）和“学习率”（learning_rate）需要联合调整才能尽可能地提高模型的准确度：想象一下，A方案是走4步，每步走3米，B方案是走5步，每步走2米，哪个方案可以更接近10米远的终点？同理，子模型越复杂，对应整体模型偏差低，方差高，故“最大叶节点数”（max_leaf_nodes）、“最大树深度”（max_depth）等控制子模型结构的参数是与Random Forest一致的。类似“分裂时考虑的最大特征数”（max_features），降低“子采样率”（subsample），也会造成子模型间的关联度降低，整体模型的方差减小，但是当子采样率低到一定程度时，子模型的偏差增大，将引起整体模型的准确度降低。还记得“初始模型”（init）是什么吗？不同的损失函数有不一样的初始模型定义，通常，初始模型是一个更加弱的模型（以“平均”情况来预测），虽说支持自定义，大多数情况下保持默认即可。在下图中，我们可以看到这些参数对Gradient Tree Boosting整体模型性能的影响： 2.3 一个朴实的方案：贪心的坐标下降法 到此为止，我们终于知道需要调整哪些参数，对于单个参数，我们也知道怎么调整才能提升性能。然而，表示模型的函数F并不是一元函数，这些参数需要共同调整才能得到全局最优解。也就是说，把这些参数丢给调参算法（诸如Grid Search）咯？对于小数据集，我们还能这么任性，但是参数组合爆炸，在大数据集上，或许我的子子孙孙能够看到训练结果吧。实际上网格搜索也不一定能得到全局最优解，而另一些研究者从解优化问题的角度尝试解决调参问题。 坐标下降法是一类优化算法，其最大的优势在于不用计算待优化的目标函数的梯度。我们最容易想到一种特别朴实的类似于坐标下降法的方法，与坐标下降法不同的是，其不是循环使用各个参数进行调整，而是贪心地选取了对整体模型性能影响最大的参数。参数对整体模型性能的影响力是动态变化的，故每一轮坐标选取的过程中，这种方法在对每个坐标的下降方向进行一次直线搜索（line search）。首先，找到那些能够提升整体模型性能的参数，其次确保提升是单调或近似单调的。这意味着，我们筛选出来的参数是对整体模型性能有正影响的，且这种影响不是偶然性的，要知道，训练过程的随机性也会导致整体模型性能的细微区别，而这种区别是不具有单调性的。最后，在这些筛选出来的参数中，选取影响最大的参数进行调整即可。 无法对整体模型性能进行量化，也就谈不上去比较参数影响整体模型性能的程度。是的，我们还没有一个准确的方法来量化整体模型性能，只能通过交叉验证来近似计算整体模型性能。然而交叉验证也存在随机性，假设我们以验证集上的平均准确度作为整体模型的准确度，我们还得关心在各个验证集上准确度的变异系数，如果变异系数过大，则平均值作为整体模型的准确度也是不合适的。在接下来的案例分析中，我们所谈及的整体模型性能均是指平均准确度，请各位留心。 2.3.1 Random Forest调参案例：Digit Recognizer 在这里，我们选取Kaggle上101教学赛中的Digit Recognizer作为案例来演示对RandomForestClassifier调参的过程。当然，我们也不要傻乎乎地手工去设定不同的参数，然后训练模型。借助sklearn.grid_search库中的GridSearchCV类，不仅可以自动化调参，同时还可以对每一种参数组合进行交叉验证计算平均准确度。 2.3.1.1 调整过程影响类参数 首先，我们需要对过程影响类参数进行调整，而Random Forest的过程影响类参数只有“子模型数”（n_estimators）。“子模型数”的默认值为10，在此基础上，我们以10为单位，考察取值范围在1至201的调参情况： # 左图为模型在验证集上的平均准确度，右图为准确度的变异系数。横轴为参数的取值。 通过上图我们可以看到，随着“子模型数”的增加，整体模型的方差减少，其防止过拟合的能力增强，故整体模型的准确度提高。当“子模型数”增加到40以上时，准确度的提升逐渐不明显。考虑到训练的效率，最终我们选择“子模型数”为200。此时，在Kaggle上提交结果，得分为：0.96500，很凑合。 2.3.1.2 调整子模型影响类参数 在设定“子模型数”（n_estimators）为200的前提下，我们依次对子模型影响类的参数对整体模型性能的影响力进行分析。 对“分裂条件”（criterion）分别取值gini和entropy，得到调参结果如下： 显见，在此问题中，“分裂条件”保持默认值gini更加合适。 对“分裂时参与判断的最大特征数”（max_feature）以1为单位，设定取值范围为28至47，得到调参结果如下： “分裂时参与判断的最大特征数”的默认值auto，即总特征数（sqrt(784)=28）的开方。通过提升该参数，整体模型的准确度得到了提升。可见，该参数的默认值过小，导致了子模型的偏差过大，从而整体模型的偏差过大。同时，我们还注意到，该参数对整体模型性能的影响是近似单调的：从28到38，模型的准确度逐步抖动提升。所以，我们可考虑将该参数纳入下一步的调参工作。 对“最大深度”（max_depth）以10为单位，设定取值范围为10到100，得到调参结果如下： 随着树的深度加深，子模型的偏差减少，整体模型的准确度得到提升。从理论上来说，子模型训练的后期，随着方差增大，子模型的准确度稍微降低，从而影响整体模型的准确度降低。看图中，似乎取值范围从40到60的情况可以印证这一观点。不妨以1为单位，设定取值范围为40到59，更加细致地分析： 有点傻眼了，怎么跟预想的不太一样？为什么模型准确度的变化在40到59之间没有鲜明的“规律”了？要分析这个问题，我们得先思考一下，少一层子节点对子模型意味着什么？若少的那一层给原子模型带来的是方差增大，则新子模型会准确度提高；若少的那一层给原子模型带来的是偏差减小，则新子模型会准确度降低。所以，细粒度的层次变化既可能使整体模型的准确度提升，也可能使整体模型的准确度降低。从而也说明了，该参数更适合进行粗粒度的调整。在训练的现阶段，“抖动”现象的发生说明，此时对该参数的调整已不太合适了。 对“分裂所需的最小样本数”（min_samples_split）以1为单位，设定取值范围为2到11，得到调参的结果： 我们看到，随着分裂所需的最小样本数的增加，子模型的结构变得越来越简单，理论上来说，首先应当因方差减小导致整体模型的准确度提升。但是，在训练的现阶段，子模型的偏差增大的幅度比方差减小的幅度更大，所以整体模型的准确度持续下降。该参数的默认值为2，调参后，最优解保持2不变。 对“叶节点最小样本数”（min_samples_leaf）以1为单位，设定取值范围为1到10，得到调参结果如下： 同“分裂所需的最小样本数”，该参数也在调参后，保持最优解1不变。 对“最大叶节点数”（max_leaf_nodes）以100为单位，设定取值范围为2500到3400，得到调参结果如下： 类似于“最大深度”，该参数的增大会带来模型准确的提升，可是由于后期“不规律”的抖动，我们暂时不进行处理。 通过对以上参数的调参情况，我们可以总结如下： 参数 默认值准确度 调整后最佳准确度 提升幅度 分裂条件（criterion） 0.964023809524 0.964023809524 0 分裂时参与判断的最大特征数（max_feature） 0.963380952381 0.964428571429 0.00104762 最大深度（max_depth） 抖动 分裂所需的最小样本数（min_samples_split） 0.963976190476 0.963976190476 0 叶节点最小样本数（min_samples_leaf） 0.963595238095 0.963595238095 0 最大叶节点数（max_leaf_nodes） 抖动 接下来，我们固定分裂时参与判断的最大特征（max_features）为38，在Kaggle上提交一次结果：0.96671，比上一次调参好了0.00171，基本与我们预期的提升效果一致。 还需要继续下一轮坐标下降式调参吗？一般来说没有太大的必要，在本轮中出现了两个发生抖动现象的参数，而其他参数的调整均没有提升整体模型的性能。还是得老调重弹：数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。在DR竞赛中，与其期待通过对RandomForestClassifier调参来进一步提升整体模型的性能，不如挖掘出更有价值的特征，或者使用自带特征挖掘技能的模型（正如此题，图分类的问题更适合用神经网络来学习）。但是，在这里，我们还是可以自信地说，通过贪心的坐标下降法，比那些用网格搜索法穷举所有参数组合，自以为得到最优解的朋友们更进了一步。 2.3.2 Gradient Tree Boosting调参案例：Hackathon3.x 在这里，我们选取Analytics Vidhya上的Hackathon3.x作为案例来演示对GradientBoostingClassifier调参的过程。 2.3.2.1 调整过程影响类参数 GradientBoostingClassifier的过程影响类参数有“子模型数”（n_estimators）和“学习率”（learning_rate），我们可以使用GridSearchCV找到关于这两个参数的最优解。慢着！这里留了一个很大的陷阱：“子模型数”和“学习率”带来的性能提升是不均衡的，在前期会比较高，在后期会比较低，如果一开始我们将这两个参数调成最优，这样很容易陷入一个“局部最优解”。在目标函数都不确定的情况下（如是否凸？），谈局部最优解就是耍流氓，本文中“局部最优解”指的是调整各参数都无明显性能提升的一种状态，所以打了引号。下图中展示了这个两个参数的调参结果： # 图中颜色越深表示整体模型的性能越高 在此，我们先直觉地选择“子模型数”为60，“学习率”为0.1，此时的整体模型性能（平均准确度为0.8253）不是最好，但是也不差，良好水准。 2.3.2.2 调整子模型影响类参数 对子模型影响类参数的调整与Random Forest类似。最终我们对参数的调整如下： 子模型数n_estimators 学习率learning_rate 叶节点最小样本数min_samples_leaf 最大深度max_depth 子采样率subsample 分裂时参与判断的最大特征数max_feature 60 0.1 12 4 0.77 10 到此，整体模型性能为0.8313，与workbench（0.8253）相比，提升了约0.006。 2.3.2.3 杀一记回马枪 还记得一开始我们对“子模型数”（n_estimators）和“学习率”（learning_rate）手下留情了吗？现在我们可以回过头来，调整这两个参数，调整的方法为成倍地放大“子模型数”，对应成倍地缩小“学习率”（learning_rate）。通过该方法，本例中整体模型性能又提升了约0.002。 2.4 “局部最优解” 目前来说，在调参工作中，广泛使用的仍是一些经验法则。Aarshay Jain对Gradient Tree Boosting总结了一套调参方法，其核心思想在于：对过程影响类参数进行调整，毕竟它们对整体模型性能的影响最大，然后依据经验，在其他参数中选择对整体模型性能影响最大的参数，进行下一步调参。这种方法的关键是依照对整体模型性能的影响力给参数排序，然后按照该顺序对的参数进行调整。如何衡量参数对整体模型性能的影响力呢？基于经验，Aarshay提出他的见解：“最大叶节点数”（max_leaf_nodes）和“最大树深度”（max_depth）对整体模型性能的影响大于“分裂所需最小样本数”（min_samples_split）、“叶节点最小样本数”（min_samples_leaf）及“叶节点最小权重总值”（min_weight_fraction_leaf），而“分裂时考虑的最大特征数”（max_features）的影响力最小。 Aarshay提出的方法和贪心的坐标下降法最大的区别在于前者在调参之前就依照对整体模型性能的影响力给参数排序，而后者是一种“很自然”的贪心过程。还记得2.3.2.1小节中我们讨论过“子模型数”（n_estimators）和“学习率”（learning_rate）的调参问题吗？同理，贪心的坐标下降法容易陷入“局部最优解”。对Random Forest调参时会稍微好一点，因为当“子模型数”调到最佳状态时，有时就只剩下诸如““分裂时参与判断的最大特征数”等Aarshay认为影响力最小的参数可调了。但是，对Gradient Tree Boosting调参时，遇到“局部最优解”的可能性就大得多。 Aarshay同样对Hackathon3.x进行了调参试验，由于特征提取方式的差异，参数赋值相同的情况下，本文的整体模型性能仍与其相差0.007左右（唉，不得不再说一次，特征工程真的很重要）。首先，在过程影响类参数的选择上，Aarshay的方法与贪心的坐标下降法均选择了“子模型数”为60，“学习率”为0.1。接下来，Aarshay按照其定义的参数对整体模型性能的影响力，按序依次对参数进行调整。当子模型影响类参数确定完成后，Aarshay的方法提升了约0.008的整体模型性能，略胜于贪心的坐标下降法的0.006。但是，回过头来继续调试“子模型数”和“学习率”之后，Aarshay的方法又提升了约0.01的整体模型性能，远胜于贪心的坐标下降法的0.002。 诶！诶！诶！少侠请住手！你说我为什么要在这篇博文中介绍这种“无用”的贪心的坐标下降法？首先，这种方法很容易凭直觉就想到。人们往往花了很多的时间去搞懂模型的参数是什么含义，对整体模型性能有什么影响，搞懂这些已经不易了，所以接下来很多人选择了最直观的贪心的坐标下降法。通过一个实例，我们更容易记住这种方法的局限性。除了作为反面教材，贪心的坐标下降法就没有意义了吗？不难看到，Aarshay的方法仍有改进的地方，在依次对参数进行调整时，还是需要像贪心的坐标下降法中一样对参数的“动态”影响力进行分析一下，如果这种影响力是“抖动”的，可有可无的，那么我们就不需要对该参数进行调整。 2.5 类别不均衡的陷阱 哈哈哈，这篇博文再次留了个陷阱，此段文字并不是跟全文一起发布！有人要说了，按照我的描述，Aarshay的调参试验不可再现啊！其实，我故意没说Aarshay的另一个关键处理：调参前的参数初始值。因为Hackathon3.x是一个类别不均衡的问题，所以如果直接先调试“最大深度”（max_depth），会发现其会保持默认值3作为最优解，而后面的调参中，“分裂所需最小样本数”（min_samples_split）、“叶节点最小样本数”（min_samples_leaf）再怎么调都没有很大作用。这是因为，正例样本远远小于反例，所以在低深度时，子模型就可能已经对正例过拟合了。所以，在类别不均衡时，只有先确定“叶节点最小样本数”（min_samples_leaf），再确定“分裂所需最小样本数”（min_samples_split），才能确定“最大深度”。而Aarshay设定的初始值，则以经验和直觉避开了这个险恶的陷阱。 如果实在觉得经验和直觉不靠谱，我还尝试了一种策略：首先，我们需要初步地调一次“子采样率”（subsample）和“分裂时考虑的最大特征数”（max_features），在此基础上依次调好“叶节点最小样本数”（min_samples_leaf）、“分裂所需最小样本数”（min_samples_split）以及“最大深度”（max_depth）。然后，按照Aarshay的方法，按影响力从大到小再调一次。通过这种方法，整体模型性能在未等比缩放过程影响类参数前，已达到约0.8352左右，比workbench相比，提升了约0.1，与Aarshay的调参试验差不多，甚至更好一点点。 回过头来，我们再次看看贪心的坐标下降法是怎么掉入这个陷阱的。在确定过程影响类参数后，贪心的坐标下降法按照“动态”的对整体模型性能的影响力大小，选择了“叶节点最小样本数”进行调参。这一步看似和上一段的描述是一致的，但是，一般来说，含随机性（“子采样率”和“分裂时考虑的最大特征数”先初步调过）的“叶节点最小样本数”要大于无随机性。举个例来说，因为增加了随机性，导致了子采样后，某子样本中只有一个正例，且其可以通过唯一的特征将其分类，但是这个特征并不是所有正例的共性，所以此时就要求“叶节点最小样本数”需要比无随机性时大。对贪心的坐标下降来说，“子采样率”和“分裂时考虑的最大特征数”在当下，对整体模型性能的影响比不上“叶节点最小样本数”，所以栽了个大跟头。 3 总结 在这篇博文中，我一反常态，花了大部分时间去试验和说明一个有瑕疵的方案。数据挖掘的工作中的方法和技巧，有很大一部分暂时还未被严谨地证明，所以有很大部分人，特别是刚入门的小青年们（也包括曾经的我），误以为其是一门玄学。实际上，尽管没有被严谨地证明，我们还是可以通过试验、分析，特别是与现有方法进行对比，得到一个近似的合理性论证。 另外，小伙伴们你们有什么独到的调参方法吗？请不要有丝毫吝啬，狠狠地将你们的独门绝技全释放在我身上吧，请大胆留言，残酷批评！ 4 参考资料 《使用sklearn进行集成学习——理论》 Complete Guide to Parameter Tuning in Gradient Boosting (GBM) in Python 坐标下降法 Digit Recognizer Hackathon3.x]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>西瓜书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[presto笔记]]></title>
    <url>%2F2017%2F07%2F12%2Fhadoop-spark%2Fpresto%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[增加kafka配置1、在/opt/presto-server-0.152/etc/catalog/增加文件kafka.properties，内容是 12345connector.name=kafkakafka.table-names=showup,click kafka.nodes=10.11.10.33:9092#kafka.hide-internal-columns-hidden=falsekafka.default-schema=rawdata 其中， kafka.table-names 跟topic名称相同，如果topic是带前缀的，比如rawdata.showup，那么schema就是rawdata。 kafka.hide-internal-columns-hidden 建表后有一系列内置column，默认这些是隐藏的，设为false使其显示。 kafka.default-schema 如果topic没有前缀，默认的schema是default，可以用该参数修改默认schema名称。 2、在etc的config.propreties中的datasources增加kafka 3、增加topic描述文件 放在etc/kafka目录中，以.json结尾，文件名和表名最好一致。例如： 12345678910111213141516171819202122232425&#123; &quot;tableName&quot;: &quot;click_dis&quot;, &quot;schemaName&quot;: &quot;rawdata&quot;, &quot;topicName&quot;: &quot;click_dis&quot;, &quot;key&quot;: &#123; &quot;dataFormat&quot;: &quot;raw&quot;, &quot;fields&quot;: [ &#123; &quot;name&quot;: &quot;kafka_key&quot;, &quot;type&quot;: &quot;VARCHAR&quot;, &quot;hidden&quot;: &quot;false&quot; &#125; ] &#125;, &quot;message&quot;: &#123; &quot;dataFormat&quot;: &quot;json&quot;, &quot;fields&quot;: [ &#123; &quot;name&quot;: &quot;dt_i&quot;, &quot;mapping&quot;: &quot;dt_i&quot;, &quot;type&quot;: &quot;BIGINT&quot; &#125; ] &#125;&#125; 4、重启presto服务器 1/opt/presto-server-0.152/bin/launcher restart 5、连接服务器测试 1/opt/jdk1.8.0_102/bin/java -jar /opt/presto-server-0.152/presto-cli --server 10.11.10.33:8082 --catalog kafka --schema rawdata 1select * from click limit 10; 其中内置column的意思是： Column name Type Description _partition_id BIGINT 包含这行数据的kafka partition的id _partition_offset BIGINT kafka partition的offset _segment_start BIGINT 在该segment中的最小offset _segment_end BIGINT 在该segment中的最大offset _segment_count BIGINT 对于一个未压缩的topic，_segment_start + _segment_count is equal to _partition_offset _message_corrupt BOOLEAN 为TRUE就说明解码器不能解析message _message VARCHAR UTF-8编码的string，只对text的topic有效 _message_length BIGINT message长度 _key_corrupt BOOLEAN 为TRUE就说明解码器不能解析key _key VARCHAR UTF-8编码的string _key_length BIGINT key的长度 查询key 1select count(1) from showup_dis where kafka_key like &apos;20161009%&apos;; 语法string转日期1date(b.data_date) 日期转string1cast(gpstime as varchar) 时间的string转timestamp1cast(&apos;2019-03-01 12:00:00&apos; as timestamp) 日期加减1cast((date(b.data_date) - interval &apos;1&apos; day) as varchar) 日期转时间戳1select to_unixtime(timestamp &apos;2018-12-27&apos;)*1000 两个日期相差的天数1date_diff(&apos;day&apos;, date(last_date), date(&apos;2019-03-05&apos;)) presto ui实现总数的统计/Users/david/david/git/yanagishima/src/main/java/yanagishima/service/PrestoServiceImpl.java 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768private void processData(StatementClient client, String datasource, String queryId, String query, PrestoQueryResult prestoQueryResult, List&lt;String&gt; columns, List&lt;List&lt;String&gt;&gt; rowDataList, long start, int limit, String userName) &#123; Duration queryMaxRunTime = new Duration(this.yanagishimaConfig.getQueryMaxRunTimeSeconds(datasource), TimeUnit.SECONDS); Path dst = getResultFilePath(datasource, queryId, false); int lineNumber = 0; int maxResultFileByteSize = yanagishimaConfig.getMaxResultFileByteSize(); int resultBytes = 0; try (BufferedWriter bw = Files.newBufferedWriter(dst, StandardCharsets.UTF_8); CSVPrinter csvPrinter = new CSVPrinter(bw, CSVFormat.EXCEL.withDelimiter('\t').withNullString("\\N").withRecordSeparator(System.getProperty("line.separator")));) &#123; csvPrinter.printRecord(columns); lineNumber++; while (client.isRunning()) &#123; Iterable&lt;List&lt;Object&gt;&gt; data = client.currentData().getData(); if (data != null) &#123; for(List&lt;Object&gt; row : data) &#123; List&lt;String&gt; columnDataList = new ArrayList&lt;&gt;(); List&lt;Object&gt; tmpColumnDataList = row.stream().collect(Collectors.toList()); for (Object tmpColumnData : tmpColumnDataList) &#123; if (tmpColumnData instanceof Long) &#123; columnDataList.add(((Long) tmpColumnData).toString()); &#125; else if (tmpColumnData instanceof Double) &#123; if(Double.isNaN((Double)tmpColumnData) || Double.isInfinite((Double) tmpColumnData)) &#123; columnDataList.add(tmpColumnData.toString()); &#125; else &#123; columnDataList.add(BigDecimal.valueOf((Double) tmpColumnData).toPlainString()); &#125; &#125; else &#123; if (tmpColumnData == null) &#123; columnDataList.add(null); &#125; else &#123; columnDataList.add(tmpColumnData.toString()); &#125; &#125; &#125; try &#123; csvPrinter.printRecord(columnDataList); lineNumber++; resultBytes += columnDataList.toString().getBytes(StandardCharsets.UTF_8).length; if(resultBytes &gt; maxResultFileByteSize) &#123; String message = String.format("Result file size exceeded %s bytes. queryId=%s, datasource=%s", maxResultFileByteSize, queryId, datasource); storeError(db, datasource, "presto", client.currentStatusInfo().getId(), query, userName, message); throw new RuntimeException(message); &#125; &#125; catch (IOException e) &#123; throw new RuntimeException(e); &#125; if (client.getQuery().toLowerCase().startsWith("show") || rowDataList.size() &lt; limit) &#123; rowDataList.add(columnDataList); &#125; else &#123; prestoQueryResult.setWarningMessage(String.format("now fetch size is %d. This is more than %d. So, fetch operation stopped.", rowDataList.size(), limit)); &#125; &#125; &#125; client.advance(); checkTimeout(db, queryMaxRunTime, start, datasource, "presto", queryId, query, userName); &#125; &#125; catch (IOException e) &#123; throw new RuntimeException(e); &#125; prestoQueryResult.setLineNumber(lineNumber); try &#123; long size = Files.size(dst); DataSize rawDataSize = new DataSize(size, DataSize.Unit.BYTE); prestoQueryResult.setRawDataSize(rawDataSize.convertToMostSuccinctDataSize()); &#125; catch (IOException e) &#123; throw new RuntimeException(e); &#125; &#125; 空值替换1coalesce]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
      <tags>
        <tag>presto</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark-streaming笔记]]></title>
    <url>%2F2017%2F07%2F12%2Fhadoop-spark%2Fspark%2Fspark-streaming%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[spark streaming的示例12345678910111213141516171819202122232425def main ( args : Array[ String ]): Unit = &#123; //关闭一些不必要的日志 Logger. getLogger ( "org.apache.spark" ). setLevel (Level. WARN ) Logger. getLogger ( "org.eclipse.jetty.server" ). setLevel (Level. OFF ) val conf = new SparkConf(). setAppName ( "wordStreaming" ). setMaster ( "local[2]" ). set ( "spark.sql.shuffle.partitions" , "10" ). set ( "spark.network.timeout" , "30s" ) . set ( "spark.shuffle.compress" , "true" ). set ( "spark.shuffle.spill.compress" , "true" ) . set ( "spark.shuffle.manager" , "sort" ) val sc = new SparkContext( conf ) // 创建 StreamingContext，1 秒一个批次。这里要用 sc ，而不是 conf ，因为 sc 已经创建了 val ssc = new StreamingContext( sc , Seconds ( 1 )) // 获得一个 DStream 负责连接 监听端口:地址 val lines = ssc . socketTextStream ( "192.168.37.129" , 9999 ) // 对每一行数据执行 Split 操作 val words = lines . flatMap ( _. split ( " " ) ) // 统计 word 的数量 val pairs = words . map ( word =&gt; ( word , 1 )) val wordCounts = pairs . reduceByKey (_ + _) // 输出结果 wordCounts . print () ssc . start () ssc . awaitTermination () &#125; 一开始会报错：Exception in thread “main” org.apache.spark.SparkException : Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at: org.apache.spark.SparkContext.( SparkContext.scala:82 ) 错误是在val ssc = new StreamingContext( conf , Seconds ( 1 )) 因为之前sc已经创建了，所以这里的conf要改成sc 之后，在 192.168.37.129上启动netcatnc -lk 9999输入hello world 再启动spark的程序，可以看出会输出1234Time: 1462790166000 ms(hello,1)(world,1) streaming读取本地文件1val lines = ssc.textFileStream(&quot;E:\\spark&quot;) 每当该文件夹内有新文件生成，就会自动读取 Spark Streaming将会监控dataDirectory目录，并且处理目录下生成的任何文件（嵌套目录不被支持）。需要注意一下三点：1 所有文件必须具有相同的数据格式2 所有文件必须在dataDirectory目录下创建，文件是自动的移动和重命名到数据目录下3 一旦移动，文件必须被修改。所以如果文件被持续的附加数据，新的数据不会被读取。对于简单的文本文件，有一个更简单的方法streamingContext.textFileStream(dataDirectory)可以被调用。文件流不需要运行一个receiver，所以不需要分配核。 spark streaming连接kafka12val topics = Set(&quot;test1&quot;)val kafkaParm = Map(&quot;metadata.broker.list&quot; -&gt; &quot;192.168.255.128:9092&quot;)]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
      <tags>
        <tag>spark-streaming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[4-1 TensorBoard网络结构]]></title>
    <url>%2F2017%2F07%2F12%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2Ftensorflow%2F%E5%9F%BA%E7%A1%80-%E8%8E%AB%E7%83%A6%E6%95%99%E7%A8%8B%2F4-1%20TensorBoard%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/4-1-tensorboard1/ 各种不同的优化器本次课程，我们会讲到Tensorflow里面的优化器。 Tensorflow 中的优化器会有很多不同的种类。最基本, 也是最常用的一种就是GradientDescentOptimizer。 Tensorflow提供了7种优化器： 搭建图纸首先从 Input 开始： 123# define placeholder for inputs to networkxs = tf.placeholder(tf.float32, [None, 1])ys = tf.placeholder(tf.float32, [None, 1]) 对于input我们进行如下修改： 首先，可以为xs指定名称为x_in: 1xs= tf.placeholder(tf.float32, [None, 1],name=&apos;x_in&apos;) 然后再次对ys指定名称y_in: 1ys= tf.placeholder(tf.loat32, [None, 1],name=&apos;y_in&apos;) 使用with tf.name_scope(&#39;inputs&#39;)可以将xs和ys包含进来，形成一个大的图层，图层的名字就是with tf.name_scope()方法里的参数。 1234with tf.name_scope(&apos;inputs&apos;): # define placeholder for inputs to network xs = tf.placeholder(tf.float32, [None, 1]) ys = tf.placeholder(tf.float32, [None, 1]) 接下来开始编辑layer ， 请看编辑前的程序片段 ： 12345678910def add_layer(inputs, in_size, out_size, activation_function=None): # add one more layer and return the output of this layer Weights = tf.Variable(tf.random_normal([in_size, out_size])) biases = tf.Variable(tf.zeros([1, out_size]) + 0.1) Wx_plus_b = tf.add(tf.matmul(inputs, Weights), biases) if activation_function is None: outputs = Wx_plus_b else: outputs = activation_function(Wx_plus_b, ) return outputs 这里的名字应该叫layer, 下面是编辑后的: 12345def add_layer(inputs, in_size, out_size, activation_function=None): # add one more layer and return the output of this layer with tf.name_scope(&apos;layer&apos;): Weights= tf.Variable(tf.random_normal([in_size, out_size])) # and so on... 在定义完大的框架layer之后，同时也需要定义每一个’框架‘里面的小部件：(Weights biases 和 activation function): 现在现对 Weights 定义： 定义的方法同上，可以使用tf.name.scope()方法，同时也可以在Weights中指定名称W。 即为： 1234567def add_layer(inputs, in_size, out_size, activation_function=None): #define layer name with tf.name_scope('layer'): #define weights name with tf.name_scope('weights'): Weights= tf.Variable(tf.random_normal([in_size, out_size]),name='W') #and so on...... 接着继续定义biases ， 定义方式同上。 12345678910def add_layer(inputs, in_size, out_size, activation_function=None): #define layer name with tf.name_scope(&apos;layer&apos;): #define weights name with tf.name_scope(&apos;weights&apos;) Weights= tf.Variable(tf.random_normal([in_size, out_size]),name=&apos;W&apos;) # define biase with tf.name_scope(&apos;Wx_plus_b&apos;): Wx_plus_b = tf.add(tf.matmul(inputs, Weights), biases) # and so on.... activation_function 的话，可以暂时忽略。因为当你自己选择用 tensorflow 中的激励函数（activation function）的时候，tensorflow会默认添加名称。 最终，layer形式如下： 1234567891011121314151617181920def add_layer(inputs, in_size, out_size, activation_function=None): # add one more layer and return the output of this layer with tf.name_scope(&apos;layer&apos;): with tf.name_scope(&apos;weights&apos;): Weights = tf.Variable( tf.random_normal([in_size, out_size]), name=&apos;W&apos;) with tf.name_scope(&apos;biases&apos;): biases = tf.Variable( tf.zeros([1, out_size]) + 0.1, name=&apos;b&apos;) with tf.name_scope(&apos;Wx_plus_b&apos;): Wx_plus_b = tf.add( tf.matmul(inputs, Weights), biases) if activation_function is None: outputs = Wx_plus_b else: outputs = activation_function(Wx_plus_b, ) return outputs 最后编辑loss部分：将with tf.name_scope()添加在loss上方，并为它起名为loss 1234567# the error between prediciton and real datawith tf.name_scope(&apos;loss&apos;): loss = tf.reduce_mean( tf.reduce_sum( tf.square(ys - prediction), eduction_indices=[1] )) 使用with tf.name_scope()再次对train_step部分进行编辑,如下： 12with tf.name_scope(&apos;train&apos;): train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) 我们需要使用 tf.summary.FileWriter() (tf.train.SummaryWriter() 这种方式已经在 tf &gt;= 0.12 版本中摒弃) 将上面‘绘画’出的图保存到一个目录中，以方便后期在浏览器中可以浏览。 这个方法中的第二个参数需要使用sess.graph ， 因此我们需要把这句话放在获取session的后面。 这里的graph是将前面定义的框架信息收集起来，然后放在logs/目录下面。 123sess = tf.Session() # get session# tf.train.SummaryWriter soon be deprecated, use followingwriter = tf.summary.FileWriter(&quot;logs/&quot;, sess.graph) 请确保你的 tensorboard 指令是在你的 logs 文件根目录执行的. 如果在其他目录下, 比如 Desktop 等, 可能不会成功看到图. 比如在下面这个目录, 你要 cd 到 project 这个地方执行 /project &gt; tensorboard --logdir logs 完整代码在 https://github.com/MorvanZhou/tutorials/blob/master/tensorflowTUT/tf14_tensorboard/full_code.py 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# View more python learning tutorial on my Youtube and Youku channel!!!# Youtube video tutorial: https://www.youtube.com/channel/UCdyjiB5H8Pu7aDTNVXTTpcg# Youku video tutorial: http://i.youku.com/pythontutorial"""Please note, this code is only for python 3+. If you are using python 2+, please modify the code accordingly."""from __future__ import print_functionimport tensorflow as tfdef add_layer(inputs, in_size, out_size, activation_function=None): # add one more layer and return the output of this layer with tf.name_scope('layer'): with tf.name_scope('weights'): Weights = tf.Variable(tf.random_normal([in_size, out_size]), name='W') with tf.name_scope('biases'): biases = tf.Variable(tf.zeros([1, out_size]) + 0.1, name='b') with tf.name_scope('Wx_plus_b'): Wx_plus_b = tf.add(tf.matmul(inputs, Weights), biases) if activation_function is None: outputs = Wx_plus_b else: outputs = activation_function(Wx_plus_b, ) return outputs# define placeholder for inputs to networkwith tf.name_scope('inputs'): xs = tf.placeholder(tf.float32, [None, 1], name='x_input') ys = tf.placeholder(tf.float32, [None, 1], name='y_input')# add hidden layerl1 = add_layer(xs, 1, 10, activation_function=tf.nn.relu)# add output layerprediction = add_layer(l1, 10, 1, activation_function=None)# the error between prediciton and real datawith tf.name_scope('loss'): loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction), reduction_indices=[1]))with tf.name_scope('train'): train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)sess = tf.Session()# tf.train.SummaryWriter soon be deprecated, use followingif int((tf.__version__).split('.')[1]) &lt; 12 and int((tf.__version__).split('.')[0]) &lt; 1: # tensorflow version &lt; 0.12 writer = tf.train.SummaryWriter('logs/', sess.graph)else: # tensorflow version &gt;= 0.12 writer = tf.summary.FileWriter("logs/", sess.graph)# tf.initialize_all_variables() no long valid from# 2017-03-02 if using tensorflow &gt;= 0.12if int((tf.__version__).split('.')[1]) &lt; 12 and int((tf.__version__).split('.')[0]) &lt; 1: init = tf.initialize_all_variables()else: init = tf.global_variables_initializer()sess.run(init)# direct to the local dir and run this in terminal:# $ tensorboard --logdir=logs]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark笔记-操作elastic search]]></title>
    <url>%2F2017%2F07%2F12%2Fhadoop-spark%2Fspark%2Fspark%E7%AC%94%E8%AE%B0-%E6%93%8D%E4%BD%9Celastic-search%2F</url>
    <content type="text"><![CDATA[最简单的例子1、在pom.xml中增加12345&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-spark_2.10&lt;/artifactId&gt; &lt;version&gt;2.2.1&lt;/version&gt;&lt;/dependency&gt; 2、在spark的main中导入org.elasticsearch.spark包12...import org.elasticsearch.spark._ 3、在spark的conf中增加如下配置1set(&quot;es.index.auto.create&quot;, &quot;true&quot;).set(&quot;es.nodes&quot;, &quot;192.168.37.129&quot;).set(&quot;es.port&quot;,&quot;9200&quot;) 其中，es.nodes是ElasticSearch的host 4、简单的写法如下1234567val conf = ...val sc = new SparkContext(conf) val numbers = Map("one" -&gt; 1, "two" -&gt; 2, "three" -&gt; 3)val airports = Map("arrival" -&gt; "Otopeni", "SFO" -&gt; "San Fran")sc.makeRDD(Seq(numbers, airports)).saveToEs("spark/docs") 也可以用case class来写1234567case class Trip(departrue: String, arrival: String) val upTrip = Trip("OTF", "SFO")val downTrip = Trip("MUC", "OTP")val rdd = sc.makeRDD(Seq(upTrip, downTrip))EsSpark.saveToEs(rdd, "spark/docs") 5、在Elastic Search的Sense中查询1GET /spark/docs/_search 返回123456789101112131415161718192021222324252627282930313233343536&#123; &quot;took&quot;: 4, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;spark&quot;, &quot;_type&quot;: &quot;docs&quot;, &quot;_id&quot;: &quot;AVSkEdTv9l_YEZuMmxgt&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;arrival&quot;: &quot;Otopeni&quot;, &quot;SFO&quot;: &quot;San Fran&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;spark&quot;, &quot;_type&quot;: &quot;docs&quot;, &quot;_id&quot;: &quot;AVSkEdZp9l_YEZuMmxgu&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;one&quot;: 1, &quot;three&quot;: 3, &quot;two&quot;: 2 &#125; &#125; ] &#125;&#125; 测试通过 与spark streaming结合123456789101112131415// 创建 StreamingContext，5秒一个批次val ssc = new StreamingContext(sc, Seconds(3))val lines = ssc.socketTextStream("192.168.37.129", 9999)// 对每一行数据执行 Split 操作val words = lines.flatMap(_.split(" "))// 统计 word 的数量val pairs = words.map(word =&gt; (word, 1))pairs.foreachRDD&#123;x =&gt;x.saveToEs("spark/words")&#125;ssc.start()ssc.awaitTermination() pairs是一个DSteamRDD，通过foreachRDD来遍历其中的每个RDD，对于每个RDD，可以saveToEs]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
      <tags>
        <tag>elastic search</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pig笔记]]></title>
    <url>%2F2017%2F07%2F12%2Fhadoop-spark%2Fpig%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[JOIN的优化1、Replicated Join ​ 当进行Join的一个表比较大，而其他的表都很小(能够放入内存)时，replicated join会非常高效。 ​ 在Join时使用 Using ‘replicated’语句来触发Replicated Join，大表放置在最左端，其余小表(可以有多个)放置在右端。 ​ 为了防止replicated join应用于大表的连接关系，pig会在这个连接关系复制的字节大小比pig.join.replicated.max.bytes属性的值大会失败 (default = 1GB)。 2、Skewed Join ​ 当进行Join的两个表的内连接时，一个表数据记录针对key的分布极其不均衡的时候使用，如果多于两个连接，要自己拆分成多个双表的连接。 ​ pig.skewedjoin.reduce.memusage属性的值指定了reduce可以占用堆内存的百分数，低的分数可以让pig执行更多的reducer，但是增加了复制的成本。性能好的范围值在0.1到0.4，但是这仅仅是一个范围。这个值取决于这个操作的可用的堆内存和这个输入的行数和倾斜。默认值是0.5。 ​ Skewed Join并没有专注于解决或者说的平衡这种不均匀的数据分布在reducer，而是确保这个Join连接能够完成而不是失败，但是会慢。他会增加5%的时间用于计算这个连接操作。 3、Merge Join ​ 当进行Join的两个表都已经用Join的键进行了排序，可以使用Merge Join。 可以在Join时使用Using ‘merge’语句来触发Merge Join，需要创建索引的表放置在右端。 另外，在进行Join之前，首先过滤掉key为Null的数据记录可以减少Join的数据量。 读取HDFS文件 比如下面加载某个模型到pig 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748private static Map&lt;String, InfoPredictor&gt; cache = new ConcurrentHashMap&lt;String, InfoPredictor&gt;(); private static AtomicBoolean isLoading = new AtomicBoolean(false); public static InfoPredictor getInfoPredictor(String modelFile) throws IOException, InterruptedException &#123; InfoPredictor model = cache.get(modelFile); if (model == null) &#123; // 保证在多线程下只执行一次 if(isLoading.compareAndSet(false, true)) &#123; model = load(modelFile);/* InfoPredict.loadDomainModel(modelFile);*/ cache.put(modelFile, model); &#125; else &#123; long maxLoadingTimeSecs = 120l; long loadingTimeSecs = 0l; while(null == cache.get(modelFile)) &#123; Thread.currentThread().sleep(5000l); loadingTimeSecs += 5; if(loadingTimeSecs &gt; maxLoadingTimeSecs) &#123; throw new IOException("try loading KNNSearcher model out times"); &#125; &#125; model = cache.get(modelFile); &#125; &#125; return model; &#125; public static InfoPredictor load(String modelFile) throws IOException &#123; FSDataInputStream fin = null; // pig下加载hdfs配置 Configuration conf = UDFContext.getUDFContext().getJobConf(); String hdfsPath = conf.get("fs.defaultFS") + modelFile; FileSystem fs = FileSystem.get(URI.create(hdfsPath), conf); fin = fs.open(new Path(hdfsPath)); ScoreIndex sIndex = new ScoreIndex(); BufferedReader in = null; in = new BufferedReader(new InputStreamReader(fin)); String line = ""; while((line = in.readLine()) != null)&#123; String[] scoreArr = line.split(","); for (int i = 0; i &lt; scoreArr.length; i++) &#123; sIndex.putScore(i + 1, (float)(Float.valueOf(scoreArr[i])/* * CorrectValue[i]*/)); &#125; &#125; sIndex.sortScore(); InfoPredict.setsIndex(sIndex); return new InfoPredictor(); &#125; UDF返回tupleUDF返回DataBag123456789101112131415161718192021222324252627282930313233public class searchPerKeywords extends EvalFunc&lt;DataBag&gt; &#123; private static TupleFactory mTupleFactory = TupleFactory.getInstance(); private static BagFactory mBagFactory = BagFactory.getInstance(); @Override public DataBag exec(Tuple input) throws IOException &#123; String keywordsFile = input.get(0).toString(); String modelFile = input.get(1).toString(); Integer featureLength = Integer.valueOf(input.get(2).toString()); Integer filtLevel = Integer.valueOf(input.get(3).toString()); String compressedFloatVec = input.get(4).toString(); if(StringUtils.isEmpty(compressedFloatVec)) &#123; return null; &#125; DataBag bag = mBagFactory.newDefaultBag(); try &#123; float[] vecs = WVUtils.String2floatArray(compressedFloatVec); KeyWordSearcher searcher = KeyWordSearcher.getSearcher(keywordsFile, modelFile); for(String matchedWord : searcher.MatchedWords(WVUtils.splitFeature(vecs, featureLength), filtLevel)) &#123; bag.add(mTupleFactory.newTuple(matchedWord)); &#125; &#125; catch (Exception e) &#123; return null; &#125; if(bag.size() == 0) &#123; return null; &#125; return bag; &#125; &#125; UDF返回tupleUDF的写法 123456789101112131415161718192021222324252627282930public Tuple exec(Tuple input) throws IOException &#123; if (input == null || input.size() != 1) &#123; return null; &#125; WebPageInfo nlp = null; String nlpPageInfo = (String) input.get(0); Tuple tup = mTupleFactory.newTuple(); if (StringUtils.isBlank(nlpPageInfo)) &#123; return null; &#125; try &#123; nlp = JSON.parseObject(nlpPageInfo, WebPageInfo.class); &#125; catch (Exception e) &#123; return null; &#125; tup.append(nlp.content); if (filter.isContaintSensitiveWord(nlp.content, 2)) &#123; tup.append("-1"); &#125; else &#123; tup.append("1"); &#125; return tup; &#125; pig的写法DetectUnsafeUrl.pig 1url_classify = FOREACH url_page_info GENERATE FLATTEN(DetectUnsafeUrl(info)) as (content, score); pig问题filter为null的报错1Exception while executing (Name: url_safety: Filter[bag] - scope-10 Operator Key: scope-10): org.apache.pig.backend.executionengine.ExecException: ERROR 0: Exception while executing [POUserFunc (Name: POUserFunc(com.buzzinate.pig.udf.webdata.DetectUnsafeUrl)[chararray] - scope-7 Operator Key: scope-7) children: null at []]: java.lang.NullPointerException 因为在udf中加了一段抛出异常 12345try &#123; nlp = JSON.parseObject(nlpPageInfo, WebPageInfo.class); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; 当有返回异常的时候，pig的filter就不能对返回结果做过滤了 1url_safety = FILTER url_classify BY score == &apos;-1&apos;; 这里就会报错。把抛出异常改为返回一个空字符串或者null就可以了]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
      <tags>
        <tag>pig</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark笔记-local开发]]></title>
    <url>%2F2017%2F07%2F12%2Fhadoop-spark%2Fspark%2Fspark%E7%AC%94%E8%AE%B0-local%E5%BC%80%E5%8F%91%2F</url>
    <content type="text"><![CDATA[mac安装spark环境https://blog.csdn.net/python_tty/article/details/72820469 安装scala2.10.7 下载后放到david/david/opt/scala 修改.bash_profile 12export SCALA_HOME=&quot;/Users/david/david/opt/scala-2.10.7&quot;export PATH=&quot;$SCALA_HOME/bin:$PATH&quot; 下载sparkhttp://spark.apache.org/downloads.html 下载的是1.6.3的版本 加到环境变量 12export SPARK_HOME=&quot;/Users/david/david/opt/spark-1.6.3-bin-hadoop2.6&quot;export PATH=&quot;$SPARK_HOME/bin:$PATH&quot; 允许ssh启动spark-shell进入到spark安装目录的sbin/目录下，执行 ./start-all 启动spark 进入到spark安装目录的bin/目录下，执行 ./spark-shell，看spark 是否安装成功 spark开发环境一、环境 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt; &lt;version&gt;1.6.1&lt;/version&gt; &lt;/dependency&gt; 二、编写local的spark程序1234567891011121314import org.apache.log4j.&#123; Level, Logger &#125;import org.apache.spark.&#123; SparkConf, SparkContext &#125;def main(args: Array[String]): Unit = &#123; //关闭一些不必要的日志 Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.WARN) Logger.getLogger(&quot;org.eclipse.jetty.server&quot;).setLevel(Level.OFF) val conf = new SparkConf().setAppName(&quot;wordSegname&quot;).setMaster(&quot;local[4]&quot;). set(&quot;spark.sql.shuffle.partitions&quot;,&quot;10&quot;).set(&quot;spark.network.timeout&quot;,&quot;30s&quot;) .set(&quot;spark.shuffle.compress&quot;,&quot;true&quot;).set(&quot;spark.shuffle.spill.compress&quot;,&quot;true&quot;) .set(&quot;spark.shuffle.manager&quot;,&quot;sort&quot;) val sc = new SparkContext(conf) &#125; 程序示例[WordTest.scala](../../../../code/spark/spark-buzzads/src/main/scala/com/iclick/word_segmentation/WordTest.scala) spark数据操作sparkRDD创建RDD1、数据集合 12 var data = Array(1,2,3,4,5,6,7,8,9) var disData = sc.parallelize(data,3) 创建一个RDD，包括1-9，分在3个分区 2、外部数据源 1textFile(path:String, minPartitions:Int) //第一个指定路径，第二个指定分区 RDD转换1、map，对每个元素执行一个指定函数产生新的RDD12val rdd1 = sc.parallelize(1 to 9, 3)val rdd2 = rdd1.map(x =&gt; x*2) spark hive locallocal的spark连接测试服务器的hive 1、复制hive环境上的hdfs-site.xml和hive-site.xml到项目的resource，如果有不识别的hostname则修改 2、写法类似于 1234567891011121314package com.mljr.spark.gps.sampleimport org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.apache.spark.sql.hive.HiveContextobject HiveTest &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf().setAppName("test").setMaster("local[2]") val sc = new SparkContext(conf) val sqlContext = new HiveContext(sc) sqlContext.sql("SELECT * FROM bdwh_tbl.tbl_s057_car_gps_position limit 1").collect.foreach(println) sc.stop() &#125;&#125; 报错 1java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.Session 这是因为在创建SQLContext实例的时候，要求spark编译的Hive版本和HiveMetaStore里面记录的Hive版本一致，我们可以通过配置hive.metastore.schema.verification参数来取消这种验证，这个参数的默认值是true，我们可以取消验证，配置如下： 1234567891011&lt;property&gt; &lt;name&gt;hive.metastore.schema.verification&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;description&gt; Enforce metastore schema version consistency. True: Verify that version information stored in metastore matches with one from Hive jars. Also disable automatic schema migration attempt. Users are required to manully migrate schema after Hive upgrade which ensures proper metastore schema migration. (Default) False: Warn if the version information stored in metastore doesn&apos;t match with one from in Hive jars. &lt;/description&gt; &lt;/property&gt; 再报错 1Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: java.net.UnknownHostException: slave1 控制日志输出级别修改log4j.properties 1log4j.rootCategory=WARN, console 默认是INFO, console， 更丰富的可以是STDOUT, DEBUG, INFO, console 精简的是WARN, console no snappyjava in java.library.pathhttps://stackoverflow.com/questions/30039976/unsatisfiedlinkerror-no-snappyjava-in-java-library-path-when-running-spark-mlli 1234567891011&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.xerial.snappy&lt;/groupId&gt; &lt;artifactId&gt;snappy-java&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt; and then adding 12345&lt;dependency&gt; &lt;groupId&gt;org.xerial.snappy&lt;/groupId&gt; &lt;artifactId&gt;snappy-java&lt;/artifactId&gt; &lt;version&gt;1.0.5&lt;/version&gt;&lt;/dependency&gt;]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka笔记]]></title>
    <url>%2F2017%2F07%2F12%2Fhadoop-spark%2Fkafka%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[多个消费者读取一个topic，多次消费不同消费者设置不同的groupid 12val kafkaParm = Map("metadata.broker.list" -&gt; "localhost:9092","auto.offset.reset" -&gt; "smallest", "group.id" -&gt; "davidtopi1c1") 每个消费者不会重复消费数据1234 kafkaStrem.foreachRDD&#123; rdd=&gt; km.updateZKOffsets(rdd)&#125;]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive笔记-orc格式读写]]></title>
    <url>%2F2017%2F07%2F12%2Fhadoop-spark%2Fhive%E7%AC%94%E8%AE%B0-orc%E6%A0%BC%E5%BC%8F%E8%AF%BB%E5%86%99%2F</url>
    <content type="text"><![CDATA[需要引入的包： 123456789101112131415&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-core&lt;/artifactId&gt; &lt;version&gt;2.6.0-mr1-cdh5.9.1&lt;/version&gt; &lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-common&lt;/artifactId&gt; &lt;version&gt;1.1.0-cdh5.9.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-exec&lt;/artifactId&gt; &lt;version&gt;1.1.0-cdh5.9.1&lt;/version&gt; &lt;/dependency&gt; 1234567891011121314151617181920212223242526272829val fs = FileSystem.get(conf);val prop = Config.getConfig("config.properties")println(sdf.format(new Date))val readerOpts = OrcFile.readerOptions(conf)val reader = OrcFile.createReader(new Path(iaxReqPath+"/ds=17-08-21/20170821000000antispam_2529853576425433.orc"), readerOpts)val inspector = reader.getObjectInspector().asInstanceOf[StructObjectInspector]val count = reader.getNumberOfRowsinfo("the count is: " + count.toString())val fields = inspector.getAllStructFieldRefs()fields.foreach &#123; x =&gt; println(x.getFieldObjectInspector.getCategory) &#125;val records = reader.rows()var n=0val loop = new Breaksloop.breakable(&#123; while(records.hasNext)&#123; if (n&gt;5) loop.break val row = records.next(null) val valueList = inspector.getStructFieldsDataAsList(row) info(valueList.get(10).toString) info(row.toString()) n = n+1 &#125;&#125;)]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>orc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop笔记]]></title>
    <url>%2F2017%2F07%2F12%2Fhadoop-spark%2Fhadoop%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[查看namenode在集群的每个节点上都有配置文件， vim /etc/hadoop/conf/hdfs-site.xml 1234567&lt;property&gt; &lt;name&gt;dfs.ha.namenodes.iclick&lt;/name&gt; &lt;value&gt;srv-buzz-cloudpmnn1.buzz.com,srv-buzz-cloudpmnn2.buzz.com&lt;/value&gt;&lt;/property&gt; 常用IO操作123456789public static void testIOUtils() throws IOException &#123;Configuration conf = new Configuration();FileSystem fs = FileSystem.get(conf);Path p = new Path(&quot;/test/in/point&quot;);FSDataInputStream fdis = fs.open(p);IOUtils.copyBytes(fdis, System.out, conf,false);IOUtils.closeStream(fdis);fs.close();&#125;]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[广告反作弊]]></title>
    <url>%2F2017%2F07%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A%2F%E5%B9%BF%E5%91%8A%E5%8F%8D%E4%BD%9C%E5%BC%8A%2F</url>
    <content type="text"><![CDATA[秒针发布过《互联网广告反作弊技术白皮书》 腾讯灯塔联手秒针、AdMaster发布 腾讯《2017广告反欺诈白皮书》日均校验40亿广告请求，识别的作弊比率在15%左右。部分行业和campaign中，作弊率有60%。 欺诈手段 虚假流量。 某APP在地推时，新增用户暴涨。95%的新增用户的共同特征： ROM编译机名称一致；指令逃逸差异数与正常用户不一致；CPU结构为X86，为PC机模拟器；文件系统类型的差异度与正常用户不一致。 某APP的新增用户中，工作室批量刷量的特征有： 安装时间具有明显的规律；手机APP安装数量一致；明显的地域集中性。 2.2 黑产技术 1、广告作弊类型分类 1）模拟器刷量：电脑模拟器刷量、手机软件模拟刷量、脚本刷量（录制行为跑循环任务）。 2）真假机用户：储备大量手机或者sim卡，利用数据线push命令到手机，手机执行命令。 3）静默安装（真机真用户假行为）。人工方式或网络传播方式将木马/具有再分发能力的应用植入到用户手机，形成僵尸网络，在用户无感知的情况下，完成App的下载、激活和删除等一系列操作。 4）羊毛党（真机真用户真行为假动机）：登录一次就删除应用、使用时长极短、留存率极低。在大部份的情况下，这种用户对业务的健康发展并无太大价值。 5）广告素材、篇幅偷换（不可见）。“1 像素广告”指在用户的手机屏幕上只展示1个像素大小的广告。这种广告，用户看不见，但统计工具可以统计到，仍然会作为曝光广告与广告主结算，给广告主带来经济损失。 6）以次充好（不匹配）。部分媒体会将广告主原本定向的一线城市用户偷偷换成二三线城市用户，达到以次充好的目的。 应对方法2.3 反作弊技术 1）用户群体数据检测。低阶技术，常见方式有： a.看留存率。真实的用户的留存曲线是一条平滑的指数衰减曲线，如果发现留存曲线存在陡升陡降的异常波动，则判断刷量者干预了数据。 b.看用户终端、网络信息。如根据经验分析渠道新增用户或者启动用户的设备排名;2G、3G、4G的使用比例分布是否正常等。 c.看用户注册信息。比如说注册昵称的分布和规律等。 以上操作均效率低下。 2）用户行为特征分析 a.单个指标。与黑IP库进行比对，是否为黑名单IP、是否为代理IP;与IMEI库进行对比，是否为为黑IMEI; b.群体指标。用户的IP、IMEI、机型、OS、位置信息、运营商、接入方式的分布是否符合先验数据的分布 c.设备一致性验证。CPU、制造商、MAC地址、IMEI、机型、操作系统的一致性验证。 简单粗暴，没有黑白转换机制，误判率高。 这些方法容易被刷量者利用。在某电商专业Android app游戏激活、注册、留存、付费、应用市场好评平台上，买主只需要很小的代价，即可刷出完全符合正常用户规律的留存率、IP分布机型分布使用时长等。 3）终端特征分析+云端交叉验证 “查”模型负责找寻黑产界的新型作弊方式，提升整体模型的覆盖率 “杀”模型负责准确识别恶意份子 “验”模型通过多业务交叉验证，负责保证“查”、“杀”模型的准确率 终端识别模块（灯塔SDK稽核模块）:该模块主要是采用机器学习算法选取系统中所有可用的信息作为特征，然后对这些特征进行运算得到该设备的指纹，可以有效识别手机模拟器、修改系统参数等行为。 基于规则的识别模块（业务自有模块）:该模块一般是通过业务经验及对历史可疑渠道的总结形成的反作弊规则，可以理解为多维组合规则，一般需根据业务成本、对渠道的容忍度设置关键变量的阈值。 基于数据挖掘的识别模块（灯塔云端模块）:该模块主要从硬件信息、用户活跃、用户行为进行多维度、多业务交叉验证，分别计算每个维度下面的不同特征值，结合决策树、LR、贝叶斯网络等多种算法进行精准的定位。分类 为了增强识别的准确性和稳定性，模块之间、模块内部均采用集成学习方法的思想，其核心思想是在模块内对同一个训练集训练不同的分类器，然后把这些分类器结合起来构成一个最终的分类器，而每一个模块可以针对不同的作弊手段进行识别，再把模块与模块结合，才能识别所有的作弊手段。 admaster《广告反欺诈研究报告》2016今年 1 月 29 日和 3 月 2 日,宝洁公司首席品牌官 Marc Pritchard 分别在美国互动广告局(InteractiveAdvertising Bureau, IAB)和美国广告主协会(Association of National Advertisers, ANA)两个年度重要峰会上进行主题发言,针对数字广告透明度和可见性标准的言论引发了全球营销圈和数字行业的热议。宝洁呼吁业界在四个方面采取行动: 数字广告采纳一套统一的可见性测量标准; 贯彻第三方测评机构 的验证审核; 提倡全面透明的代理公司合同机制; 预防广告欺诈。 AdMaster先后推出了 BlueAir、定投识别 (VOA)、监播实录 (SNAP) 等技术产品,逐步建立了“全方位、深层次、多角度”的广告反欺诈解决方案。尤其在程序化购买中,AdMaster 在投放前预判 (Pre-bid),即事前广告反欺诈技术。 BlueAir可以对智能电视IP、地域、频次以及User Agent等维度的异常流量进行甄别，将行为逻辑上不正常的设备加入黑名单，从而保障广告投放的安全。同时，与海信、康佳、创维、欢网等硬件厂家共同建立智能电视设备白名单，以便从设备维度进一步甄别流量真实性。 定投识别 监播实录 投放前预判。在流量请求、广告未展现时,根据历史流量质量进行排查,从而提前避免广告在无效流量上的投放。 无效流量类型1、广告可见性问题引发的低质量流量 目前媒体的环境导致广告不易可见。AdMaster 在 AdServing 广告投放管理技术上能够实现广告可见性的预估判断。在多种广告形式的后测方面,通过独创的模型评估广告可见性表现。 2、机器人无效流量（Non-Human Traffic） 从最初的 cookie 和 IP不变的前提下,反复刷新页面和点击广告,造成广告曝光和点击的增加,到通过木马或者恶意程序控制海量人肉刷机、伪造大量 IP 与设备信息进行模拟访问、或将 IP 和 cookie、User Agent 一起进行轮替的流量造假方式,都属于机器人无效流量。 BlueAir 广告反欺诈技术能够结合历史异常数据,能够在流量请求、广告未展现时,即根据历史流量质量判断此流量的质量,在投放前通过 Pre-bid 判断出流量的异常,杜绝流量造假现象发生。 3、视频类无效流量 (1)针对剧目投偏现象,AdMaster 定投识别 (VOA) 功能通过 Referrer/ 剧目 ID 等方式解析 广告曝光时所播放剧目名称,并与广告主定投的剧目内容进行匹配。在移动端定投评估中,高诚信度的视频媒体也提供高度配合。剧目投偏比例,一目了然。 (2)针对时有发生的曝光代码调用,但是素材未正确展示的广告欺诈现象,AdMaster 利用监播实录 (SNAP)功能,采用类似于“神秘访客”概念的方法从海量抽样监测,将视频内容播放前的所有贴片内容录制下来,并通过图像识别与适当的素材进行对比,判断素材是否被正确展示,以及是否按照要求展示。 4、智能电视无效流量 支持智能电视广告的监测模式一般有 3 种 : 分别是第三方 SDK 监测、C2S 和 S2S 两种 API。前两种相比 S2S 更为安全,也更容易监测流量异常情况,可以说 C2S API 是智能电视广告监测安全的起点和基础(S2S API 传输方式目前很难识别无效流量)。 inmobi《移动广告反作弊白皮书》没技术 反作弊措施 1）剔除自动流量。 识别机器人脚本，分析展示和点击的质量。 2）数据信号双重检测。 将媒体共享的人群信息和从SDK收集的信息比对，对所有无效信息定位和删除。 [部分有关 广告联盟作弊 与反作弊资料收集]（http://cwiki.apachecn.org/pages/viewpage.action?pageId=4882639） roadmap中，AD fraud的业界措施 监控广告投放的效果； 保存曝光设备的ip 记录用户在landing页的行为 网页分析 基于数据挖掘的识别模块（灯塔云端模块）:该模块主要从硬件信息、用户活跃、用户行为进行多维度、多业务交叉验证，分别计算每个维度下面的不同特征值，结合决策树、LR、贝叶斯网络等多种算法进行精准的定位。为了增强识别的准确性和稳定性，模块之间、模块内部均采用集成学习方法的思想，其核心思想是在模块内对同一个训练集训练不同的分类器，然后把这些分类器结合起来构成一个最终的分类器，而每一个模块可以针对不同的作弊手段进行识别，再把模块与模块结合，才能识别所有的作弊手段。 这个方法未来肯定要实现的，大致是通过集成学习的方式，综合硬件信息，网页特征，用户行为 最初我们只有url信息，提取url的特征判断投放的网页是否安全； 后面对每条 前期，通过分析url内容，判断网页是否安全，是否为虚假网页，网页内容与广告品牌是否有冲突。 后面，分析曝光的详细信息，包括设备信息，用户特征，判断是否为作弊流量；分析网页元素和广告位置，判断广告可见性。 再后面，在对url和黑名单有一定积累的基础上，在投放或竞价前检测投放环境，主动识别和屏蔽非安全流量]]></content>
      <categories>
        <category>计算广告</category>
      </categories>
      <tags>
        <tag>反作弊</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elastic search笔记]]></title>
    <url>%2F2017%2F07%2F12%2FElastic%20Search%2Felastic-search%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[过滤字段中为空的通过filter中的exists，如 12345678910111213141516171819GET iclick_persona/iclick/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123;&quot;term&quot;: &#123; &quot;province&quot;: &#123; &quot;value&quot;: &quot;CN_IN_SG&quot; &#125; &#125;&#125; ], &quot;filter&quot;: &#123; &quot;exists&quot;: &#123; &quot;field&quot;: &quot;articles.titles&quot; &#125; &#125; &#125; &#125;&#125; 大索引按天拆开好像不能直接从A索引中抽取一部分到B索引 python从文件插入到ES12345678910111213141516171819202122232425def set_pois_data(es, input_file, index_name=&quot;pois&quot;, doc_type_name=&quot;iclick&quot;): #读入数据 fp = open(input_file) #创建ACTIONS ACTIONS = [] for line in fp: fields = line.strip(&apos;\n&apos;).split(&quot;\t&quot;) name = fields[0] location = fields[2] + &quot;,&quot; + fields[3] action = &#123; &quot;_index&quot;: index_name, &quot;_type&quot;: doc_type_name, &quot;_source&quot;: &#123; &quot;name&quot; : name, &quot;pois&quot; : location, &#125; &#125; #print action ACTIONS.append(action) # 批量处理 success, _ = bulk(es, ACTIONS, index=index_name, raise_on_error=True) print(&apos;Performed %d actions&apos; % success) 复制index的数据到另一个12345678import pyesconn = pyes.es.ES(&quot;http://10.xx.xx.xx:8305/&quot;)search = pyes.query.MatchAllQuery().search(bulk_read=1000)hits = conn.search(search, &apos;store_v1&apos;, &apos;client&apos;, scan=True, scroll=&quot;30m&quot;, model=lambda _,hit: hit)for hit in hits: #print hit conn.index(hit[&apos;_source&apos;], &apos;store_v2&apos;, &apos;client&apos;, hit[&apos;_id&apos;], bulk=True)conn.flush() 统计数组的元素个数的sum、avg1234567891011121314151617181920212223242526GET iclick_persona/iclick/_search?size=0&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123;&quot;term&quot;: &#123; &quot;device&quot;: &#123; &quot;value&quot;: &quot;PC&quot; &#125; &#125;&#125; ], &quot;filter&quot;: [ &#123;&quot;script&quot;:&#123;&quot;script&quot;:&quot;doc[&apos;articles.domains&apos;].values.length&gt;1&quot;&#125;&#125; ] &#125; &#125;, &quot;aggs&quot;: &#123; &quot;NAME&quot;: &#123; &quot;avg&quot;: &#123; &quot;script&quot;: &#123; &quot;inline&quot;: &quot;doc[&apos;articles.domains&apos;].values.length&quot; &#125; &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>Elastic Search</category>
      </categories>
      <tags>
        <tag>elastic search</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单聚类算法]]></title>
    <url>%2F2017%2F07%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E7%AE%80%E5%8D%95%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[canopy 12345678while D is not empty select element d from D to initialize canopy c remove d from D Loop through remaining elements in D if distance between d_i and c &lt; T1 : add element to the canopy c if distance between d_i and c &lt; T2 : remove element from D end add canopy c to the list of canopies C]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[路径规划算法]]></title>
    <url>%2F2017%2F07%2F12%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E8%B7%AF%E5%BE%84%E8%A7%84%E5%88%92%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[TSP、VRP VRPVehicle Routing Problem 假设在一个供求关系系统中，车辆从货源取货，配送到对应的若干配送点。车辆存在最大载货量，且配送可能有时间限制。需要合理安排取货时间，组织适当的行车路线，使用户需求得到满足，同时使某个代价函数最小，比如总工作时间最少、路径最短等。 可以看出TSP问题是VRP问题的一种简单特殊形式。因此，VRP也是一种NP hard 问题。 目前解决此种问题的办法有多种，主要以启发式算法为主。包括退火算法、遗传算法、蚁群算法、禁忌算法等。 https://blog.csdn.net/ldotn/article/details/53366882 C-W节约算法和遗传算法 C-W节约算法：基本思想是把各点单独与货源相连，构成若干条仅含一个配送点的线路，总费用为两倍从原点到各点的距离费用；然后计算将点 i 和点 j 连接在一条线路上费用节约值： S(i,j) = C_{oi}+C_{io}+C_{oj}+C_{jo}-(C_{oi}+C_{ij}+C_{jo}) \\= C_{oi}+C_{oj}+C_{ij}具体步骤：（1）计算节约值S(i,j)，按从大到小排序（2）考虑表格中最大元素Smax（i,j）Smax（i,j），对应点i和j，按条件进行操作： 若i和j均不在构成线路上，则得到线路 o -&gt; i -&gt;j -&gt;o，转到（3） 若i或j在已构成线路上，但不是内点 0 -&gt; i -&gt;o，则可连接，转到（3） 若i和j位于已构成不同线路上，且均不是内点，则连接得到线路，转到（3） 若i和j位于已构成的同一线路，则不连接，转到（3）（3）划去第i行和第j列，即i点不能再到其他点，j点也不能由其他店到达（4）若所有元素均被划去，则得到完整线路，算法终止；否则，在没有划去的元素中选最大元素，转至（2）。 作者：LDOTN来源：CSDN原文：https://blog.csdn.net/ldotn/article/details/53366882?utm_source=copy版权声明：本文为博主原创文章，转载请附上博文链接！ 专栏 | 从架构到算法，详解美团外卖订单分配内部机制https://mp.weixin.qq.com/s?src=11&amp;timestamp=1539566532&amp;ver=1183&amp;signature=9jq4HLmibsnw8OrowBhYA9CXJN79KssHyV0G4Kra2LrXD95V-RHwziumoBV1pwJEsK-4kEiHILRjucx7fM4kyp*s61FEF3tEYd40N3CFBEvakfY1Kj0-pWEjqIYTm4G-&amp;new=1 外卖订单的分配问题一般可建模为带有若干复杂约束的 DVRP（Dynamic Vehicle Routing Problem）问题。这类问题一般可表述为：有一定数量的骑手，每名骑手身上有若干订单正在配送过程中，在过去一段时间（如 1 分钟）内产生了一批新订单，已知骑手的行驶速度、任意两点间的行驶距离、每个订单的出餐时间和交付时间（骑手到达用户所在地之后将订单交付至用户所需的时间），那么如何将这批新订单在正确的时间分配至正确的骑手，使得用户体验得到保证的同时，骑手的配送效率最高。]]></content>
      <categories>
        <category>算法与数据结构</category>
      </categories>
      <tags>
        <tag>树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[本地操作服务器hadoop]]></title>
    <url>%2F2017%2F07%2F11%2Fhadoop-spark%2F%E6%9C%AC%E5%9C%B0%E6%93%8D%E4%BD%9C%E6%9C%8D%E5%8A%A1%E5%99%A8hadoop%2F</url>
    <content type="text"><![CDATA[配置1、添加hadoop的必备jar包 123456789101112&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.5.0-cdh5.3.2&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.5.0-cdh5.3.2&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; 2、如果只要读取数据，直接配namenode地址12val conf = new Configurationconf.set("fs.defaultFS","hdfs://10.11.40.207:9000/") 2 读取数据12345678910111213141516171819import org.apache.hadoop.fs.Pathimport org.apache.hadoop.conf.Configurationimport org.apache.hadoop.fs.FileSystemimport java.io.InputStreamReaderimport java.io.BufferedReaderval fs = FileSystem.get(conf)val path = new Path("/user/david/testdir/a.txt")val in = fs.open(path)val buff = new BufferedReader(new InputStreamReader(in))var str = buff.readLinewhile (str != null) &#123; println(str) str = buff.readLine&#125;buff.closein.closefs.close]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2000%2F01%2F01%2F%E6%80%BB%E7%BB%93%E4%B8%8E%E6%80%9D%E8%80%83%2F%E5%BE%85%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"></content>
      <categories>
        <category>总结与思考</category>
      </categories>
  </entry>
</search>
