<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title></title>
    <url>%2F2019%2F11%2F11%2FKafka%2Fkafka%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[下载0.10.0.0，解压在 1/Users/david/david/git/kafka-0.10.0.0-src 编译 12cd /Users/david/david/git/kafka-0.10.0.0-srcgradle idea 用idea导入目录，选择gradle导入。 4、启动项目 kafka的启动入口类是 1core/src/main/scala/kafka/Kafka 启动需要添加server.properties配置文件路径。 然后，这样启动是不会打印日志的，所以要添加log4j.properties文件。在config文件夹下有log4j.properties文件，在core项目下创建resources文件夹，把log4j.properties文件复制过去。 然后启动项目，这样就可以看到打印日志了。(记得修改server.properties里的zk地址和logDir地址) 问题记录 1、执行gradle idea时，遇到报错 123456789101112131415161718192021222324&gt; Configure project :Building project &apos;core&apos; with Scala version 2.10.6FAILURE: Build failed with an exception.* Where:Build file &apos;/Users/david/david/git/kafka-0.10.0.0-src/build.gradle&apos; line: 230* What went wrong:A problem occurred evaluating root project &apos;kafka-0.10.0.0-src&apos;.&gt; Failed to apply plugin [class &apos;org.gradle.api.plugins.scala.ScalaBasePlugin&apos;] &gt; Could not create task &apos;:core:compileScala&apos;. &gt; No such property: useAnt for class: org.gradle.api.tasks.scala.ScalaCompileOptions* Try:Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.* Get more help at https://help.gradle.orgDeprecated Gradle features were used in this build, making it incompatible with Gradle 6.0.Use &apos;--warning-mode all&apos; to show the individual deprecation warnings.See https://docs.gradle.org/5.4.1/userguide/command_line_interface.html#sec:command_line_warningsBUILD FAILED in 43s 解决方法： 123456vim build.gradleScalaCompileOptions.metaClass.daemonServer = trueScalaCompileOptions.metaClass.fork = trueScalaCompileOptions.metaClass.useAnt = falseScalaCompileOptions.metaClass.useCompileDaemon = false 继续报错 123456789101112131415161718192021222324&gt; Configure project :Building project &apos;core&apos; with Scala version 2.10.6FAILURE: Build failed with an exception.* Where:Build file &apos;/Users/david/david/git/kafka-0.10.0.0-src/build.gradle&apos; line: 373* What went wrong:A problem occurred evaluating root project &apos;kafka-0.10.0.0-src&apos;.&gt; Failed to apply plugin [id &apos;org.scoverage&apos;] &gt; Could not create an instance of type org.scoverage.ScoverageExtension. &gt; You can&apos;t map a property that does not exist: propertyName=testClassesDir* Try:Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.* Get more help at https://help.gradle.orgDeprecated Gradle features were used in this build, making it incompatible with Gradle 6.0.Use &apos;--warning-mode all&apos; to show the individual deprecation warnings.See https://docs.gradle.org/5.4.1/userguide/command_line_interface.html#sec:command_line_warningsBUILD FAILED in 4s 问题原因是gradle用的是新版，与旧版不兼容。修改以下行为 1classpath &apos;org.scoverage:gradle-scoverage:2.5.0&apos;]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F10%2F20%2FKafka%2F%E6%89%8B%E5%8A%A8%E5%AE%89%E8%A3%85Kafka%2F</url>
    <content type="text"><![CDATA[部署下载0.10.0.0版 http://kafka.apache.org/downloads 解压到 1/Users/david/soft/kafka_2.10-0.10.0.0 启动 12bin/zookeeper-server-start.sh config/zookeeper.properties &amp;bin/kafka-server-start.sh config/server.properties &amp; 单机模式一个副本一个分区create创建topic，list查看topic列表 1234$ bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test[2019-10-20 11:03:58,460] INFO Created log for partition [test,0] in /tmp/kafka-logs with properties &#123;compression.type -&gt; producer, message.format.version -&gt; 0.10.0-IV1, file.delete.delay.ms -&gt; 60000, max.message.bytes -&gt; 1000012, message.timestamp.type -&gt; CreateTime, min.insync.replicas -&gt; 1, segment.jitter.ms -&gt; 0, preallocate -&gt; false, min.cleanable.dirty.ratio -&gt; 0.5, index.interval.bytes -&gt; 4096, unclean.leader.election.enable -&gt; true, retention.bytes -&gt; -1, delete.retention.ms -&gt; 86400000, cleanup.policy -&gt; delete, flush.ms -&gt; 9223372036854775807, segment.ms -&gt; 604800000, segment.bytes -&gt; 1073741824, retention.ms -&gt; 604800000, message.timestamp.difference.max.ms -&gt; 9223372036854775807, segment.index.bytes -&gt; 10485760, flush.messages -&gt; 9223372036854775807&#125;. (kafka.log.LogManager)[2019-10-20 11:03:58,462] INFO Partition [test,0] on broker 0: No checkpointed highwatermark is found for partition [test,0] (kafka.cluster.Partition) 123$ bin/kafka-topics.sh --list --zookeeper localhost:2181 testtest 接着启动一个producer，并生成一条消息 123$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic testfirst messagesecond message 启动一个consumer 1234$ bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginningfirst messagesecond message 一个副本多个分区创建多个分区的my-partitioned-topic主题，然后用describe查看详细信息，验证3个分区，且每个分区有以下5个属性。 topic partition。从0开始 Leader。当前分区负责读写的节点，只有主副本才会接受消息读写。 Replicas。分区的复制节点列表，与topic的副本数量有关，默认只有一个副本，即主副本。 Isr。同步状态的副本，是Replicas的子集，必须是存活的，且都能赶上主副本。 123$ bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 3 --topic my-partitioned-topic[2019-10-20 12:10:19,194] INFO Created log for partition [my-partitioned-topic,2] in /tmp/kafka-logs with properties &#123;compression.type -&gt; producer, message.format.version -&gt; 0.10.0-IV1, file.delete.delay.ms -&gt; 60000, max.message.bytes -&gt; 1000012, message.timestamp.type -&gt; CreateTime, min.insync.replicas -&gt; 1, segment.jitter.ms -&gt; 0, preallocate -&gt; false, min.cleanable.dirty.ratio -&gt; 0.5, index.interval.bytes -&gt; 4096, unclean.leader.election.enable -&gt; true, retention.bytes -&gt; -1, delete.retention.ms -&gt; 86400000, cleanup.policy -&gt; delete, flush.ms -&gt; 9223372036854775807, segment.ms -&gt; 604800000, segment.bytes -&gt; 1073741824, retention.ms -&gt; 604800000, message.timestamp.difference.max.ms -&gt; 9223372036854775807, segment.index.bytes -&gt; 10485760, flush.messages -&gt; 9223372036854775807&#125;. (kafka.log.LogManager) 123456$ bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-partitioned-topicTopic:my-partitioned-topic PartitionCount:3 ReplicationFactor:1 Configs: Topic: my-partitioned-topic Partition: 0 Leader: 0 Replicas: 0 Isr: 0 Topic: my-partitioned-topic Partition: 1 Leader: 0 Replicas: 0 Isr: 0 Topic: my-partitioned-topic Partition: 2 Leader: 0 Replicas: 0 Isr: 0 为了验证消息是否写到主题分区的日志目录，可以查看日志目录，其中以log结果的是二进制日志格式，可以用strings查看。可以看到，刚才创建的还没有消息，但kafka已经提前创建了文件夹和对应的文件 12345678910111213141516$ tree /tmp/kafka-logs/tmp/kafka-logs├── cleaner-offset-checkpoint├── meta.properties├── my-partitioned-topic-0│ ├── 00000000000000000000.index│ └── 00000000000000000000.log├── my-partitioned-topic-1│ ├── 00000000000000000000.index│ └── 00000000000000000000.log├── my-partitioned-topic-2│ ├── 00000000000000000000.index│ └── 00000000000000000000.log├── recovery-point-offset-checkpoint└── replication-offset-checkpoint 1$ strings /tmp/kafka-logs/my-partitioned-topic-0/00000000000000000000.log 插入几条记录 1$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic my-partitioned-topic 其中recovery-point-offset-checkpoint记录offset 1234567$ cat recovery-point-offset-checkpoint03 =&gt; 一共有3个分区my-partitioned-topic 1 0 =&gt; 单个partition的offsetmy-partitioned-topic 2 0my-partitioned-topic 0 0 1234567$ cat replication-offset-checkpoint03my-partitioned-topic 2 2my-partitioned-topic 1 2my-partitioned-topic 0 2 再启动一个消费者并订阅，可以看到消息没有按照生产的顺序读取。这是因为Kafka不保证全局顺序，只保证分区级别的消息顺序。 12345678$ bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic my-partitioned-topic --from-beginningm2m5m1m4m3m6 分布式模式搭建过程略。 查看上面创建的有多个副本的主题分区信息，副本数为3，每个分区都会分布在3个服务端节点上: Isr的含义 分区中的所有副本统称为AR（Assigned Repllicas）。所有与leader副本保持一定程度同步的副本（包括Leader）组成ISR（In-Sync Replicas），ISR集合是AR集合中的一个子集。 消息会先发送到leader副本，然后follower副本才能从leader副本中拉取消息进行同步，同步期间内follower副本相对于leader副本而言会有一定程度的滞后。前面所说的“一定程度”是指可以忍受的滞后范围，这个范围可以通过参数进行配置。与leader副本同步滞后过多的副本（不包括leader）副本，组成OSR(Out-Sync Relipcas),由此可见：AR=ISR+OSR。在正常情况下，所有的follower副本都应该与leader副本保持一定程度的同步，即AR=ISR,OSR集合为空。 Leader副本负责维护和跟踪ISR集合中所有的follower副本的滞后状态，当follower副本落后太多或者失效时，leader副本会吧它从ISR集合中剔除。如果OSR集合中follower副本“追上”了Leader副本，之后再ISR集合中的副本才有资格被选举为leader，而在OSR集合中的副本则没有机会（这个原则可以通过修改对应的参数配置来改变）————————————————原文链接：https://blog.csdn.net/weixin_43975220/article/details/93190906 下面手动停止一个 Kafka服务节点，来模拟 Kafka集群中一个服务端节点出现者机的情况: 再次查看创建的主题的信息，可以看到原先落在主副本编号为3的节点，分区的主副本会转移。比如， my-replicated-topic3主题的P1分区，主副本原先是3，现在变为2。另外，虽然每个分区的Replicas没有变化，但 Isr都不再包含 3: 这里能看到编号为2的节点有3个分区在上面。 为了保证主副本会负载均衡到所有的服务器 ， 可以执行preferred-replica-election脚本来手动执行平衡操作， 即选择Replicas的第一个副本作为分区的主副本 。 比如，分区Pl的副本集等于[3, 2, 0]，当前的主副本编号为2，那么就要将分区Pl的主副本从现有的2迁移到3上。 执行完平衡操作后 ， 再次查看分区信息，可 以看到分区 的主副本确实发生了转移 :]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F09%2F30%2FKafka%2Fkafka%E6%B6%88%E8%B4%B9%E8%80%85%2F</url>
    <content type="text"><![CDATA[Consumer概念Consumer属于group。一个group里的consumer订阅同一个主题，每个consumer接收一部分分区的消息。 假设主题T1有4个分区，我们创建了consumer C1，它是群组G1中唯一消费者，用它订阅主题T1。消费者C1将收到主题T1全部4个分区的消息。 若在G1中新增一个C2，那么每个consumer分别从两个分区接收消息。 若有4个consumer，每个都分配一个分区 如果继续增加consumer数量且超过分区数，那么有一部分会闲置 往组里增加consumer是横向伸缩消费能力的主要方式。consumer经常做一些高延迟的事情，比如复杂计算等。这种情况下，单个consumer无法跟上数据生成的速度，所以可以增加更多consumer，让它们分担负载，每个consumer只处理部分分区的消息，这就是横向伸缩的主要手段。 若新增一个G2，将从T1接收所有消息，与G1不影响。 消费者群组与分区再均衡在一个group中，一个新的consumer加入，读取的是本由其他consumer读取的消息。当一个consumer被关闭或发生崩溃时，它就离开群组，原本读取的分区由其他消费者来读取。 在topic发生变化时，比如管理员添加了新的分区，会发生分区重分配。分区所有权从一个consumer转到另一个consumer，成为再均衡（rebalance）。有了rebalance，就可以放心的添加或移除消费者。不过在正常情况下，不希望发生rebalance。 在rebalance时，consumer无法读取消息，会造成整个group一段时间的不可用。 另外，当分区被重新分配给另一个消费者时，消费者当前的读取状态会丢失，它有可能还需要去刷新缓存，在它重新恢复状态之前会拖慢应用。 如何安全的Rebalance？如何避免不必要的Rebalance？ consumer通过向群组协调器（coordinator，也是一个broker）发送心跳来维持他们和群组的从属关系以及它们对分区的所有权关系。只要consumer以正常时间间隔发送心跳，就被认为是活跃的，说明还在读取分区里的消息。consumer会在轮询消息或提交偏移量时发送心跳。若停止发送心跳时间过长，会话就会过期，coordinator认为它已经死亡，就会触发一次Rebalance。 当消费者崩溃，停止读取消息，coordinator会等待几秒，确认死亡才会触发Rebalance。在这几秒钟，死掉的consumer不会读取分区消息。在清理consumer时，consumer会通知coordinator它将要离开群组，coordinator会立即触发一次Rebalance。（死掉了还能通知？） 心跳在最近版本中的变化 在0.10.1中，引入一个独立的心跳线程。可以在轮询消息的空档发送心跳。这样，发送心跳的频率和消息轮询的频率之间是相互独立的。在新版kafka中，可以指定消费者在离开群组并触发Rebalance之前可以有多长时间不进行消息轮询，这样可以避免活锁（livelock），比如有时应用并没有崩溃，只是由于某些原因导致无法正常运行。这个配置与session.timeout.ms是相互独立的，后者用于控制检测consumer发生崩溃的时间与停止发送心跳的时间。没懂 创建Kafka消费者在读取消息前，需先创建一个KafkaConsumer对象。有三个必要的属性：bootstrap.servers，key.deserializer和value.deserializer。 第四个属性group.id不是必须的，指定KafkaConsumer属于哪一个group。创建不属于任何一个群组的消费者也是可以的，只是这样不太常见。 订阅topicsubscribe()方法接受一个topic列表作为参数 1consumer.subscribe(Collections.singletonList(&quot;customerCountries&quot;)); 也可以传入一个正则，可以匹配多个主题。如果有人创建了新的主题，并且主题的名字与正则匹配，那么会立即触发一次Rebalance，消费者就可以读取新添加的主题。 1consumer.subscribe(&quot;test.*&quot;); 轮询消息轮询是consumer API的核心，通过一个简单的轮询向服务器请求数据。一旦consumer订阅主题，轮询就处理所有细节，包括群组协调、分区Rebalance、发送心跳和获取数据。开发者通过API来处理从分区返回的数据。 1234567891011try &#123; while (true) &#123; // 持续对kafka进行轮询，否则会被认为已经死亡，它的分区会被移交给group中其他消费者。poll传入的是一个超时时间，指定了方法在多久之后可以返回，不管有没有可用的数据都要返回。若设为0，poll()会立即返回，否则会在指定毫秒数内一直等待broker返回数据 ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); // poll返回一个记录列表。每条记录包含topic、分区、offset和key value。通过遍历来逐条处理。 for (ConsumerRecord&lt;String, String&gt; record: records) &#125;&#125; finally &#123; // 在退出前关闭消费者。网络连接和socket也会随之关闭，并立即触发一次Rebalance，而不是等待group的coordinator发现它不再发送心跳并认定它已死亡，因此那样需要更长的时间。 consumer.close(); &#125; 轮询不仅是获取数据这么简单。在第一次调用新消费者的poll()时，会负责查找GroupCoordinator，然后加入group，接受分配的分区。如果发生Rebalance，整个过程也是在轮询期间进行的。心跳也是从轮询发出去的。所以，要确保轮询期间所做的任何处理工作都应该尽快完成。 消费者的配置fetch.min.bytes 指定从服务器获取记录的最小字节数。broker在收到consumer请求时，会等到有足够数据才返回。这样可以降低consumer和broker的工作量。 fetch.max.wait.ms 通过上一个参数告诉kafka，在有足够数据的时候才返回。而该参数是指定broker的等待时间，默认是500ms。 若该参数设为100ms，fetch.min.bytes设为1MB，在kafka接收到consumer请求后，要么返回1MB，要么在100ms后返回所有可用的数据，就看哪个条件先得到满足。 max.partitions.fetch.bytes 指定服务器从每个分区里返回给消费者的最大字节数。默认值是1MB。即，KafkaConumser.poll()从每个分区返回的记录不超过该参数指定的字节。 该参数的值必须比broker能接收的最大消息字节数（max.message.size）大，否则consumer可能无法读取，导致consumer一直挂起重试。不懂 session.timeout.ms 指定consumer在认为死亡前，可以与服务器断开连接的时间。默认是3s。如果在指定时间诶没有发送心跳，就认为是已经死亡，会触发Rebalance。该参数与headrbeat.interval.ms紧密相关，headrbeat.interval.ms指定poll向coordinator发送心跳的频率，session.timeout.ms则指定可以多久不发送心跳。所以，一般要同时修改这两个。一般headrbeat.interval.ms是session.timeout.ms的三分之一。 如果参数值过小，可以更快检测和恢复崩溃节点，但长时间的轮询或GC可能导致非预期的Rebalance。若设的大，可以减少意外的Rebalance，但检测节点崩溃需要更长时间。 auto.offset.reset 指定consumer在读取一个没有offset的分区，或offset无效情况下该如何处理。默认是latest，是指在offset无效时，从最新记录开始读取数据（从consumer启动后生成的记录）。另一个值是earliest，是指在offset无效时，从起始位置读取分区记录。 enable.auto.commit 指定consumer是否自动提交offset，默认是true。为了尽量避免重复数据和丢失，可以设为false。如果设为true，可以通过auto.commit.interval.ms来控制提交的频率。 partition.assignment.strategy PartitionAssignor根据给定的consumer和topic，决定哪些分区应该被分配给哪个consumer。kafka有两个默认的分配策略。 Range 该策略会把主题的若干个连续的分区分配给消费者。假设悄费者C1和消费者C2同时订阅了主题T1和主题T2 ,井且每个主题有3 个分区。那么消费者C1有可能分配到过两个主题的分区0 和分区1 ,而消费者C2分配到这两个主题的分区2 。因为每个主题拥有奇数个分区,而分配是在主题内独立完成的,第一个消费者最后分配到比第二个消费者更多的分区。只要使用了Range 策略,而且分区数量无怯被消费者数量整除,就会出现这种情况。 RangeRobin 该策略把主题的所有分区逐个分配给消费者。如果使用RangeRobin策略来给消费者C1和消费者C2分配分区,那么消费者C1将分到主题T1的分区0和分区2以及主题T2的分区1 ,消费者C2将分配到主题T1的分区1以及主题口的分区0和分区2。一般来说，如果所有消费者都订阅相同的主题(这种情况很常见)，RangeRobin策略会给所有消费者分配相同数量的分区(或最多就差一个分区)。 client.id broker用它来标记从客户端发过来的消息，通常被用在日志、度量指标和配额里。 max.poll.records 用于控制单词调用call()方法能够返回的记录数量，可以帮你控制在轮询里需要处理的数据量。 receive.buffer.bytes和send.buffer.bytes socket在读写数据时用到的TCP缓冲区也可以设置大小。如果它们被设为-1 ,就使用操作系统的默认值。如果生产者或消费者与broker处于不同的数据中心内,可以适当增大这些值,因为跨数据中心的网络一般都有比较高的延迟和比较低的带宽。 提交和偏移量每次调用poll，总是返回由producer写入Kafka但还没有被consumer读取过的记录，我们因此可以追踪哪些记录是被group里面哪个消费者读取的。 把更新分区当前位置的操作叫做提交。 P80 问题： 1]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F09%2F28%2FKafka%2Fkafka%E7%94%9F%E4%BA%A7%E8%80%85%2F</url>
    <content type="text"><![CDATA[生产者概览两种使用场景： 1、不允许丢失或重复，低延迟，对吞吐要求高 2、可以少量丢失或重复，高延迟 向kafka发送消息的主要步骤 从创建一个producerRecorder对象开始。 1）若指定分区，则分区器不做事情，直接把指定分区返回；若没指定分区，则分区器会根据ProducerRecord对象的键来选择一个分区。 2）记录被添加到一个记录批次中，这个批次所有消息会被发送到相同的主题和分区上。 3）有一个独立线程把这些记录批次发送到相应broker上。 4）服务器接收后返回一个响应。若成功写入，就返回一个RecordMetaData对象，包含主题和分区信息，以及记录再分区中的offset。若失败，会重试几次。 创建生产者3个必选属性：bootstrap.servers 指定broker地址清单，格式为host:port。清单里不需要包含所有broker地址，生产者会从给定broker中查找其他broker信息。不过建议至少要提供两个，防止其中一个宕机。 key.serializer 将java对象设置为字节数组。 value.serializer 12345private Properties kafkaProps = new Properties();kafkaProps.put(&quot;bootstrap.servers&quot;, &quot;broker1:9092,broker2:9092);kafkaProps.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);kafkaProps.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);producer = new KafkaProducer&lt;String, String&gt;(kafkaProps); 实例化producer后，接下里可以发送消息，3种方式。 发送并忘记（fire-and-forget) 同步发送 send()之后返回一个Future对象，调用get()方法进行等待，就可以知道是否发送成功。 KafkaProducer一般会发生两类错误。 一类是可重试错误，可以通过重发消息来解决。比如链接错误，通过再次建立连接；“无主（no leader）”错误，通过重新为分区选举首领来解决。 一类无法通过重试解决，比如“消息太大”异常。对于这类，kafka不会重试，直接抛出异常。 异步发送 12345678910private class DemoProducerCallback implements Callback &#123; @Override public void onCompletion(RecordMetadata recordMetadata, Exception e) &#123; if (e != null) &#123; e.printStackTrace(); &#125; &#125;&#125;producer.send(record, new DemoProducerCallback()); 生产者的配置acks 指定必须要有多少个分区收到消息，生产者才会认为消息写入是成功的。对消息丢失的可能性有重要影响。 若acks=0，producer成功写入消息前不会等待任何来自服务器的响应。即，若当中出现问题，producer无感知，消息也就丢了。好处是，不需要等待响应，可以以网络能支持的最大速度发送消息，吞吐量最大。 若acks=1，只要集群首领收到消息，consumer就会收到成功响应。若无法到达leader节点（比如leader节点崩溃，新的leader还没被选举出来），会收到一个错误响应，然后producer会重发消息。但若一个没有收到消息的节点成为新首领，消息还是会丢失 没懂。此时，吞吐量取决于用的是同步还是异步（同步慢，异步快）。 若acks=all，最安全，保证不止一个服务器接收到消息，就算有服务器崩溃，整个集群仍然可以运行。不过延迟是最高的，因为要等到不止一个节点。 buffer.memory 设置缓冲区大小，producer用它缓冲要发到服务器的消息。若应用发送消息的速度&gt; 发送到服务器的速度，会导致producer空间不足。此时，send()要么被阻塞，要么抛异常。取决于如何设置block.on.buffer.full参数（在0.9.0.0中被替换为max.block.ms，表示在抛出异常前可以阻塞一段时间）。 compression.type 压缩类型，snappy，gzip或lz4。若比较关注性能和网络带宽，推荐用snappy。gzip占CPU多，但压缩比高，在带宽有限的情况下使用。 retries 重试次数。若达到这个次数，返回错误。默认时，每次重试间隔100ms，通过retry.backoff.ms改变这个时间间隔。 建议在设置这两个参数前，测试一下恢复一个崩溃节点需要多久（比如所有分区选举出首领需要多久），让总的重试时间比kafka从崩溃中恢复的时间长。否则，producer会过早的放弃重试。 因为producer会自动重试，代码就没必要处理可重试的错误，只需要处理不可重试的错误或重试次数超出上限的情况。 batch.size 当有多个消息要发送到同一个分区，producer会放到同一个批次里。参数指定一个批次可以使用的内存大小，按照字节数计算（而不是消息个数）。一个批次不用等到被填满，半满甚至只包括一个的也可能被发送（根据linger.ms）。所以就算设置的很大，也不会造成延迟，只是会占用更多内存而已。但如果设置太小，producer需要更频繁发送，会增加一些额外开销。 linger.ms consumer在发送批次前等待更多消息加入批次的时间。KafkaProducer会在批次填满，或linger.ms达到上限后发送批次。 client.id 可以是任意字符串，用它识别消息来源，还可以用在日志和配额指标里。 max.in.flight.requests.per.connection 指定producer在收到响应前可以发多少消息。值越高，就会占用越多内存。 不是每次发一个批次吗，这个参数有什么作用。貌似设的大可以发送多个批次。 设为1可以保证消息是按照发送顺序写入服务器的，即使发生了重试。 timeout.ms, request.timeout.ms和metadata.fetch.timeout.msrequest.timeout.ms指定了生产者在发送数据时等待服务器返回响应的时间。 metadata.fetch.timeout.ms指定producer在获取元数据（比如目标分区首领是谁）时等待服务器响应的时间。若超时，要么重试，要么返回错误。 timeout.ms指定broker等待同步副本返回消息确认的时间，与acks的配置相匹配——如果再指定时间内没有收到同步副本的确认，则broker返回一个错误。 max.block.ms 在调用send()方法或使用partitionsFor()方法获取元数据时producer的阻塞时间。当producer的缓冲区已满，或没有可用元数据时，这些方法就会阻塞。阻塞超过参数时，producer抛出超时异常。 max.request.size 控制producer发送的请求大小。假设为1MB，则可以发送的单个最大消息是1MB，或生产者可以在单个请求里发送一个批次，该批次包含1000个消息，每个消息大小是1KB。另外，borker对可接受消息也有限制（message.max.bytes），所以两边配置要可以匹配。 receive.buffer.bytes和send.buffer.bytes 指定TCP socket接收和发送数据包的缓冲区大小。若设为-1，就是用os的默认值。若producer和consumer与broker处于不同数据中心，则可以适当增大这些值，因为跨数据中心的网络一般有较高延迟和较低带宽。 序列化器分区若key设为null，且使用默认分区器，则记录将被随机发送到topic内各个可用的分区上。分区器采用轮询（Round Robin）算法将消息均衡分区到各个分区上。 若key不为null，且使用默认分区器，则kafka会对key进行hash（用kafka自己的hash算法，即使改变java版本，hash值不会变化），再根据hash值把消息映射到特定分区上。 这里的关键是，同一个key会分到同一个分区，所以在进行映射时，会使用topic所有分区，而不仅是可用分区。那么，若写入数据的分区不可用，就会发生错误，但这种情况很少发生。 只有不改变partition个数的情况下，key和partition的映射才能保持不变。不过，一旦topic增加新的分区，新的数据可能被写到其他分区上。所以，如果要用key来映射分区，最好在创建topic时就把分区规划好，而且永远不要增加新分区。 实现自定义分区策略按key可能造成不平衡，需要给一些大的key分配单独分区，再用hash处理其他的。 没懂的记录 1 2]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F09%2F27%2FKafka%2F%E7%99%BE%E9%97%AEKafka%2F</url>
    <content type="text"><![CDATA[1、Topic的配置项num.partitions topic包含多少个分区，一旦设定后，只能增加不能减少；若需要减少，得新增一个topic。 kafka通过partitions对topic进行横向扩展，当有新broker加入时，通过分区个数实现集群的负载均衡。一般的，topic的partitions的个数大于broker个数。 log.retention.hours 过期时间。默认值是168小时，也就是一周。还有两个其他参数log.retention.minutes和log.retention.ms。如果指定多个，kafka优先使用最小值。 log.retention.bytes 通过保留的字节数判断过期。作用在每个分区。即假设有8个partitions，且设为1G，那该topic最多保留8GB的数据。所以，当topic的分区个数增加时，整个topic可保留的数据也随之增加。如果也设置了hours，只要任意一个条件满足，消息就会被删除。 log.segment.bytes 上面设置在日志片段上，而不是在单个消息上。当消息到达broker时，它们被追加到分区的当前日志片段上。当日志片段大小超过该参数上限（默认是1GB），当前日志片段会被关闭，产生一个新的日志片段。 如果一个日志片段被关闭，就开始等待过期。该参数值越小，就会频繁关闭和分配新文件，从而降低磁盘写入的整体效率。 如果topic消息量不大，那么如何调整该参数大小就很重要。比如，一个topic每天100MB，而该参数是默认值，那么10天才能填满一个日志片段。由于日志片段在被关闭之前是不会过期的，所以如果log.retention,ms=1周，则segment最多需要17天才会过期。 segment大小也会影响通过timestamp获取offset。在获取offset时，kafka会检查分区最后修改时间大于指定timestamp的segment，让该segment的前一个的最后修改时间小于指定timestamp。然后，kafka返回该segment文件开头的offset。 segment越小，结果越准确。（不准有什么影响） message.max.bytes broker设置这个来限制单个消息的大小，默认是1MB。若生产者发生的消息超过这个大小，不仅不会被接收，还会收到broker返回的错误信息。跟其他与字节相关配置参数一样，该参数指的是压缩后的消息大小，实际大小可以大于这个值。 该值对性能有很大影响。值越大，负责网络连接和请求的线程就要花更多时间处理这些请求。还会增加磁盘写入块的大小，从而影响IO吞吐量。 在服务端和客户端之间协调消息大小 客户端的fetch.message.max.bytes必须与服务端的参数进行协调。若该值更小，则消费者无法获取大的消息，导致消费者被阻塞。 在集群里的broker配置replica.fetch.max.bytes时，遵循同样原则。 2、如何选定分区数量需要考虑： 1、topic需要多大吞吐量，每秒100KB还是1GB？ 2、从单个分区读取数据的最大吞吐量是多少？每个分区一般都会有一个消费者，如果知道消费者将写入数据库的速度不超过每秒50MB，那从一个分区读取数据的吞吐量不需要超过每秒50MB。 3、每个broker包含的分区个数，可用的磁盘空间和网络带宽。 4、如果消息按照不同键写入分区，则为已有主题增加分区就会困难。（为什么，键已经分布好了？） 5、单个broker对分区个数有限制，因为分区越多，占用内存越多，完成首领选举需要时间越长。 如果估算出topic的吞吐量和消费者的吞吐量，可以用 topic吞吐量 / 消费者吞吐量 = 分区个数即，如果每秒要从topic写入和读取1GB数据，且每个消费者每秒可处理50MB数据，那至少需要20个分区，这样可以让20个消费者同时读取这些分区。 每个分区的大小，经验值是25GB。 3、过期时间是怎么判断的根据时间保留数据是通过检查磁盘上日志文件片段的最后修改时间来实现的。最后修改时间是日志片段的关闭事件，也就是文件里最后一个消息的timestamp。 但是，如果用管理工具在服务器间移动分区，最后修改时间就不准了。时间误差可能导致分区过多的保留数据。 4、需要多少broker取决于： 1、需要多少磁盘空间来保留数据，以及单个broker有多少空间可用。若整个集群要10TB，每个broker可以保存2TB，则需要5个broker。若启用数据复制，至少还需要一倍的空间，但要取决于配置的复制系数是多少。 2、集群处理请求的能力。通常与网络接口处理客户端流量的能力有关，特别是当多个消费者存在或在数据保留期间流量发生波动。（这一段不太懂P25) 5、kafka和zookeeper消费者可以选择将偏移量提交给zookeeper或kafka，还可以选择提交偏移量的时间间隔。 若提交到zookeeper，那么在每个提交时间上，消费者会为每个消息的分区往zookeeper写入一次偏移量。合理提交间隔是1分钟，因为这刚好是group的某个消费者发生失效时能读取到重复消息的时间。 并且，这些提交对zookeeper来说流量不算小，特别是当集群中有多个消费者的时候。若zookeeper无法处理太大的流量，就有必要使用长一点的提交时间间隔。 kafka在0.9.0.0后，引入一个新的消费者接口，允许broker直接维护group信息、topic信息、offset信息。建议使用该接口，消除对zookeeper的依赖。 6、kafka如何保证顺序kafka可以保证同一个partition的消息是有序的。即，若producer按顺序发送消息，broker可以按顺序写入partition，consumer会按同样顺序消费。 若吧retires&gt;0，同时把max.in.flight.requests.per.connection&gt;1，那么，若第一个批次消息写入失败，而第二个批次写入成功，broker会重试第一个批次，若第一个批次也成功，则两个批次的顺序就反过来了。 如果要求消息有序，那么写入成功也是很关键的，所以不建议retires=0，可以把max.in.flight.requests.per.connection=1，这样producer发送第一批时，就不会有其他消息发给broker。不过这样会严重影响producer的吞吐量，所以只有在对消息的顺序有严格要求时才这么做。 看看canal怎么实现的 7、如何安全的Rebalance？如何避免不必要的Rebalance？consumer通过向群组协调器（coordinator，也是一个broker）发送心跳来维持他们和群组的从属关系以及它们对分区的所有权关系。只要consumer以正常时间间隔发送心跳，就被认为是活跃的，说明还在读取分区里的消息。consumer会在轮询消息或提交偏移量时发送心跳。若停止发送心跳时间过长，会话就会过期，coordinator认为它已经死亡，就会触发一次Rebalance。 当消费者崩溃，停止读取消息，coordinator会等待几秒，确认死亡才会触发Rebalance。在这几秒钟，死掉的consumer不会读取分区消息。在清理consumer时，consumer会通知coordinator它将要离开群组，coordinator会立即触发一次Rebalance。（死掉了还能通知？） 没讲完，后面补充 8、分配分区是怎样的一个过程？当consumer要加入group时，会向coordinator发送一个JoinGroup请求。第一个加入group的consumer将成为“群主”。群主从coordinator那里获取群组的成员列表（列表中包含了所有最近发送过心跳的消费者，他们被认为是活跃的）。并负责给每个consumer分配分区。它使用一个实现了PartitionAssingor接口来决定哪些分区应该被分配给哪个消费者。 kafka内置两种分配策略。 分配完成后，群主把分配情况列表发送给coordinator，coordinator再把这些信息发给所有消费者。每个consumer只能看到自己的分配消息，只有群主知道group中所有consumer的分配信息。这个过程在Rebalance时重复发生。 9、消费时如何保证线程安全在同一个group中，无法让一个线程运行多个consumer。按照规则，一个consumer使用一个线程。最好是把消费者的逻辑封装在自己的对象里，然后使用java的ExecutorService启动多个线程，使每个消费者运行在自己的线程上。 10、消费者如何提交offsetconsumer往一个_consumer_offset的特殊topic发送消息，消息里包含每个分区的offset。如果消费者一直运行，offset就没有意义。但如果触发Rebalance后，每个consumer可能分配到新的分区，而不是之前处理的那个，为了能继续之前工作，consumer需要读取每个分区最后一次提交的offset。 若提交的offset &lt; 处理的最后一个消息的offset，那么处于中间的就会重复处理。 反之，就会丢失消息 KafkaConsumer API有多种提交offset的方式。 自动提交若enable.auto.commit=true，则每过5s，会自动把poll()接收到的最大offset提交。提交时间间隔由auto.commit.interval.ms控制，默认5s。自动提交也是在轮询中进行的。每次轮询会检查是否该提交offset。 可能的风险是： ​ 假设使用默认的5s一次，在最近一次提交后3s发生了Rebalance，发生之后，consumer从最后一次提交的offset开始读。但此时offset已经落后了3s，所以3s内的消息会被重复处理。 所以，自动提交时可能有重复消息的风险。 提交当前offset把auto.commit.offset=false，让程序决定何时提交。使用commitSync()提交offset最简单可靠。这个api会提交由poll()方法返回的最新offset，提交成功后马上返回，否则抛出异常。 采用这种方式，要确保处理完所有记录后调用commitSync，否则有消息丢失的风险。 异步提交手动提交的不足时，在broker对提交请求作出回应之前，程序会一直阻塞，这样会限制吞吐量。可以通过降低提交频率来提升吞吐量，但如果发生Rebalance，会增加重复消息的量。 使用异步提交，只管发送提交，无需等待响应。 12345while(true) &#123; ... // 提交最后一个offset consumer.commitAsync();&#125; 这种方式也有重复消费的风险： ​ 假设我们发出一个请求用于提交偏移量2000 ,这个时候发生了短暂的通信问题,服务器收不到请求,自然也不会作出任何响应。与此同时,我们处理了另外一批消息,并成功提交了偏移量3000。如commitAsync()重新尝试提交偏移量2000 ,它有可能在偏移量3000之后提交成功。这个时候如果发生再均衡,就会出现重复消息。 异步提交也支持回调 重试异步提交 我们可以使用一个单调递增的序列号来维护异步提交的顺序。在每次提交偏移量之后或在回调里提交偏移量时递增序列号。在进行重试前,先检查回调的序列号和即将提交的偏移量是否相等,如果相等,说明没有新的提交,那么可以安全地进行重试。如果序列号比较大,说明有一个新的提交已经发送出去了,应该停止重试。 同步和异步组合提交偶尔的提交失败不进行重试问题不大，但是在关闭consumer或Rebalance前最后一次提交，要确保提交成功。这个时候一般组合用commitAsync()和commitSync()。 Kafka如何选举？在Kafka里面，Master/Slave的选举，有2步：第1步，先通过ZK在所有机器中，选举出一个KafkaController；第2步，再由这个Controller，决定每个partition的Master是谁，Slave是谁。因为有了选举功能，所以kafka某个partition的master挂了，该partition对应的某个slave会升级为主对外提供服务。 与RocketMQ的区别转自：https://www.jianshu.com/p/c474ca9f9430 namesrv VS zk​ 1、我们可以对比下kafka和rocketMq在协调节点选择上的差异，kafka通过zookeeper来进行协调，而rocketMq通过自身的namesrv进行协调。 ​ 2、kafka在具备选举功能，在Kafka里面，Master/Slave的选举，有2步：第1步，先通过ZK在所有机器中，选举出一个KafkaController；第2步，再由这个Controller，决定每个partition的Master是谁，Slave是谁。因为有了选举功能，所以kafka某个partition的master挂了，该partition对应的某个slave会升级为主对外提供服务。 ​ 3、rocketMQ不具备选举，Master/Slave的角色也是固定的。当一个Master挂了之后，你可以写到其他Master上，但不能让一个Slave切换成Master。那么rocketMq是如何实现高可用的呢，其实很简单，rocketMq的所有broker节点的角色都是一样，上面分配的topic和对应的queue的数量也是一样的，Mq只能保证当一个broker挂了，把原本写到这个broker的请求迁移到其他broker上面，而并不是这个broker对应的slave升级为主。 ​ 4、rocketMq在协调节点的设计上显得更加轻量，用了另外一种方式解决高可用的问题，思路也是可以借鉴的。 kafka部署图 rocketmq部署图 关于吞吐量1、首先说明下面的几张图片来自于互联网共享，也就是我后面参考文章里面的列出的文章。 2、kafka在消息存储过程中会根据topic和partition的数量创建物理文件，也就是说我们创建一个topic并指定了3个partition，那么就会有3个物理文件目录，也就说说partition的数量和对应的物理文件是一一对应的。 3、rocketMq在消息存储方式就一个物流问题，也就说传说中的commitLog，rocketMq的queue的数量其实是在consumeQueue里面体现的，在真正存储消息的commitLog其实就只有一个物理文件。 4、kafka的多文件并发写入 VS rocketMq的单文件写入，性能差异kafka完胜可想而知。 5、kafka的大量文件存储会导致一个问题，也就说在partition特别多的时候，磁盘的访问会发生很大的瓶颈，毕竟单个文件看着是append操作，但是多个文件之间必然会导致磁盘的寻道。 参考文章分布式消息队列RocketMQ与Kafka架构上的巨大差异之1 — 为什么RocketMQ要去除ZK依赖？ 分布式消息队列RocketMQ与Kafka架构上的巨大差异之2 — CommitLog与ConsumeQueue RocketMQ与Kafka对比 Kafka vs RocketMQ—— Topic数量对单机性能的影响 问题 1 2]]></content>
  </entry>
</search>
