<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[机器学习随笔]]></title>
    <url>%2F2018%2F01%2F30%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9A%8F%E7%AC%94%2F</url>
    <content type="text"><![CDATA[根据数据采样来估计概率分布，往往可以用极大似然估计法。这种做法需要假定参数符合一个先验分布。贝叶斯分类用的就是这个思路。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python容器]]></title>
    <url>%2F2018%2F01%2F12%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FPYTHON%2FPython%E5%AE%B9%E5%99%A8%2F</url>
    <content type="text"><![CDATA[列表[]列表是可变的，这是它区别于字符串和元组的最重要的特点，一句话概括即：列表可以修改，而字符串和元组不能。 创建直接创建 12list1 = [&apos;a&apos;,&apos;b&apos;]list2 = [1,2] list函数创建 12list3 = list(&quot;hello&quot;)print list3 输出 [‘h’, ‘e’, ‘l’, ‘l’, ‘o’] 1lista = [0] * 6 过滤1[elem for elem in li if len(elem) &gt; 1] 划分1234567a=[1,2,3,4]#不包括1a[:1]#输出[1]#包括2a[2:]#输出[3、4] 把列表中某个值划分出去123if featureVec[axis] == value: reducedFeatVec = featureVec[:axis] reducedFeatVec.extend(featureVec[axis+1:]) 二维列表12345dataSet = [[1, 1, &apos;yes&apos;], [1, 1, &apos;yes&apos;], [1, 0, &apos;no&apos;], [0, 1, &apos;no&apos;], [0, 1, &apos;no&apos;]] 合并1234circle_file = glob.glob(os.path.join(self.resource_dir, &apos;circle/*.png&apos;))table_file = glob.glob(os.path.join(self.resource_dir, &apos;table/*.png&apos;)) # 直接相加 self.jump_file = [cv2.imread(name, 0) for name in circle_file + table_file] generator转list1234import jieba# jieba的cut返回的是一个generatora = jieba.cut(&apos;我喜欢吃土豆&apos;)b = list(a) 列表扩展的两种方式123456789a=[1,2,3]b=[4,5,6]a.append(b)[1,2,3,[4,5,6]]a.extend(b)[1,2,3,4,5,6] 元组()元组与列表一样，也是一种序列，唯一不同的是元组不能被修改（字符串其实也有这种特点）。 创建123456t1=1,2,3t2="jeffreyzhao","cnblogs"t3=(1,2,3,4)t4=()t5=(1,)print t1,t2,t3,t4,t5 输出： (1, 2, 3) (‘jeffreyzhao’, ‘cnblogs’) (1, 2, 3, 4) () (1,) 从上面我们可以分析得出： a、用逗号分隔一些值，元组自动创建完成； b、元组大部分时候是通过圆括号括起来的； c、空元组可以用没有包含内容的圆括号来表示； d、只含一个值的元组，必须加个逗号（,）； list转元组tuple函数和序列的list函数几乎一样：以一个序列作为参数并把它转换为元组。如果参数就是元组，那么该参数就会原样返回 12345678t1=tuple([1,2,3])t2=tuple(&quot;jeff&quot;)t3=tuple((1,2,3))print t1print t2print t3t4=tuple(123)print t4 输出： (1, 2, 3)(‘j’, ‘e’, ‘f’, ‘f’)(1, 2, 3) t4=tuple(123)TypeError: ‘int’ object is not iterable 词典{}12345678prices = &#123; &apos;A&apos;:123, &apos;B&apos;:450.1, &apos;C&apos;:12, &apos;E&apos;:444,&#125;prices[&apos;A&apos;] 判断词典是否包含某个元素1234labelCount=&#123;&#125;for feature in dataSet: label = feature[-1] if label not in labelCount[label]: labelCount[label] = 0 词典的遍历iteritems 12345678910sentences = [&quot;我喜欢吃土豆&quot;,&quot;土豆是个百搭的东西&quot;,&quot;我不喜欢今天雾霾的北京&quot;]words = []for doc in sentences: words.append(list(jieba.cut(doc)))dic = corpora.Dictionary(words)for word,index in dic.token2id.iteritems(): print word + &apos;, index: &apos; + str(index) 在3.x 里 用 items()替换iteritems() 增加元素1234567891011121314#比如有个词典action = &#123; &quot;_index&quot;: elastic_urls_index, &quot;_type&quot;: doc_type_name, &quot;_id&quot;: data[0], &quot;_source&quot;: &#123; &quot;iclick_id&quot;: data[0], &quot;onsite_id&quot;: data[1], &quot;create_time&quot;: self.today_2 &#125;&#125;#要增加元素data[&apos;_soupyrce&apos;][&apos;age&apos;] = &apos;aa&apos; 提取文本的高频词1234567891011121314documents = ["Human machine interface for lab abc computer applications", "A survey of user opinion of computer system response time"]stoplist = set('for in and'.split())texts = [ [word for word in document.lower().split() if word not in stoplist ] for document in documents]from collections import defaultdictfrequency = defaultdict(int)for text in texts: for word in text: frequency[word]+=1texts = [ [word for word in text if frequency[word]&gt;1] for text in texts ] 映射mapping集合set定义1aaa = set() 增加1aaa.add(1) 判断是否在集合1if 1 in aaa: 数组转集合12a = [11,22,33,44,11,22] b = set(a) 通过set去除停用词123456documents = [&quot;Human machine interface for lab abc computer applications&quot;, &quot;A survey of user opinion of computer system response time&quot;]stoplist = set(&apos;for in and&apos;.split())texts = [ [word for word in document.lower().split() if word not in stoplist ] for document in documents] set增加数据123vocabSet = set([])for document in dataSet: vocabSet = vocabSet | set(document)]]></content>
      <categories>
        <category>PYTHON</category>
      </categories>
      <tags>
        <tag>容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python编码问题]]></title>
    <url>%2F2017%2F11%2F09%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FPYTHON%2FPython%E7%BC%96%E7%A0%81%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[困扰了很长时间的问题，找到一篇解释不错的文章，转载并整理之。 ‘ascii’ codec can’t decode byte 0xe4 in position 0: ordinal not in range(128) python的编码是unicode -&gt; str，解码是str -&gt; unicode 关于文件开头的”编码指示”，也就是-*- coding: -*-这个语句。Python 默认脚本文件都是 UTF-8 编码的，当文件中有非 UTF-8 编码范围内的字符的时候就要使用”编码指示”来修正. 关于 sys.defaultencoding，这个在解码没有明确指明解码方式的时候使用。比如我有如下代码： 1234#! /usr/bin/env python # -*- coding: utf-8 -*- s = &apos;中文&apos; # 注意这里的 str 是 str 类型的，而不是 unicode s.encode(&apos;gb18030&apos;) 这句代码将 s 重新编码为gb18030的格式，即进行unicode -&gt; str的转换。因为 s 本身就是 str类型的，因此 Python 会自动的先将 s 解码为unicode，然后再编码成 gb18030。因为解码是python自动进行的，我们没有指明解码方式，python 就会使用sys.defaultencoding指明的方式来解码。很多情况下 sys.defaultencoding 是ANSCII，如果 s 不是这个类型就会出错。 拿上面的情况来说，我的 sys.defaultencoding是anscii，而 s 的编码方式和文件的编码方式一致，是 utf8 的，所以出错了:UnicodeDecodeError: &#39;ascii&#39; codec can&#39;t decode byte 0xe4 in position 0: ordinal not in range(128) Unicode和UTF8的区别 unicode指的是万国码，是一种“字码表”。而utf-8是这种字码表储存的编码方法。unicode不一定要由utf-8这种方式编成bytecode储存，也可以使用utf-16,utf-7等其他方式。目前大多都以utf-8的方式来变成bytecode。 python中字符串类型分为byte string 和unicode string两种。 如果在python文件中指定编码方式为utf-8(#coding=utf-8)，那么所有带中文的字符串都会被认为是utf-8编码的byte string（例如：mystr=”你好”），但是在函数中所产生的字符串则被认为是unicode string问题就出在这边，unicode string 和byte string是不可以混合使用的，一旦混合使用了，就会产生这样的错误。例如： 1self.response.out.write(&quot;你好&quot;+self.request.get(&quot;argu&quot;)) ​ 以下有两个解决方法： 第一种,是明确的指示出 s 的编码方式 1234#! /usr/bin/env python # -*- coding: utf-8 -*- s = &apos;中文&apos; s.decode(&apos;utf-8&apos;).encode(&apos;gb18030&apos;) 第二种,更改sys.defaultencoding为文件的编码方式 12345#! /usr/bin/env python # -*- coding: utf-8 -*- import sys #要重新载入sys。因为 Python 初始化后会删除 sys.setdefaultencoding 这个方 法reload(sys) sys.setdefaultencoding(&apos;utf-8&apos;)]]></content>
      <categories>
        <category>PYTHON</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>编码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树]]></title>
    <url>%2F2017%2F09%2F11%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[1 决策树公式符号 \begin{align} &Ent(X)\ \ \ 熵 \\ &Gain(X)\ \ \ 信息增益\\ &Gini(X)\ \ \ 基尼指数\\ &D\ \ \ 训练集\\ &A\ \ \ 训练集的某个特征\\ &N\ \ \ 特征A的类别总数\\ &K\ \ \ 标签分类的数量\\ \end{align}1.1 关键步骤-python实现创建决策树分支的createBranch()伪代码函数 123456789检查数据集中每个子项是否属于同一个分类： IF YES return 类标签； ELSE 寻找划分数据集的最好特征； 划分数据集； 创建分支节点； for 每个划分的子集 递归调用createBranch()并增加返回结果到分支节点中 return 分支节点 对label的分类计算熵 12345678910111213141516def calcEnt(dataSet): labelNum = len(dataSet) ent = 0.0 #定义字典存放每个类别的count统计 labelCounts = &#123;&#125; #统计每个label的个数 for featureVec in dataSet: #最后一列是label label = featureVec[-1] if label not in labelCounts.keys(): labelCounts[label] = 0 labelCounts[label] += 1 #计算概率以及熵 for key in labelCounts: prob = float(labelCounts[key]) / labelNum ent -= prob * log(2, prob) return ent 对数据集进行划分 12345678def splitDataSet(dataSet, axis, value): subDataSet = [] for featureVec in dataSet: if featureVec[axis] == value: reducedFeatVec = featureVec[:axis] reducedFeatVec.extend(featureVec[axis+1:]) resDataSet.append(reducedFeatVec) return subDataSet 选出最好的数据集划分方式 信息增益 熵的定义是 Ent(X) = -\sum_{i=1}^{n}p(x_i)log_2p(x_i)n是类别总数。 条件熵$Ent(Y|X)$表示在已知X的条件下Y的不确定性，定义为给定X时Y的条件概率分布的熵对X的期望 Ent(Y|X)=\sum_{i=1}^np_iEnt(Y|X=x_i)对于训练集D以及其中的特征A，熵就是 Ent(D) = -\sum_{k=1}^K \frac {|C_k|}{|D|} log_2\frac{|C_k|}{|D|}其中，K是标签分类的数量，$C_k$是每个分类的样本数 条件熵就是 \begin{aligned} Ent(D|A) &=\sum_{i=1}^N\frac{|D_i|}{|D|}Ent(D_i) \\ &=\sum_{i=1}^N\frac{|D_i|}{|D|}(-\sum_{k=1}^K \frac {|D_{ik}|}{|D_i|} log_2\frac{|D_{ik}|}{|D_i|}) \end{aligned}其中，N是特征A的类别总数，$D_i$是特征A的每种类别的数量。 信息增益就是两者之差 Gain(D,A)=Ent(D)-Ent(D|A)信息增益也称为互信息。 找出信息增益最大的来划分数据集 12345678910111213141516171819202122def chooseBestFeature(dataSet): #feature数量，最后一列是label numFeature = len(dataSet[0]-1) bestInfoGain = 0.0 bestFeature = -1 #先计算熵 baseEntropy = calcEnt(dataSet) for i in range(numFeature): #首先需要知道该特征有几个值 uniqueValue = set([sample[i] for sample in dataSet]) #用set去重是最快方法 newEntropy = 0.0 #对于每个特征，计算条件熵 for value in uniqueValue: #用这个特征划分数据集 subDataSet = splitDataSet(dataSet, i, value) newEntropy += calcEnt(subDataSet) #计算信息增益 infoGain = baseEntropy-newEntropy if infoGain &gt; bestInfoGain: bestInfoGain = infoGain bestFeature = ireturn bestFeature 如果所有特征都处理过了，但是类标签依然不是唯一的，用投票决定 1234567def majorityCnt(classList): classCount=&#123;&#125; for vote in classList: if vote not in classCount.keys() classCount[vote] = 0 classCount[vote] += 1 sortedClassCount = sorted(classCount, key = operator.itemgetter(1), reverse = True) return sortedClassCount[0][0] 1.2 ID3算法与上面的步骤类似。但是ID3只有树的生成，容易过拟合。 1.3 C4.5算法与ID3相比，C4.5用信息增益比来选择特征。 信息增益比 在面对类别比较少的离散数据时，两者差不多。但如果面对连续的数据（如体重、身高、年龄、距离等），或者每列数据没有明显的类别之分（最极端的例子的该列所有数据都独一无二）。 那么根据信息增益公式，$Ent(D)$不变，当数据独一无二时， Ent(D|A)=\sum_{i=1}^n \frac {1}{n}Ent(D_i)这样$Ent(D|A)$最小，程序会倾向于这种划分，导致划分效果差。 信息增益比的公式为 Gain_R(D,A)=\frac {Gain(D,A)}{Ent(D)}可以理解成对分支数目的惩罚项。 1.5 CART算法CART是分类与回归树，由特征选择、树的生成和剪枝组成。 CART是在给定输入变量X条件下输出随机变量Y的条件概率分布的方法。CART假设决策树是二叉树，内部结点特征的取值为是和否，约定左是右否。 决策树等价于递归的二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定的条件下输出的条件概率分布。 1.5.1 CART的生成 递归构建二叉树的过程。回归树用最小二乘，分类树用基尼指数。 1）回归树 2）分类树 假设有K个类，样本点属于第k类的概率是$p_k$，则基尼指数定义为 Gini(p) = \sum_{k=1}^K p_k(1-p_k) = 1-\sum_{k=1}^K p_k^2=1-\sum_{k=1}^K(\frac{|C_k|}{|D|})^2如果是两分类问题，则概率分布的基尼指数为 Gini(P)=2p(1-p)若样本集合D根据特征A是否取某一值a被划分为$D_1$和$D_2$两部分，即 D_1=\{(x,y)\in D | A(x)=a\}, D_2=D-D_1则在特征A的条件下，集合D的基尼指数为 Gini(D,A)=\frac {|D_1|}{|D|}Gini(D_1)+\frac {|D_2|}{|D|}Gini(D_2)Gini越大，样本集合的不确定性越大，与熵相似。 算法过程 12345678输入：训练集D，停止条件输出：CART决策树从根结点递归对每个结点进行以下操作，构建二叉树：1）对每个特征和可能的取值a，根据A=a的为是或否，将D分割成D1和D2，计算基尼指数2）选出基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。3）对两个子结点递归调用#1、#2，直至满足停止条件4）生成CART决策树 3）CART剪枝 1.6 决策树的剪枝 如何判断剪枝后泛华性能提升？ 用留出法。将一部分训练集作为验证集。 剪枝是为了解决过拟合。通过极小化决策树整体的损失函数来实现。设树T的叶结点个数为$|T|$，t是T的叶结点，该叶结点有$Nt$个样本点，其中k类的样本点有$N{tk}$个，则损失函数定义为 C_{\alpha}(T) = \sum_{t=1}^T N_tEnt_t(T) + \alpha|T|由于 Ent_t(T) = - \sum_{k=1}^K \frac {N_{tk}}{N_t} log_2\frac {N_{tk}}{N_t}则令 C(T) = - \sum_{t=1}^T\sum_{k=1}^KN_{tk}log_2\frac {N_{tk}}{N_t}于是 C_\alpha(T) = C(T) +\alpha|T|这里，$C(T)$表示训练数据的预测误差，$|T|$表示模型复杂度，$\alpha$控制两者影响，较大时选择较简单的树，反之亦然，等于0时就不考虑模型复杂度。 两种剪枝思路 预剪枝（Pre-Pruning） 构造的同时剪枝。比如设一个阈值，熵减小的数量小于这个阈值，即使还可以继续降低熵，也停止继续创建分支。 有些分支虽然当前划分时性能下降，但后续划分有可能又会提高，仅根据当前验证集来判断是否要继续划分，往往会导致欠拟合。 后剪枝（Post-Pruning） 三种主要方法 1）REP错误率降低剪枝 简单粗暴，对每个非叶结点的子树，用其替换一个叶结点，类别用子树覆盖训练样本中类最多的代替。这样产生的简化树再跟原树比较在测试数据集中的效果。若错误更少就替换。算法以Bottom-up的方式遍历所有的子树，直到没有任何改进时，终止。 2）PEP悲观剪枝 1.7 连续和缺失值处理1）连续值离散化 jueceshu最简单的策略是二分法，也是C4.5采用的机制。 对于连续属性a，可以考察包含n-1个元素的候选划分点集合 T_a=\left \{ \frac {a^i+a^{i+1}} {2} | 1 \leqslant i \leqslant n-1 \right \}即把区间的中位点作为候选划分点，然后像离散值那样考察划分点，再选出最优的划分点。 2）缺失值 考虑：①如何在属性值缺失的情况下进行划分属性选择？②给定划分属性，若样本在该属性的值缺失，如何划分？ 靠权重。在判定划分时，权重相等，用已知的样本来划分属性。对于每个划分属性，若属性缺失，将缺失的记录根据属性的每个划分所占比例作为权重，分到属性的每个子结点中。 1.8 多变量决策树非子结点不再针对某个属性，而是多个属性的线性组合。即，每个非子结点都是一个线性分类器。 2、随机森林见《集成学习》 3、GBDT参考 统计学习方法 决策树的剪枝问题]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>决策树</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[贝叶斯分类器]]></title>
    <url>%2F2017%2F09%2F11%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[贝叶斯判定准则（Bayes decision rule）：为最小化总体风险，只需在每个样本上选择那个能使条件风险$R(c|x)$最小的类别标记。 我定义了一个条件风险和一个判断分类的准则，如果这个准则能使条件风险最小，也就能使总体风险最小。 朴素贝叶斯假设数据是独立分布的，即属性条件独立性假设 （attribute conditional independence assumption）。对已知类别，假定所有属性相互独立。这一假设使朴素贝叶斯变得简单，但会牺牲一定的分类准确度。 然后，如果属性中有连续值的属性，在计算概率时又假定数据符合正态分布。 贝叶斯分类器有两种实现方式， 基于伯努利模型，不考虑样本中特征出现的次数，只考虑出不出现，相当于假设每个特征是同等权重的。 基于多项式模型，也考虑出现次数 机器学习实战的代码 从文档中创建词典 12345def createVocabList(dataSet): vocabSet = set([]) #create empty set for document in dataSet: vocabSet = vocabSet | set(document) #union of the two sets return list(vocabSet) 给定一个词典和输入文档，如果某个词出现，就给词典的下标置1，就是创建词袋 1234567def setOfWords2Vec(vocabList, inputSet): returnVec = [0]*len(vocabList) for word in inputSet: if word in vocabList: returnVec[vocabList.index(word)] = 1 else: print "the word: %s is not in my Vocabulary!" % word return returnVec 训练朴素贝叶斯分类器 1234567891011121314151617181920def trainNB0(trainMatrix,trainCategory): numTrainDocs = len(trainMatrix) numWords = len(trainMatrix[0]) # 计算分为第一类的概率p(c)，分类是0和1所以就直接相加 pAbusive = sum(trainCategory)/float(numTrainDocs) # 防止最后某一个的概率是0 p0Num = ones(numWords); p1Num = ones(numWords) #change to ones() p0Denom = 2.0; p1Denom = 2.0 #change to 2.0 for i in range(numTrainDocs): if trainCategory[i] == 1: # 矩阵粒度的加法，对应位置相加 p1Num += trainMatrix[i] # 这里为何是所有特征相加，回去再看看 p1Denom += sum(trainMatrix[i]) else: p0Num += trainMatrix[i] p0Denom += sum(trainMatrix[i]) p1Vect = log(p1Num/p1Denom) #change to log() p0Vect = log(p0Num/p0Denom) #change to log() return p0Vect,p1Vect,pAbusive 预测 12345678def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1): # 之前用了log，于是概率相乘就转化为相加，最后再加上p(c) p1 = sum(vec2Classify * p1Vec) + log(pClass1) #element-wise mult p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1) if p1 &gt; p0: return 1 else: return 0]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>贝叶斯</tag>
        <tag>西瓜书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（转）LDA]]></title>
    <url>%2F2017%2F08%2F13%2FNLP%2FGensim-LDA%2F</url>
    <content type="text"><![CDATA[原理gensim-LDAhttp://blog.csdn.net/whzhcahzxh/article/details/17528261 引用gensim包，gensim包中引用corpora,models, similarities，分别做语料库建立，模型库和相似度比较库 from gensim import corpora, models, similarities import jieba 1、分词123456sentences = [&quot;我喜欢吃土豆&quot;,&quot;土豆是个百搭的东西&quot;,&quot;我不喜欢今天雾霾的北京&quot;]words=[]for doc in sentences:# 结巴分词返回的是一个generator，要用list()转成list words.append(list(jieba.cut(doc)))print words 输出： [[u’\u6211’, u’\u559c\u6b22’, u’\u5403’, u’\u571f\u8c46’], [u’\u571f\u8c46’, u’\u662f’, u’\u4e2a’, u’\u767e’, u’\u642d’, u’\u7684’, u’\u4e1c\u897f’], [u’\u6211’, u’\u4e0d’, u’\u559c\u6b22’, u’\u4eca\u5929’, u’\u96fe’, u’\u973e’, u’\u7684’, u’\u5317\u4eac’]] 此时输出的格式为unicode，不影响后期运算，因此我保留不变，如果想看分词结果可以用循环输出jieba分词结果 2、分词结果构造词典12345dic = corpora.Dictionary(words)# 词袋中的所有词print dic# 每个词和编号print dic.token2id 输出： Dictionary(15 unique tokens: [u’\u973e’, u’\u5403’, u’\u5317\u4eac’, u’\u7684’, u’\u4e1c\u897f’]…) {u’\u973e’: 14, u’\u5403’: 0, u’\u5317\u4eac’: 12, u’\u7684’: 9, u’\u4e1c\u897f’: 4, u’\u4e2a’: 5, u’\u642d’: 6, u’\u662f’: 7, u’\u6211’: 3, u’\u559c\u6b22’: 1, u’\u4eca\u5929’: 11, u’\u571f\u8c46’: 2, u’\u4e0d’: 10, u’\u96fe’: 13, u’\u767e’: 8} 为方便看数据： 12for word,index in dic.token2id.iteritems(): print word +&quot; 编号为:&quot;+ str(index) 输出： 北京 编号为:12搭 编号为:6的 编号为:9喜欢 编号为:1不 编号为:10东西 编号为:4土豆 编号为:2霾 编号为:14是 编号为:7个 编号为:5雾 编号为:13百 编号为:8今天 编号为:11我 编号为:3吃 编号为:0 3、生成语料库词袋模型 12corpus = [dic.doc2bow(text) for text in words]print corpus 输出： [[(0, 1), (1, 1), (2, 1), (3, 1)], [(2, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1)], [(1, 1), (3, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1)]] 4、TFIDF变换通过语料库得到tfidf的值，由tfidf来描述句子 123456789#通过语料库得到tfidf模型tfidf = models.TfidfModel(corpus)vec = [(0, 1), (4, 1)]print tfidf[vec]#对corpus的每个文档中的每个词计算tfidf，得到的结果是，每个文档的每个词都是一个元组，包括id和tfidf值corpus_tfidf = tfidf[corpus]for doc in corpus_tfidf: print doc 输出： [(0, 0.7071067811865475), (4, 0.7071067811865475)][(0, 0.8425587958192721), (1, 0.3109633824035548), (2, 0.3109633824035548), (3, 0.3109633824035548)][(2, 0.16073253746956623), (4, 0.4355066251613605), (5, 0.4355066251613605), (6, 0.4355066251613605), (7, 0.4355066251613605), (8, 0.4355066251613605), (9, 0.16073253746956623)][(1, 0.1586956620869655), (3, 0.1586956620869655), (9, 0.1586956620869655), (10, 0.42998768831312806), (11, 0.42998768831312806), (12, 0.42998768831312806), (13, 0.42998768831312806), (14, 0.42998768831312806)] vec是查询文本向量，比较vec和训练中的三句话相似度 123index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features=14)sims = index[tfidf[vec]]print list(enumerate(sims)) 输出： [(0, 0.59577906), (1, 0.30794966), (2, 0.0)] 表示和第1句话相似度为59.578%，和第二句话的相似度位30.79%，第三句没有相似度， 我们看看vec这句话是什么：0为吃，4为东西，所以vec这句话可以是[“吃东西”]或者[“东西吃”] 而第一句话”我喜欢吃土豆”,”土豆是个百搭的东西”明显有相似度，而第三句话”我不喜欢今天雾霾的北京”，相似度几乎为0，至于为什么第一句比第二句更相似，就需要考虑TfIdf document representation和cosine similarity measure了 回到tfidf转换，接着训练LSI模型，假定三句话属于2个主题， 1234lsi = models.LsiModel(corpus_tfidf, id2word=dic, num_topics=2)lsiout=lsi.print_topics(2)print lsiout[0]print lsiout[1] 输出： 0.532“吃” + 0.290“喜欢” + 0.290“我” + 0.258“土豆” + 0.253“霾” + 0.253“雾” + 0.253“北京” + 0.253“今天” + 0.253“不” + 0.166“东西”0.393“百” + 0.393“搭” + 0.393“东西” + 0.393“是” + 0.393“个” + -0.184“霾” + -0.184“雾” + -0.184“北京” + -0.184“今天” + -0.184“不” 这就是基于SVD建立的两个主题模型内容 将文章投影到主题空间中 123corpus_lsi = lsi[corpus_tfidf]for doc in corpus_lsi: print doc 输出： [(0, -0.70861576320682107), (1, 0.1431958007198823)][(0, -0.42764142348481798), (1, -0.88527674470703799)][(0, -0.66124862582594512), (1, 0.4190711252114323)] 因此第一三两句和主题一相似，第二句和主题二相似 同理做个LDA 1234567lda = models.LdaModel(corpus_tfidf, id2word=dic, num_topics=2)ldaOut=lda.print_topics(2)print ldaOut[0]print ldaOut[1]corpus_lda = lda[corpus_tfidf]for doc in corpus_lda: print doc 得到的结果每次都变，给一次的输出： 0.077吃 + 0.075北京 + 0.075雾 + 0.074今天 + 0.073不 + 0.072霾 + 0.070喜欢 + 0.068我 + 0.062的 + 0.061土豆0.091吃 + 0.073搭 + 0.073土豆 + 0.073个 + 0.073是 + 0.072百 + 0.071东西 + 0.066我 + 0.065喜欢 + 0.059霾[(0, 0.31271095988105352), (1, 0.68728904011894654)][(0, 0.19957991735916861), (1, 0.80042008264083142)][(0, 0.80940337254233863), (1, 0.19059662745766134)] 第一二句和主题二相似，第三句和主题一相似 结论和LSI不一样，我估计这和样本数目太少，区别度不高有关，毕竟让我来区分把第一句和哪一句分在一个主题，我也不确定 输入一句话，查询属于LSI得到的哪个主题类型，先变成词袋模型，然后查询LSI： 12345query = "雾霾"query_bow = dic.doc2bow(list(jieba.cut(query)))print query_bowquery_lsi = lsi[query_bow]print query_lsi 输出: [(13, 1), (14, 1)][(0, 0.50670602027401368), (1, -0.3678056037187441)] 与第一个主题相似 比较和第几句话相似，用LSI得到的索引接着做，并排序输出 12345index = similarities.MatrixSimilarity(lsi[corpus])sims = index[query_lsi]print list(enumerate(sims))sort_sims = sorted(enumerate(sims), key=lambda item: -item[1])print sort_sims 输出： [(0, 0.90161765), (1, -0.10271341), (2, 0.99058259)][(2, 0.99058259), (0, 0.90161765), (1, -0.10271341)] 可见和第二句话相似度很高，因为只有第二句话出现了雾霾两个词，可是惊讶的是和第一句话的相似度也很高，这得益于LSI模型的算法：在A和C共现，B和C共现的同时，可以找到A和B的相似度 代码位于blogcodes/gensim_lda.py]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>gensim</tag>
        <tag>LDA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Gensim-Tutorials]]></title>
    <url>%2F2017%2F08%2F12%2FNLP%2FGensim-Tutorials%2F</url>
    <content type="text"><![CDATA[http://radimrehurek.com/gensim/tutorial.html Gensim 使用Python标准logging模块来记录log，使用方法是 12import logginglogging.basicConfig(format=&apos;%(asctime)s : %(levelname)s : %(message)s&apos;, level=logging.INFO)]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>gensim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vim命令]]></title>
    <url>%2F2017%2F07%2F12%2F%E5%B7%A5%E5%85%B7%E5%92%8C%E7%8E%AF%E5%A2%83%2Fvim%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[查找并删除:g/要删除的内容/d 查找 /要查找的 按n就是下一个 删除一行 dd 查找匹配的个数 :%s/refering_site/&amp;/gn 或 :%s/17779//gn :%s/“opxcreativeid”:16650//gn 查找并替换%s/源字符串/目的字符串/g 如 :%s/\/home\/weinan/\/opt\/pig_home\/bshare_etl/g :%s/gpadmin/gpxmo/g :%s/\t-1/\t1/g :%s/“//g :%s/16-06-28/16-06-29/g 多行变1行大写V选中行+shift J 替换每行的行首、行尾12:1,$ s/^/HELLO/g:1,$ s/$/WORLD/g sh文件的编码转成unix查看用:set fileformat 修改用:set fileformat=unix 编码从latin1转成utf8:e ++enc=cp936:set fileencoding=utf-8]]></content>
      <categories>
        <category>工具和环境</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka笔记]]></title>
    <url>%2F2017%2F07%2F12%2Fhadoop-spark%2Fkafka%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[多个消费者读取一个topic，多次消费不同消费者设置不同的groupid 12val kafkaParm = Map("metadata.broker.list" -&gt; "localhost:9092","auto.offset.reset" -&gt; "smallest", "group.id" -&gt; "davidtopi1c1") 每个消费者不会重复消费数据1234 kafkaStrem.foreachRDD&#123; rdd=&gt; km.updateZKOffsets(rdd)&#125;]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[presto笔记]]></title>
    <url>%2F2017%2F07%2F12%2Fhadoop-spark%2Fpresto%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[增加kafka配置1、在/opt/presto-server-0.152/etc/catalog/增加文件kafka.properties，内容是 12345connector.name=kafkakafka.table-names=showup,click kafka.nodes=10.11.10.33:9092#kafka.hide-internal-columns-hidden=falsekafka.default-schema=rawdata 其中， kafka.table-names 跟topic名称相同，如果topic是带前缀的，比如rawdata.showup，那么schema就是rawdata。 kafka.hide-internal-columns-hidden 建表后有一系列内置column，默认这些是隐藏的，设为false使其显示。 kafka.default-schema 如果topic没有前缀，默认的schema是default，可以用该参数修改默认schema名称。 2、在etc的config.propreties中的datasources增加kafka 3、增加topic描述文件 放在etc/kafka目录中，以.json结尾，文件名和表名最好一致。例如： 12345678910111213141516171819202122232425&#123; &quot;tableName&quot;: &quot;click_dis&quot;, &quot;schemaName&quot;: &quot;rawdata&quot;, &quot;topicName&quot;: &quot;click_dis&quot;, &quot;key&quot;: &#123; &quot;dataFormat&quot;: &quot;raw&quot;, &quot;fields&quot;: [ &#123; &quot;name&quot;: &quot;kafka_key&quot;, &quot;type&quot;: &quot;VARCHAR&quot;, &quot;hidden&quot;: &quot;false&quot; &#125; ] &#125;, &quot;message&quot;: &#123; &quot;dataFormat&quot;: &quot;json&quot;, &quot;fields&quot;: [ &#123; &quot;name&quot;: &quot;dt_i&quot;, &quot;mapping&quot;: &quot;dt_i&quot;, &quot;type&quot;: &quot;BIGINT&quot; &#125; ] &#125;&#125; 4、重启presto服务器 1/opt/presto-server-0.152/bin/launcher restart 5、连接服务器测试 1/opt/jdk1.8.0_102/bin/java -jar /opt/presto-server-0.152/presto-cli --server 10.11.10.33:8082 --catalog kafka --schema rawdata 1select * from click limit 10; 其中内置column的意思是： Column name Type Description _partition_id BIGINT 包含这行数据的kafka partition的id _partition_offset BIGINT kafka partition的offset _segment_start BIGINT 在该segment中的最小offset _segment_end BIGINT 在该segment中的最大offset _segment_count BIGINT 对于一个未压缩的topic，_segment_start + _segment_count is equal to _partition_offset _message_corrupt BOOLEAN 为TRUE就说明解码器不能解析message _message VARCHAR UTF-8编码的string，只对text的topic有效 _message_length BIGINT message长度 _key_corrupt BOOLEAN 为TRUE就说明解码器不能解析key _key VARCHAR UTF-8编码的string _key_length BIGINT key的长度 查询key 1select count(1) from showup_dis where kafka_key like &apos;20161009%&apos;;]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
      <tags>
        <tag>presto</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark-sql笔记]]></title>
    <url>%2F2017%2F07%2F12%2Fhadoop-spark%2Fspark-sql%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[spark sql可以用基本的SQL语法，或者hiveQL。Spark SQL也能从hive中读数据。 Data Frame的来源可以是：结构化数据、hive表、外部数据库或者RDD Starting Point: SQLContextThe entry point into all functionality in Spark SQL is the SQLContext class, or one of its descendants. To create a basic SQLContext, all you need is a SparkContext. 12345val sc: SparkContext // An existing SparkContext.val sqlContext = new org.apache.spark.sql.SQLContext(sc)// this is used to implicitly convert an RDD to a DataFrame.import sqlContext.implicits._ 在SQLContext的基础上，也要创建一个HiveContext，可以用Hive的udf，也能读hive表。HiveContext是一个独立的包，不需要安装hive Creating DataFramesWith a SQLContext, applications can create DataFrames from an existing RDD, from a Hive table, or from data sources. As an example, the following creates a DataFrame based on the content of a JSON file: 1234567val sc: SparkContext // An existing SparkContext.val sqlContext = new org.apache.spark.sql.SQLContext(sc)val df = sqlContext.read.json("examples/src/main/resources/people.json")// Displays the content of the DataFrame to stdoutdf.show() Creating Datasets123456789101112131415161718import org.apache.spark.sql.SQLContext......// Encoders for most common types are automatically provided by importing sqlContext.implicits._val sqlContext = SQLContext.getOrCreate(sc)import sqlContext.implicits._val ds = Seq(1, 2, 3).toDS()ds.map(_ + 1).collect() // Returns: Array(2, 3, 4)// Encoders are also created for case classes.case class Person(name: String, age: Long)val ds = Seq(Person("Andy", 32)).toDS()// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name.val path = "examples/src/main/resources/people.json"val people = sqlContext.read.json(path).as[Person] 在DataFrame中创建表并查询12345678val sqlContext = SQLContext.getOrCreate(sc)import sqlContext.implicits._//把andy和32匹配到name和age列。//先创建一个DataFrame，再注册为tableval ds = sc.parallelize(Seq(("Andy", 32))).toDF("name","age") ds.registerTempTable("ds1")sqlContext.sql("select * from ds1") sparkSQL链接GP1、maven中增加包一开始试过8.2的包就不行12345&lt;dependency&gt; &lt;groupId&gt;postgresql&lt;/groupId&gt; &lt;artifactId&gt;postgresql&lt;/artifactId&gt; &lt;version&gt;8.3-606.jdbc3&lt;/version&gt;&lt;/dependency&gt; 2、连接jdbc12345val jdbcDF = sqlContext.read.format("jdbc").options( Map("url" -&gt; "jdbc:postgresql://10.1.1.230:5432/xmo_dw", "user"-&gt;"david_xu", "password"-&gt;"w7dtfxHD", "dbtable" -&gt; "(select * from xmo_dw.bshare_blacklist_tagid) as aa")).load().show() 在spark sql命令行中测试连接/usr/lib/spark/bin/spark-sql —jars /home/david/jars/postgresql-8.3-606.jdbc4.jar 12345678910CREATE TEMPORARY TABLE temp_imageviewsUSING org.apache.spark.sql.jdbcOPTIONS ( driver "org.postgresql.Driver", url "jdbc:postgresql://10.1.1.230:5432/xmo_dw", user "david_xu", password "w7dtfxHD", dbtable "(select date_i,opxpid::text from xmo_dw.imageviews where date_i=20160512 limit 20000000) as aa", numPartitions "6"); 1234567891011121314cd /usr/lib/spark/bin//usr/lib/spark/bin/spark-sql --executor-memory 100gadd jar /home/wilson/sparksql/postgresql-8.3-606.jdbc3.jar;set spark.sql.shuffle.partitions=20; CREATE TEMPORARY TABLE temp_imageviewsUSING org.apache.spark.sql.jdbcOPTIONS ( driver "org.postgresql.Driver", url "jdbc:postgresql://10.1.1.230:5432/xmo_dw", user "david_xu", password "w7dtfxHD", dbtable "(select timeslot,record_server,referring_site ,opxsid from xmo_dw.imageviews where date_i=20160515 limit 5000000) as aa", numPartitions "6"); spark sql读取HDFS建表12345678910111213/usr/lib/spark/bin/spark-sql -e "CREATE TEMPORARY TABLE rtbreq_hourUSING org.apache.spark.sql.bytesOPTIONS ( paths '$&#123;rtbreq_path&#125;');create TEMPORARY TABLE rtbreq_tanx_hourUSING org.apache.spark.sql.bytesOPTIONS( paths '$&#123;rtbreq_tanx_path&#125;');select ip,count(1) cnt from (select ip from rtbreq_hour union all select ip from rtbreq_tanx_hour) a group by ip having cnt &gt; 30 order by cnt desc;select bxid,count(1) cnt from (select bxid from rtbreq_hour union all select bxid from rtbreq_tanx_hour) a group by bxid having cnt &gt; 1000 order by cnt desc;" &gt; rtbreq/$day/$hour.txt]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
      <tags>
        <tag>spark-sql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark-streaming笔记]]></title>
    <url>%2F2017%2F07%2F12%2Fhadoop-spark%2Fspark-streaming%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[spark streaming的示例12345678910111213141516171819202122232425def main ( args : Array[ String ]): Unit = &#123; //关闭一些不必要的日志 Logger. getLogger ( "org.apache.spark" ). setLevel (Level. WARN ) Logger. getLogger ( "org.eclipse.jetty.server" ). setLevel (Level. OFF ) val conf = new SparkConf(). setAppName ( "wordStreaming" ). setMaster ( "local[2]" ). set ( "spark.sql.shuffle.partitions" , "10" ). set ( "spark.network.timeout" , "30s" ) . set ( "spark.shuffle.compress" , "true" ). set ( "spark.shuffle.spill.compress" , "true" ) . set ( "spark.shuffle.manager" , "sort" ) val sc = new SparkContext( conf ) // 创建 StreamingContext，1 秒一个批次。这里要用 sc ，而不是 conf ，因为 sc 已经创建了 val ssc = new StreamingContext( sc , Seconds ( 1 )) // 获得一个 DStream 负责连接 监听端口:地址 val lines = ssc . socketTextStream ( "192.168.37.129" , 9999 ) // 对每一行数据执行 Split 操作 val words = lines . flatMap ( _. split ( " " ) ) // 统计 word 的数量 val pairs = words . map ( word =&gt; ( word , 1 )) val wordCounts = pairs . reduceByKey (_ + _) // 输出结果 wordCounts . print () ssc . start () ssc . awaitTermination () &#125; 一开始会报错：Exception in thread “main” org.apache.spark.SparkException : Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at: org.apache.spark.SparkContext.( SparkContext.scala:82 ) 错误是在val ssc = new StreamingContext( conf , Seconds ( 1 )) 因为之前sc已经创建了，所以这里的conf要改成sc 之后，在 192.168.37.129上启动netcatnc -lk 9999输入hello world 再启动spark的程序，可以看出会输出1234Time: 1462790166000 ms(hello,1)(world,1) streaming读取本地文件1val lines = ssc.textFileStream(&quot;E:\\spark&quot;) 每当该文件夹内有新文件生成，就会自动读取 Spark Streaming将会监控dataDirectory目录，并且处理目录下生成的任何文件（嵌套目录不被支持）。需要注意一下三点：1 所有文件必须具有相同的数据格式2 所有文件必须在dataDirectory目录下创建，文件是自动的移动和重命名到数据目录下3 一旦移动，文件必须被修改。所以如果文件被持续的附加数据，新的数据不会被读取。对于简单的文本文件，有一个更简单的方法streamingContext.textFileStream(dataDirectory)可以被调用。文件流不需要运行一个receiver，所以不需要分配核。 spark streaming连接kafka12val topics = Set(&quot;test1&quot;)val kafkaParm = Map(&quot;metadata.broker.list&quot; -&gt; &quot;192.168.255.128:9092&quot;)]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
      <tags>
        <tag>spark-streaming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark笔记-local开发]]></title>
    <url>%2F2017%2F07%2F12%2Fhadoop-spark%2Fspark%E7%AC%94%E8%AE%B0-local%E5%BC%80%E5%8F%91%2F</url>
    <content type="text"><![CDATA[一、环境 12345 &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt; &lt;version&gt;1.6.1&lt;/version&gt; &lt;/dependency&gt; 如果要进shell模式增加环境变量123HADOOP_HOME D:\gitlab\hadoop2.6_Win_x64\binSPARK_HOME D:\spark-1.4.0-bin-hadoop2.6SPARK_CLASSPATH &lt;font face=&quot;Verdana, Arial, Helvetica, sans-serif&quot;&gt;D:\spark-1.4.0-bin-hadoop2.6\bin&lt;/font&gt; 输入spark-shell进入 二、编写local的spark程序1234567891011121314import org.apache.log4j.&#123; Level, Logger &#125;import org.apache.spark.&#123; SparkConf, SparkContext &#125;def main(args: Array[String]): Unit = &#123; //关闭一些不必要的日志 Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.WARN) Logger.getLogger(&quot;org.eclipse.jetty.server&quot;).setLevel(Level.OFF) val conf = new SparkConf().setAppName(&quot;wordSegname&quot;).setMaster(&quot;local[4]&quot;). set(&quot;spark.sql.shuffle.partitions&quot;,&quot;10&quot;).set(&quot;spark.network.timeout&quot;,&quot;30s&quot;) .set(&quot;spark.shuffle.compress&quot;,&quot;true&quot;).set(&quot;spark.shuffle.spill.compress&quot;,&quot;true&quot;) .set(&quot;spark.shuffle.manager&quot;,&quot;sort&quot;) val sc = new SparkContext(conf) &#125; spark数据操作sparkRDD创建RDD1、数据集合 12 var data = Array(1,2,3,4,5,6,7,8,9) var disData = sc.parallelize(data,3) 创建一个RDD，包括1-9，分在3个分区 2、外部数据源 1textFile(path:String, minPartitions:Int) //第一个指定路径，第二个指定分区 RDD转换1、map，对每个元素执行一个指定函数产生新的RDD12val rdd1 = sc.parallelize(1 to 9, 3)val rdd2 = rdd1.map(x =&gt; x*2) RDD actionreducecollect 所有元素以array形式返回countfirsttake返回数据集中前N个元素takeSample 返回随机元素takeOrderedsaveAdTextFilecountByKeyforeach]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark笔记-操作elastic search]]></title>
    <url>%2F2017%2F07%2F12%2Fhadoop-spark%2Fspark%E7%AC%94%E8%AE%B0-%E6%93%8D%E4%BD%9Celastic-search%2F</url>
    <content type="text"><![CDATA[最简单的例子1、在pom.xml中增加12345&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-spark_2.10&lt;/artifactId&gt; &lt;version&gt;2.2.1&lt;/version&gt;&lt;/dependency&gt; 2、在spark的main中导入org.elasticsearch.spark包12...import org.elasticsearch.spark._ 3、在spark的conf中增加如下配置1set(&quot;es.index.auto.create&quot;, &quot;true&quot;).set(&quot;es.nodes&quot;, &quot;192.168.37.129&quot;).set(&quot;es.port&quot;,&quot;9200&quot;) 其中，es.nodes是ElasticSearch的host 4、简单的写法如下1234567val conf = ...val sc = new SparkContext(conf) val numbers = Map("one" -&gt; 1, "two" -&gt; 2, "three" -&gt; 3)val airports = Map("arrival" -&gt; "Otopeni", "SFO" -&gt; "San Fran")sc.makeRDD(Seq(numbers, airports)).saveToEs("spark/docs") 也可以用case class来写1234567case class Trip(departrue: String, arrival: String) val upTrip = Trip("OTF", "SFO")val downTrip = Trip("MUC", "OTP")val rdd = sc.makeRDD(Seq(upTrip, downTrip))EsSpark.saveToEs(rdd, "spark/docs") 5、在Elastic Search的Sense中查询1GET /spark/docs/_search 返回123456789101112131415161718192021222324252627282930313233343536&#123; &quot;took&quot;: 4, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;spark&quot;, &quot;_type&quot;: &quot;docs&quot;, &quot;_id&quot;: &quot;AVSkEdTv9l_YEZuMmxgt&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;arrival&quot;: &quot;Otopeni&quot;, &quot;SFO&quot;: &quot;San Fran&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;spark&quot;, &quot;_type&quot;: &quot;docs&quot;, &quot;_id&quot;: &quot;AVSkEdZp9l_YEZuMmxgu&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;one&quot;: 1, &quot;three&quot;: 3, &quot;two&quot;: 2 &#125; &#125; ] &#125;&#125; 测试通过 与spark streaming结合123456789101112131415// 创建 StreamingContext，5秒一个批次val ssc = new StreamingContext(sc, Seconds(3))val lines = ssc.socketTextStream("192.168.37.129", 9999)// 对每一行数据执行 Split 操作val words = lines.flatMap(_.split(" "))// 统计 word 的数量val pairs = words.map(word =&gt; (word, 1))pairs.foreachRDD&#123;x =&gt;x.saveToEs("spark/words")&#125;ssc.start()ssc.awaitTermination() pairs是一个DSteamRDD，通过foreachRDD来遍历其中的每个RDD，对于每个RDD，可以saveToEs]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
      <tags>
        <tag>elastic search</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark笔记-新词发现]]></title>
    <url>%2F2017%2F07%2F12%2Fhadoop-spark%2Fspark%E7%AC%94%E8%AE%B0-%E6%96%B0%E8%AF%8D%E5%8F%91%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[12val conf = new SparkConf().setAppName("wordSegname").setMaster("local[4]").set("spark.sql.shuffle.partitions","10").set("spark.network.timeout","30s") local[4]是指在本地运行，用4核CPU。 spark.sql.shuffle.partitions是指partition的数量。SparkSQL在运行时，将一个查询任务分解成多个task，一个task就是一个partition。默认是200个partition，而如果实际集群只能并行3个task，则跑完200个partition要200/3=67次。 spark.network.timeout是指所有网络通信的超时时间，默认是120s 123456789val word1=sc.textFile(path).map&#123;x=&gt; val x_filter=x.replaceAll("\p&#123;Punct&#125;", " ").replaceAll("\pP", " ") .replaceAll(" ", " ").replaceAll("[" + AtomsUitl.stopwords + "]", " ").replaceAll("\p&#123;Blank&#125;", " ").replaceAll("\p&#123;Space&#125;", " ").replaceAll("\p&#123;Cntrl&#125;", " ") x_filter &#125; replaceAll中是正则表达式。上文中，是将所有的特殊字符都用空格代替 AtomsUitl.stopwords停用词是”的很了么呢是嘛个都也比还这于不与才上用就好在和对挺去后没说” sc.textFile读取文件后，生成一个RDD，以行为单位，所以后面的map是对每行的操作 打印出所有的元素，用 word1.foreach { x =&gt; println(x) } 12345678val word_document = word1.zipWithIndex.filter &#123; x =&gt; !StringUtils.isBlank(x._1) &#125;.flatMap &#123; x =&gt; val arr = ArrayBuffer(String, Int) val line = x._1.split(" ") //对于每一行，都用空格分割 for (i &lt;- line) &#123; arr += ((i, x._2.toInt)) //分割后，每一个tuple加到数组中 &#125; arr &#125;.map &#123; x =&gt; (x.1.trim, x.2) &#125;.filter(x =&gt; !StringUtils.isBlank(x._1)) zipWithIndex用带有index的来压缩RDD，索引从0开始 word1.zipWithIndex.foreach { x =&gt; println(x) } (ab ef,0) (cd,1) 上述代码得到的结果是 (ab,0) (ef,0) (cd,1) 12345678val wordleft = word.map(x =&gt; AtomsUitl.reverse(x)).map &#123; x =&gt; "" + x + "" &#125;.flatMap &#123; x =&gt; var arr = ArrayBufferString for (y &lt;- 1 to AtomsUitl.len(x) - 2) &#123; arr += AtomsUitl.substring(x, y, Math.min(maxLen + y, AtomsUitl.len(x))) &#125; arr&#125;.sortBy(x =&gt; x) 将每个句子倒序排列，提取每个子集 今$ 四期星天今 处言语然自 天今$ 星天今$ 期星天今$ 然自$ 理处言语然 自$ 言语然自 语然自$ $ 12345678val wordleft_caculate = wordleft.map &#123; s =&gt; val first = AtomsUitl.substring(s, 0, 1).toString (first, s) &#125;.groupBy(f =&gt; f._1).map &#123; x =&gt; x._2 &#125;wordleft_caculate.foreach&#123;x=&gt; println(x.iterator.next())&#125; groupBy之后得到 (期, CompactBuffer((期,期星天今$))等 这个是Iterable，可迭代的。可以转换为一个迭代器x.iterator.next()。迭代出来就是 (期,期星天今$) 等]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>新词发现</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark笔记]]></title>
    <url>%2F2017%2F07%2F12%2Fhadoop-spark%2Fspark%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[算圆周率pi1234567891011121314151617import org.apache.spark.SparkConf;import org.apache.spark.SparkContext;case class PerTypeson[T,S](var name:T,var age:S) &#123;&#125;object SparkTest&#123; def main(args: Array[String]) &#123; val conf = new SparkConf().setAppName("Spark Pi").setMaster("local") //关键 val sc = new SparkContext(conf) val NUM_SAMPLES=100000 val count=sc.parallelize(1 to NUM_SAMPLES).map&#123;i=&gt; val x=Math.random() val y=Math.random() if(x*x+y*y&lt;1)1 else 0 &#125;.reduce(_+_) println("pi is rougly"+4.0*count/NUM_SAMPLES) &#125;&#125; 读取外部文件和链接数据库（用spark 1.6的版本） 12345678910111213141516171819202122232425262728import org.apache.spark.SparkConf;import org.apache.spark.SparkContextimport org.apache.spark.ml.classification.LogisticRegressionimport org.apache.spark.sql.SQLContextimport scala.collection.mutable.Mapcase class Person(var name:String)object SparkTest&#123; def main(args: Array[String]) &#123; // val conf = new SparkConf().setAppName("Spark Pi").setMaster("spark://hadoop:7070") //关键 val conf = new SparkConf().setAppName("Spark Pi").setMaster("local") //关键 val sc = new SparkContext(conf)// val textfile=sc.textFile("C:\\Users\\Administrator\\Desktop\\分词.txt")// textfile.collect().foreach(println) val sqlContext=new SQLContext(sc) val df=sqlContext.read.json("F:\\people.json") df.cache() println(df.select("age").show()) df.registerTempTable("df1") println(sqlContext.sql("select * from df1 where age=19")) val map=Map("url" -&gt; "jdbc:mysql://localhost:3306/test", "user"-&gt;"root","password"-&gt;"") map+=("dbtable" -&gt;"class") "dbtable" -&gt; "SELECT * FROM iteblog" val jdbc=sqlContext.read.format("jdbc").options(map).load() println(jdbc.show(1))// val lr = new LogisticRegression().setMaxIter(10) &#125;&#125; 创建DataFrame并简单操作DataFrame123456789101112131415161718192021222324252627282930313233343536import org.apache.spark.SparkConf;import org.apache.spark.SparkContextimport org.apache.spark.ml.classification.LogisticRegressionimport org.apache.spark.sql.SQLContextimport scala.collection.mutable.Mapcase class Person(var name:String)case class Employee(age: Int, name: String)object SparkTest&#123; def main(args: Array[String]) &#123; // val conf = new SparkConf().setAppName("Spark Pi").setMaster("spark://hadoop:7070") //关键 val conf = new SparkConf().setAppName("Spark Pi").setMaster("local") //关键 val sc = new SparkContext(conf) val sqlContext=new SQLContext(sc) //第一种方式就创建DataFrame，读取外部文件 println("第一种方式就创建DataFrame，读取外部文件") val textfile=sc.textFile("C:\\Users\\Administrator\\Desktop\\分词.txt") val df_person=textfile.map(x=&gt;Person(x)) val df_test=sqlContext.createDataFrame(df_person).withColumnRenamed("name","anmoyi") println(df_test.filter(df_test("anmoyi").contains("使用")).count()) //第二种方式创建DataFrame println("第二种方式创建DataFrame，通过List和case类的方式创建") val listOfEmployee=List(Employee(1,"zhou"),Employee(1,"zhou"),Employee(2,"mei"),Employee(3,"xu")) val emFrame=sqlContext.createDataFrame(listOfEmployee) println(emFrame.show()) emFrame.registerTempTable("employeeTable") val sortedByNameEmployees = sqlContext.sql("select * from employeeTable order by name desc") println(sortedByNameEmployees.show()) println(emFrame.groupBy("age").count().show()) println(emFrame.select(emFrame("name"),emFrame("age"),(emFrame("age")+1).as("age1")).show()) println(sortedByNameEmployees.show()) //第三种方式通过TupleN来创建DataFrame println("第三种方式通过TupleN，元祖的方式来创建DataFrame") val mobiles=sqlContext.createDataFrame(Seq((1,"Android"), (2, "iPhone"))).toDF("age","mobile") println(mobiles.show()) &#125;&#125; Spark中统计相关的东西spark shell中增加依赖包 bin/spark-shell --packages com.databricks:spark-csv_2.10:1.0.3 12345678910111213141516171819import org.apache.spark.SparkConfimport org.apache.spark.SparkContextimport org.apache.spark.sql.SQLContextimport org.apache.spark.sql.functions._ //包含了常见的统计函数和数学函数import org.apache.log4j.&#123;Level, Logger&#125;//import com.databricks.spark.csv._object Test &#123; def main(args: Array[String]):Unit=&#123;// 屏蔽不必要的日志显示在终端上Logger.getLogger("org.apache.spark").setLevel(Level.WARN)Logger.getLogger("org.eclipse.jetty.server").setLevel(Level.OFF) val conf = new SparkConf().setAppName("stastic").setMaster("local") //关键 val sc = new SparkContext(conf) val sqlContext=new SQLContext(sc) import sqlContext.implicits._ //用于隐式转化，可以由RDD直接转换为DataFrame val df = sc.parallelize(0 until 10).toDF("id").withColumn("rand1", rand(10)) .withColumn("rand2", rand(seed=27)).withColumn("rand3",rand(20)) println(df.columns) println(df.describe().show()) Spark中的多对多JOIN如果存在多对多的情况下，则是以乘法得到最后结果，并不是以某列多的情况 12345678910111213141516171819202122val df2=sc.parallelize(0 until 6).toDF("id").withColumn("age",rand(10))println(df.join(df2,df("id")===df2("id"),"left").show()) //左链接println(df.join(df2,df("id")===df2("id"),"right").show()) //右链接println(df.join(df2,df("id")===df2("id"),"outer").show()) //全链接println(df.join(df2,df("id")===df2("id"),"inner").show()) //inner 链接df.join(df2, $"df1Key" === $"df2Key")df.join(df2).where($"df1Key" === $"df2Key")df.join(df2, Seq("user_id", "user_name")) println("统计函数开始") println(df.groupBy($"id").agg(Map( "rand1" -&gt; "avg", "rand2" -&gt; "max", "rand3" -&gt; "min" )).show()) println(df.drop("rand1").show()) println(df.stat.corr("rand1","rand2")) println(df.stat.cov("rand1", "rand2")) val df1=sqlContext.createDataFrame(Seq((1, 1), (1, 2), (2, 1), (2, 1), (2, 3), (3, 2), (3, 3))).toDF("key", "value") println(df1.stat.crosstab("key","value").show()) &#125;&#125;]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas笔记]]></title>
    <url>%2F2017%2F07%2F12%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FPYTHON%2Fpandas%2F</url>
    <content type="text"><![CDATA[1、DataFrame1.1 创建随机引入pandas包 12345In [1]: import pandas as pdIn [2]: import numpy as npIn [3]: import matplotlib.pyplot as plt 创建一个Series，pandas可以生成一个默认的索引 1s = pd.Series([1,3,5,np.nan,6,8]) 通过numpy创建DataFrame，包含一个日期索引，以及标记的列 123456789101112dates = pd.date_range(&apos;20170101&apos;, periods=6)df = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list(&apos;ABCD&apos;))dfOut[4]: A B C D2016-10-10 0.630275 1.081899 -1.594402 -2.5716832016-10-11 -0.211379 -0.166089 -0.480015 -0.3467062016-10-12 -0.416171 -0.640860 0.944614 -0.7566512016-10-13 0.652248 0.186364 0.943509 0.0532822016-10-14 -0.430867 -0.494919 -0.280717 -1.3274912016-10-15 0.306519 -2.103769 -0.019832 0.035211 其中，np.random.randn可以返回一个随机数组 123np.random.randn(1,2)Out[11]: array([[-2.67809797, 1.49728361]]) np.random.rand 随机样本位于[0,1)中 np.random.randn 从标准正态分布$N=(\mu , \sigma ^2)$中返回样本，默认的范围是$N(0,1)$，等价于np.random.standard_normal 如果要返回2*4的$N(3,6.25)$的随机分布，可知均值是3，标准差是2.5，则 12&gt; 3+2.5*np.random.randn(2,4)&gt; 如果在matlib模块中使用，则返回的是matrix而不是array 1234import numpy.matlibnp.matlib.randn(1,2)Out[13]: matrix([[ 0.13107513, -0.87977247]]) 1.2 通过dict创建12345678910111213df2 = pd.DataFrame(&#123; &apos;A&apos; : 1., &apos;B&apos; : pd.Timestamp(&apos;20130102&apos;), &apos;C&apos; : pd.Series(1,index=list(range(4)),dtype=&apos;float32&apos;), &apos;D&apos; : np.array([3] * 4,dtype=&apos;int32&apos;), &apos;E&apos; : pd.Categorical([&quot;test&quot;,&quot;train&quot;,&quot;test&quot;,&quot;train&quot;]), &apos;F&apos; : &apos;foo&apos; &#125;) Out[20]: A B C D E F0 1.0 2013-01-02 1.0 3 test foo1 1.0 2013-01-02 1.0 3 train foo2 1.0 2013-01-02 1.0 3 test foo3 1.0 2013-01-02 1.0 3 train foo 有几个方法可以构造一个Timestamp对象 pd.Timestamp 1234567891011121314151617181920import pandas as pdfrom datetime import datetime as dtp1=pd.Timestamp(2017,6,19)p2=pd.Timestamp(dt(2017,6,19,hour=9,minute=13,second=45))p3=pd.Timestamp(&quot;2017-6-19 9:13:45&quot;)print(&quot;type of p1:&quot;,type(p1))print(p1)print(&quot;type of p2:&quot;,type(p2))print(p2)print(&quot;type of p3:&quot;,type(p3))print(p3)(&apos;type of p1:&apos;, &lt;class &apos;pandas.tslib.Timestamp&apos;&gt;)2017-06-19 00:00:00(&apos;type of p2:&apos;, &lt;class &apos;pandas.tslib.Timestamp&apos;&gt;)2017-06-19 09:13:45(&apos;type of p3:&apos;, &lt;class &apos;pandas.tslib.Timestamp&apos;&gt;)2017-06-19 09:13:45 to_datetime() 123456789101112131415import pandas as pdfrom datetime import datetime as dtp4=pd.to_datetime(&quot;2017-6-19 9:13:45&quot;)p5=pd.to_datetime(dt(2017,6,19,hour=9,minute=13,second=45))print(&quot;type of p4:&quot;,type(p4))print(p4)print(&quot;type of p5:&quot;,type(p5))print(p5)(&apos;type of p4:&apos;, &lt;class &apos;pandas.tslib.Timestamp&apos;&gt;)2017-06-19 09:13:45(&apos;type of p5:&apos;, &lt;class &apos;pandas.tslib.Timestamp&apos;&gt;)2017-06-19 09:13:45 在df2对象中，每一列都有对应的dtypes 12345678910df2.dtypesOut[30]: A float64B datetime64[ns]C float32D int32E categoryF objectdtype: object 1.3 查看数据参考Basics section 查看head和tail 12df.head(1)df.tail(3) 查看index、column和数据 123df.indexdf.columnsdf.values 显示数据的快速统计 1234567891011df.describe()Out[19]: A B C Dcount 6.000000 6.000000 6.000000 6.000000mean 0.073711 -0.431125 -0.687758 -0.233103std 0.843157 0.922818 0.779887 0.973118min -0.861849 -2.104569 -1.509059 -1.13563225% -0.611510 -0.600794 -1.368714 -1.07661050% 0.022070 -0.228039 -0.767252 -0.38618875% 0.658444 0.041933 -0.034326 0.461706max 1.212112 0.567020 0.276232 1.071804 转置 1df.T 通过列名来排序 12#对于矩阵，axis=0表示行，1表示列df.sort_index(axis=1, ascending=False) 通过某一列的数值排序 1df.sort_values(by=&apos;B&apos;) 1.4 选择选择某一列 1df[&apos;A&apos;] 选择某几行 123df[0:3]#也可以通过行的索引来选择，但是不能单独写某一行df[&apos;20130102&apos;:&apos;20130104&apos;] 2、Series参考10 Minutes to pandas pandas中Timestamp类用法讲解]]></content>
      <categories>
        <category>PYTHON</category>
      </categories>
      <tags>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（转）HDFS基本文件常用命令]]></title>
    <url>%2F2017%2F07%2F12%2Fhadoop-spark%2F%EF%BC%88%E8%BD%AC%EF%BC%89HDFS%E5%9F%BA%E6%9C%AC%E6%96%87%E4%BB%B6%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[-ls path列出path目录下的内容，包括文件名，权限，所有者，大小和修改时间。 -lsr path与ls相似，但递归地显示子目录下的内容。 -du path显示path下所有文件磁盘使用情况下，用字节大小表示，文件名用完整的HDFS协议前缀表示。 -dus path与-du相似，但它还显示全部文件或目录磁盘使用情况 -mv src dest在HDFS中，将文件或目录从HDFS的源路径移动到目标路径。 -cp src dest在HDFS中，将src文件或目录复制到dest。 –rm path删除一个文件或目录 –rmr path删除一个文件或递归删除目录注意：这里的mv cp操作的源路径和目的路径都是在HDFS中的路径文件 –put localSrc dest将本地文件或目录localSrc上传到HDFS中的dest路径。 –copyFromLocal localSrc dest与-put命令相同 –moveFromLocal localSrc dest将文件或目录从localSrc上传到HDFS中的dest目录，再删除本地文件或目录localSrc。12 –get [-crc] src localDest将文件或目录从HDFS中的src拷贝到本地文件系统localDest。13 –getmerge src localDest [addnl]将在HDFS中满足路径src的文件合并到本地文件系统的一个文件localDest中。14 –cat filename显示文件内容到标准输出上。 -copyToLocal [-crc] src localDest与-get命令相同。16 -moveToLocal [-crc] src localDest与-get命令相似，但拷贝结束后，删除HDFS上原文件。17 -mkdir path在HDFS中创建一个名为path的目录，如果它的上级目录不存在，也会被创建，如同linux中的mkidr –p。18 -setrep [-R] [-w] rep path设置目标文件的复制数。19 -touchz path创建一个文件。时间戳为当前时间，如果文件本就存在就失败，除非原文件长充为0。20 -test –[ezd] path如果路径(path)存在，返回1，长度为0(zero)，或是一个目录(directory)。21 –stat [format] path显示文件所占块数(%b)，文件名(%n)，块大小(%n)，复制数(%r)，修改时间(%y%Y)。22 –tail [-f] file显示文件最后的1KB内容到标准输出。23 –chmod [-R] [owner][:[group]] path…递归修改时带上-R参数，mode是一个3位的8进制数，或是[augo]+/-{rwxX}。24 –chgrp [-R] group设置文件或目录的所有组，递归修改目录时用-R参数。25 –help cmd显示cmd命令的使用信息，你需要把命令的“-”去掉复制到本地hadoop fs -copyToLocal /tmp/admaster_xid_15-05-08/part-r-00298 /home/david/查看文件夹大小hadoop fs -du /shortdata/xmo_info | awk ‘{ sum=$1 ;dir2=$3 ; hum[10243]=”Gb”;hum[10242]=”Mb”;hum[1024]=”Kb”; for (x=1024**3; x&gt;=1024; x/=1024){ if (sum&gt;=x) { printf “%.2f %s \t %s\n”,sum/x,hum[x],dir2;break } }}’]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
      <tags>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 配置和使用]]></title>
    <url>%2F2017%2F07%2F12%2F%E5%B7%A5%E5%85%B7%E5%92%8C%E7%8E%AF%E5%A2%83%2FDocker-%E9%85%8D%E7%BD%AE%E5%92%8C%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[0、基本概念镜像 Docker镜像通过镜像ID进行识别。镜像ID是一个64字符的十六进制的字符串。但是当我们运行镜像时，通常我们不会使用镜像ID来引用镜像，而是使用镜像名来引用。要列出本地所有有效的镜像，可以使用命令 $ docker images 镜像可以发布为不同版本，这种机制我们称之为标签（Tag）。 容器 Docker容器可以使用命令创建： $ docker run imagename 它会在所有的镜像层之上增加一个可写层。这个可写层有运行在CPU上的进程，而且有两个不同的状态：运行态（Running）和退出态（Exited）。这就是Docker容器。当我们使用docker run启动容器，Docker容器就进入运行态，当我们停止Docker容器时，它就进入退出态。 对容器的变更是写入到容器的文件系统的，而不是写入到Docker镜像中的。 用同一个镜像启动多个Docker容器，这些容器启动后都是活动的，彼此还是相互隔离的。我们对其中一个容器所做的变更只会局限于那个容器本身。 如果对容器的底层镜像进行修改，那么当前正在运行的容器是不受影响的，不会发生自动更新现象。 1、配置1.1 安装12$ sudo apt-get update$ sudo apt-get install docker 编辑/etc/default/docker增加如下配置，自定义docker镜像存放路径 1DOCKER_OPTS=&quot;--graph=/home/david/opt/docker&quot; sudo service docker start #启动sudo service docker stop #关闭sudo service docker restart #重启 1.2 pull一个镜像查看镜像：此时应该没有镜像docker images 下载镜像 sudo docker pull ubuntu 1.3 创建容器docker ps命令 123$ sudo docker ps #列出当前所有正在运行的container$ sudo docker ps -l #列出最近一次启动的，且正在运行的container$ sudo docker ps -a #列出所有的container 启动容器，并且进入到Ubuntu容器的bash命令 1$ sudo docker run -itv /home/david/docker:/home/davida/gitlab ubuntu /bin/bash 其中， 1.4 提交容器退出容器，使用 docker commit 命令来提交更新后的副本。 1$ sudo docker commit -m 'xmo run evn' -a 'david' bde4f2f2db5f ubuntu-ruby:v1 其中，-m 来指定提交的说明信息，跟我们使用的版本控制工具一样；-a 可以指定更新的作者信息；之后是用来创建镜像的容器的ID；最后指定目标镜像的仓库名和 tag 信息。创建成功后会返回这个镜像的 ID 信息。v1是tag（版本号） 1$ sudo docker inspect ubuntu-ruby 查看详细信息 1.5 Dockerfile提交容器Dockerfile的指令是忽略大小写的，建议使用大写，使用 # 作为注释，每一行只支持一条指令，每条指令可以携带多个参数。Dockerfile的指令根据作用可以分为两种，构建指令和设置指令。构建指令用于构建image，其指定的操作不会在运行image的容器上执行；设置指令用于设置image的属性，其指定的操作将在运行image的容器中执行。 1）FROM 基础image 基础image可以是官方远程仓库中的，也可以位于本地仓库。 1FROM &lt;image&gt; 或者 1FROM &lt;image&gt;:&lt;tag&gt; 制定某个tag版本 1.4 用SSH访问容器先安装ssh 12apt-get updateapt-get install openssh-server 需要修改/etc/ssh/sshd_config文件中内容 1234RSAAuthentication yes #启用 RSA 认证PubkeyAuthentication yes #启用公钥私钥配对认证方式AuthorizedKeysFile .ssh/authorized_keys #公钥文件路径（和上面生成的文件同）PermitRootLogin yes #root能使用ssh登录 重启sshd服务 12/etc/init.d/ssh stop/etc/init.d/ssh start 2、常用命令docker start和docker run 从图片可以看出，docker run 命令先是利用镜像创建了一个容器，然后运行这个容器。 docker run命令类似于git pull命令。git pull命令就是git fetch 和 git merge两个命令的组合，同样的，docker run就是docker create和docker start两个命令的组合。 docker psdocker ps 命令会列出所有运行中的容器。这隐藏了非运行态容器的存在，如果想要找出这些容器，我们需要使用docker ps -a docker imagesdocker images命令会列出了所有顶层（top-level）镜像。实际上，在这里我们没有办法区分一个镜像和一个只读层，所以我们提出了top-level 镜像。只有创建容器时使用的镜像或者是直接pull下来的镜像能被称为顶层（top-level）镜像，并且每一个顶层镜像下面都隐藏了多个镜像层。 docker images -a docker images –a命令列出了所有的镜像，也可以说是列出了所有的可读层。如果你想要查看某一个image-id下的所有层，可以使用docker history来查看。 docker stop docker stop命令会向运行中的容器发送一个SIGTERM的信号，然后停止所有的进程。 docker commit docker commit命令将容器的可读写层转换为一个只读层，这样就把一个容器转换成了不可变的镜像。 dockesr exec docker exec 命令会在运行中的容器执行一个新进程。 docker save docker save命令会创建一个镜像的压缩文件，这个文件能够在另外一个主机的Docker上使用。和export命令不同，这个命令为每一个层都保存了它们的元数据。这个命令只能对镜像生效。 docker export docker export命令创建一个tar文件，并且移除了元数据和不必要的层，将多个层整合成了一个层，只保存了当前统一视角看到的内容（expoxt后 的容器再import到Docker中，通过docker images –tree命令只能看到一个镜像；而save后的镜像则不同，它能够看到这个镜像的历史镜像）。 参考Ubuntu 15.04下安装Docker Docker的镜像和容器的区别 Docker容器和镜像区别 Docker学习笔记（3）— 如何使用Dockerfile构建镜像]]></content>
      <categories>
        <category>工具和环境</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo + Github 搭建博客入门]]></title>
    <url>%2F2017%2F07%2F12%2F%E5%B7%A5%E5%85%B7%E5%92%8C%E7%8E%AF%E5%A2%83%2FHexo%20%2B%20Github%20%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[1、安装node.js12345$ sudo add-apt-repository ppa:chris-lea/node.js$ sudo apt-get update$ sudo apt-get install nodejs#原方法没有这一步，但是后面的操作会提示npm command not found$ sudo apt-get install npm 2、安装hexo1$ sudo npm install hexo -g 3、初始博客的根目录12$ cd ~/myblog$ hexo init mac的根目录在 1/Users/david/david/myblog 4、在github上新建仓库名称必须是 1gitusername.github.io 我的就是 1Schwimmer.github.io 并将本地的SSH KEY添加到git上（略） 5、让博客可以发布到git1）安装hexo-deployer-Git（不然会出现ERROR Deployer not found: git） 1npm install hexo-deployer-git --save 2） 配置你hexo博客根目录下的_config.yml文件(应该是最下面一行，修改成你的github) 1234deploy: type: git repo: git@github.com:Schwimmer/Schwimmer.github.io.git branch: master tips 冒号后面一定要跟空格 6、hexo常用命令12345hexo clean #清除缓存hexo new &quot;title&quot; #新建文章hexo g #生成html，或hexo generatehexo s #在本地启动服务，启动后访问localhost:4000就可以打开，或hexo serverhexo d #发布到git，发布后访问https://schwimmer.github.io/就可以打开，或hexo deploy tips 我目前用的新建文章的方法，就是直接在source/_posts/下面新建md文件 可以偷懒写成 cd ~/myblog hexo clean;hexo g;hexo s 或 hexo clean;hexo g;hexo d 7、支持数学公式更新： 现在用 如何在 hexo 中支持 Mathjax？ 搭建一个支持LaTEX的hexo博客 1npm install hexo-math --save 这时如果你会发现出了一些问题，原因是hexo先用marked.js渲染，然后再交给MathJax渲染。在marked.js渲染的时候下划线_是被escape掉并且换成了&lt;em&gt;标签，即斜体字，另外LaTeX中的\\也会被转义成一个\，这样会导致MathJax渲染时不认为它是一个换行符了。 为了使Marked.js与MathJax共存，打开node_modules/marked/lib/marked.js并做如下改动 123456Step 1: escape: /^\\([\\`*&#123;&#125;\[\]()# +\-.!_&gt;])/,11替换成 escape: /^\\([`*\[\]()# +\-.!_&gt;])/,11这一步是在原基础上取消了对\\,\&#123;,\&#125;的转义(escape)Step 2: em: /^\b_((?:[^_]|__)+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,11替换成 em:/^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, 1https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;&lt;/script&gt; 8、安装主题转自：NexT主题安装教程 9、文章阅读计数转自：Hexo添加不蒜子和LeanCloud统计无标题文章 找到站点的themes/next/layout/_partials目录下的footer.swig文件。插入代码如下。 123456789101112131415161718192021&#123;% if theme.copyright %&#125;&lt;div class=&quot;powered-by&quot;&gt; &#123;&#123; __(&apos;footer.powered&apos;, &apos;&lt;a class=&quot;theme-link&quot; href=&quot;https://hexo.io&quot;&gt;Hexo&lt;/a&gt;&apos;) &#125;&#125;&lt;/div&gt;&lt;div class=&quot;theme-info&quot;&gt; &#123;&#123; __(&apos;footer.theme&apos;) &#125;&#125; - &lt;a class=&quot;theme-link&quot; href=&quot;https://github.com/iissnan/hexo-theme-next&quot;&gt; NexT.&#123;&#123; theme.scheme &#125;&#125; &lt;/a&gt;&lt;/div&gt;# 此位置插入以下代码&lt;div&gt;&lt;script async src=&quot;https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js&quot;&gt;&lt;/script&gt;本站总访问量 &lt;span id=&quot;busuanzi_value_site_pv&quot;&gt;&lt;/span&gt; 次&amp;nbsp&amp;nbsp&amp;nbsp本站访客数&lt;span id=&quot;busuanzi_value_site_uv&quot;&gt;&lt;/span&gt;人次&lt;/div&gt;&#123;% endif %&#125; 10、增加图片1 把主页配置文件_config.yml 里的post_asset_folder:这个选项设置为true 2 在你的hexo目录下执行这样一句话npm install hexo-asset-image --save，这是下载安装一个可以上传本地图片的插件，来自dalao：dalao的git 3 等待一小段时间后，再运行hexo n &quot;xxxx&quot;来生成md博文时，/source/_posts文件夹内除了xxxx.md文件还有一个同名的文件夹 4 最后在xxxx.md中想引入图片时，先把图片复制到xxxx这个文件夹中，然后只需要在xxxx.md中按照markdown的格式引入图片： ![你想输入的替代文字](xxxx/图片名.jpg) 注意：xxxx是这个md文件的名字，也是同名文件夹的名字，你想引入的图片就只需要放入xxxx这个文件夹内就好了，很像引用相对路径。 5 最后检查一下，hexo g生成页面后，进入public\2017\02\26\index.html文件中查看相关字段，可以发现，html标签内的语句是&lt;img src=&quot;2017/02/26/xxxx/图片名.jpg&quot;&gt;，而不是&lt;img src=&quot;xxxx/图片名.jpg&gt;。这很重要，关乎你的网页是否可以真正加载你想插入的图片。 11、sitemap 插件Hexo Seo优化让你的博客在google搜索排名第一 12&lt;meta name=&quot;google-site-verification&quot; content=&quot;Mx7Ikp0IpBtTbSpHDTBV0_CMJA-E8CLn8NRIrwyq5m4&quot; /&gt;&lt;meta name=&quot;baidu-site-verification&quot; content=&quot;ZBTsWx4NdC&quot; /&gt; 12、首页显示文章摘要 进入hexo博客项目的themes/next目录 用文本编辑器打开_config.yml文件 搜索”auto_excerpt”,找到如下部分： 12345# Automatically Excerpt. Not recommand.# Please use &lt;!-- more --&gt; in the post to control excerpt accurately.auto_excerpt: enable: false length: 150 把enable改为对应的false改为true，然后hexo d -g，再进主页，问题就解决了！ 13、启用分类和标签1、在博客的开头要加上 123456title: Hexo + Github 搭建博客入门date: 2017-07-12 11:49:53categories: &quot;工具和环境&quot;tags: - hexodescription: 2、修改主题配置文档 123456menu: home: / categories: /categories #about: /about archives: /archives tags: /tags 3、hexo加上page 如分类 1hexo new page categories 然后打开source/categories/index.md，增加一行 1type: &quot;categories&quot; 增加标签也是一样 1hexo new page tags 然后打开source/tags/index.md，增加一行 1type: &quot;tags&quot; 谷歌与百度的站点地图，前者适用于其他搜索引擎，用来手动提交以增加收录 安装： 12npm install hexo-generator-sitemap@1 --savenpm install hexo-generator-baidu-sitemap@0.1.1 --save _config.yml添加代码： 12baidusitemap: path: baidusitemap.xml 谷歌的sitemap.xml不需要写到配置文件中，自动生效。 在主页后面加/baidusitemap.xml可以看到baidusitemap（谷歌同理），将该网址它提交给百度搜索：百度站长平台，贴吧账号无法在这里使用。 不过由于Github禁止了百度爬虫，百度无法抓取其中的URL： 绑定代码到Coding创建coding仓库 上传公钥 https://coding.net/user/account/setting/keys 测试SSH Key 是否配置成功 1ssh -T git@git.coding.net 用来部署Hexo博客的Coding项目地址为：git@git.coding.net:ddxy1986/DavidXu-Blog deploy的配置改为 123456deploy: type: git repo: github: git@github.com:Schwimmer/Schwimmer.github.io.git coding: git@git.coding.net:ddxy1986/DavidXu-Blog branch: master 配置Coding项目的Pages服务开启Coding项目的Pages服务 踩过的坑1）若启动时报错 1234 Error: Warning: Permanently added &apos;github.com,192.30.253.112&apos; (RSA) to the list of known hosts.sign_and_send_pubkey: signing failed: agent refused operationPermission denied (publickey).fatal: Could not read from remote repository. 处理是 12$ eval &quot;$(ssh-agent -s)&quot;$ ssh-add 参考：ubuntu下使用hexo搭建博客]]></content>
      <categories>
        <category>工具和环境</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux命令]]></title>
    <url>%2F2017%2F07%2F12%2F%E5%B7%A5%E5%85%B7%E5%92%8C%E7%8E%AF%E5%A2%83%2Flinux%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[各种压缩解压缩rar 解压缩 unrar e XXX.rar 7z 7za x word2vec_c_from_weixin.7z -r -o./ 注意-o和后面的路径之间没有空格 查看gz压缩文件行数 zcat *.gz | wc -l tar 压缩 tar -czvf tracking_ingest_BJIDCnopig.tar.gz tracking_ingest_BJIDCnopig 解压缩 tar -zxvf greenplum-db-4.2.3.1.tar -C /usr/local/ gz gunzip -c gp_dump_1_1_20160806093243.gz &gt; gp_dump_1_1_20160806093243 zip unzip archive_name.zip rar 安装：sudo apt install unrar卸载：sudo apt-get remove unrar rar常用命令主要有:e 将文件解压到当前目录 例:rar e test.rar​ 注:用e解压的话，不仅原来的file1.txt和file2.txt被解压到当前目录，就连dir1里面的所有文件​ 也被解压到当前目录下，不能保持压缩前的目录结构，如果想保持压缩前的目录结构，用x解压x 带路径解压文档中内容到当前目录 例:rar x test.rar​ 这样解压的话，dir1就会保持原来的目录结构 查看OS版本lsb_release -a 看某个端口是否在使用netstat -tunlp |grep 9527 统计指定文件的大小du -c -h adgroup.BJ1.20160623* 看linux某个软件的版本rpm -qa | grep mapr 看文件的指定行 sed -n ‘5,10p’ filename 这样你就可以只查看文件的第5行到第10行。 文件链接ln -s greenplum-db-4.2.3.1/ greenplum-db 停止crontab服务这个命令在red hat当中常用,有的linux发行版本中没有这个命令.$ service crond start //启动服务$ service crond stop //关闭服务$ service crond restart //重启服务 2.linux发行版本没有service这个命令时：/etc/init.d/cron stop/etc/init.d/cron start cronjob路径 /var/spool/cron/ 查看已安装版本号sudo apt-get install apt-show-versions 用apt-show-versions查看 若查看单个软件包的版本 apt-show-versions –p 查看可升级的软件包 apt-show-versions –u centos中查看已安装的包 1yum list installed |grep mysql 查看可以安装的包 yum install mysql mysql-server mysql-devel 指定文件大小总和du -m 201604 | awk ‘{sum += $1}; END{print sum}’ 查看目录结构tree -a 文件夹搜索文件find . -name “*.py” dpkgdpkg命令常用格式如下：sudo dpkg -I iptux.deb#查看iptux.deb软件包的详细信息，包括软件名称、版本以及大小等（其中-I等价于—info）sudo dpkg -c iptux.deb#查看iptux.deb软件包中包含的文件结构（其中-c等价于—contents）sudo dpkg -i iptux.deb#安装iptux.deb软件包（其中-i等价于—install）sudo dpkg -l iptux#查看iptux软件包的信息（软件名称可通过dpkg -I命令查看，其中-l等价于—list）sudo dpkg -L iptux#查看iptux软件包安装的所有文件（软件名称可通过dpkg -I命令查看，其中-L等价于—listfiles）sudo dpkg -s iptux#查看iptux软件包的详细信息（软件名称可通过dpkg -I命令查看，其中-s等价于—status）sudo dpkg -r iptux#卸载iptux软件包（软件名称可通过dpkg -I命令查看，其中-r等价于—remove） #清空文件 > filename #linux 匹配tab ctrl+M+tab #在行首添加字符 sed ‘s/^/HEAD&amp;/g’ test.file #在行尾添加字符 sed ‘s/$/&amp;TAIL/g’ test.file 查找文件夹最近修改的文件查找最近30分钟修改的当前目录下的.php文件 1find . -name &apos;*.php&apos; -mmin -30 查找最近24小时修改的当前目录下的.php文件 1find . -name &apos;*.php&apos; -mtime 0 查找最近24小时修改的当前目录下的.php文件，并列出详细信息 1find . -name &apos;*.inc&apos; -mtime 0 -ls 查找当前目录下，最近24-48小时修改过的常规文件。 1find . -type f -mtime 1 查找当前目录下，最近1天前修改过的常规文件。 1find . -type f -mtime +1 删除 find . -name “*.pyc” | xargs rm ping端口telnet 1.1.1.1 8080 文件去重sort -k2n file | uniq &gt; a.out 当file中的重复行不再一起的时候，uniq没法删除所有的重复行。经过排序后，所有相同的行都在相邻，因此uniq可以正常删除重复行。 统计文件夹大小并排序du -sh * | sort -rn | head -5]]></content>
      <categories>
        <category>工具和环境</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scp免密码登录]]></title>
    <url>%2F2017%2F07%2F12%2F%E5%B7%A5%E5%85%B7%E5%92%8C%E7%8E%AF%E5%A2%83%2Fscp%E5%85%8D%E5%AF%86%E7%A0%81%E7%99%BB%E5%BD%95%2F</url>
    <content type="text"><![CDATA[假设A要免密码传输文件到B 在A上创建秘钥 1ssh-keygen -t rsa 拷贝id_rsa.pub到B的.ssh，并改名为authorized_keys 注意要修改权限 A机器 .ssh目录，以及/home/当前用户 需要700权限，参考以下操作调整 sudo chmod 700 ~/.ssh sudo chmod 700 /home/当前用户 B机器 .ssh目录下的authorized_keys文件需要600或644权限，参考以下操作调整 sudo chmod 600 ~/.ssh/authorized_keys]]></content>
      <categories>
        <category>工具和环境</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elastic search笔记]]></title>
    <url>%2F2017%2F07%2F12%2FElastic%20Search%2Felastic-search%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[过滤字段中为空的通过filter中的exists，如 12345678910111213141516171819GET iclick_persona/iclick/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123;&quot;term&quot;: &#123; &quot;province&quot;: &#123; &quot;value&quot;: &quot;CN_IN_SG&quot; &#125; &#125;&#125; ], &quot;filter&quot;: &#123; &quot;exists&quot;: &#123; &quot;field&quot;: &quot;articles.titles&quot; &#125; &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>Elastic Search</category>
      </categories>
      <tags>
        <tag>elastic search</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利率计算]]></title>
    <url>%2F2017%2F07%2F12%2F%E6%95%B0%E5%AD%A6%2F%E5%88%A9%E7%8E%87%E8%AE%A1%E7%AE%97%2F</url>
    <content type="text"><![CDATA[等额本息 1234每月月供额=〔贷款本金×月利率×(1＋月利率)＾还款月数〕÷〔(1＋月利率)＾还款月数-1〕每月应还利息=贷款本金×月利率×〔(1+月利率)^还款月数-(1+月利率)^(还款月序号-1)〕÷〔(1+月利率)^还款月数-1〕每月应还本金=贷款本金×月利率×(1+月利率)^(还款月序号-1)÷〔(1+月利率)^还款月数-1〕总利息=还款月数×每月月供额-贷款本金 等额本金 12345每月月供额=(贷款本金÷还款月数)+(贷款本金-已归还本金累计额)×月利率每月应还本金=贷款本金÷还款月数每月应还利息=剩余本金×月利率=(贷款本金-已归还本金累计额)×月利率每月月供递减额=每月应还本金×月利率=贷款本金÷还款月数×月利率总利息=还款月数×(总贷款额×月利率-月利率×(总贷款额÷还款月数)*(还款月数-1)÷2+总贷款额÷还款月数) 信用卡账单分期真实年化利率 年利率利息率=利息量÷本金÷时间×100% IRR内部收益率 (IRR) 的定义是：净现值 (NPV) 为零时的折现率。 综合考虑了每期的流入流出现金的量和时间，加权出来的结果。IRR实质上是一个折现率，用IRR折现时会达到该项目的净现值NPV为0的状态。也可以理解为一个项目的预期收益率。举例来说，如IRR为8%，可以简单解释为以8%的利率借钱投资于此项目，刚好可以不赚不赔。]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>利率</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elastic Search 配置和使用]]></title>
    <url>%2F2017%2F07%2F12%2FElastic%20Search%2FElastic%20Search%E9%85%8D%E7%BD%AE%E5%92%8C%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[官网：https://www.elastic.co/products/elasticsearch 最好的教程：https://es.xiaoleilu.com/ docker的ELK环境：https://hub.docker.com/r/sebp/elk/ ES 5.4中文文档 http://cwiki.apachecn.org/pages/viewpage.action?pageId=4260364 0、基本概念接近实时（NRT） Elasticsearch是一个接近实时的搜索平台。这意味着，从索引一个文档直到这个文档能够被搜索到有一个轻微的延迟（通常是1秒）。 集群（cluster） 一个集群就是由一个或多个节点组织在一起，它们共同持有你整个的数据，并一起提供索引和搜索功能。一个集群一个唯一的名字标识，这个名字默认就是 “elasticsearch”。这个名字是重要的，因为一个节点只能通过指定某个集群的名字，来加入这个集群。在产品环境中显式地设定这个名字是一个好 习惯，但是使用默认值来进行测试/开发也是不错的。 节点（node） 一个节点是你集群中的一个服务器，作为集群的一部分，它存储你的数据，参与集群的索引和搜索功能。和集群类似，一个节点也是由一个名字来标识的，默认情况 下，这个名字是一个随机的漫威漫画角色的名字，这个名字会在启动的时候赋予节点。这个名字对于管理工作来说挺重要的，因为在这个管理过程中，你会去确定网 络中的哪些服务器对应于Elasticsearch集群中的哪些节点。 一个节点可以通过配置集群名称的方式来加入一个指定的集群。默认情况下，每个节点都会被安排加入到一个叫做“elasticsearch”的集群中，这意 味着，如果你在你的网络中启动了若干个节点，并假定它们能够相互发现彼此，它们将会自动地形成并加入到一个叫做“elasticsearch”的集群中。 在一个集群里，只要你想，可以拥有任意多个节点。而且，如果当前你的网络中没有运行任何Elasticsearch节点，这时启动一个节点，会默认创建并加入一个叫做“elasticsearch”的集群。 12Relational DB -&gt; Databases -&gt; Tables -&gt; Rows -&gt; ColumnsElasticsearch -&gt; Indices -&gt; Types -&gt; Documents -&gt; Fields 索引（index） 一个索引就是一个拥有几分相似特征的文档的集合。比如说，你可以有一个客户数据的索引，另一个产品目录的索引，还有一个订单数据的索引。一个索引由一个名 字来标识（必须全部是小写字母的），并且当我们要对对应于这个索引中的文档进行索引、搜索、更新和删除的时候，都要使用到这个名字。 在一个集群中，如果你想，可以定义任意多的索引。 类型（type） 在一个索引中，你可以定义一种或多种类型。一个类型是你的索引的一个逻辑上的分类/分区，其语义完全由你来定。通常，会为具有一组共同字段的文档定义一个 类型。比如说，我们假设你运营一个博客平台并且将你所有的数据存储到一个索引中。在这个索引中，你可以为用户数据定义一个类型，为博客数据定义另一个类 型，当然，也可以为评论数据定义另一个类型。 文档（document） 一个文档是一个可被索引的基础信息单元。比如，你可以拥有某一个客户的文档，某一个产品的一个文档，当然，也可以拥有某个订单的一个文档。文档以 JSON（Javascript Object Notation）格式来表示，而JSON是一个到处存在的互联网数据交互格式。 在一个index/type里面，只要你想，你可以存储任意多的文档。注意，尽管一个文档，物理上存在于一个索引之中，文档必须被索引/赋予一个索引的type。 分片和复制（shards &amp; replicas） 一个索引可以存储超出单个结点硬件限制的大量数据。比如，一个具有10亿文档的索引占据1TB的磁盘空间，而任一节点都没有这样大的磁盘空间；或者单个节点处理搜索请求，响应太慢。 为了解决这个问题，Elasticsearch提供了将索引划分成多份的能力，这些份就叫做分片。当你创建一个索引的时候，你可以指定你想要的分片的数量。每个分片本身也是一个功能完善并且独立的“索引”，这个“索引”可以被放置到集群中的任何节点上。 分片之所以重要，主要有两方面的原因： - 允许你水平分割/扩展你的内容容量 - 允许你在分片（潜在地，位于多个节点上）之上进行分布式的、并行的操作，进而提高性能/吞吐量 至于一个分片怎样分布，它的文档怎样聚合回搜索请求，是完全由Elasticsearch管理的，对于作为用户的你来说，这些都是透明的。 在一个网络/云的环境里，失败随时都可能发生，在某个分片/节点不知怎么的就处于离线状态，或者由于任何原因消失了，这种情况下，有一个故障转移机制是非 常有用并且是强烈推荐的。为此目的，Elasticsearch允许你创建分片的一份或多份拷贝，这些拷贝叫做复制分片，或者直接叫复制。 复制之所以重要，有两个主要原因： - 在分片/节点失败的情况下，提供了高可用性。因为这个原因，注意到复制分片从不与原/主要（original/primary）分片置于同一节点上是非常重要的。 - 扩展你的搜索量/吞吐量，因为搜索可以在所有的复制上并行运行 总之，每个索引可以被分成多个分片。一个索引也可以被复制0次（意思是没有复制）或多次。一旦复制了，每个索引就有了主分片（作为复制源的原来的分片）和 复制分片（主分片的拷贝）之别。分片和复制的数量可以在索引创建的时候指定。在索引创建之后，你可以在任何时候动态地改变复制的数量，但是你事后不能改变 分片的数量。 默认情况下，Elasticsearch中的每个索引被分片5个主分片和1个复制，这意味着，如果你的集群中至少有两个节点，你的索引将会有5个主分片和另外5个复制分片（1个完全拷贝），这样的话每个索引总共就有10个分片。 1、安装-5.5.01.1 ElasticSearch下载的 elasticsearch-5.5.0.tar.gz kibana-5.5.0-linux-x86_64.tar.gz 解压到/home/david/opt，在主目录直接运行 1$ bin/elasticsearch 启动服务，启动后，访问localhost:9200，若出现 12345678910111213&#123; &quot;name&quot; : &quot;Jr1It8C&quot;, &quot;cluster_name&quot; : &quot;elasticsearch&quot;, &quot;cluster_uuid&quot; : &quot;JMo_h3-USdegKS1yZ0WCnA&quot;, &quot;version&quot; : &#123; &quot;number&quot; : &quot;5.5.0&quot;, &quot;build_hash&quot; : &quot;260387d&quot;, &quot;build_date&quot; : &quot;2017-06-30T23:16:05.735Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;6.6.0&quot; &#125;, &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125; 则说明安装成功。 1.2 Marvel5.0后集成到了x-pack中 1）安装X-pack到elasticsearch 1$ bin/elasticsearch-plugin install x-pack 2）安装到kibana 1bin/kibana-plugin install x-pack 用户名elastic 密码changeme 1.3 关闭服务关闭Elastic search 1ps -ef | grep elastic 关闭kibana 1fuser -n tcp 5601 2、第一个例子摘自教程 假设我们刚好在Megacorp工作，这时人力资源部门出于某种目的需要让我们创建一个员工目录，这个目录用于促进人文关怀和用于实时协同工作，所以它有以下不同的需求： 数据能够包含多个值的标签、数字和纯文本。 检索任何员工的所有信息。 支持结构化搜索，例如查找30岁以上的员工。 支持简单的全文搜索和更复杂的短语(phrase)搜索 高亮搜索结果中的关键字 能够利用图表管理分析这些数据 2.1 索引员工文档索引含义的区分 你可能已经注意到索引(index)这个词在Elasticsearch中有着不同的含义，所以有必要在此做一下区分: 索引（名词） 如上文所述，一个索引(index)就像是传统关系数据库中的数据库，它是相关文档存储的地方，index的复数是indices 或indexes。 索引（动词） 「索引一个文档」表示把一个文档存储到索引（名词）里，以便它可以被检索或者查询。这很像SQL中的INSERT关键字，差别是，如果文档已经存在，新的文档将覆盖旧的文档。 倒排索引 传统数据库为特定列增加一个索引，例如B-Tree索引来加速检索。Elasticsearch和Lucene使用一种叫做倒排索引(inverted index)的数据结构来达到相同目的。 创建一个员工目录 每个文档的类型为employee。 employee类型归属于索引megacorp。 megacorp索引存储在Elasticsearch集群中。 1234567891011121314151617181920212223242526PUT /megacorp/employee/1&#123; &quot;first_name&quot; : &quot;John&quot;, &quot;last_name&quot; : &quot;Smith&quot;, &quot;age&quot; : 25, &quot;about&quot; : &quot;I love to go rock climbing&quot;, &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ]&#125;PUT /megacorp/employee/2&#123; &quot;first_name&quot; : &quot;Jane&quot;, &quot;last_name&quot; : &quot;Smith&quot;, &quot;age&quot; : 32, &quot;about&quot; : &quot;I like to collect rock albums&quot;, &quot;interests&quot;: [ &quot;music&quot; ]&#125;PUT /megacorp/employee/3&#123; &quot;first_name&quot; : &quot;Douglas&quot;, &quot;last_name&quot; : &quot;Fir&quot;, &quot;age&quot; : 35, &quot;about&quot;: &quot;I like to build cabinets&quot;, &quot;interests&quot;: [ &quot;forestry&quot; ]&#125; 2.2 检索文档1GET /megacorp/employee/1 响应的内容中包含一些文档的元信息，John Smith的原始JSON文档包含在_source字段中。 1234567891011121314151617&#123; &quot;_index&quot;: &quot;megacorp&quot;, &quot;_type&quot;: &quot;employee&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;found&quot;: true, &quot;_source&quot;: &#123; &quot;first_name&quot;: &quot;John&quot;, &quot;last_name&quot;: &quot;Smith&quot;, &quot;age&quot;: 25, &quot;about&quot;: &quot;I love to go rock climbing&quot;, &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ] &#125;&#125; 简单搜索 1GET /megacorp/employee/_search 使用关键字_search来取代原来的文档ID。响应内容的hits数组中包含了我们所有的三个文档。默认情况下搜索会返回前10个结果。 12345678910111213141516171819202122232425262728293031&#123; "took": 6, "timed_out": false, "_shards": &#123; "total": 5, "successful": 5, "failed": 0 &#125;, "hits": &#123; "total": 3, "max_score": 1, "hits": [ &#123; "_index": "megacorp", "_type": "employee", "_id": "2", "_score": 1, "_source": &#123; "first_name": "Jane", "last_name": "Smith", "age": 32, "about": "I like to collect rock albums", "interests": [ "music" ] &#125; &#125;, ...... ] &#125;&#125; 接下来，让我们搜索姓氏中包含“Smith”的员工。要做到这一点，我们将在命令行中使用轻量级的搜索方法。这种方法常被称作查询字符串(query string)搜索，因为我们像传递URL参数一样去传递查询语句： 1GET /megacorp/employee/_search?q=last_name:Smith 2.3 使用DSL语句查询查询字符串搜索便于通过命令行完成特定(ad hoc)的搜索，但是它也有局限性（参阅简单搜索章节）。Elasticsearch提供丰富且灵活的查询语言叫做DSL查询(Query DSL),它允许你构建更加复杂、强大的查询。 DSL(Domain Specific Language特定领域语言)以JSON请求体的形式出现。我们可以这样表示之前关于“Smith”的查询: 12345678GET /megacorp/employee/_search&#123; &quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;last_name&quot; : &quot;Smith&quot; &#125; &#125;&#125; 2.4 更复杂的搜索我们让搜索稍微再变的复杂一些。我们依旧想要找到姓氏为“Smith”的员工，但是我们只想得到年龄大于30岁的员工。我们的语句将添加过滤器(filter),它使得我们高效率的执行一个结构化搜索： 1234567891011121314151617GET /megacorp/employee/_search&#123; &quot;query&quot; : &#123; &quot;filtered&quot; : &#123; &quot;filter&quot; : &#123; &quot;range&quot; : &#123; &quot;age&quot; : &#123; &quot;gt&quot; : 30 &#125; &lt;1&gt; &#125; &#125;, &quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;last_name&quot; : &quot;smith&quot; &lt;2&gt; &#125; &#125; &#125; &#125;&#125; 5.0以后的DSL语法变了 3、数据3.1 文档一个文档不只有数据。它还包含了元数据(metadata)——关于文档的信息。三个必须的元数据节点是： 节点 说明 _index 文档存储的地方 _type 文档代表的对象的类 _id 文档的唯一标识 _index索引(index)类似于关系型数据库里的“数据库”——它是我们存储和索引关联数据的地方。 提示： 事实上，我们的数据被存储和索引在分片(shards)中，索引只是一个把一个或多个分片分组在一起的逻辑空间。然而，这只是一些内部细节——我们的程序完全不用关心分片。对于我们的程序而言，文档存储在索引(index)中。剩下的细节由Elasticsearch关心既可。 我们将会在《索引管理》章节中探讨如何创建并管理索引，但现在，我们将让Elasticsearch为我们创建索引。我们唯一需要做的仅仅是选择一个索引名。这个名字必须是全部小写，不能以下划线开头，不能包含逗号。让我们使用website做为索引名。 _type在应用中，我们使用对象表示一些“事物”，例如一个用户、一篇博客、一个评论，或者一封邮件。每个对象都属于一个类(class)，这个类定义了属性或与对象关联的数据。user类的对象可能包含姓名、性别、年龄和Email地址。 在关系型数据库中，我们经常将相同类的对象存储在一个表里，因为它们有着相同的结构。同理，在Elasticsearch中，我们使用相同类型(type)的文档表示相同的“事物”，因为他们的数据结构也是相同的。 每个类型(type)都有自己的映射(mapping)或者结构定义，就像传统数据库表中的列一样。所有类型下的文档被存储在同一个索引下，但是类型的映射(mapping)会告诉Elasticsearch不同的文档如何被索引。 我们将会在《映射》章节探讨如何定义和管理映射，但是现在我们将依赖Elasticsearch去自动处理数据结构。 _type的名字可以是大写或小写，不能包含下划线或逗号。我们将使用blog做为类型名。 _idid仅仅是一个字符串，它与_index和_type组合时，就可以在Elasticsearch中唯一标识一个文档。当创建一个文档，你可以自定义_id，也可以让Elasticsearch帮你自动生成。 3.2 索引一个文档自定义ID123456PUT /website/blog/123&#123; &quot;title&quot;: &quot;My first blog entry&quot;, &quot;text&quot;: &quot;Just trying this out...&quot;, &quot;date&quot;: &quot;2014/01/01&quot;&#125; Elasticsearch的响应： 12345678910111213&#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;blog&quot;, &quot;_id&quot;: &quot;123&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;created&quot;: true&#125; _version ：Elasticsearch中每个文档都有版本号，每当文档变化（包括删除）都会使_version增加。 自增ID123456POST /website/blog/&#123; &quot;title&quot;: &quot;My second blog entry&quot;, &quot;text&quot;: &quot;Still trying this out...&quot;, &quot;date&quot;: &quot;2014/01/01&quot;&#125; 自动生成的ID有22个字符长，URL-safe, Base64-encoded string universally unique identifiers, 或者叫 UUIDs。 3.3 检索想要从Elasticsearch中获取文档，我们使用同样的_index、_type、_id，但是HTTP方法改为GET： 1GET /website/blog/123?pretty pretty在任意的查询字符串中增加pretty参数，类似于上面的例子。会让Elasticsearch美化输出(pretty-print)JSON响应以便更加容易阅读。_source字段不会被美化，它的样子与我们输入的一致。 {&quot;found&quot;: true}。这意味着文档已经找到。 如果我们请求一个不存在的文档，依旧会得到一个JSON，不过found值变成了false。 此外，HTTP响应状态码也会变成&#39;404 Not Found&#39;代替&#39;200 OK&#39;。我们可以在curl后加-i参数得到响应头： 1curl -i -XGET http://localhost:9200/website/blog/124?pretty 检索文档的一部分通常，GET请求将返回文档的全部，存储在_source参数中。但是可能你感兴趣的字段只是title。请求个别字段可以使用_source参数。多个字段可以使用逗号分隔： 1GET /website/blog/123?_source=title,text 或者你只想得到_source字段而不要其他的元数据，你可以这样请求： 1GET /website/blog/123/_source 3.4 更新整个文档文档在Elasticsearch中是不可变的——我们不能修改他们。如果需要更新已存在的文档，我们可以使用《索引文档》章节提到的index API 重建索引(reindex) 或者替换掉它。 123456PUT /website/blog/123&#123; &quot;title&quot;: &quot;My first blog entry&quot;, &quot;text&quot;: &quot;I am starting to get the hang of this...&quot;, &quot;date&quot;: &quot;2014/01/02&quot;&#125; 在响应中，我们可以看到Elasticsearch把_version增加了，且result是updated update API。这个API 似乎 允许你修改文档的局部，但事实上Elasticsearch遵循与之前所说完全相同的过程，这个过程如下： 从旧文档中检索JSON 修改它 删除旧文档 索引新文档 唯一的不同是update API完成这一过程只需要一个客户端请求既可，不再需要get和index请求了。 3.5 创建新文档请记住_index、_type、_id三者唯一确定一个文档。所以要想保证文档是新加入的，最简单的方式是使用POST方法让Elasticsearch自动生成唯一_id： 12POST /website/blog/&#123; ... &#125; 如果要确保是create操作 1）使用op_type查询参数： 12PUT /website/blog/123?op_type=create&#123; ... &#125; 2）在URL后加/_create做为端点： 12PUT /website/blog/123/_create&#123; ... &#125; 如果包含相同的_index、_type和_id的文档已经存在，Elasticsearch将返回409 Conflict响应状态码 3.6 删除文档使用DELETE方法： 1DELETE /website/blog/123 删除一个文档也不会立即从磁盘上移除，它只是被标记成已删除。Elasticsearch将会在你之后添加更多索引的时候才会在后台进行删除内容的清理。 3.7 MappingElasticSearch的Mapping之字段类型 （一）核心数据类型： （1）string： 默认会被分词，一个完整示例如下 12345678910111213141516171819&quot;status&quot;: &#123; &quot;type&quot;: &quot;string&quot;, //字符串类型 &quot;index&quot;: &quot;analyzed&quot;//分词，不分词是：not_analyzed ，设置成no，字段将不会被索引 &quot;analyzer&quot;:&quot;ik&quot;//指定分词器 &quot;boost&quot;:1.23//字段级别的分数加权 &quot;doc_values&quot;:false//对not_analyzed字段，默认都是开启，分词字段不能使用，对排序和聚合能提升较大性能，节约内存 &quot;fielddata&quot;:&#123;&quot;format&quot;:&quot;disabled&quot;&#125;//针对分词字段，参与排序或聚合时能提高性能，不分词字段统一建议使用doc_value &quot;fields&quot;:&#123;&quot;raw&quot;:&#123;&quot;type&quot;:&quot;string&quot;,&quot;index&quot;:&quot;not_analyzed&quot;&#125;&#125; //可以对一个字段提供多种索引模式，同一个字段的值，一个分词，一个不分词 &quot;ignore_above&quot;:100 //超过100个字符的文本，将会被忽略，不被索引 &quot;include_in_all&quot;:ture//设置是否此字段包含在_all字段中，默认是true，除非index设置成no选项 &quot;index_options&quot;:&quot;docs&quot;//4个可选参数docs（索引文档号） ,freqs（文档号+词频），positions（文档号+词频+位置，通常用来距离查询），offsets（文档号+词频+位置+偏移量，通常被使用在高亮字段）分词字段默认是position，其他的默认是docs &quot;norms&quot;:&#123;&quot;enable&quot;:true,&quot;loading&quot;:&quot;lazy&quot;&#125;//分词字段默认配置，不分词字段：默认&#123;&quot;enable&quot;:false&#125;，存储长度因子和索引时boost，建议对需要参与评分字段使用 ，会额外增加内存消耗量 &quot;null_value&quot;:&quot;NULL&quot;//设置一些缺失字段的初始化值，只有string可以使用，分词字段的null值也会被分词 &quot;position_increament_gap&quot;:0//影响距离查询或近似查询，可以设置在多值字段的数据上火分词字段上，查询时可指定slop间隔，默认值是100 &quot;store&quot;:false//是否单独设置此字段的是否存储而从_source字段中分离，默认是false，只能搜索，不能获取值 &quot;search_analyzer&quot;:&quot;ik&quot;//设置搜索时的分词器，默认跟ananlyzer是一致的，比如index时用standard+ngram，搜索时用standard用来完成自动提示功能 &quot;similarity&quot;:&quot;BM25&quot;//默认是TF/IDF算法，指定一个字段评分策略，仅仅对字符串型和分词类型有效 &quot;term_vector&quot;:&quot;no&quot;//默认不存储向量信息，支持参数yes（term存储），with_positions（term+位置）,with_offsets（term+偏移量），with_positions_offsets(term+位置+偏移量) 对快速高亮fast vector highlighter能提升性能，但开启又会加大索引体积，不适合大数据量用 &#125; 4、结构化查询DSLmatch 相当于and should 相当于or must_not 相当于not Query精确查询 Matchhttps://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl.html match查询 12345678GET /_search&#123; &quot;query&quot;: &#123; &quot;match&quot; : &#123; &quot;message&quot; : &quot;this is a test&quot; &#125; &#125;&#125; Filter判断某个字段不为空 123&quot;filter&quot;: [ &#123; &quot;script&quot;:&#123;&quot;script&quot;:&quot;doc[&apos;interests&apos;].values.length==60&quot;&#125; &#125; ] should5、聚合统计对查询的结果聚合12345678910111213141516171819GET iclick_persona/iclick/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;term&quot;: &#123; &quot;articles.domains&quot;: &quot;www.baby-kingdom.com&quot; &#125;&#125;, &#123; &quot;match&quot;: &#123; &quot;province&quot;: &quot;HK&quot; &#125;&#125;, &#123;&quot;match&quot;: &#123;&quot;interests&quot;: &quot;20&quot;&#125;&#125; ] &#125; &#125;, &quot;aggs&quot;: &#123; &quot;interests&quot;: &#123; &quot;terms&quot;: &#123;&quot;field&quot;: &quot;interests&quot;, &quot;size&quot;: 50 &#125; &#125; &#125;&#125; 统计月活跃度 123456789101112131415161718192021222324GET iclick_persona/iclick/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;term&quot;: &#123; &quot;articles.domains&quot;: &quot;www.baby-kingdom.com&quot; &#125;&#125;, &#123; &quot;term&quot;: &#123; &quot;province&quot;: &quot;HK&quot; &#125;&#125; ], &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;create_time&quot;: &#123;&quot;gte&quot; : &quot;2017-08-10&quot;&#125; &#125; &#125; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;create_time&quot;: &#123; &quot;terms&quot;: &#123;&quot;field&quot;: &quot;create_time&quot;,&quot;size&quot;: 50,&quot;order&quot;: &#123; &quot;_term&quot;: &quot;asc&quot; &#125; &#125; &#125; &#125;&#125; 看香港的人群每天有多少 123456789101112131415161718192021GET iclick_persona/iclick/_search?size=0&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;province&quot;: &quot;HK&quot; &#125;&#125; ] &#125; &#125;, &quot;aggs&quot;: &#123; &quot;dates&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;create_time&quot;, &quot;size&quot;: 100, &quot;order&quot;: &#123; &quot;_term&quot;: &quot;asc&quot; &#125; &#125; &#125; &#125;&#125; DELETE按条件删除 1234567891011121314151617181920212223POST iclick_persona/iclick/_delete_by_query&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;term&quot;: &#123; &quot;create_time&quot;: &#123; &quot;value&quot;: &quot;2017-07-15&quot; &#125; &#125; &#125;, &#123; &quot;term&quot;: &#123; &quot;province&quot;: &#123; &quot;value&quot;: &quot;HK&quot; &#125; &#125; &#125; ] &#125; &#125;&#125;]]></content>
      <categories>
        <category>Elastic Search</category>
      </categories>
      <tags>
        <tag>Elastic Search</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记-最大熵]]></title>
    <url>%2F2017%2F07%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%9C%80%E5%A4%A7%E7%86%B5%2F</url>
    <content type="text"><![CDATA[1、最大熵原理日常生活中，很多事情的发生表现出一定的随机性，试验的结果往往是不确定的，也不知道这个随机现象所服从的概率分布。最大熵的实质就是，在已知部分知识的前提下，关于未知分布最合理的推断就是符合已知知识最不确定或者最随机的推断。任何其他的选择都意味着我们增加了其他的约束和假设。 将最大熵应用到分类，就是最大熵模型。给定一个训练集： T = \{ (x_1,y_1), (x_2,y_2),..., (x_N,y_N)\}其中$x_i \in X$是输入，$y_i \in Y$是输出，X和Y表示输入和输出空间。N为样本数。目标是，利用最大熵原理选出一个最好的分类模型，即对于任意给定的输入$x \in X$，可以以概率$p(y|x)$输出$y \in Y$ 。 按照最大熵原理，应该优先保证模型满足已知的所有约束。思路是，从训练数据T中抽取若干有用的特征，要求这些特征在T上关于经验分布$\tilde{p}(x,y)$的数学期望与它们在模型中关于$p(x,y)$的数学期望相等。这样，一个特征就是一个约束了。 这里就涉及到，特征如何刻画？经验分布如何表示？ 2、特征函数假设通过特征选择，抽取若干特征。特征通常由特征函数来表示。例如 f(x,y) =\left\{\begin{matrix} \begin{aligned} & 1，若x,y满足某个事实 \\ & 0，否则 \end{aligned} \end{matrix}\right.这里的特征不是指输入的某个特征，而是指输入和输出共同的特征。 例如，假设我们需要判断“打”是动词还是量词，已知的训练数据有 (x1,y1)=(一打火柴，量词); (x2,y2)=(三打啤酒，量词); (x3,y3)=(打电话，动词); (x4,y4)=(打篮球，动词); 通过观察，发现“打”前面是数字时，是量词，“打”后面是名词时，是动词。这就是从训练数据中提取的两个特征，可分别用特征函数表示为 3、经验分布经验（概率）分布就是通过对训练集T进行统计得到的分布，用$\tilde p$表示。这里列举两个经验分布 \tilde p(x,y) = \frac {count(x,y)} {N} , \tilde p(x)=\frac {count(x)} {N}其中，count表示出现的次数。 4、约束条件对于任意一个特征函数f，$E{\tilde p}f$ 表示f在训练数据T上关于$\tilde p(x,y)$的数学期望， $E{p}f$ 表示f在训练数据T上关于$p(x,y)$的数学期望。按照期望的定义，我们有 E_{\tilde p}f=\sum_{x,y}\tilde p(x,y)f(x,y) E_{ p}f=\sum_{x,y} p(x,y)f(x,y)其中，p(x,y)是未知的，而建模的目标是生成$p(y|x)$，因此，根据Bayes定理，$p(x,y)=p(x)p(y|x)$。在样本数量足够的条件下，$p(x)$可以用$\tilde p(x)$近似表示。这样 E_{ p}f=\sum_{x,y} \tilde p(x)p(y|x)f(x,y)对于概率分布$p(y|x)$，我们希望特征f的期望值应该和从训练集中得到的特征期望值是一致的，因此，增加约束 E_{ p}f=E_{\tilde p}f假设我们从训练集中抽取了n个特征，相应的，便有n个特征函数$f_i(i=1,2,…,n)$以及n个约束条件 C_i:E_{ p}(f_i)=E_{\tilde p}(f_i) \tag {3-1} 关于约束条件的几何解释 （a）：P是所有可能的概率空间，此时没有约束条件，所有的概率模型$p(y|x)$都是允许的； （b）：增加了一个线性约束条件$C_1$，此时，目标分布$p(y|x)$只能落在由$C_1$定义的线段上； （c）：在（b）的基础上增加了另一个约束条件$C_2$ ，且$C_1 \cap C_2 \neq \varnothing$。此时，目标分布只能落在交点上，即被唯一确定； （d）：在（b）基础上增加了另一个约束$C_3$，且$C_1 \cap C_2 = \varnothing$，此时不存在能够同时满足$C_1$和$C_3$的$p(y|x)$。 利用（3-1）定义的约束条件，我们定义P的一个子空间 C=\{p \in P | E_p(f_i)={\tau}_i, i=1,2,...,n\}5、最大熵模型由于我们的目标是获得一个条件分布，因此这里也采用相应的条件熵 H(p(y|x))=-\sum_{x,y} \tilde p(x)p(y|x)\log p(y|x)可以看出这里也是用$\tilde p(x)$来近似$p(x)$。以下将$H(p(y|x))$简记为$H(p)$。至此，可以给出最大熵模型的完整描述。 对于给定的训练集T，特征函数$f_i(x,y), i=1,2,…n$，最大熵模型就是求解 \underset {p \in C} {max} \ \ H(p) = \begin{pmatrix} -\sum_{x,y} \tilde p(x)p(y|x)\log p(y|x) \end{pmatrix}, \\ s.t. \sum_y p(y|x)=1 \tag {5-1} \\ s.t. \ C=\{p \in P | E_p(f_i)={\tau}_i, i=1,2,...,n\}其中的s.t.是为了保证$p(y|x)$是一个（合法的）条件概率分布。 等价于一个求极小值问题 \underset {p \in C} {min} \ \ -H(p) = \begin{pmatrix} \sum_{x,y} \tilde p(x)p(y|x)\log p(y|x) \end{pmatrix}, \\ s.t. \sum_y p(y|x)=1 \tag {5-2} \\ s.t. \ C=\{p \in P | E_p(f_i)={\tau}_i, i=1,2,...,n\}6、模型求解对于5-1的求解，主要思路和步骤如下： 利用Lagrange乘子将最大熵模型由一个带约束的最优化问题转为无约束的最优化问题，这是一个极小极大问题（min max）。 利用对偶问题等价性，转化为求解上一步得到的极大/极小问题的对偶问题，也是一个极大极小问题。 6.1 原始问题和对偶问题根据（5-2），引入拉格朗日乘子$\lambda=(\lambda_0,\lambda_1,…,\lambda_n)^T$，定义拉格朗日函数 L(p,\lambda) = -H(p) + \lambda_0(1-\sum_y p(y|x))+\sum_{i=1}^n\lambda_i(\tau_i-E_p(f_i)) \tag{6-1}利用对偶性，求解（6-1）的原始问题表示为： \underset {p \in C} {min}\ \underset {\lambda} {max}\ L(p,\lambda) \tag{6-2}对偶问题为： \underset {\lambda} {max}\ \underset {p \in C} {min}\ L(p,\lambda) \tag{6-3}由于$H(p)$是关于p的凸函数，因此要求解最大熵模型，只需求解对偶问题（6-3）即可。 6.1.1 指数形式的解首先求解内部的极小问题。由于$\underset {p \in C} {min}\ L(p,\lambda)$是关于$\lambda$的函数，将其记做： \Psi (\lambda) =\underset {p \in C} {min}\ L(p,\lambda) = L(p_{\lambda}, \lambda) \tag {6-4}其中 p_{\lambda}=\underset {p \in C} {argmin}\ L(p,\lambda)=p_{\lambda}(y|x) \tag {6-5}根据拉格朗日乘子法，求$L(p,\lambda)$对$p(y|x)$的偏导，得（求解过程略）： p_{\lambda}=\frac {1} {Z_{\lambda}(x)} \ \exp(\sum_{i=1}^n \lambda_i f_i(x,y)) \tag{6-6}其中， Z_{\lambda}(x)=\sum_y \exp(\sum_{i=1}^n \lambda_i f_i(x,y)) \tag{6-7}称为规范化因子（normalizing factor）。注意，此时已经没有$\lambda_0$了。 由（6-6）定义的$p_{\lambda}$就是最大熵模型的解，它具有指数形式。其中，$\lambda_i$就是特征$f_i$的权重，越大表示特征越重要。 6.1.2 最大似然估计得到对偶问题的内层极小值问题的解之后，接着求解外层的极大值问题$\underset {\lambda} {max} \ \Psi(\lambda)$。 设其解为 \lambda^* = \underset {\lambda} {argmax} \ \Psi(\lambda) \tag{6-8}则最大熵模型的解为 p^*=p_{\lambda^*} \tag{6-9}根据推导，最大化$\Psi(\lambda)$与最大似然估计是等价的！ 7、最优化方法通用的方法有梯度下降，拟牛顿法等，最大熵模型有两个量身定做的方法：通用迭代尺度法（Generalized Iterative Scaling，GIS）和改进的迭代尺度法（Impoved Iterative Scaling，IIS）。 7.1 GIS算法 算法1： S1：初始化参数，令$\lambda=0$ S2：计算$E_{\tilde p}(f_i),\ i=1,2,…,n$ S3：执行一次迭代，对参数做一次刷新。 ​ 计算$E{p{\lambda}}(f_i)$ ​ FOR i=1,2,…,n DO { ​ $\lambdai\ += \ \eta \log\frac {E{\tilde p}(fi)} {E{p_{\lambda}}(f_i)}$ ​ } S4：检查是否收敛，若未收敛则继续S3 其中，$\eta$是学习率，在实际中取$\frac {1} {C}$，$$，表示训练数据中包含特征最多的那个样本所包含的特征个数。 \Delta\lambda_i=\eta \log\frac {E_{\tilde p}(f_i)} {E_{p_{\lambda}}(f_i)}是校正量。 每次迭代，先用当前的权重估算每个特征$fi$在训练数据中的概率分布的期望，然后逐个与相应的经验分布的期望比较，其偏差程度通过$\log\frac {E{\tilde p}(fi)} {E{p_{\lambda}}(f_i)}$来进行刻画。 收敛条件就是当两次迭代的$\lambda$在一个较小的范围。 GIS每次迭代时间很长，不太稳定，容易溢出，一般不会使用。 7.2 IIS算法与GIS的不同主要在$\Delta\lambda_i$的计算上。IIS通过求解方程 \sum_{x,y} \tilde p(x)p(y|x)f_i(x,y)\exp(\Delta\lambda_i\sum_{i=1}^nf_i(x,y))=\tilde p(f_i)1）若$\sum{i=1}^nf_i(x,y)$为常数，即对任意样本(x,y)，都有$\sum{i=1}^nf_i(x,y)=C$，则 \Delta\lambda_i=\frac {1} {C} \log\frac {E_{\tilde p}(f_i)} {E_{p_{\lambda}}(f_i)}此时，IIS可以看做是GIS的一种推广。 2）若$\sum_{i=1}^nf_i(x,y)$不是常数，则需要通过数值方式来求解$\Delta\lambda_i$，如牛顿法。 8、优缺点优点是：在建模时，只需要集中精力选取特征，不需要花费精力考虑如何使用这些特征，可以灵活使用不同类型的特征。 缺点是计算量大。 参考 【1】 最大熵学习笔记 【2】统计学习方法]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>最大熵</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop笔记]]></title>
    <url>%2F2017%2F07%2F12%2Fhadoop-spark%2Fhadoop%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[查看namenode在集群的每个节点上都有配置文件， vim /etc/hadoop/conf/hdfs-site.xml 1234567&lt;property&gt; &lt;name&gt;dfs.ha.namenodes.iclick&lt;/name&gt; &lt;value&gt;srv-buzz-cloudpmnn1.buzz.com,srv-buzz-cloudpmnn2.buzz.com&lt;/value&gt;&lt;/property&gt;]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模型评估与选择]]></title>
    <url>%2F2017%2F07%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9%2F</url>
    <content type="text"><![CDATA[1、模型评估方法 可重复采样：在训练集小，难以划分训练/测试集是有用。此外，能产生多个不同训练集，对集成学习等方法有很大的好处。但是会改变初始数据集分布。 在初始数据量足够时，用留出法或者交叉验证法。 1.1 留出法Hold-out将数据集D分成两个互斥的集合。 训练/测试集尽量保证数据一致性，用分层采样，正负样本同比例。 由于单次估计结果往往不可靠，使用留出法时，一般要采用若干次随机划分，重复进行实验后取平均值作为评估值。 ‘ 1.2 交叉验证法将D分成k个大小相似的互斥子集，每个子集用分层采样得到。 每次用k-1个子集的并集作为训练集，余下的子集作为测试集。这样获得k组训练/测试集。最终返回是k个测试结果的均值。 常用10折交叉验证。 1.3 可重复采样bootstrapping sampling：给定包含m个样本的数据集D，我们进行采样产生数据集$D’$，每次随机从D中挑选一个样本，将其拷贝放入$D’$，再将样本放回D。重复m次，得到包含m个样本的$D’$。 样本在m次采样中始终不被采到的概率是$(1-\frac 1 m)^m$，取极限得到 {\lim_{m \mapsto \infty }}(1-\frac 1 m)^m \mapsto \frac 1 e\approx0.368即通过bootstrapping，D中有36.8%的样本未出现在$D’$中，于是可以将$D’$作为训练集，$D-D’$作为测试集，这样可以有1/3个未出现在训练集的样本用于测试。测试结果称为“包外估计”（out-of-bag estimate）。 1.4 调参 我们在模型评估时往往用来确定算法和参数。当这些确定后，要用所有的D再训练一次，才是最终的模型。 2、性能度量回归最常用的是“均方误差”（mean squared error） E(f;D) = \frac 1 m \sum_{i=1}^m(f(x_i)-y_i )^2更一般的，对于数据分布D和概率密度函数$p(\cdot )$，均方误差可描述为 E(f;D) = \int_{x \in D}(f(x)-y)^2p(x)dx分类的性能度量更复杂 2.1 错误率和精度错误率：分类错误的样本占总样本的比例 精度：分类正确的样本占总样本的比例]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>西瓜书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈在线最优化求解算法-以CTR预测模型为例]]></title>
    <url>%2F2017%2F07%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%B5%85%E8%B0%88%E5%9C%A8%E7%BA%BF%E6%9C%80%E4%BC%98%E5%8C%96%E6%B1%82%E8%A7%A3%E7%AE%97%E6%B3%95-%E4%BB%A5CTR%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B%E4%B8%BA%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[[TOC] 1、最优化求解问题通常，我们需要求解的最优化问题有如下三类： （1）无约束优化问题： X=\arg \underset{X}{min}f(X)含义是求解X，令目标函数$f(X)$最小。 对于这类问题，在$f(X)$ 是凸函数的前提下，通常做法就是对$f(X)$ 求导，并令$\frac {\partial} {\partial X} f(X) =0$ ，求解可以得到最优值。 凸函数 如果$f(x)$是定义在N维向量空间上的实变量函数，对于在$f(x)$的定义域C上的任意两个点$x_1$和$x_2$，以及任意[0,1]之间的值t都有： f(tX_1 + (1-t)X_2) \leq tf(X_1)+(1-t)f(X_2)\\ \forall X_1,X_2 \in C,\ \ 0 \leq t \leq 1 则称$f(x)$是凸函数。一个函数是凸函数是其存在最优解的充要条件。 此外，如果$f(x)$满足 f(tX_1 + (1-t)X_2)< tf(X_1)+(1-t)f(X_2)\\ \forall X_1,X_2 \in C,\ \ 0 \leq t \leq 1 则$f(x)$为严格凸函数。如下图所示，左边是严格凸函数，右边是凸函数 （2）有等式约束的最优化问题： X=\arg \underset{X}{min}f(X)\\ s.t. h_k(X)=0;k=1,2,...,n含义是在n个等式约束$h_k(X)$ 的条件下求解X，另目标函数$f(X)$最小。 针对有等式的最优化问题，采用拉格朗日乘数法进行求解，通过拉格朗日系数$A=[a_1,a_2,…,a_n]^T$ 把等式约束和目标函数组合成一个式子 X=\arg \underset{X}{min}[f(X)+ A^TH(X)]相当于转化成无约束最优化求解问题，解决方法是分别对X，A求偏导并令其等于0。 （3）不等式约束的优化问题求解 ： X=\arg \underset{X}{min}f(X)\\ s.t. h_k(X)=0;k=1,2,...,n\\ g_l(X)\leq 0;l=1,2,...,m对于不等式约束，通过KKT条件求解。将所有的约束和目标函数写为一个式子 L(X,A,B)=f(X)+A^TH(X)+B^TG(X)KKT条件是说最优值必须满足以下条件： \frac \partial {\partial X} L(X,A,B)=0\\ H(X)=0\\ B^TG(X)=0KKT条件是求解最优值的必要条件，要使其成为充要条件，还需要f(x)为凸函数。 2、批量最优化求解算法一些定义： $i=1,2,…,N$表示向量维度 $j=1,2,…,M$表示样本个数 $t=1,2,…$表示迭代次数 2.1 批量和随机求解我们面对的最优化问题都是无约束的最优化问题（有约束的也可以转成无约束的），因此通常可以将其描述为 W=\arg \underset{W}{min}\ l(W,Z)\\ Z=\{ (X_j,y_j) | j=1,2,...,M \}\\ y_j=h(W,X_j) \tag {2-1-1}就是在已知训练集的情况下，求使得目标函数最小的权重矩阵。其中，$Z$是训练集，$\mathbf{X}$是特征向量，$X_j$是其中一个样本，$Y$是预测值，$y_j$是其中一个样本对应的预测值。一共有M个样本。$h(W,X_j)$ 是特征向量到预测值的映射函数，$ l(W,Z)$ 最优化求解的目标函数，也称为损失函数，$W$ 为特征权重，也就是在损失函数中需要求解的参数。 损失函数一般包括损失项和正则项 常用的损失函数有： （1）平方损失函数（线性回归） 最小二乘法（Ordinary Least Squares）是常用的一种平方损失函数，最小二乘的基本原理是：最优拟合直线应该是使各点到回归直线的距离和最小的直线，即平方和最小。 线性回归的映射函数为： h(W,X_j)=W^TX_j损失函数可以表示为 l(W,Z)=\sum_{j=1}^M (y_j-W^TX_j)^2（2）Logistics损失函数（逻辑回归） 逻辑回归的映射函数为： h(W,X_j)=\frac 1 {1+e^{-W^TX_j}} logistic函数的优点是： 1、他的输入范围是$-\infty \rightarrow + \infty $ ，输出范围是(0,1)，正好满足概率分布为（0，1）的要求。我们用概率去描述分类器，自然比单纯的某个阈值要方便很多； 2、是一个单调上升的函数，具有良好的连续性，不存在不连续点。 由于该函数服从伯努利分布（0-1分布），通过最大似然估计，对于每一维的权重W，损失函数可以表示为 l(W,Z)=(Y-h_W(\mathbf X))X 推导过程 令 h_W(X) = \frac 1 {1+e^{-W^T\mathbf X}}该函数服从伯努利分布（一次点击要么成功，要么失败，通过训练集可以知道不同特征组合下成功和失败的概率） P(Y=1 | \mathbf X;W) = h_W(\mathbf X)\\ P(Y=0 | \mathbf X;W) = 1-h_W(\mathbf X)则概率分布函数为 P(Y|\mathbf X;W) = (h_W(\mathbf X))^Y*(1-h_W(\mathbf X))^{1-Y}（也就是说，我们有样本，通过样本能知道概率分布，那么我们需要知道得到这个概率分布的最有可能的参数W。即我们通过样本知道一些特征组合下的点击率，现在需要求概率函数中的系数。） 我们假设样本数据相互独立，所以它们的联合分布可以表示为各边际分布的乘积，用似然函数表示为： \begin{aligned} L(W)=P(Y|\mathbf X;W) &= (h_W(\mathbf X))^Y(1-h_W(\mathbf X))^{1-Y}\\ &=\prod_{j=1}^M(h_W(X_j))^{y_j}(1-h_W(X_j))^{1-y_j} \end{aligned} \tag {2-1-2}从而，损失函数的求解，可以转化为求最有可能导致这样概率分布的W，也就是求L(W)的最大值。最简单的方法就是对W求偏导，并令导数为零。 在多数情况下，直接对变量进行求导反而会使得计算式子更加的复杂，此时可以借用对数函数。由于对数函数是单调增函数，因此与（2-1-2）具有相同的最大值，上式变为 \begin{aligned} l(W) &= Log\ L(W)\\ &=\sum_{j=1}^M(y_jln\ h(X_j)+(1-y_j)ln\ (1-h(X_j))) \end{aligned}对其求关于W的偏导 首先求logistic函数的导数，得（最后一个X是对$W^TX$的求导） h_W^{'}(\mathbf X) = h_W(\mathbf X)(1-h_W(\mathbf X))X 推导过程如下 为了求解方便，将l(W)转为（其实1/M没用，完全可以去掉，不懂为何要加上） J(W) = -\frac {1}{M} l(W)则就变成求J(W)的最小值。求偏导的过程如下： 最后得到目标函数（损失函数）为： \frac {\partial }{\partial W}J(W) =-\frac{1}{M} (Y-h_W(\mathbf X))X 对于损失函数的求解，一个典型的方法就是梯度下降法，由于损失函数是凸函数，因此沿着梯度下降的方向找到最小点。 假设样本总数为n，批量梯度下降是： Repeat\ until\ convergence \{ \\ W^{(t+1)} := W^t - \eta^t\triangledown _{W}l(W^{t},Z) \\ \}\\ \tag{1-2}而随机梯度下降（SGD）是： Repeat\ until\ convergence \{ \\ for\ j=1\ to\ M, \{ \\ W^{(t+1)} := W^t - \eta^t\triangledown _{W}l(W^{t},Z_j) \\ \}两者的区别是： 前者每次更新$W$都需要遍历一次整个样本集合；而后者在遍历样本集合的时候，每个样本都能改变$W$ ，有更快的收敛速度 。由于SGD针对观测到的随机一条数据进行权重的更新，很适合进行增量计算，实现梯度下降的online模式。 2.2 正则化正则化的主要目的是防止过拟合。对于损失函数构成的模型，可能会出现有些权重很大，有些权重很小的情况，导致过拟合，使得模型的复杂度提高，泛化能力较差（对未知数据的预测能力）。 而正则化就是对损失函数中权重的限制，限制其模不要太大： W=\arg \underset{W}{min}\ l(W,Z)\\ s.t. \Psi(W)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>最优化问题</tag>
        <tag>CTR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单聚类算法]]></title>
    <url>%2F2017%2F07%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E7%AE%80%E5%8D%95%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[canopy 12345678while D is not empty select element d from D to initialize canopy c remove d from D Loop through remaining elements in D if distance between d_i and c &lt; T1 : add element to the canopy c if distance between d_i and c &lt; T2 : remove element from D end add canopy c to the list of canopies C]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive笔记-orc格式读写]]></title>
    <url>%2F2017%2F07%2F12%2Fhadoop-spark%2Fhive%E7%AC%94%E8%AE%B0-orc%E6%A0%BC%E5%BC%8F%E8%AF%BB%E5%86%99%2F</url>
    <content type="text"><![CDATA[需要引入的包： 123456789101112131415&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-core&lt;/artifactId&gt; &lt;version&gt;2.6.0-mr1-cdh5.9.1&lt;/version&gt; &lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-common&lt;/artifactId&gt; &lt;version&gt;1.1.0-cdh5.9.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-exec&lt;/artifactId&gt; &lt;version&gt;1.1.0-cdh5.9.1&lt;/version&gt; &lt;/dependency&gt; 1234567891011121314151617181920212223242526272829val fs = FileSystem.get(conf);val prop = Config.getConfig("config.properties")println(sdf.format(new Date))val readerOpts = OrcFile.readerOptions(conf)val reader = OrcFile.createReader(new Path(iaxReqPath+"/ds=17-08-21/20170821000000antispam_2529853576425433.orc"), readerOpts)val inspector = reader.getObjectInspector().asInstanceOf[StructObjectInspector]val count = reader.getNumberOfRowsinfo("the count is: " + count.toString())val fields = inspector.getAllStructFieldRefs()fields.foreach &#123; x =&gt; println(x.getFieldObjectInspector.getCategory) &#125;val records = reader.rows()var n=0val loop = new Breaksloop.breakable(&#123; while(records.hasNext)&#123; if (n&gt;5) loop.break val row = records.next(null) val valueList = inspector.getStructFieldsDataAsList(row) info(valueList.get(10).toString) info(row.toString()) n = n+1 &#125;&#125;)]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>orc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集成学习]]></title>
    <url>%2F2017%2F07%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[个体学习器一般是弱学习器。 弱学习器是指泛华性能略优于随机猜测的学习器，例如二分上略高于50%的学习器。 要获得好的集成，个体学习器应“好而不同”，即个体学习器要有一定的准确性和多样性（学习器之间有差异）。 理论上，假设个体学习器的误差是相互独立，那么随着学习器数量增大，集成的错误率将指数下降，最终趋向于零。 但实际上不可能相互独立。且准确性和多样性本身就是矛盾的，追求准确性就要牺牲多样性。所以如何产生并结合“好而不同”的学习器，是集成学习研究的核心。 根据集成的方式不同， 1）个体学习器存在强依赖性，必须串行生成，如Boosting； 2）个体学习器间不存在强依赖关系，可同时并行生成，如Bagging和随机森林。 1、Boosting工作机制： 先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使前一个做错的样本在后续得到更多关注，然后基于调整后的样本分布来训练下一个基学习器；重复迭代s达到T。 代表是AdaBoost. 1.1 算法描述假设$y_i \in {-1,+1}$ AdsBoost有多种推广方式，一般的是基于加性模型（additive model），即用基学习器的线性组合 H(x) = \sum_{t=1}^T a_th_t(x)来最小化指数损失函数 l_{exp}(H|D) =E_{x \sim D}[e^{-f(x)H(x)}]求偏导 \frac {\partial l_{exp(H|D)}} {\partial H(x)} =后面看不懂了。。。 2、Bagging 3、随机森林]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>西瓜书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[查找树]]></title>
    <url>%2F2017%2F07%2F12%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E6%9F%A5%E6%89%BE%E6%A0%91%2F</url>
    <content type="text"><![CDATA[动态查找树主要有：二叉查找树（Binary Search Tree），平衡二叉查找树（Balanced Binary Search Tree），红黑树 (Red-Black Tree )，B-tree/B+-tree/ B*-tree (B~Tree)。前三者是典型的二叉查找树结构，其查找的时间复杂度*O(log2N)*与树的深度相关，那么降低树的深度自然对查找效率是有所提高的；还有一个实际问题：就是大规模数据存储中，实现索引查询这样一个实际背景下，树节点存储的元素数量是有限的（如果元素数量非常多的话，查找就退化成节点内部的线性查找了），这样导致二叉查找树结构由于树的深度过大而造成磁盘I/O读写过于频繁，进而导致查询效率低下（为什么会出现这种情况，待会在外部存储器-磁盘中有所解释），那么如何减少树的深度（当然是不能减少查询的数据量），一个基本的想法就是：采用多叉树结构（由于树节点元素数量是有限的，自然该节点的子树数量也就是有限的）。 这样我们就提出了一个新的查找树结构——多路查找树。根据平衡二叉树的启发，自然就想到平衡多路查找树结构，也就是B树 1、B树1.1 原理二叉搜索树： 所有非叶子结点至多拥有两个儿子（ Left 和 Right ）； 所有结点存储一个关键字； 非叶子结点的左指针指向小于其关键字的子树，右指针指向大于其关键字的子树； 1.2 搜索方法 B 树的搜索，从根结点开始，如果查询的关键字与结点的关键字相等，那么就命中；否则，如果查询关键字比 结点关键字小，就进入左儿子；如果比结点关键字大，就进入右儿子；如果左儿子或右儿子的指针为空，则报告找不到相应的关键字； 如果 B 树的所有非叶子结点的左右子树的 结点数目均保持差不多（平衡），那么 B 树的搜索性能逼近二分查找；但它比连续内存空间的二分查找的优点是，改变 B 树结构（插入与删除结点）不需要移动大段的内存数据，甚至通常是常数开销；如 但 B 树在经过多次插入与删除后，有可能导致不同的结构： 右边也是一个 B 树，但 它的搜索性能已经是线性的了；同样的关键字集合有可能导致不同的树结构索引；所以，使用 B 树还 要考虑尽可能让 B 树保持左图的结构，和避免右图的结构，也就是所谓的“平衡”问题； 实际使用的 B 树都是在原 B 树的基 础上加上平衡算法，即“平衡二叉树”；如何保持 B 树结点分布均匀的平衡算法是平衡二叉树的 关键；平衡算法是一种在 B 树中插入和删除结点的策略； 2、B-树1.1 原理是一种多路搜索树（并不是二叉的）： ​ 1. 定义任意非叶子结点最多只有 M 个儿 子；且 M&gt;2 ； ​ 2. 根结点的儿子数为 [2, M] ； ​ 3. 除根结点以外的非叶子结点的儿子数为 [M/2, M] ； ​ 4. 每个结点存放至少 M/2-1 （取 上整）和至多 M-1 个关键字；（至少 2 个关键 字） ​ 5. 非叶子结点的关键字个数 = 指向儿 子的指针个数 -1 ； ​ 6. 非叶子结点的关键字： K[1], K[2], …, K[M-1] ；且 K[i] &lt; K[i+1] ； ​ 7. 非叶子结点的指针： P[1], P[2], …, P[M] ；其中 P[1] 指向关键字小于 K[1] 的子树， P[M] 指向关键字大于K[M-1] 的子树，其它 P[i] 指 向关键字属于 (K[i-1], K[i]) 的子树； ​ 8. 所有叶子结点位于同一层； 如：（ M=3 ） B- 树的特性： ​ 1. 关键字集合分布在整颗树中； ​ 2. 任何一个关键字出现且只出现在一个结点中； ​ 3. 搜索有可能在非叶子结点结束； ​ 4. 其搜索性能等价于在关键字全集内做一次二分查找； ​ 5. 自动层次控制； ​ 由于限制了除根结点以外的非叶子结点，至少含有 M/2 个儿子，确保了结点的至少利用率，其最底搜索性能为： 其中， M 为设定的非叶子结点最多子树个 数， N 为关键字总数； ​ 所以 B- 树的性能总是等价于二分查找 （与 M 值无关），也就没有 B 树平衡 的问题； ​ 由于 M/2 的限制，在插入结点时，如果 结点已满，需要将结点分裂为两个各占 M/2 的结点；删除结点时，需将两个不足 M/2 的 兄弟结点合并； 1.2 方法参考 BTree,B-Tree,B+Tree,B*Tree都是什么 B-tree/B+tree/B*tree]]></content>
      <categories>
        <category>算法与数据结构</category>
      </categories>
      <tags>
        <tag>树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[广告反作弊]]></title>
    <url>%2F2017%2F07%2F12%2F%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A%2F%E5%B9%BF%E5%91%8A%E5%8F%8D%E4%BD%9C%E5%BC%8A%2F</url>
    <content type="text"><![CDATA[秒针发布过《互联网广告反作弊技术白皮书》 腾讯灯塔联手秒针、AdMaster发布广告反欺诈白皮书 部分有关 广告联盟作弊 与反作弊资料收集]]></content>
      <categories>
        <category>计算广告</category>
      </categories>
      <tags>
        <tag>反作弊</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive笔记]]></title>
    <url>%2F2017%2F07%2F12%2Fhadoop-spark%2Fhive%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[查询时如何去掉重复数据假设数据为 1234name adx tran_id cost tsck 5 125.168.10.0 33.00 1407234660ck 5 187.18.99.00 33.32 1407234661ck 5 125.168.10.0 33.24 1407234661 1select * from (select *,row_number() over (partition by tran_id order by timestamp asc) num from table) t where t.num=1; 附上：ROW_NUMBER() OVER函数的基本用法 语法：ROW_NUMBER() OVER(PARTITION BY COLUMN ORDER BY COLUMN) 简单的说row_number()从1开始，为每一条分组记录返回一个数字，这里的ROW_NUMBER() OVER (ORDER BY xlh DESC) 是先把xlh列降序，再为降序以后的没条xlh记录返回一个序号。示例： 123456&gt; xlh row_num &gt; 1700 1 &gt; 1500 2 &gt; 1085 3 &gt; 710 4 &gt; &gt; row_number() OVER (PARTITION BY COL1 ORDER BY COL2) 表示根据COL1分组，在分组内部根据 COL2排序，而此函数计算的值就表示每组内部排序后的顺序编号（组内连续的唯一的) 查询ES表报错1Failed with exception java.io.IOException:org.elasticsearch.hadoop.rest.EsHadoopInvalidRequest: The number of slices [1126] is too large. It must be less than [1024]. This limit can be set by changing the [index.max_slices_per_scroll] index level settin 修改es的设置 1234567891011PUT /megacorp/_settings&#123; &quot;index&quot;: &#123; &quot;max_slices_per_scroll&quot; : 1126 &#125;&#125; 切换队列1set mapred.job.queue.name=data; sqoop切换队列是 1-D mapred.job.queue.name=data]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java出错汇总]]></title>
    <url>%2F2017%2F07%2F12%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FJAVA%2Fjava%E5%87%BA%E9%94%99%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[MD5线程不安全MessageDigest线程不安全，多线程下会报错 12345678910111213java.lang.ArrayIndexOutOfBoundsException at java.lang.System.arraycopy(Native Method) at sun.security.provider.DigestBase.engineUpdate(DigestBase.java:114) at java.security.MessageDigest$Delegate.engineUpdate(MessageDigest.java:584) at java.security.MessageDigest.update(MessageDigest.java:335) at com.buzzinate.common.util.hash.MD5Util.getMD5String(MD5Util.java:98) at com.buzzinate.common.util.hash.MD5Util.getMD5String(MD5Util.java:94) 改用apache的commons-codec 123456789&lt;dependency&gt; &lt;groupId&gt;commons-codec&lt;/groupId&gt; &lt;artifactId&gt;commons-codec&lt;/artifactId&gt; &lt;version&gt;1.10&lt;/version&gt; &lt;/dependency&gt; 使用方法 1234567public static String encodeMD5Hex(String data) &#123; return DigestUtils.md5Hex(data); &#125; 该方法是线程安全的 Eclipse闪退每次闪退后都提示查看\workspace.metadata.log，发现有如下异常信息记录： 1234!ENTRY org.eclipse.e4.ui.workbench.swt 4 2 2016-08-23 08:42:49.516 !MESSAGE Problems occurred when invoking code from plug-in: &quot;org.eclipse.e4.ui.workbench.swt&quot;. !STACK 0 java.lang.IllegalArgumentException: Argument cannot be null 出现该问题的原因是：由于项目没有正常关闭运行而导致”workbench.xmi”中的”persistedState”标签还保持在运行时的配置造成的。 解决办法： 找到&lt;workspace&gt;/.metadata/.plugins/org.eclipse.e4.workbench/workbench.xmi文件，将其删掉，再重启Eclipse，恢复正常。]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>java错误</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java笔记]]></title>
    <url>%2F2017%2F07%2F12%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FJAVA%2Fjava%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[list转array123List&lt;Double&gt; scores = new ArrayList&lt;Double&gt;();Double[] arrays = scores.toArray(new Double[0]);]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[maven常用命令]]></title>
    <url>%2F2017%2F07%2F12%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FJAVA%2Fmaven%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[创建一个简单的Java工程：mvn archetype:create -DgroupId=com.mycompany.example -DartifactId=Example 创 建一个java的web工程：mvn archetype:create -DarchetypeGroupId=org.apache.maven.archetypes -DarchetypeArtifactId=maven-archetype-webapp -DgroupId=com.mycompany.app -DartifactId=my-webapp 打包：mvn package 编译：mvn compile 编译测试程序：mvn test-compile 清空：mvn clean 运行测试：mvn test 生成站点目录: mvn site 生成站点目录并发布：mvn site-deploy 安装当前工程的输出文件到本地仓库: mvn install 安 装指定文件到本地仓库：mvn install:install-file -DgroupId= -DartifactId= -Dversion=1.0.0 -Dpackaging=jar -Dfile= 查看实际pom信息: mvn help:effective-pom 分析项目的依赖信息：mvn dependency:analyze 或 mvn dependency:tree 跳过测试运行maven任务： mvn -Dmaven.test.skip=true XXX 生成eclipse项目文件: mvn eclipse:eclipse 查看帮助信息：mvn help:help 或 mvn help:help -Ddetail=true 查看插件的帮助信息：mvn :help，比如：mvn dependency:help 或 mvn ant:help 等等。 跳过测试安装: mvn clean install -Dmaven.test.skip=true mvn jetty:run mvn tomcat:run mvn clean package -Dmaven.test.skip=true mvn clean install -Dmaven.test.skip=true 如果要修改生成jar包的名称，要改 build -&gt;finalname属性 maven导出jar包需要在pom文件的build节点的plugins节点内添加一个plugin，plugin内容如下：maven-dependency-plugin 12345678910111213141516171819202122232425262728293031323334353637&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;copy-dependencies&lt;/id&gt; &lt;phase&gt;prepare-package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;copy-dependencies&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;outputDirectory&gt;$&#123;project.build.directory&#125;/lib&lt;/outputDirectory&gt; &lt;overWriteReleases&gt;false&lt;/overWriteReleases&gt; &lt;overWriteSnapshots&gt;false&lt;/overWriteSnapshots&gt; &lt;overWriteIfNewer&gt;true&lt;/overWriteIfNewer&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; maven引用本地项目 maven上传至中央仓库1、确认settings.xml中是否有用户密码 1234567891011121314151617181920212223&lt;servers&gt; &lt;server&gt; &lt;id&gt;buzzinate&lt;/id&gt; &lt;username&gt;buzzinate&lt;/username&gt; &lt;password&gt;buzzinate&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;buzzinate-snapshots&lt;/id&gt; &lt;username&gt;buzzinate&lt;/username&gt; &lt;password&gt;buzzinate&lt;/password&gt; &lt;/server&gt;&lt;/servers&gt; 2、如果是SNAPSHOT版本，在POM中增加 123456789101112131415 &lt;distributionManagement&gt; &lt;snapshotRepository&gt; &lt;id&gt;buzzinate-snapshots&lt;/id&gt; &lt;name&gt;snapshots name&lt;/name&gt; &lt;url&gt;http://17.21.6.1:8081/nexus/content/repositories/snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt; &lt;/distributionManagement&gt; &lt;/project&gt; 重点是id需要跟settings中一致 3、增加源码插件，上传后可以看到源码和javadoc 1234567891011121314151617181920212223242526272829&lt;plugin&gt; &lt;!-- 源码插件 --&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-source-plugin&lt;/artifactId&gt; &lt;version&gt;2.1&lt;/version&gt; &lt;!-- 发布时自动将源码同时发布的配置 --&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;attach-sources&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;jar&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; 4、执行mvn deploy 如果有时候编译失败，可能是因为网络不通，导致元数据读取失败了，用 mvn compile -U 强制更新 部署到服务器复制jar包，新建conf，复制配置文件新建lib，复制lib下的jar包新建shell，编写启动脚本 123456export JAVA_HOME=/usr/local/lib/jvm/jdk1.7.0_71/CLASSPATH=conf:log4jtest.jarfor f in lib/*.jar; do CLASSPATH=$&#123;CLASSPATH&#125;:$fdone$JAVA_HOME/bin/java -server -Dfile.encoding=UTF-8 -Xms2G -Xmx2G -XX:+UseConcMarkSweepGC -XX:+CMSIncrementalMode -XX:+CMSIncrementalPacing -XX:CMSIncrementalDutyCycleMin=0 -XX:CMSIncrementalDutyCycle=10 -XX:MaxNewSize=1024M -XX:MaxPermSize=256M -XX:+DisableExplicitGC -cp $CLASSPATH com.iclick.rocket.examples.main.log4jMain &gt; test.log 其中，Xms1G JVM初始分配的堆内存]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala笔记]]></title>
    <url>%2F2017%2F07%2F12%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FJAVA%2Fscala%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[获取目录下所有文件路径 123456789101112import java.io.PrintWriterimport java.io.Fileimport scala.reflect.io.Directoryobject Test &#123; def main(args: Array[String]): Unit = &#123; val dir = new File(&quot;F:\\joke\\DCIM\\299MEDIA&quot;) val children = dir.listFiles() for ( d &lt;- children) println(d) &#125;&#125; 结果为123456F:\joke\DCIM\299MEDIA\YDXJ0580.THMF:\joke\DCIM\299MEDIA\YDXJ0580_thm.mp4F:\joke\DCIM\299MEDIA\YDXJ0581.THMF:\joke\DCIM\299MEDIA\YDXJ0581_thm.mp4F:\joke\DCIM\299MEDIA\YDXJ0582.THMF:\joke\DCIM\299MEDIA\YDXJ0582_thm.mp4]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu java反编译]]></title>
    <url>%2F2017%2F07%2F12%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FJAVA%2Fubuntu-java%E5%8F%8D%E7%BC%96%E8%AF%91%2F</url>
    <content type="text"><![CDATA[从https://sourceforge.net/projects/jadx/下载 假设安装路径为~/opt/jadx 12cd ~/opt/jadx/jadx/build/jadx/bin./jadx-gui 打开gui，拖入jar包或者class文件]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>java反编译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pig笔记]]></title>
    <url>%2F2017%2F07%2F12%2Fhadoop-spark%2Fpig%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[JOIN的优化1、Replicated Join ​ 当进行Join的一个表比较大，而其他的表都很小(能够放入内存)时，replicated join会非常高效。 ​ 在Join时使用 Using ‘replicated’语句来触发Replicated Join，大表放置在最左端，其余小表(可以有多个)放置在右端。 ​ 为了防止replicated join应用于大表的连接关系，pig会在这个连接关系复制的字节大小比pig.join.replicated.max.bytes属性的值大会失败 (default = 1GB)。 2、Skewed Join ​ 当进行Join的两个表的内连接时，一个表数据记录针对key的分布极其不均衡的时候使用，如果多于两个连接，要自己拆分成多个双表的连接。 ​ pig.skewedjoin.reduce.memusage属性的值指定了reduce可以占用堆内存的百分数，低的分数可以让pig执行更多的reducer，但是增加了复制的成本。性能好的范围值在0.1到0.4，但是这仅仅是一个范围。这个值取决于这个操作的可用的堆内存和这个输入的行数和倾斜。默认值是0.5。 ​ Skewed Join并没有专注于解决或者说的平衡这种不均匀的数据分布在reducer，而是确保这个Join连接能够完成而不是失败，但是会慢。他会增加5%的时间用于计算这个连接操作。 3、Merge Join ​ 当进行Join的两个表都已经用Join的键进行了排序，可以使用Merge Join。 可以在Join时使用Using ‘merge’语句来触发Merge Join，需要创建索引的表放置在右端。 另外，在进行Join之前，首先过滤掉key为Null的数据记录可以减少Join的数据量。]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
      <tags>
        <tag>pig</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python笔记]]></title>
    <url>%2F2017%2F07%2F12%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FPYTHON%2Fpython%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[1 容器http://blog.chedushi.com/archives/8331 1.1 序列Python中有6种内建的序列。其中列表和元组是最常见的类型。其他包括字符串、Unicode字符串、buffer对象和xrange对象。 1.1.1 列表[]列表是可变的，这是它区别于字符串和元组的最重要的特点，一句话概括即：列表可以修改，而字符串和元组不能。 1、创建 类似于javascript的数组 12list1 = [&apos;a&apos;,&apos;b&apos;]list2 = [1,2] 2、list函数 12list3 = list(&quot;hello&quot;)print list3 输出 [‘h’, ‘e’, ‘l’, ‘l’, ‘o’] 3、过滤 1[elem for elem in li if len(elem) &gt; 1] 列表的划分123456789a=[1,2,3,4]#不包括1a[:1]#包括2a[2:]if featureVec[axis] == value: reducedFeatVec = featureVec[:axis] reducedFeatVec.extend(featureVec[axis+1:]) 取数组的第0维 dataSet.shape[0] 列表的合并1234circle_file = glob.glob(os.path.join(self.resource_dir, &apos;circle/*.png&apos;))table_file = glob.glob(os.path.join(self.resource_dir, &apos;table/*.png&apos;)) # 直接相加 self.jump_file = [cv2.imread(name, 0) for name in circle_file + table_file] generator转list123import jiebaa = jieba.cut(&apos;我喜欢吃土豆&apos;)b = list(a) 1.1.2 元组()元组与列表一样，也是一种序列，唯一不同的是元组不能被修改（字符串其实也有这种特点）。 1、创建 123456t1=1,2,3t2=&quot;jeffreyzhao&quot;,&quot;cnblogs&quot;t3=(1,2,3,4)t4=()t5=(1,)print t1,t2,t3,t4,t5 输出： (1, 2, 3) (‘jeffreyzhao’, ‘cnblogs’) (1, 2, 3, 4) () (1,) 从上面我们可以分析得出： a、逗号分隔一些值，元组自动创建完成； b、元组大部分时候是通过圆括号括起来的； c、空元组可以用没有包含内容的圆括号来表示； d、只含一个值的元组，必须加个逗号（,）； 2、tuple函数 tuple函数和序列的list函数几乎一样：以一个序列（注意是序列）作为参数并把它转换为元组。如果参数就是元组，那么该参数就会原样返回 12345678t1=tuple([1,2,3])t2=tuple(&quot;jeff&quot;)t3=tuple((1,2,3))print t1print t2print t3t4=tuple(123)print t45 输出： (1, 2, 3)(‘j’, ‘e’, ‘f’, ‘f’)(1, 2, 3) Traceback (most recent call last):File “F:\Python\test.py”, line 7, in t4=tuple(123)TypeError: ‘int’ object is not iterable 列表的扩展123456789a=[1,2,3]b=[4,5,6]a.append(b)[1,2,3,[4,5,6]]a.extend(b)[1,2,3,4,5,6] 1.1.3 词典{}12345678prices = &#123; &apos;A&apos;:123, &apos;B&apos;:450.1, &apos;C&apos;:12, &apos;E&apos;:444,&#125;prices[&apos;A&apos;] 判断词典是否包含某个元素1234labelCount=&#123;&#125;for feature in dataSet: label = feature[-1] if label not in labelCount[label]: labelCount[label] = 0 字段的遍历12for key in labelCount: print labelCount[key] 词典排序词典增加元素参考onsite.py 1234567891011121314#比如有个词典action = &#123; "_index": elastic_urls_index, "_type": doc_type_name, "_id": data[0], "_source": &#123; "iclick_id": data[0], "onsite_id": data[1], "create_time": self.today_2 &#125;&#125;#要增加元素data['_soupyrce']['age'] = 'aa' 1.1.4 集合定义 1aaa = set() 增加 1aaa.add(1) 判断是否在集合 1if 1 in aaa: 数组转集合 12a = [11,22,33,44,11,22] b = set(a) 1.2 映射1.3 集合 集合的操作创建二维数组12345dataSet = [[1, 1, &apos;yes&apos;], [1, 1, &apos;yes&apos;], [1, 0, &apos;no&apos;], [0, 1, &apos;no&apos;], [0, 1, &apos;no&apos;]] 定义字典，若不存在则插入123labelCounts = &#123;&#125;if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0 提取文档高频词 1234567891011121314documents = ["Human machine interface for lab abc computer applications", "A survey of user opinion of computer system response time"]stoplist = set('for in and'.split())texts = [ [word for word in document.lower().split() if word not in stoplist ] for document in documents]from collections import defaultdictfrequency = defaultdict(int)for text in texts: for word in text: frequency[word]+=1texts = [ [word for word in text if frequency[word]&gt;1] for text in texts ] 统计集合中出现某个字符的个数1234567for weibo in myWeiBo: if rconn.get(&quot;%s:Cookies:%s--%s&quot; % (spiderName, weibo[0], weibo[1])) is None: # &apos;SinaSpider:Cookies:账号--密码&apos;，为None即不存在。 cookie = getCookie(weibo[0], weibo[1]) if len(cookie) &gt; 0: rconn.set(&quot;%s:Cookies:%s--%s&quot; % (spiderName, weibo[0], weibo[1]), cookie) # 统计SinaSpider:Cookies的个数 cookieNum = &quot;&quot;.join(rconn.keys()).count(&quot;SinaSpider:Cookies&quot;) json操作1.1 json转字符串1result = json.loads(s) 1.2 遍历json的key将json当做dict，用dict的方法遍历 12for k,v in result.items(): print result[k] 1.3 读json文件1234567import jsonf = open(&quot;/home/david/keywordjson&quot;)result = json.load(f,encoding=&apos;utf-8&apos;)for k,v in result.items(): print result[k] 文件操作2.1 读CSV文件csv文件的格式如 12id,click,hour,C1,C2,C3,C4,C5123,1,14091123,a,b,c,d,e 读取的方式为： 12345from csv import DictReader# t是每行的index，row是每行的具体数据，dict型for t, row in enumerate(DictReader(open(path))): ID = row[&apos;id&apos;] 2.2 按行读普通文件12345678910for line in open('../result/ctrout-featurelist-1-10.dat'): #要去掉换行符 arr = line.strip('\n').split(',') if len(arr) &gt; 0: try: prectr.append(float(arr[0])) onlinectr.append(float(arr[1])) isclick.append(1 if arr[2]=='true' else 0) except ValueError: continue 上述方法不够严谨，为防止读取时出错，应该用 1234567file = open(filePath) try: tempSet = set() for line in file: tempSet.add(line) finally: file.close() 内置函数3.1 hash返回对象的hash值，返回的哈希值是使用一个整数表示，通常使用在字典里，以便实现快速查询键值。参数object输入是数字类型时，是根据数值来计算的，比如1和1.0计算出来是一样的哈希值，因此说这个函数是不区分不同的数值类型。 pandashttp://pandas.pydata.org/pandas-docs/stable/10min.html 引入pandas包 12345In [1]: import pandas as pdIn [2]: import numpy as npIn [3]: import matplotlib.pyplot as plt 通过numpy创建DataFrame，包含一个日期索引 1dates = pd.date_range(&apos;20170101&apos;, periods=6) 日期操作12345678910111213141516171819202122232425import datetimedstr = '20170228'd2 = datetime.datetime.strptime(dstr,'%Y%m%d') + datetime.timedelta(days=1)d2str = d2.strftime('%Y%m%d')结果是20170301获取当前日期并格式化import timetime.strftime('%Y%m%d',time.localtime(time.time()))time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time()))日期加减#datetime.timedelta(days, seconds, microseconds)d1 = datetime.date.today()加一天：d2 = d1 + datetime.timedelta(1)減一天：d2 = d1 + datetime.timedelta(-1)当天日期的格式化datetime.date.today().strftime('%Y%m%d') 字符串字符串拼接1234runToday=date.today().strftime(&apos;%y-%m-%d&apos;)b=time.strptime(runToday,&apos;%y-%m-%d&apos;)btime = datetime(*b[:3])&quot;,&quot;.join([&quot;/shortdata/persona/xid_present_info/&quot; + (btime - timedelta(x)).strftime(&apos;%y-%m-%d&apos;) for x in range(2,13)]) 1&apos;/shortdata/persona/xid_present_info/17-05-08,/shortdata/persona/xid_present_info/17-05-07,/shortdata/persona/xid_present_info/17-05-06,/shortdata/persona/xid_present_info/17-05-05,/shortdata/persona/xid_present_info/17-05-04,/shortdata/persona/xid_present_info/17-05-03,/shortdata/persona/xid_present_info/17-05-02,/shortdata/persona/xid_present_info/17-05-01,/shortdata/persona/xid_present_info/17-04-30,/shortdata/persona/xid_present_info/17-04-29,/shortdata/persona/xid_present_info/17-04-28&apos; 字符串切割数字除法后转float12# 加个点即可scale = state.shape[1] / 720. 循环12345678documents = ["Human machine interface for lab abc computer applications", "A survey of user opinion of computer system response time", "The EPS user interface management system", "System and human system engineering testing of EPS"]stoplist = set('for a of the and to in'.split())texts = [[word for word in document.lower().split() if word not in stoplist] for document in documents] hdfs操作https://snakebite.readthedocs.io/en/latest/client.html 代码规范定义类123456class model_evaluation(object): def __init__(self, prectrArr, onlinectrArr, isclickArr): self.prectrArr = prectrArr self.onlinectrArr = onlinectrArr self.isclickArr = isclickArr main函数123456def main(argv=None): if argv == None: argv = sys.argv if __name__ == &quot;__main__&quot;: main() nohup执行nohup python -u download_bing_api.py &gt;&gt; result.log &amp; 可以输出print的内容 max初级技巧 12tmp = max(1,2,4)print(tmp) 1234#可迭代对象a = [1, 2, 3, 4, 5, 6]tmp = max(a)print(tmp) 中级技巧：key属性的使用 当key参数不为空时，就以key的函数对象为判断的标准。如果我们想找出一组数中绝对值最大的数，就可以配合lamda先进行处理，再找出最大值 123a = [-9, -8, 1, 3, -4, 6]tmp = max(a, key=lambda x: abs(x))print(tmp) 高级技巧：找出字典中值最大的那组数据 如果有一组商品，其名称和价格都存在一个字典中，可以用下面的方法快速找到价格最贵的那组商品： 12345678910prices = &#123; &apos;A&apos;:123, &apos;B&apos;:450.1, &apos;C&apos;:12, &apos;E&apos;:444,&#125;# 在对字典进行数据操作的时候，默认只会处理key，而不是value# 先使用zip把字典的keys和values翻转过来，再用max取出值最大的那组数据max_prices = max(zip(prices.values(), prices.keys()))print(max_prices) # (450.1, &apos;B&apos;) 画图画圆 12345678import numpy as np import matplotlib.pyplot as plt from matplotlib.patches import Polygon import matplotlib.patches as mpatches for i in range(0,r,600): plt.plot(np.cos(t)*i, np.sin(t)*i)plt.show() 去掉坐标轴 123ax=plt.subplot(111)ax.set_xticks([]) ax.set_yticks([]) 进程subprocess运行python的时候，我们都是在创建并运行一个进程。像Linux进程那样，一个进程可以fork一个子进程，并让这个子进程exec另外一个程序。在Python中，我们通过标准库中的subprocess包来fork一个子进程，并运行一个外部的程序。 如 1cat = subprocess.Popen([&quot;hadoop&quot;, &quot;fs&quot;, &quot;-du&quot;, abspath], stdout=subprocess.PIPE) Popen对象创建后，主程序不会自动等待子进程完成。我们必须调用对象的wait()方法，父进程才会等待 (也就是阻塞block)，举例： 123&gt;&gt;&gt; import subprocess&gt;&gt;&gt; child = subprocess.Popen([&apos;ping&apos;,&apos;-c&apos;,&apos;4&apos;,&apos;blog.linuxeye.com&apos;])&gt;&gt;&gt; print &apos;parent process&apos; 从运行结果中看到，父进程在开启子进程之后并没有等待child的完成，而是直接运行print。对比等待的情况: 1234&gt;&gt;&gt; import subprocess&gt;&gt;&gt; child = subprocess.Popen(&apos;ping -c4 blog.linuxeye.com&apos;,shell=True)&gt;&gt;&gt; child.wait()&gt;&gt;&gt; print &apos;parent process&apos; 从运行结果中看到，父进程在开启子进程之后并等待child的完成后，再运行print。此外，你还可以在父进程中对子进程进行其它操作，比如我们上面例子中的child对象: 1234child.poll() # 检查子进程状态child.kill() # 终止子进程child.send_signal() # 向子进程发送信号child.terminate() # 终止子进程 子进程的PID存储在child.pid，子进程的标准输入、标准输出和标准错误如下属性分别表示: 123child.stdinchild.stdoutchild.stderr Python模块安装方法一、方法1： 单文件模块直接把文件拷贝到 $python_dir/Lib 二、方法2： 多文件模块，带setup.py 下载模块包，进行解压，进入模块文件夹，执行：python setup.py install 三、 方法3：easy_install 方式 先下载ez_setup.py,运行python ez_setup 进行easy_install工具的安装，之后就可以使用easy_install进行安装package了。easy_install packageNameeasy_install package.egg 四、 方法4：pip 方式 先进行pip工具的安裝：easy_install pip（pip 可以通过easy_install 安裝，而且也会装到 Scripts 文件夹下。） 安裝：pip install PackageName 更新：pip install -U PackageName 移除：pip uninstall PackageName 搜索：pip search PackageName 帮助：pip help 输出日志123456789101112131415161718192021logger = logging.getLogger("eventToKafka")logger.setLevel(logging.DEBUG)# 建立一个filehandler来把日志记录在文件里，级别为debug以上fh = logging.FileHandler("/home/hdbatch/fh.log")fh.setLevel(logginng.DEBUG)# 建立一个streamhandler来把日志打在CMD窗口上，级别为error以上ch = logging.StreamHandler()ch.setLevel(logginng.ERROR)# 设置日志格式formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")ch.setFormatter(formatter)fh.setFormatter(formatter)#将相应的handler添加在logger对象中logger.addHandler(ch)logger.addHandler(fh)# 开始打日志logger.debug("debug message")logger.info("info message")logger.warn("warn message")logger.error("error message")logger.critical("critical message") 输出到excel1234567891011121314151617181920212223242526import xlwtfrom datetime import datetime style0 = xlwt.easyxf(&apos;font: name Times New Roman, color-index red, bold on&apos;,num_format_str=&apos;#,##0.00&apos;)style1 = xlwt.easyxf(num_format_str=&apos;D-MMM-YY&apos;) wb = xlwt.Workbook(encoding=&apos;utf-8&apos;)ws = wb.add_sheet(&apos;A Test Sheet&apos;)ln = 1for line in open(&apos;/home/david/taxo.txt&apos;): arr = line.strip(&apos;\n&apos;).split(&apos;|&apos;) if len(arr) &gt; 0: try: print arr[0] ws.write(ln,0,arr[0]) print arr[1] ws.write(ln,1,arr[1]) print arr[2] ws.write(ln,2,arr[2]) ln += 1 except ValueError: print ValueError.message continue wb.save(&apos;/home/david/example.xls&apos;) latin1转utf8很多语料库下载后都是latin1编码，是乱码 判断变量是否为None123456789三种主要的写法有：第一种：if X is None;第二种：if not X；当X为None, False, 空字符串&quot;&quot;, 0, 空列表[], 空字典&#123;&#125;, 空元组()这些时，not X为真，即无法分辨出他们之间的不同。第三种：if not X is None; 在Python中，None、空列表[]、空字典{}、空元组()、0等一系列代表空和无的对象会被转换成False。除此之外的其它对象都会被转化成True。 在命令if not 1中，1便会转换为bool类型的True。not是逻辑运算符非，not 1则恒为False。因此if语句if not 1之下的语句，永远不会执行。 对比：foo is None 和 foo == None 示例： 123456789&gt;&gt;&gt; class Foo(object): def __eq__(self, other): return True&gt;&gt;&gt; f = Foo()&gt;&gt;&gt; f == NoneTrue&gt;&gt;&gt; f is NoneFalse 输入参数判断12345678import argparseparser = argparse.ArgumentParser()parser.add_argument(&quot;-v&quot;, &quot;--verbosity&quot;, help=&quot;increase output verbosity&quot;)args = parser.parse_args()if args.verbosity: print &quot;verbosity turned on&quot; 一种是通过一个-来指定的短参数，如-h； 一种是通过--来指定的长参数，如--help 这两种方式可以同存，也可以只存在一个。通过解析后，其值保存在args.verbosity变量中用法如下： 1234567891011121314151617yarving@yarving-VirtualBox /tmp $ python prog.py -v 1verbosity turned onyarving@yarving-VirtualBox /tmp $ python prog.py --verbosity 1verbosity turned onyarving@yarving-VirtualBox /tmp $ python prog.py -h usage: prog.py [-h] [-v VERBOSITY]optional arguments: -h, --help show this help message and exit -v VERBOSITY, --verbosity VERBOSITY increase output verbosityyarving@yarving-VirtualBox /tmp $ python prog.py -v usage: prog.py [-h] [-v VERBOSITY]prog.py: error: argument -v/--verbosity: expected one argument 或者这样使用 12345678910parser = argparse.ArgumentParser() parser.add_argument('--phone', default='Android', choices=['Android', 'IOS'], type=str, help='mobile phone OS') parser.add_argument('--sensitivity', default=2.045, type=float, help='constant for press time') parser.add_argument('--serverURL', default='http://localhost:8100', type=str, help='ServerURL for wda Client') parser.add_argument('--resource', default='resource', type=str, help='resource dir') parser.add_argument('--debug', default=None, type=str, help='debug mode, specify a directory for storing log files.') args = parser.parse_args() # print(args) AI = WechatAutoJump(args.phone, args.sensitivity, args.serverURL, args.debug, args.resource) type定义了输入的数据类型，如果不符合会报错 搜索指定目录下的文件123import glob# 搜索目录下的所有png文件，返回的是所有路径的列表glob.glob(os.path.join(self.resource_dir, &apos;circle/*.png&apos;)) 复制文件12import shutilshutil.copyfile(&apos;state.png&apos;, os.path.join(self.debug, &apos;state_&#123;:03d&#125;.png&apos;.format(self.step)))]]></content>
      <categories>
        <category>PYTHON</category>
      </categories>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[本地操作服务器hadoop]]></title>
    <url>%2F2017%2F07%2F11%2Fhadoop-spark%2F%E6%9C%AC%E5%9C%B0%E6%93%8D%E4%BD%9C%E6%9C%8D%E5%8A%A1%E5%99%A8hadoop%2F</url>
    <content type="text"><![CDATA[配置1、添加hadoop的必备jar包 123456789101112&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.5.0-cdh5.3.2&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.5.0-cdh5.3.2&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; 2、如果只要读取数据，直接配namenode地址12val conf = new Configurationconf.set("fs.defaultFS","hdfs://10.11.40.207:9000/") 2 读取数据12345678910111213141516171819import org.apache.hadoop.fs.Pathimport org.apache.hadoop.conf.Configurationimport org.apache.hadoop.fs.FileSystemimport java.io.InputStreamReaderimport java.io.BufferedReaderval fs = FileSystem.get(conf)val path = new Path("/user/david/testdir/a.txt")val in = fs.open(path)val buff = new BufferedReader(new InputStreamReader(in))var str = buff.readLinewhile (str != null) &#123; println(str) str = buff.readLine&#125;buff.closein.closefs.close]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JPA连接池问题]]></title>
    <url>%2F2016%2F05%2F21%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FJAVA%2FJPA%E8%BF%9E%E6%8E%A5%E6%B1%A0%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[用hibernate的JPA框架连接MySql并提供API接口，往往过一夜就会报错 1org.hibernate.exception.JDBCConnectionException: Unable to acquire JDBC Connection 根据stackflow上的解释，因为有太多connection，导致无法建立新的connection。因此需要设置连接池 连接池的作用 1.JDBC数据库连接池的必要性 在使用开发基于数据库的web程序时，传统的模式基本是按以下步骤： 在主程序（如servlet、beans）中建立数据库连接。 进行sql操作 断开数据库连接。 这种模式开发，存在的问题: 普通的JDBC数据库连接使用 DriverManager 来获取，每次向数据库建立连接的时候都要将 Connection 加载到内存中，再验证用户名和密码(得花费0.05s～1s的时间)。需要数据库连接的时候，就向数据库要求一个，执行完成后再断开连接。这样的方式将会消耗大量的资源和时间。数据库的连接资源并没有得到很好的重复利用.若同时有几百人甚至几千人在线，频繁的进行数据库连接操作将占用很多的系统资源，严重的甚至会造成服务器的崩溃。 对于每一次数据库连接，使用完后都得断开。否则，如果程序出现异常而未能关闭，将会导致数据库系统中的内存泄漏，最终将导致重启数据库。 这种开发不能控制被创建的连接对象数，系统资源会被毫无顾及的分配出去，如连接过多，也可能导致内存泄漏，服务器崩溃. 2.数据库连接池（connection pool） 为解决传统开发中的数据库连接问题，可以采用数据库连接池技术。 数据库连接池的基本思想就是为数据库连接建立一个“缓冲池”。预先在缓冲池中放入一定数量的连接，当需要建立数据库连接时，只需从“缓冲池”中取出一个，使用完毕之后再放回去。 数据库连接池负责分配、管理和释放数据库连接，它允许应用程序重复使用一个现有的数据库连接，而不是重新建立一个。 数据库连接池在初始化时将创建一定数量的数据库连接放到连接池中，这些数据库连接的数量是由最小数据库连接数来设定的。无论这些数据库连接是否被使用，连接池都将一直保证至少拥有这么多的连接数量。连接池的最大数据库连接数量限定了这个连接池能占有的最大连接数，当应用程序向连接池请求的连接数超过最大连接数量时，这些请求将被加入到等待队列中。 3.数据库连接池技术的优点 (1)资源重用：由于数据库连接得以重用，避免了频繁创建，释放连接引起的大量性能开销。在减少系统消耗的基础上，另一方面也增加了系统运行环境的平稳性。 (2)更快的系统反应速度:数据库连接池在初始化过程中，往往已经创建了若干数据库连接置于连接池中备用。此时连接的初始化工作均已完成。对于业务请求处理而言，直接利用现有可用连接避免了数据库连接初始化和释放过程的时间开销，从而减少了系统的响应时间 (3)新的资源分配手段对于多应用共享同一数据库的系统而言，可在应用层通过数据库连接池的配置实现某一应用最大可用数据库连接数的限制避免某一应用独占所有的数据库资源. (4)统一的连接管理，避免数据库连接泄露在较为完善的数据库连接池实现中，可根据预先的占用超时设定，强制回收被占用连接，从而避免了常规数据库连接操作中可能出现的资源泄露。 4.c3p0数据库连接池 设置的方法参见 How to configure the C3P0 connection pool in Hibernate 配置后的persistence.xml 12345678910111213141516171819202122232425&lt;properties&gt; &lt;property name="hibernate.dialect" value="org.hibernate.dialect.MySQL5Dialect" /&gt; &lt;property name="hibernate.connection.driver_class" value="com.mysql.jdbc.Driver" /&gt; &lt;property name="hibernate.connection.username" value="usr_dba" /&gt; &lt;property name="hibernate.connection.password" value="4rfv%TGB^YHN" /&gt; &lt;property name="hibernate.connection.url" value="jdbc:mysql://10.11.10.44:3306/symphony" /&gt; &lt;property name="hibernate.max_fetch_depth" value="3" /&gt; &lt;property name="hibernate.hbm2ddl.auto" value="update" /&gt; &lt;!-- 下面开始c3p0的配置 --&gt; &lt;property name="hibernate.connection.provider_class" value="org.hibernate.service.jdbc.connections.internal.C3P0ConnectionProvider"/&gt; &lt;!-- 最小连接数 --&gt; &lt;property name="hibernate.c3p0.min_size" value="1"/&gt; &lt;!-- 最大连接数 --&gt; &lt;property name="hibernate.c3p0.max_size" value="100"/&gt; &lt;!-- 获得连接的超时时间,如果超过这个时间,会抛出异常，单位（毫秒） --&gt; &lt;property name="hibernate.c3p0.timeout" value="10"/&gt; &lt;!-- 指定连接池里最大缓存多少个Statement对象 --&gt; &lt;property name="hibernate.c3p0.max_statements" value="100"/&gt; &lt;!-- 每隔3000秒检查连接池里的空闲连接 ，单位是（秒）--&gt; &lt;property name="hibernate.c3p0.idle_test_period" value="3000"/&gt; &lt;!-- 当连接池里面的连接用完的时候，C3P0自动一次性获取多少个新的连接 --&gt; &lt;property name="hibernate.c3p0.acquire_increment" value="5"/&gt; &lt;!-- 每次都验证连接是否可用 --&gt; &lt;property name="hibernate.c3p0.validate" value="true"/&gt;&lt;/properties&gt;]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>JPA</tag>
      </tags>
  </entry>
</search>
