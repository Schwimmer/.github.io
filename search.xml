<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[如何阅读代码]]></title>
    <url>%2F2018%2F03%2F25%2F%E6%80%BB%E7%BB%93%E4%B8%8E%E6%80%9D%E8%80%83%2F%E5%A6%82%E4%BD%95%E9%98%85%E8%AF%BB%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[从宏观到微观的原则。 即明确读代码的目的，找到关键问题，忽略中间的细节。 比如今天读w2v的增量学习代码，花了太多时间读小模型的具体实现，又看了不少理论，其实增量学习的逻辑只在后面一点，浪费了大量时间。]]></content>
      <categories>
        <category>总结与思考</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[相关文章]]></title>
    <url>%2F2018%2F03%2F23%2F%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A%2F%E7%9B%B8%E5%85%B3%E6%96%87%E7%AB%A0%2F</url>
    <content type="text"><![CDATA[腾讯2017广告大赛的代码https://github.com/BladeCoda/Tencent2017_Final_Coda_Allegro 在线广告中基于转化率提升的竞价策略https://zhuanlan.zhihu.com/p/24801130]]></content>
      <categories>
        <category>计算广告</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[读取文件内容并写到kafka]]></title>
    <url>%2F2018%2F03%2F23%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FPYTHON%2F%E8%AF%BB%E5%8F%96%E6%96%87%E4%BB%B6%E5%86%85%E5%AE%B9%E5%B9%B6%E5%86%99%E5%88%B0kafka%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200#!/usr/bin/env python2# -*- coding: utf-8 -*-from kafka.producer import KafkaProducer import sysimport loggingimport osimport datetimeimport randomimport threading import timeparentPath = "/opt/git/tracking_ingest_HK/app/event"finishedLogPath = parentPath+"/finished_log.dat"logger = logging.getLogger("eventToKafka")logger.setLevel(logging.DEBUG)# 建立一个filehandler来把日志记录在文件里，级别为debug以上fh = logging.FileHandler(parentPath+"/event_to_kafka.log")fh.setLevel(logging.DEBUG)# 建立一个streamhandler来把日志打在CMD窗口上，级别为error以上ch = logging.StreamHandler()ch.setLevel(logging.ERROR)# 设置日志格式formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")ch.setFormatter(formatter)fh.setFormatter(formatter)#将相应的handler添加在logger对象中logger.addHandler(ch)logger.addHandler(fh)class Kafka_producer(): """ 使用kafka的生产模块 """ def __init__(self, kafkatopic, kafkapartition): self.kafkaTopic = kafkatopic self.kafkaPartition=kafkapartition self.producer = KafkaProducer(bootstrap_servers = ['10.11.70.141:9092','10.11.70.142:9092','10.11.70.143:9092']) def sendjsondata(self, jsonData): try: producer = self.producer producer.send(self.kafkaTopic, jsonData) #producer.flush() except Exception, e: logger.error(e) def flush(self): producer = self.producer producer.flush() def sendBatchJsonData(self, jsonData): try: curcount = len(jsonData)/self.kafkaPartition for i in range(0, self.kafkaPartition): start = i * curcount if i != (self.kafkaPartition - 1): end = (i+1) * curcount curdata = jsonData[start:end] self.producer.send(self.kafkaTopic, curdata) self.producer.flush() else: curdata = jsonData[start:] self.producer.send(self.kafkaTopic, curdata) self.producer.flush() except Exception, e: logger.error(e)def searchFile(path, keyword): fpList = [] for filename in os.listdir(path): fp = os.path.join(path, filename) if os.path.isfile(fp) and keyword in filename: fpList.append(fp) return fpListdef insertIntoSet(filePath): file = open(filePath) try: tempSet = set() for line in file: tempSet.add(line.replace('\n','')) except Exception, e: logger.error(e) finally: file.close() return tempSet class calthread(threading.Thread): #初始化函数 def __init__(self,threadname,cond,startN,endN,files): threading.Thread.__init__(self,name = threadname) self.cond = cond self.startN = startN self.endN = endN self.files = files #业务函数 def run(self): for i in range(self.startN,self.endN + 1): filePath = self.files[i] logger.info("current file is " + filePath) producer = Kafka_producer("event", 1) file = open(filePath) try: fileLines = 0 for line in file: arr = line.strip().split('\t') if len(arr) &gt; 0: try: producer.sendjsondata(arr[2]) producer.flush() #随机打印日志 if random.random() &lt; 0.00001: logger.info(arr[2]) fileLines += 1 except Exception, e: logger.error("current wrong file is %s" % (filePath)) logger.error("The wrong event log is %s" % (arr[2])) logger.error(e) continue logger.info("insert into kafka %s lines" % (str(fileLines))) except Exception, e: logger.error(e) finally: file.close()def main(argv=None): if argv == None: argv = sys.argv #获取线程锁 cond = threading.Condition() #已处理日志 finishedLog = set() finishedFile = open(finishedLogPath) try: for line in finishedFile: finishedLog.add(line.strip('\n')) finally: finishedFile.close() #incoming日志 incomingLog = set(searchFile("/mapr/hkidc.hadoop.iclick/staging/tracking/incoming/", "event.HK")) #待处理日志写入finished_log.dat todoLog = incomingLog - finishedLog if len(todoLog) == 0: return for i in todoLog: print(i) outfile = open(finishedLogPath, 'a') try: for i in todoLog: outfile.write(i + "\n") finally: outfile.close() todoList = list(todoLog) alen = len(todoList) threadN = alen #执行线程对象列表 threadL = [] t = alen / threadN logger.info( "初始化线程" ) for x in range(0,threadN): startN = x*t endN = 0 if((x+1)*t &gt;= alen): endN = alen - 1 else: if(x == threadN - 1): endN = alen - 1 else: endN = (x+1)*t - 1 #向列表中存入新线程对象 threadTT = calthread("Thread--"+str(x),cond,startN,endN,todoList) threadL.append(threadTT) #总计完成线程计数器 logger.info("Start time of threadCal--"+ str(time.time())) for a in threadL: a.start() logger.info("done")if __name__ == "__main__": main()]]></content>
      <categories>
        <category>PYTHON</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python snippets]]></title>
    <url>%2F2018%2F03%2F23%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FPYTHON%2Fpython%20snippets%2F</url>
    <content type="text"><![CDATA[创建vocab传入一个二维数组，统计所有单词，并返回一个list 方法：遍历每行， 123456789101112dataSet = [[&apos;my&apos;, &apos;dog&apos;, &apos;has&apos;, &apos;flea&apos;, &apos;problems&apos;, &apos;help&apos;, &apos;please&apos;], [&apos;maybe&apos;, &apos;not&apos;, &apos;take&apos;, &apos;him&apos;, &apos;to&apos;, &apos;dog&apos;, &apos;park&apos;, &apos;stupid&apos;], [&apos;my&apos;, &apos;dalmation&apos;, &apos;is&apos;, &apos;so&apos;, &apos;cute&apos;, &apos;I&apos;, &apos;love&apos;, &apos;him&apos;], [&apos;stop&apos;, &apos;posting&apos;, &apos;stupid&apos;, &apos;worthless&apos;, &apos;garbage&apos;], [&apos;mr&apos;, &apos;licks&apos;, &apos;ate&apos;, &apos;my&apos;, &apos;steak&apos;, &apos;how&apos;, &apos;to&apos;, &apos;stop&apos;, &apos;him&apos;], [&apos;quit&apos;, &apos;buying&apos;, &apos;worthless&apos;, &apos;dog&apos;, &apos;food&apos;, &apos;stupid&apos;]] def createVocabList(dataSet): vocabSet = set([]) for document in dataSet: vocabSet = vocabSet | set(document) return list(vocabSet) 词袋模型 传入一个vocab，一个doc，返回词袋模型的list gensim如何生成词袋模型 数据的初始化 初始化一个长度为n的，初始值为0的数组 1a = [0] * len(n) 或者用Numpy 12from numpy import *a = zeros(n) 如果初始值为1 1a = ones(n) 读取csv转为numpy假设第一行是描述，第二行起是数据；第一列是标签，后面是特征项 12345import numpy as npdata = np.loadtxt(open(&apos;sample.csv&apos;,&apos;rb&apos;), delimiter=&apos;,&apos;, skiprows=1)y_train = data[:,0]x_train = data[:,1:-1]]]></content>
      <categories>
        <category>PYTHON</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[scala和java混编项目创建]]></title>
    <url>%2F2018%2F03%2F23%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FJAVA%2Fscala%E5%92%8Cjava%E6%B7%B7%E7%BC%96%E9%A1%B9%E7%9B%AE%E5%88%9B%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[现在有一个能用的scala和java混编项目，于是可以在这个基础上创建新的。步骤如下： 1、新建一个java的maven项目。 2、拷贝原项目中pom.xml的build和dependencies到新的pom.xml 3、在src/main下面新建scala目录 4、复制原项目在src/main下面的assembly目录 5、修改scala编译的版本到2.10.5]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[所有回文子串]]></title>
    <url>%2F2018%2F03%2F23%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E6%89%80%E6%9C%89%E5%9B%9E%E6%96%87%E5%AD%90%E4%B8%B2%2F</url>
    <content type="text"><![CDATA[一种是用了3重循环的方法 1234567891011121314151617181920212223242526272829#include&lt;stdio.h&gt;#include&lt;string.h&gt;int main(int argc, char *argv[])&#123; char a[505]; int n,len,begin,maxBegin,i,j; freopen("29.in","r",stdin); scanf("%s",a); n=strlen(a); for(len=2;len&lt;=n;len++)//枚举子串的所有可能的长度 &#123; maxBegin=n-len; for(begin=0;begin&lt;=maxBegin;begin++)//枚举子串的开始点 &#123; j=begin+len-1; for(i=begin;i&lt;j;i++,j--) //遍历当前子串（a[i]~a[begin+len-1]）,判断是否回文串 &#123; if(a[i]!=a[j]) break; &#125; if(i&gt;=j)//是回文串 &#123; j=begin+len-1; for(i=begin;i&lt;=j;i++) printf("%c",a[i]); printf("\n"); &#125; &#125; &#125; return 0;&#125;]]></content>
      <categories>
        <category>算法与数据结构</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[敏感词过滤]]></title>
    <url>%2F2018%2F03%2F23%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E6%95%8F%E6%84%9F%E8%AF%8D%E8%BF%87%E6%BB%A4%2F</url>
    <content type="text"><![CDATA[转自 敏感词过滤的算法原理之DFA算法]]></content>
      <categories>
        <category>算法与数据结构</category>
      </categories>
      <tags>
        <tag>DFA算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA面试题]]></title>
    <url>%2F2018%2F03%2F23%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FJAVA%2FJAVA%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[HashMap和HashTableHashMap通过hashcode对其内容进行快速查找，而 TreeMap中所有的元素都保持着某种固定的顺序，如果你需要得到一个有序的结果你就应该使用TreeMap（HashMap中元素的排列顺序是不固定的）。 HashMap和TreeMaphttp://blog.csdn.net/fujiakai/article/details/51585767 1、继承的父类不同 Hashtable继承自Dictionary类，而HashMap继承自AbstractMap类。但二者都实现了Map接口。 2、线程安全性不同 ​ Hashtable 中的方法是Synchronize的，而HashMap中的方法在缺省情况下是非Synchronize的。在多线程并发的环境下，可以直接使用Hashtable，不需要自己为它的方法实现同步，但使用HashMap时就必须要自己增加同步处理。 3、contains方法 HashMap把Hashtable的contains方法去掉了，改成containsValue和containsKey，因为contains方法容易让人引起误解。 Hashtable则保留了contains，containsValue和containsKey三个方法，其中contains和containsValue功能相同。 4、key和value是否允许为null Hashtable中，key和value都不允许出现null值。HashMap中，null可以作为键，这样的键只有一个；可以有一个或多个键所对应的值为null。因此，在HashMap中不能由get()方法来判断HashMap中是否存在某个键， 而应该用containsKey()方法来判断。 5、遍历方式的内部实现上不同 Hashtable、HashMap都使用了 Iterator。而由于历史原因，Hashtable还使用了Enumeration的方式 。 6、hash值不同 HashTable直接使用对象的hashCode。而HashMap重新计算hash值。 hashCode是jdk根据对象的地址或者字符串或者数字算出来的int类型的数值。 Hashtable在求hash值对应的位置索引时，用取模运算，而HashMap在求位置索引时，则用与运算，且这里一般先用hash&amp;0x7FFFFFFF后，再对length取模，&amp;0x7FFFFFFF的目的是为了将负的hash值转化为正值，因为hash值有可能为负数，而&amp;0x7FFFFFFF后，只有符号外改变，而后面的位都不变。]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[树的算法题]]></title>
    <url>%2F2018%2F03%2F23%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E6%A0%91%E7%9A%84%E7%AE%97%E6%B3%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[二叉树插入12345678910111213141516171819202122232425262728class TreeNode: def __init__(self, val, left, right): self.val = val self.left = left self.right = right class BinarySearchTree: def insert(self, root, val): if root == None: root = TreeNode(val, None, None) else: if val &lt; root.val: root.left = self.insert(root.left, val) if val &gt; root.val: root.right = self.insert(root.right, val) return root def preOrder(self, root): if root: print root.val self.preOrder(root.left) self.preOrder(root.right)Tree = BinarySearchTree()root = Nonefor i in [1,2,3]: root = Tree.insert(root, i)Tree.preOrder(root) 有序数组创建二叉树123456789101112class Solution: def sortedArrayToBST(self, num): if not num: return None mid = len(num)//2 #“//”表示整数除法；“/”浮点数除法； root = TreeNode(num[mid]) left = num[:mid] right = num[mid+1:] root.left = self.sortedArrayToBST(left) root.right = self.sortedArrayToBST(right) return root 遍历二叉树algorithms/ 前序 根左右 中序 左根右 后序 左右根 递归方式 1234567891011121314151617181920class Tree(object): def __init__(self,data,left,right): self.data=data self.left=left self.right=rightdef post_visit(Tree): if Tree: post_visit(Tree.left) post_visit(Tree.right) print Tree.datadef pre_visit(Tree): if Tree: print Tree.data pre_visit(Tree.left) pre_visit(Tree.right)def in_visit(Tree): if Tree: in_visit(Tree.left) print Tree.data in_visit(Tree.right) 非递归 1234567891011121314151617181920212223class TreeNode: def __init__(self,value=None,leftNode=None,rightNode=None): self.value = value self.leftNode = leftNode self.rightNode = rightNode class Tree: def __init__(self,root=None): self.root = root def preOrder(self): if not self.root: return stackNode = [] stackNode.append(self.root) while stackNode: node = stackNode.pop() print node.value, if node.rightNode: stackNode.append(node.rightNode) if node.leftNode: stackNode.append(node.leftNode) 平衡二叉树输入一棵二叉树，判断该二叉树是否是平衡二叉树。 1234567891011121314151617class Solution: def getDepth(self , Root): if Root == None: return 0; lDepth = self.getDepth(Root.left); rDepth = self.getDepth(Root.right); return max(lDepth , rDepth) + 1; def IsBalanced_Solution(self, pRoot): if not pRoot: return True lDepth = self.getDepth(pRoot.left); rDepth = self.getDepth(pRoot.right); diff = lDepth - rDepth; if diff &lt; -1 or diff &gt; 1: return False; return self.IsBalanced_Solution(pRoot.left) and self.IsBalanced_Solution(pRoot.right); 判断一棵树是否为另一棵的子树给定两棵二叉树，判断T2是否是T1某棵子数的结构。 T1序列化成字符串str1; T2序列化成字符串str2; 用KMP算法判断str1中是否包含str2:如果str1包含str2,说明T1包含于T2结构一致的子树。 KMP算法 12345678910111213141516171819202122232425#KMP def kmp_match(s, p): m = len(s); n = len(p) cur = 0#起始指针cur table = partial_table(p) while cur&lt;=m-n: for i in range(n): if s[i+cur]!=p[i]: cur += max(i - table[i-1], 1)#有了部分匹配表,我们不只是单纯的1位1位往右移,可以一次移动多位 break else: return True return False #部分匹配表 def partial_table(p): '''''partial_table("ABCDABD") -&gt; [0, 0, 0, 0, 1, 2, 0]''' prefix = set() postfix = set() ret = [0] for i in range(1,len(p)): prefix.add(p[:i]) postfix = &#123;p[j:i+1] for j in range(1,i+1)&#125; ret.append(len((prefix&amp;postfix or &#123;''&#125;).pop())) return ret]]></content>
      <categories>
        <category>算法与数据结构</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[最长公共子序列]]></title>
    <url>%2F2018%2F03%2F23%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97%2F</url>
    <content type="text"><![CDATA[转自http://blog.csdn.net/littlethunder/article/details/25637173 序列a共同拥有m个元素，序列b共同拥有n个元素，假设a[m-1]==b[n-1]，那么a[:m]和b[:n]的最长公共子序列长度就是a[:m-1]和b[:n-1]的最长公共子序列长度+1；假设a[m-1]!=b[n-1]，那么a[:m]和b[:n]的最长公共子序列长度就是MAX（a[:m-1]和b[:n]的最长公共子序列长度，a[:m]和b[:n-1]的最长公共子序列长度）。 12345678910111213141516171819202122232425262728293031323334353637383940def lcs(a,b): lena=len(a) lenb=len(b) c=[[0 for i in range(lenb+1)] for j in range(lena+1)] flag=[[0 for i in range(lenb+1)] for j in range(lena+1)] for i in range(lena): for j in range(lenb): if a[i]==b[j]: c[i+1][j+1]=c[i][j]+1 flag[i+1][j+1]='ok' elif c[i+1][j]&gt;c[i][j+1]: c[i+1][j+1]=c[i+1][j] flag[i+1][j+1]='left' else: c[i+1][j+1]=c[i][j+1] flag[i+1][j+1]='up' return c,flagdef printLcs(flag,a,i,j): if i==0 or j==0: return if flag[i][j]=='ok': printLcs(flag,a,i-1,j-1) print(a[i-1],end='') elif flag[i][j]=='left': printLcs(flag,a,i,j-1) else: printLcs(flag,a,i-1,j) a='ABCBDAB'b='BDCABA'c,flag=lcs(a,b)for i in c: print(i)print('')for j in flag: print(j)print('')printLcs(flag,a,len(a),len(b))print('') 上图是执行结果，第一个矩阵是计算公共子序列长度的，能够看到最长是4；第二个矩阵是构造这个最优解用的；最后输出一个最优解BCBA。]]></content>
      <categories>
        <category>算法与数据结构</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试经历]]></title>
    <url>%2F2018%2F03%2F23%2F%E6%80%BB%E7%BB%93%E4%B8%8E%E6%80%9D%E8%80%83%2F%E9%9D%A2%E8%AF%95%E7%BB%8F%E5%8E%86%2F</url>
    <content type="text"><![CDATA[触宝 20180314提的一些问题： 转化如何反馈到投放中？ DSP广告追求的平滑投放有什么意义？ 转化可以更加均衡，也能防止一些作弊行为。 公司的广告投放流程中有哪些是比较有特色的？ CVR和CTR预测有什么不同的难点，侧重点？ 如何检验CTR预测的效果？ 因为没有均衡采样，线下用AUC会不准。为什么不准？ 线上如何检验？直接看实际CTR有什么问题？ LR中的w和b分别是如何优化的？ 性别预测用贝叶斯，特征之间有依赖如何解决？ 这种时候用贝叶斯的效果就不好了，可以改用随机森林，GBDT等优秀分类器试试。 加上惩罚项。 每个算法还有什么可以改进的地方？ LR样本不均衡如何处理？对AUC有什么影响？如何均衡？从不均衡到均衡的过程中权重会如何变化？ 2345 20180315求一个字符串中所有的回文子串 一次调查表明意大利三分之一的汽车都是法拉利，并且在那之中一半的车都是红色的。如果你在意大利的街头看到一辆红色的汽车驶来，请问它是法拉利的可能性有多大？ 一位农民想搞明白是什么因素影响了他的牛奶产量。他记录了每天的气温（30 - 40 度）、湿度（60 - 90%）、饲料消耗（2000 - 2500 千克）以及牛奶产量（500 - 1000 升）。假设问题是要预测每天的牛奶产量，你会如何处理数据并建立模型？ 你有一个电商网站，当用户点击一个商品打开详情页面时，你想基于商品特征和用户的购买历史为用户推荐 5 个其他的商品显示在页面的底部。你需要哪些服务和数据表来实现这个功能？请写一个查询语句或一段过程式代码来返回所要推荐的 5 个商品。 特征比样本多的情况怎么办 矩阵就不是满秩的，如果数据集的特征比样本点还多怎么办？是否还可以使用线性回归来做预测？答案是否定的，因为在计算$(X^TX)^{-1}$ 的时候会出错。 为了解决这个问题，统计学家引入了岭回归（ridge regression）的概念。简单说来，岭回归就是在矩阵 $X^TX$加一个 $λI$ 使得矩阵非奇异，进而能对 $X^X+\lambda I$ 求逆。在这种情况下，回归系数的计算公式变为： w=(X^TX+\lambda I)^{-1}X^Ty 支持向量机中核函数的目的？如何选择？多分类怎么做？样本不平衡的影响？如何解决样本不平衡？ 核函数，他们公司首先用线性核函数，速度快。径向基慢。 数据量增大后，模型会遇到哪些瓶颈，如何解决？ 分为在线模型和离线模型。在线模型快速响应，离线模型扔到hadoop跑全量。 回归模型的优缺点？回归模型遇到分类样本如何解决？ 比较排序算法时间复杂度下界为nlogn的证明 排序算法的比较是两两进行的，所以可以抽象成一棵二叉树，相互比较的数分别是左右叶子结点，，比较的结果存储在父节点中，依此类推。那么算法的时间复杂度就是取决于树的深度。如果要对n个数字进行比较排序，则需要进行n!次，即该二叉树有n!片叶子。 一棵深度为d的二叉树拥有的叶子结点数最大为2d个，则具有n!片叶子的二叉树的深度为logn!。 logn!=logn+log(n-1)+log(n-1)+…+log(2)+log(1)≥logn+log(n-1)+log(n-2)+…+log(n/) ≥(n/2)log(n/2)≥(n/2)log(n/10) ≥(n/2)logn-(n/2)log10=(n/2)logn=O(nlogn) 所以比较排序的算法时间复杂度为O(nlogn) 爱奇艺 20180316 中序遍历二叉树（非递归） 二叉树的插入 从1-200中任意选出101个自然数,其中一个数必是另一个数的整数倍 1 0-1背包问题的变形2 LCS最长公共子序列问题3 平衡二叉树判定 word2vec是有监督还是无监督 头条 word2vec为什么快 word2vec与神经网络的区别 负采样模型，采样率对向量的影响]]></content>
      <categories>
        <category>总结与思考</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[maven技巧]]></title>
    <url>%2F2018%2F03%2F23%2F%E5%B7%A5%E5%85%B7%E5%92%8C%E7%8E%AF%E5%A2%83%2Fmaven%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[常用命令 123456789101112131415161718创建一个简单的Java工程：mvn archetype:create -DgroupId=com.mycompany.example -DartifactId=Example创 建一个java的web工程：mvn archetype:create -DarchetypeGroupId=org.apache.maven.archetypes -DarchetypeArtifactId=maven-archetype-webapp -DgroupId=com.mycompany.app -DartifactId=my-webapp打包：mvn package编译：mvn compile编译测试程序：mvn test-compile清空：mvn clean运行测试：mvn test生成站点目录: mvn site生成站点目录并发布：mvn site-deploy安装当前工程的输出文件到本地仓库: mvn install安 装指定文件到本地仓库：mvn install:install-file -DgroupId=&lt;groupId&gt; -DartifactId=&lt;artifactId&gt; -Dversion=1.0.0 -Dpackaging=jar -Dfile=&lt;myfile.jar&gt;查看实际pom信息: mvn help:effective-pom分析项目的依赖信息：mvn dependency:analyze 或 mvn dependency:tree跳过测试运行maven任务： mvn -Dmaven.test.skip=true XXX生成eclipse项目文件: mvn eclipse:eclipse查看帮助信息：mvn help:help 或 mvn help:help -Ddetail=true查看插件的帮助信息：mvn &lt;plug-in&gt;:help，比如：mvn dependency:help 或 mvn ant:help 等等。跳过测试安装: mvn clean install -Dmaven.test.skip=true 打包时包含lib的jar包fat jar方式打包 需要在pom文件的build节点的plugins节点内添加一个plugin，plugin内容如下：maven-dependency-plugin 12345678910111213141516171819&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;copy-dependencies&lt;/id&gt; &lt;phase&gt;prepare-package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;copy-dependencies&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;outputDirectory&gt;$&#123;project.build.directory&#125;/lib&lt;/outputDirectory&gt; &lt;overWriteReleases&gt;false&lt;/overWriteReleases&gt; &lt;overWriteSnapshots&gt;false&lt;/overWriteSnapshots&gt; &lt;overWriteIfNewer&gt;true&lt;/overWriteIfNewer&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; maven上传至中央仓库1、确认settings.xml中是否有用户密码 123456789101112&lt;servers&gt; &lt;server&gt; &lt;id&gt;buzzinate&lt;/id&gt; &lt;username&gt;buzzinate&lt;/username&gt; &lt;password&gt;buzzinate&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;buzzinate-snapshots&lt;/id&gt; &lt;username&gt;buzzinate&lt;/username&gt; &lt;password&gt;buzzinate&lt;/password&gt; &lt;/server&gt;&lt;/servers&gt; 2、如果是SNAPSHOT版本，在POM中增加 12345678 &lt;distributionManagement&gt; &lt;snapshotRepository&gt; &lt;id&gt;buzzinate-snapshots&lt;/id&gt; &lt;name&gt;snapshots name&lt;/name&gt; &lt;url&gt;http://117.121.96.174:8081/nexus/content/repositories/snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt; &lt;/distributionManagement&gt; &lt;/project&gt; 重点是id需要跟settings中一致 3、增加源码插件，上传后可以看到源码和javadoc 123456789101112131415&lt;plugin&gt; &lt;!-- 源码插件 --&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-source-plugin&lt;/artifactId&gt; &lt;version&gt;2.1&lt;/version&gt; &lt;!-- 发布时自动将源码同时发布的配置 --&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;attach-sources&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;jar&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; 4、执行mvn deploy 5、如果有时候编译失败，可能是因为网络不通，导致元数据读取失败了，用 mvn compile -U 强制更新 setting.xml文件参考：maven设置———setting.xml文件学习 部署到服务器1、mvn clean package打包，如果要修改生成jar包的名称，要改 build -&gt;finalname属性 2、服务器上，复制jar包，新建conf，复制配置文件新建lib，复制lib下的jar包新建shell，编写启动脚本 123456export JAVA_HOME=/usr/local/lib/jvm/jdk1.7.0_71/CLASSPATH=conf:log4jtest.jarfor f in lib/*.jar; do CLASSPATH=$&#123;CLASSPATH&#125;:$fdone$JAVA_HOME/bin/java -server -Dfile.encoding=UTF-8 -Xms2G -Xmx2G -XX:+UseConcMarkSweepGC -XX:+CMSIncrementalMode -XX:+CMSIncrementalPacing -XX:CMSIncrementalDutyCycleMin=0 -XX:CMSIncrementalDutyCycle=10 -XX:MaxNewSize=1024M -XX:MaxPermSize=256M -XX:+DisableExplicitGC -cp $CLASSPATH com.iclick.rocket.examples.main.log4jMain &gt; test.log 其中，Xms1G JVM初始分配的堆内存 报错和解决setting问题12No marketplace entries found to handle maven-compiler-plugin:2.3.2:compile in Eclipse. Please see Help for more information.No marketplace entries found to handle maven-compiler-plugin:2.3.2:testCompile in Eclipse. Please see Help for more information. 在window-preferences-maven-user settings改为本地的maven配置如果还有问题，检查settings.xml文件 导入项目时报错安装weka-stable失败解决：用mvn install -Dmaven.test.skip=true跳过单元测试 jdk-tools12Description Resource Path Location TypeThe container &apos;Maven Dependencies&apos; references non existing library &apos;/home/david/.m2/repository/jdk/tools/jdk.tools/1.7/jdk.tools-1.7.jar&apos; pig-ext Build path Build Path Problem 解决：直接在terminal输入 1mvn install:install-file -DgroupId=jdk.tools -DartifactId=jdk.tools -Dpackaging=jar -Dversion=1.7 -Dfile=tools.jar -DgeneratePom=true 在目标路径新增了相关目录，接着下载jdk.tools-1.7.jar并cp过去类似的增加cascading、riffle、thirdparty 然后再mvn clean package 之后报错 12Description Resource Path Location Typemaven-dependency-plugin (goals &quot;copy-dependencies&quot;, &quot;unpack&quot;) is not supported by m2e. pom.xml /pig-ext line 175 Maven Project Build Lifecycle Mapping Problem 解决：在plugins之前，build下面添加如下pluginManagement 12345678910111213141516171819202122232425262728&lt;pluginManagement&gt;&lt;plugins&gt;&lt;plugin&gt;&lt;groupId&gt;org.eclipse.m2e&lt;/groupId&gt;&lt;artifactId&gt;lifecycle-mapping&lt;/artifactId&gt;&lt;version&gt;1.0.0&lt;/version&gt;&lt;configuration&gt;&lt;lifecycleMappingMetadata&gt;&lt;pluginExecutions&gt;&lt;pluginExecution&gt;&lt;pluginExecutionFilter&gt;&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;&lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;&lt;versionRange&gt;[2.0,)&lt;/versionRange&gt;&lt;goals&gt;&lt;goal&gt;copy-dependencies&lt;/goal&gt;&lt;/goals&gt;&lt;/pluginExecutionFilter&gt;&lt;action&gt;&lt;ignore /&gt;&lt;/action&gt;&lt;/pluginExecution&gt;&lt;/pluginExecutions&gt;&lt;/lifecycleMappingMetadata&gt;&lt;/configuration&gt;&lt;/plugin&gt;&lt;/plugins&gt;&lt;/pluginManagement&gt; 如果还不行然后在plugins中增加一个 123456789101112131415161718&lt;plugin&gt;&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;&lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;&lt;executions&gt;&lt;execution&gt;&lt;id&gt;copy-dependencies&lt;/id&gt;&lt;phase&gt;package&lt;/phase&gt;&lt;goals&gt;&lt;goal&gt;copy-dependencies&lt;/goal&gt;&lt;/goals&gt;&lt;configuration&gt;&lt;outputDirectory&gt;$&#123;project.build.directory&#125;/lib&lt;/outputDirectory&gt;&lt;excludeTransitive&gt;false&lt;/excludeTransitive&gt;&lt;stripVersion&gt;true&lt;/stripVersion&gt;&lt;/configuration&gt;&lt;/execution&gt;&lt;/executions&gt;&lt;/plugin&gt; 并把原来的maven-dependency-plugin注释掉 报错 Missing artifact jdk.tools:jdk.tools:jar:1.7 输入 1mvn install:install-file -DgroupId=jdk.tools -DartifactId=jdk.tools -Dpackaging=jar -Dversion=1.7 -Dfile=tools.jar -DgeneratePom=true 并且在pom中加上 1&lt;dependency&gt; &lt;groupId&gt;jdk.tools&lt;/groupId&gt; &lt;artifactId&gt;jdk.tools&lt;/artifactId&gt; &lt;version&gt;1.7&lt;/version&gt;&lt;/dependency&gt; 或者把java Build Path里面的Libraries中的JRE System Libraries(JRE7)改为jdk7 编码GBK的不可映射字符在POM中加上 1234567891011121314&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.7&lt;/source&gt; &lt;target&gt;1.7&lt;/target&gt; &lt;encoding&gt;utf8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;]]></content>
      <categories>
        <category>工具与环境</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[eclipse技巧]]></title>
    <url>%2F2018%2F03%2F23%2F%E5%B7%A5%E5%85%B7%E5%92%8C%E7%8E%AF%E5%A2%83%2Feclipse%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[1234打开 Eclipse -&gt; Window -&gt; Perferences -&gt; Java -&gt; Editor -&gt; Content Assist，在右边最下面一栏找到 auto-Activation ，下面有三个选项，找到第二个“Auto activation triggers for Java：”选项在其后的文本框中会看到一个“.”存在。这表示：只有输入“.”之后才会有代码提示和自动补全，我们要修改的地方就是这里。把该文本框中的“.”换掉，换成“if.”，这样，你在Eclipse里面写Java代码就可以做到按“abcdefghijklmnopqrstuvwxyz.”中的任意一个字符都会有代码提示。]]></content>
      <categories>
        <category>工具与环境</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[sublime技巧]]></title>
    <url>%2F2018%2F03%2F23%2F%E5%B7%A5%E5%85%B7%E5%92%8C%E7%8E%AF%E5%A2%83%2Fsublime%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[去掉空行CTRL+H打开replace功能，勾选上左侧的regular expression，并填写 find what栏 : \s+$ （正则表达式） replace with栏 : （这行留空） 接着点replace all即可 放大字体command+]]></content>
      <categories>
        <category>工具与环境</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[新词发现]]></title>
    <url>%2F2018%2F03%2F23%2FNLP%2F%E6%96%B0%E8%AF%8D%E5%8F%91%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[代码在 1/spark-buzzads/src/main/scala/com/iclick/word_segmentation/WordSegment.scala 自己测试的代码在 1/spark-test/src/main/scala/com/iclick/word_segmentation/WordTest.scala 新词发现的原理是 中文新词发现算法解析 互联网时代的社会语言学：基于SNS的文本数据挖掘 从凝固度和自由度两个角度考虑你通过一元分词的相邻词组合而成的“新词”是否是语境中真正的一个词。 算法原理把可能成词的文本片段全部提取出来，再跟已有词库比较，找出新词。 怎样的文本片段可以认为是词？ 凝固度光看词频是不够的，比如“的电影”出现300次，“电影院”出现100次。因为电影和院凝固的更紧一些。 首先要枚举凝固方式 令 p(x) 为文本片段 x 在整个语料中出现的概率，那么我们定义“电影院”的凝合程度就是 p(电影院) 与 p(电) · p(影院) 比值和 p(电影院) 与 p(电影) · p(院) 的比值中的较小值，“的电影”的凝合程度则是 p(的电影) 分别除以 p(的) · p(电影) 和 p(的电) · p(影) 所得的商的较小值。 自由度把一个文本片段的自由运用程度定义为它的左邻字信息熵和右邻字信息熵中的较小值。如果太小，就不能成单独的词（因为都是跟着左、右一起出现的）。 抽词后，按照频度从高到低排序。 定义候选词的最大长度，再为出现频数、凝固程度和自由程度各设定一个阈值，然后只需要提取出所有满足阈值要求的候选词即可。 左右邻字的熵取较小值。 熵较小，说明该词往往是固定搭配，不能成词。 示例文本是 ab我 ef cd 步骤1、对语料用标点分隔，这里不需要去掉停用词。 2、在设定的最长词长度内，提取所有可能成词的词（至少大于2,小于max长度），并保存每个词的左邻近和右邻近字。 3、计算 123val conf = new SparkConf().setAppName(&quot;wordSegname&quot;).setMaster(&quot;local[4]&quot;). set(&quot;spark.sql.shuffle.partitions&quot;,&quot;10&quot;).set(&quot;spark.network.timeout&quot;,&quot;30s&quot;) local[4]是指在本地运行，用4核CPU。 spark.sql.shuffle.partitions是指partition的数量。SparkSQL在运行时，将一个查询任务分解成多个task，一个task就是一个partition。默认是200个partition，而如果实际集群只能并行3个task，则跑完200个partition要200/3=67次。 spark.network.timeout是指所有网络通信的超时时间，默认是120s 123456val word1=sc.textFile(path).map&#123;x=&gt; val x_filter=x.replaceAll(&quot;\p&#123;Punct&#125;&quot;, &quot; &quot;).replaceAll(&quot;\pP&quot;, &quot; &quot;) .replaceAll(&quot; &quot;, &quot; &quot;).replaceAll(&quot;[&quot; + AtomsUitl.stopwords + &quot;]&quot;, &quot; &quot;).replaceAll(&quot;\p&#123;Blank&#125;&quot;, &quot; &quot;).replaceAll(&quot;\p&#123;Space&#125;&quot;, &quot; &quot;).replaceAll(&quot;\p&#123;Cntrl&#125;&quot;, &quot; &quot;) x_filter &#125; 停用词，标点等转换为空格。 replaceAll中是正则表达式。上文中，是将所有的特殊字符都用空格代替 AtomsUitl.stopwords停用词是”的很了么呢是嘛个都也比还这于不与才上用就好在和对挺去后没说” sc.textFile读取文件后，生成一个RDD，以行为单位，所以后面的map是对每行的操作 1val sum_document = word1.count() 打印出所有的元素，用 word1.foreach { x =&gt; println(x) } 12345678val word_document = word1.zipWithIndex.filter &#123; x =&gt; !StringUtils.isBlank(x._1) &#125;.flatMap &#123; x =&gt; val arr = ArrayBuffer[(String, Int)]() val line = x._1.split(&quot; &quot;) //对于每一行，都用空格分割 for (i &lt;- line) &#123; arr += ((i, x._2.toInt)) //分割后，每一个tuple加到数组中 &#125; arr &#125;.map &#123; x =&gt; (x._1.trim, x._2) &#125;.filter(x =&gt; !StringUtils.isBlank(x._1)) zipWithIndex用带有index的来压缩RDD，索引从0开始 word1.zipWithIndex.foreach { x =&gt; println(x) } (ab ef,0) (cd,1) 上述代码得到的结果是 (ab,0) (ef,0) (cd,1) 1234567891011121314151617val wordleft = word.map(x =&gt; AtomsUitl.reverse(x)).map &#123; x =&gt; &quot;&quot; + x + &quot;&quot; &#125;.flatMap &#123; x =&gt; var arr = ArrayBufferString for (y &lt;- 1 to AtomsUitl.len(x) - 2) &#123; // arr+=x.substring(y, Math.min(maxLen + y, x.length())) arr += AtomsUitl.substring(x, y, Math.min(maxLen + y, AtomsUitl.len(x))) &#125; arr&#125;.sortBy(x =&gt; x) 将每个句子倒序排列，提取每个子集 今$ 四期星天今 处言语然自 天今$ 星天今$ 期星天今$ 然自$ 理处言语然 自$ 言语然自 语然自$ $ \123456789101112131415161718val wordleft_caculate = wordleft.map &#123;​ s =&gt;​ val first = AtomsUitl.substring(s, 0, 1).toString​ (first, s)​ &#125;.groupBy(f =&gt; f._1).map &#123;​ x =&gt; x._2​ &#125;wordleft_caculate.foreach&#123;x=&gt; println(x.iterator.next())&#125; \ groupBy之后得到 (期, CompactBuffer((期,期星天今$))等 这个是Iterable，可迭代的。可以转换为一个迭代器x.iterator.next()。迭代出来就是 (期,期星天今$) 等 结果如： [維他命,105,5.103775510263332,2.120730528309974,41,3362341] [红庙路口,10,3.969704104467617,1.3592367006650063,6,3362341] [红裙艳丽,8,4.516740790602718,1.2554823251787535,4,3362341] [绛侯,85,3.85163302224936,1.8033243982880292,37,3362341] 其中，第一列是新词，第二列是总词频，第三列是凝聚度，第四列是左熵右熵取最小值，第五列是出现该词的文档数，最后一列是总文档数]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[kafka的log存储]]></title>
    <url>%2F2018%2F03%2F23%2Fhadoop-spark%2Fkafka%E7%9A%84log%E5%AD%98%E5%82%A8%2F</url>
    <content type="text"><![CDATA[转自https://www.cnblogs.com/dorothychai/p/6181058.html]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[GBDT]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FGBDT%2F</url>
    <content type="text"><![CDATA[原理简单易学的GBDT原理 GBDT公式推导 梯度提升算法Freidman提出了梯度提升算法，该方法是利用最速下降法的近似方法，其关键是利用损失函数的负梯度在当前模型的值 -[{\partial L(y,f(x_i)) \over \partial f(x_i)}]_{f(x) = f_{m-1}(x)} 这个公式中，每个参数什么意思 作为回归问题算法中的残差的近似值，拟合一个回归模型。 其算法流程如下： $F0(x) = argmin\rho \sum _{i=1}^N L(y_i, \rho)$ For $m = 1$ to $M$ do: $\qquad \tilde yi = -[{\partial L(y,F(x_i)) \over \partial F(x_i)}]{F(x) = F_{m-1}(x)}, i = 1, N$ $\qquad am = argmin{a,\beta}\sum_{i=1}^N[\tilde y_i - \beta h(x_i; a)]^2$ $\qquad \rhom = argmin\rho \sum{i=1}^N L(y_i, F{m-1}(x_i) + \rho h(x_i; a_m))$ $\qquad Fm(x) = F{m-1}(x) + \rho_m h(x;a_m)$ endForendAlgorighm 其中$h(x_i;a_m)$表示基本分类器（weak learner or base learner），4中$a_m$表示拟合负梯度能力最好的分类器参数负梯度只是表示下降的方向，但是下降多少没有确定，5中$\rho_m$可以认为是下降最快的步长，可以让Loss最小，可以用线性搜索的方式来估计$\rho_m$的值 为何这里不直接利用负梯度来调节，而是需要用一个分类器来拟合呢？因为这里的负梯度是在训练集上求出的，不能被泛化测试集中。我们的参数是在一个函数空间里面，不能使用例如SGD这样的求解方式。使用一个分类器来拟合，是一个泛化的方式。 回归树当我们的基本分类器是一个包含J个节点的回归树时，回归树模型可以表示为 h(x;\{b_j, R_j\}_1^J) = \sum_{b=j}^Jb_jI(x\in R_j) \qquad (8)其中${ R_j }_1^J$不相交的区域，它们的集合覆盖了预测值的空间，${ b_j }_1^J$是叶子节点的值，可以认为是模型$h$的系数 利用回归树模型，算法流程6中的公式可以被替换为：F_m(x) = F_{m-1}(x) + \rho_m \sum_{j=1}^J b_{jm}I(x \in R_{jm})\qquad (9) 其中${ R_{jm} }_1^J$是第m次迭代生成的树所产生的区域。第m次迭代的树用来预测流程3中由流程4中平方误差产生的${\tilde y_i}_i^N$ ${ b{jm}}$可以被表示为 $$b{jm} = ave{x_i \in R{jm}} \tilde y_i$$ 即用平均值表示该叶子节点拟合的值 有了下降的方向，我们还需要最好的步长，缩放因子$\rho_m$是流程5中线性搜索方式的一种解决方案 从上面可以看出，我们是先求的$b{jm}$，然后在求解$\rho_m$，我们能否同时求解呢？另$\gamma{jm} = \rho{m}b{jm}$，公式9可以被表示为：F_m(x) = F_{m-1}(x) + \sum_{j=1}^J \gamma_{jm}I(x \in R_{jm})\qquad (10) 通过优化如下公式来获取最优的系数$\gamma_{jm}$： \{\gamma_{jm}\}_1^J = argmin_{\ \gamma_j {\ _1^J}}\sum_{i=1}^N L\left(y_i, F_{m-1}(x_i) + \sum_{j=1}^J\gamma_jI(x \in R_{jm})\right)\qquad 1)由于回归树产生的叶子节点各个区域之间是不相交的，且所有的样本最终都会属于某个叶子节点，所以公式11可以表示为： \gamma_{jm} = argmin_\gamma \sum_{x_i\in R_{jm}} L(y_i, F_{m-1}(x_i) + \gamma)给定当前$F{m-1}(i)$，$\gamma{jm}$可以作为叶子节点的值，该值可以看做是基于损失函数L的每个叶子节点的最理想的常数更新值，也可以认为$\gamma_{jm}$是即有下降方向又有下降步长的值。 综上，用回归树作为基本分类器的梯度提升算法流程可以如下表示： $F0(x) = argmin\rho \sum _{i=1}^N L(y_i, \rho)$ For $m = 1$ to $M$ do: $\qquad \tilde yi = -[{\partial L(y,F(x_i)) \over \partial F(x_i)}]{F(x) = F_{m-1}(x)}, i = 1, N$ $\qquad {R_{jm}}_1^J = J-terminal\, node\, tree({ \tilde y_i, x_i }_i^N)$ $\qquad \gamma{jm} = argmin\gamma \sum{x_i\in R{jm}} L(yi, F{m-1}(x_i) + \gamma)$ $\qquad Fm(x) = F{m-1}(x) + \sum{j=1}^J \gamma{jm}I(x \in R_{jm})$ endForendAlgorighm 其中3是计算残差（利用损失函数的负梯度在当前模型的值作为残差的近似值），4是拟合一颗含有J个叶子节点的回归树，5是估计回归树叶子节点的值 下面我们看一下二元分类、多元分类、回归中残差的计算、叶子节点值的估计。 Two-class logistic regression and classification我们用negative binomial log-likehood作为我们的损失函数： L(y, F) = log(1 + exp(-2yF)), y \in {-1, 1}\qquad (12)其中F(x) = {1\over2}log\left[{Pr(y=1|x) \over Pr(y=-1|x)}\right]\qquad (13)公式13是logit函数，log odds 如上公式是Freidman的论文中使用的公式，我认为使用在逻辑回归中常见的$L(y, F) = ylogF + (1-y)log(1-F)$，其中$F(z) ={ 1\over{1+exp(-z)}}$也可以 计算残差：\tilde y_i = -[{\partial L(y,F(x_i)) \over \partial F(x_i)}]_{F(x) = F_{m-1}(x)} = {2y_i\over 1+exp(2y_iF_{m-1}(x_i))}\qquad(14) 叶子节点值的估计： \gamma_jm = argmin_\gamma \sum_{x_i \in R_{jm}} log(1+exp(-2y_i(F_{m-1}(x_i) + \gamma)))\qquad (15)可以通过一步Newton-Raphson来近似公式15，估计结果为： \gamma_{jm} = {\sum_{x_i \in R_{jm}}\tilde y_i \over {\sum_{x_i \in R_{jm}}}|\tilde y_i|(2-|\tilde y_i|)}最终得到的$F_M(x)$与对数几率 log-odds相关，我们可以用来进行概率估计 F(x) = {1\over2}log\left({p \over 1-p}\right)e^{2F(x)} = {p\over(1-p)}P_+(x) = p = {e^{2F(x)}\over 1+e^{2F(x)}} = {1\over1+e^{-2F(x)}}P_-(x) = 1-p = {1\over1+e^{2F(x)}}有了概率之后，我们接下来就可以利用概率进行分类 Multi-class logistic regression and classification我们使用multi-class log-loss作为损失函数： L(\{y_k, F_k(x)\}_1^K) = -\sum_{k=1}^K y_klogp_k(x)\qquad(16)其中使用softmax来计算概率：p_k(x) = exp(F_k(x)) / \sum_{l=1}^Kexp(F_l(x))\qquad(17) 从公式17可以得出，对于多分类问题，我们需要为每个类别创建一颗回归树$F_l(x)\, l=1,2,…,k$ 计算残差： \tilde y_{ik} = -[{\partial L(\{y_{il},F_l(x_i)\}_{l=1}^K) \over \partial F_k(x_i)}]_{\{F_l(x) = F_{l, m-1(x)}\}_1^K} = y_{ik} - p_{k,m-1(i)}\qquad (18)我们假定共分为3类，那么logloss为： L = -y_1log{exp(F_1(x))\over exp(F_1(x)) + exp(F_1(x)) + exp(F_1(x))} -y_2log{exp(F_2(x))\over exp(F_1(x)) + exp(F_1(x)) + exp(F_1(x))} _3log{exp(F_3(x))\over exp(F_1(x)) + exp(F_1(x)) + exp(F_1(x))}{\partial L \over \partial F_1(x)} = -y_1 + y_1p_1 + y_2p_2 + y_3p_3{\partial L \over \partial F_2(x)} = y_1p_1 - y_2 + y_2p_2 + y_3p_3{\partial L \over \partial F_3(x)} = y_1p_1 + y_2p_2 - y_3 + y_3p_3如果当期样本的类别为(1,0,0)，那么 {\partial L \over \partial F_1(x)} = -1 + p1{\partial L \over \partial F_2(x)} = p_2{\partial L \over \partial F_3(x)} = p_3取负梯度，则 -{\partial L \over \partial F_1(x)} = 1 - p_1-{\partial L \over \partial F_2(x)} = -p_2 = 0 - p_2-{\partial L \over \partial F_3(x)} = -p_3 = 0 - p_3符合公式18中的$\tilde y{ik} = y{ik} - p_{k,m-1(x_i)}$ 叶子节点值的估计： \{r_{jkm}\} = argmin_{\gamma_{jk}}\sum_{i=1}^N \sum_{k=1}^K \phi \left( y_{ik}, F_{k,m-1}(x_i) + \sum_{j=1}^J\gamma_{jk}I(x_i \in {jm})\}\right)\qquad(19)可以通过一步Newton-Raphson来近似公式19，估计结果为： \gamma_{jkm} = {K-1\over K}{\sum_{x_i \in R_{jkm}}\tilde y_{ik} \over {\sum_{x_i \in R_{jkm}}}|\tilde y_{ik}|(1-|\tilde y_{ik}|)}Regression我们使用Least-squares作为损失函数：L(y, F) = {(y-F)^2\over 2} 计算残差：\tilde y_i = -[{\partial L(y,F(x_i)) \over \partial F(x_i)}]_{F(x) = F_{m-1}(x)} = {y_i - F_{m-1}(x_i)}\qquad(20) 叶子节点值的估计： \gamma_{jm} = argmin_\gamma \sum_{x_i \in R_{jm}} {1\over 2}(y_i - (F_{m-1}(x_i) + \gamma))^2\qquad (21)\gamma_{jm} = argmin_\gamma \sum_{x_i \in R_{jm}} {1\over 2}(y_i - F_{m-1}(x_i) - \gamma)^2\gamma_{jm} = argmin_\gamma \sum_{x_i \in R_{jm}} {1\over 2}(\tilde y_i - \gamma)^2容易得出以下结果：\gamma_{jm} = ave_{x_i \in R_{jm}} \tilde y_i 回归树的创建拟合残数是一个回归问题，所以在分割样本时，我们不会采用基尼指数（Gini）、信息增益（IG）等用于分类的标准。我们可以选用MSE(mean square error impurity criterion)作为分割样本的标准。也可是采用Friedman在论文中的the least-squares improvement criterion，公式如下： i_2(R_l, R_r) = {w_lw_r\over w_l + w_r}(\bar y_l - \bar y_r)^2其中$\bar y_l \, \bar y_r$分别是左右孩子的平均值，$w_l \, w_r$分别是左右孩子对应的权重和 本文是针对具体的损失函数进行的相关推导，泛化能力差，大家可以参考xgboost作者的这篇文章，作者进行了更加一般的推导，这一个抽象的形式对于实现机器学习工具也是非常有帮助的。 引用：Greedy Function Approximation: A Gradient Boosting Machine 与RF的区别实现GBDT的python实现 python配置XGB参考 gbdt http://www.itopmarketing.com/index.php/News/show/id/7951/lmid/197/utm_source/tuicool/utm_medium/referral/ 【关于点击率模型，你知道这三点就够了】 http://www.cnblogs.com/zhouxiaohui888/p/6008368.html 【xgboost原理及应用】 官网介绍的原理：https://xgboost.readthedocs.io/en/latest/model.html 原理的中文解释http://dataunion.org/15787.html https://github.com/dmlc/xgboost https://github.com/Schwimmer/xgboost/tree/master/jvm-packages/xgboost4j-example/src/main/scala/ml/dmlc/xgboost4j/scala/example/spark GBDT （Gradient Boost Decision Tree）是一种常用的非线性模型[6][7][8][9]，它基于集成学习中的boosting思想[10]，每次迭代都在减少残差的梯度方向新建立一颗决策树，迭代多少次就会生成多少颗决策树。 GBDT 的思想使其具有天然优势可以发现多种有区分性的特征以及特征组合，决策树的路径可以直接作为LR输入特征使用，省去了人工寻找特征、特征组合的步骤。这种通过 GBDT 生成LR特征的方式（ GBDT +LR），业界已有实践（Facebook，Kaggle-2014），且效果不错 来源： http://blog.csdn.net/lilyth_lilyth/article/details/48032119 XGBoost http://blog.csdn.net/dusj1993/article/details/51925387【 在集群上部署xgboost踩过的坑】 http://blog.csdn.net/u010306433/article/details/51403894 【xgboost 分布式部署教程】 mvn install:install-file -Dfile=D:\code\jar_package\xgboost4j-spark-0.5-jar-with-dependencies.jar -DgroupId=ml.dmlc -DartifactId=xgboost4j -Dversion=0.5 -Dpackaging=jar ml.dmlc xgboost4j 0.7 \1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556http://blog.csdn.net/zhangweijiqn/article/details/53214186XGBoost4J:Distributed XGBoost for Scala/Java (XFBoost 4 JVM environment)，目前看来还比较小众，文档不够多，网上xgboost4j的资料也很少，社区不够活跃。Portable Distributed XGBoost in Spark, Flink and Dataflow: http://dmlc.ml/2016/03/14/xgboost4j-portable-distributed-xgboost-in-spark-flink-and-dataflow.htmlScala和Spark等分布式的包在jvm-packages下。安装：http://xgboost.readthedocs.io/en/latest/jvm/目前安装仅支持从源码安装:$ git clone --recursive https://github.com/dmlc/xgboost$ cd xgboost/jvm-packages$ mvn package安装scala/java的jvm版xgboost:$ cd jvm-packages/xgboost4j$ mvn install:install-file -Dfile=target/xgboost4j-0.7.jar -DgroupId=ml.dmlc -DartifactId=xgboost4j -Dversion=0.7 -Dpackaging=jar安装spark版的xgboost:$ cd jvm-packages/xgboost4j-spark$ mvn install:install-file -Dfile=target/xgboost4j-spark-0.7.jar -DgroupId=ml.dmlc -DartifactId=xgboost4j-spark -Dversion=0.7 -Dpackaging=jarMaven pom.xml file:&lt;dependency&gt;&lt;groupId&gt;ml.dmlc&lt;/groupId&gt;&lt;artifactId&gt;xgboost4j-spark&lt;/artifactId&gt;&lt;version&gt;0.7&lt;/version&gt;&lt;/dependency&gt;Spark code，参考https://github.com/dmlc/xgboost/tree/master/jvm-packages，分别有RDD版本和DataFrame版本:importml.dmlc.xgboost4j.scala.spark.XGBoost\ 用maven安装xgboost4j时候的坑 执行 mvn clean -DskipTests=true package 1、checkstyle报错 xgboost4j的pom中增加了checkstyple插件，在win7下package会报错 \12345678method name must match patternno-trailing-spaces等\ 解决：在pom.xml中去掉checkstyle插件 \1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556​ &lt;plugin&gt;​ &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;​ &lt;artifactId&gt;maven-checkstyle-plugin&lt;/artifactId&gt;​ &lt;version&gt;2.17&lt;/version&gt;​ &lt;configuration&gt;​ &lt;configLocation&gt;checkstyle.xml&lt;/configLocation&gt;​ &lt;failOnViolation&gt;true&lt;/failOnViolation&gt;​ &lt;/configuration&gt;​ &lt;executions&gt;​ &lt;execution&gt;​ &lt;id&gt;checkstyle&lt;/id&gt;​ &lt;phase&gt;validate&lt;/phase&gt;​ &lt;goals&gt;​ &lt;goal&gt;check&lt;/goal&gt;​ &lt;/goals&gt;​ &lt;/execution&gt;​ &lt;/executions&gt;​ &lt;/plugin&gt;​ &lt;executions&gt;​ &lt;execution&gt;​ &lt;id&gt;checkstyle&lt;/id&gt;​ &lt;phase&gt;validate&lt;/phase&gt;​ &lt;goals&gt;​ &lt;goal&gt;check&lt;/goal&gt;​ &lt;/goals&gt;​ &lt;/execution&gt;​ &lt;/executions&gt;\ 2、用scala的例子BasicWalkThrough.scala报错 \1234ERROR [main] (DMatrix.java:41) - java.io.FileNotFoundException: File /lib/xgboost4j.dll was not found inside JAR.\ 参考：https://github.com/dmlc/xgboost/issues/1148 https://www.ibm.com/developerworks/community/blogs/jfp/entry/Installing_XGBoost_For_Anaconda_on_Windows?lang=en 需要装MINGW，然后make jvm，但是会报错 \1234mingw32-make: *** No rule to make target &apos;jvm&apos;. Stop.\ 应该在有MakeFile的目录下执行，也就是根目录D:\gitlab\xgboost。并且，要从git上手动下载项目中的dmlc-core和rabit，再执行，继续报错 \123456789101112131415161718192021222324252627282930process_begin: CreateProcess(NULL, uname, ...) failed.mingw32-make: Makefile:44: pipe: No errorg++ -std=c++0x -Wall -Wno-unknown-pragmas -Iinclude -Idmlc-core/include -Irabit/include -O3 -funroll-loops -msse2 -fopenmp -MM -MT build/learner.o src/learner.cc &gt;build/learner.dg++ -c -std=c++0x -Wall -Wno-unknown-pragmas -Iinclude -Idmlc-core/include -Irabit/include -O3 -funroll-loops -msse2 -fopenmp src/learner.cc -o build/learner.o子目录或文件 -p 已经存在。处理: -p 时出错。子目录或文件 build 已经存在。处理: build 时出错。Makefile:113: recipe for target &apos;build/logging.o&apos; failedmingw32-make: *** [build/logging.o] Error 1\ 应该用git-bash来执行，然后接着报错 \12\ https://github.com/dmlc/xgboost/issues/1267 跟着一步步执行，再报错 \1234cc1plus.exe: sorry, unimplemented: 64-bit mode not compiled in\ windows安装失败，在ubuntu环境下安装]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Numpy]]></title>
    <url>%2F2018%2F03%2F17%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FPYTHON%2FNumpy%2F</url>
    <content type="text"><![CDATA[NumPy数组的维数称为轴（axes），轴的个数叫秩（rank），一维数组的秩为1，二维数组的秩为2。 ndarrayNumPy的数组类被称作ndarray。通常被称作数组。 Numpy库中的矩阵模块为ndarray对象，有很多属性：T，data, dtype,flags,flat,imag,real,size, itemsize,nbytes,ndim,shape,strides,ctypes,base等等。 创建ndarray123import numpy as npvector = np.array([10,20,30])matrix=np.array([[1,2,3],[4,5,6],[7,8,9]]) 随机array12# 每个数在0-1之间np.random.rand(2,3) 12array([[ 0.83114518, 0.41367896, 0.78876733], [ 0.01419067, 0.20610305, 0.35582215]]) 1234# 在0-5之间生成随机数np.random.rand(2,3)*5# 或者np.dot(5,np.random.rand(2,3)) 123import numpy as np# 指定生成随机数的范围np.random.randint(0, 20, size=[2,3]) 12array([[ 3, 4, 4], [13, 2, 17]]) 创建空的array1np.zeros((5,1)) 注意有两层括号，因为参数是一个shape 随机数的seed1numpy.random.seed() seed( ) 用于指定随机数生成时所用算法开始的整数值，如果使用相同的seed( )值，则每次生成的随即数都相同，如果不设置这个值，则系统根据时间来自己选择这个值，此时每次生成的随机数因时间差异而不同。 123456from numpy import *num=0while(num&lt;5): random.seed(5) print(random.random()) num+=1 每次都一样 123456from numpy import *num=0random.seed(5)while(num&lt;5): print(random.random()) num+=1 每次不一样 也就是一次有效。 axis理解Stackoverflow系列(1) -Python Pandas与Numpy中axis参数的二义性 shape12vector_shape = vector.shapematrix_shape = matrix.shape (3,) (3, 3) reshape生成新矩阵 12345678910111213141516&gt;&gt;&gt; xdataarray([2, 4, 0, 3, 5, 2, 6, 0, 1, 8])&gt;&gt;&gt; len(xdata)10&gt;&gt;&gt; xdata.reshape(5,-1)array([[2, 4], [0, 3], [5, 2], [6, 0], [1, 8]])&gt;&gt;&gt; xdata.reshape(5,2)array([[2, 4], [0, 3], [5, 2], [6, 0], [1, 8]]) 指定了第一维后，第二维可以不指定，写为-1 flatten把(a,b,c,d)的X reshape为(b*c*d, a) 1X_flatten = X.reshape(-1, X.shape[0]) 获得x的转置 1print(x.T) 123[[1 4 7] [2 5 8] [3 6 9]] 返回数组内部的信息 1print(x.flags) 123456C_CONTIGUOUS : TrueF_CONTIGUOUS : FalseOWNDATA : TrueWRITEABLE : TrueALIGNED : TrueUPDATEIFCOPY : False 将数组变为1维数组，并获取其中的一部分数据 1print(x.flat[2:6]) 1[3 4 5 6] 将值赋给1维数组，再转化成有原有数组的大小形式 12x.flat=4;xprint(x) 123[[4 4 4] [4 4 4] [4 4 4]] 轴的个数（秩） 1print(x.ndim) 2 数组的维度，翻坠一个整数构成的元组。元组的长度就是秩 1print(x.shape) (3,3) 可以取任意维的shape 12# 取到第二维print(x.shape[:2]) 数组元素的总数 1print(x.size) 9 取数组元素123x = np.array([2, 4, 0, 3, 5])# 不包括倒数第一个x[:-1] [2,4,0,3] 123x=np.array([[1,2,3],[4,5,6],[7,8,9]])# 二维数组，逗号前后表示要取的行和列，:就是全部取，0:2就是取第0列和第1列，不包括第2列print(x[:,0:2]) 123[[1 2] [4 5] [7 8]] 如果只取一列，下面这种形式就会变成一个一位数组，要加上一个[]，才可以维持原有的二维数组的形式。 1print(x[:,-1]) 1[3 6 9] 1print(x[:,[-1]]) 123[[3] [6] [9]] 比较排序numpy教程：排序、搜索和计数 默认是升序排序。 1234567list1 = [[1,3,2], [3,5,4]]array = numpy.array(list1)array = sort(array, axis=1) #对第1维升序排序#array = sort(array, axis=0) #对第0维print(array)[[1 2 3] [3 4 5]] 降序排序的实现: 123array = -sort(-array, axis=1) #降序[[3 2 1] [5 4 3]] 参考 【1】numpy中的ndarray方法和属性 运算、索引、切片http://blog.csdn.net/liangzuojiayi/article/details/51534164 矩阵的各类乘法dot product点积 a \cdot b = a_1b_1+a_2b_2...a_nb_n1234567x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]### VECTORIZED DOT PRODUCT OF VECTORS ###tic = time.process_time()dot = np.dot(x1,x2)toc = time.process_time()print ("dot = " + str(dot) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms") 只有两个值是普通数组的时候才可以是点积，如果是np.array，则dot会变成矩阵乘法。也就是 123x1 = np.array([[1,2,3]])x2 = np.array([[1,2,3]])np.dot(x1,x2) 会报错 1ValueError: shapes (1,3) and (1,3) not aligned: 3 (dim 1) != 1 (dim 0) outer product外积1234x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]### VECTORIZED OUTER PRODUCT ###outer = np.outer(x1,x2) element-wise multipulation按位乘1mul = np.multiply(x1,x2) general dot product矩阵乘法12W = np.random.rand(3,len(x1))dot = np.dot(W,x1) 可以看出dot既可以用作点积，也可以执行矩阵乘法 linspace创建等差数列，默认是创建50个，一般写成 1x2 = np.linspace(1,10,10) fromfunction1234567&gt;&gt;&gt; def func (i):... return i%4+1&gt;&gt;&gt; np. fromfunction(func, (10 ,))array([ 1., 2., 3., 4., 1., 2., 3., 4., 1., 2.]) 首先，定义一个func函数，模4加1。然后，调用np对象的fromfunction内建函数，第一个参数是我们自定义的func,第二个参数（m,n），他处理的逻辑是这个样的：第一行第一列取（0，0）带入func函数，第一行第二列取（0，1）带入func函数，第一行第三列取（0，2）带入func函数……循环往复，直到取到值（0,n-1）带入函数以后，开始取第二行。。因为我们定义的函数只有一个参数，所以m从0取到9即可。最后返回数列：array([ 1., 2., 3., 4., 1., 2., 3., 4., 1., 2.]) 。 Broadcasting广播用以描述numpy中对两个形状不同的阵列进行数学计算的处理机制。较小的阵列“广播”到较大阵列相同的形状尺度上，使它们对等以可以进行数学计算。广播提供了一种向量化阵列的操作方式，因此Python不需要像C一样循环。广播操作不需要数据复制，通常执行效率非常高。然而，有时广播是个坏主意，可能会导致内存浪费以致计算减慢。 Numpy操作通常由成对的阵列完成，阵列间逐个元素对元素地执行。最简单的情形是两个阵列有一样的形状，例如： 1234&gt;&gt;&gt; a = np.array([1.0, 2.0, 3.0])&gt;&gt;&gt; b = np.array([2.0, 2.0, 2.0])&gt;&gt;&gt; a * barray([ 2., 4., 6.]) Numpy的广播机制放宽了对阵列形状的限制。最简单的情形是一个阵列和一个尺度值相乘： 1234&gt;&gt;&gt; a = np.array([1.0, 2.0, 3.0])&gt;&gt;&gt; b = 2.0&gt;&gt;&gt; a * barray([ 2., 4., 6.]) 上面两种结果是一样的，我们可以认为尺度值b在计算时被延展得和a一样的形状。延展后的b的每一个元素都是原来尺度值的复制。延展的类比只是一种概念性的。实际上，Numpy并不需要真的复制这些尺度值，所以广播运算在内存和计算效率上尽量高效。 上面的第二个例子比第一个更高效，因为广播在乘法计算时动用更少的内存。 matrixarray转matrix12s = np.array([5,5,0,0,0,5])np.matrix(s) 加载数据loadtxt1numpy.loadtxt(fname, dtype=&lt;type &apos;float&apos;&gt;, comments=&apos;#&apos;, delimiter=None, converters=None, skiprows=0, usecols=None, unpack=False, ndmin=0)[source] 参数 fname : file, str, or pathlib.Path File, filename, or generator to read. If the filename extension is .gz or .bz2, the file is first decompressed. Note that generators should return byte strings for Python 3k. dtype : data-type, optional Data-type of the resulting array; default: float. If this is a structured data-type, the resulting array will be 1-dimensional, and each row will be interpreted as an element of the array. In this case, the number of columns used must match the number of fields in the data-type. comments : str or sequence, optional The characters or list of characters used to indicate the start of a comment; default: ‘#’. delimiter : str, optional The string used to separate values. By default, this is any whitespace. converters : dict, optional A dictionary mapping column number to a function that will convert that column to a float. E.g., if column 0 is a date string: converters = {0: datestr2num}. Converters can also be used to provide a default value for missing data (but see also genfromtxt):converters = {3: lambda s: float(s.strip() or 0)}. Default: None. skiprows : int, optional Skip the first skiprows lines; default: 0. usecols : int or sequence, optional Which columns to read, with 0 being the first. For example, usecols = (1,4,5) will extract the 2nd, 5th and 6th columns. The default, None, results in all columns being read. New in version 1.11.0. Also when a single column has to be read it is possible to use an integer instead of a tuple. E.g usecols = 3 reads the fourth column the same way as usecols = (3,)` would. unpack : bool, optional If True, the returned array is transposed, so that arguments may be unpacked using x, y, z = loadtxt(...). When used with a structured data-type, arrays are returned for each field. Default is False. ndmin : int, optional The returned array will have at least ndmin dimensions. Otherwise mono-dimensional axes will be squeezed. Legal values: 0 (default), 1 or 2. New in version 1.6.0. 返回 out : ndarray Data read from the text file. genfromtxt12import numpynfl = numpy.genfromtxt(&quot;data.csv&quot;, delimiter=&quot;,&quot;) 12# U75就是将每个值作为一个75 byte的unicode来读取world_alcohol = np.genfromtxt(&apos;world_alcohol.csv&apos;, dtype=&apos;U75&apos;, skip_header=1, delimiter=&apos;,&apos;) matrix转数组1np.argsort(y_score, kind=&quot;mergesort&quot;)[::-1] 随机数字的矩阵12345678910import numpy as npnumpy_matrix = np.random.randint(10, size=[5,2])‘’‘array([[1, 0], [8, 4], [0, 5], [2, 9], [9, 9]])’‘’ 获取排序后数据位置的下标12345import numpy as npdd=np.mat([4,5,1]) dd1 = dd.argsort()print ddprint dd1 #matrix([[2, 0, 1]], dtype=int64) expbroadcast运算 12x = np.array([1,2,3])np.exp(x) sumbroadcast运算。 1234def softmax(x): x_exp = np.exp(x) x_sum = np.sum(x_exp, axis=1, keepdims=True) s = x_exp/x_sum squeeze从数组的形状中删除单维条目，即把shape中为1的维度去掉 12x = np.array([[[0], [1], [2]]]) np.squeeze(x) 1array([0, 1, 2]) 如果本来就是(1,1)的矩阵，则变成常数 12cost = np.array([[1]])cost = np.squeeze(cost) 得到1，cost的shape变成() 获取符合条件的行列集合数据如 12341,1,1,0,0,00,1,1,1,1,01,0,0,1,1,00,0,0,1,1,0 第一列作为y_train，后面矩阵作为x_train，需要获取y_train中为1的x_train的行 12pos_rows = (y_train == 1)x_train[pos_rows,:] 还有个例子 12vector = numpy.array([5, 10, 15, 20])vector == 10 [False, True, False, False] 123456matrix = numpy.array([ [5, 10, 15], [20, 25, 30], [35, 40, 45] ]) matrix == 25 12345[ [False, False, False], [False, True, False], [False, False, False]] 比如要找第二列中是25的那一行 12345678matrix = np.array([ [5, 10, 15], [20, 25, 30], [35, 40, 45] ]) second_column_25 = (matrix[:,1] == 25) # 等同于print(matrix[second_column_25]) print(matrix[second_column_25, :]) 123[ [20, 25, 30]] 多个条件的比较 12vector = numpy.array([5, 10, 15, 20])equal_to_ten_and_five = (vector == 10) &amp; (vector == 5) [False, False, False, False] 12vector = numpy.array([5, 10, 15, 20])equal_to_ten_or_five = (vector == 10) | (vector == 5) [True, True, False, False] 也可以根据比较的结果改变值 1234vector = numpy.array([5, 10, 15, 20])equal_to_ten_or_five = (vector == 10) | (vector == 5)vector[equal_to_ten_or_five] = 50print(vector) true的都变成了50 [50, 50, 15, 20] c下每个特征的总和 c下特征为1的文件总和 c下文件总和 c下词汇总和]]></content>
      <categories>
        <category>PYTHON</category>
      </categories>
      <tags>
        <tag>numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数学问题综合]]></title>
    <url>%2F2018%2F03%2F17%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E7%BB%BC%E5%90%88%2F</url>
    <content type="text"><![CDATA[从1-200中任意选出101个自然数,其中一个数必是另一个数的整数倍把这200个数分类如下： 以上共分为100类，即100个抽屉。显然在同一类中的数若不少于两个，那么这类中的任意两个数都有倍数关系。从中任取101个数，根据抽屉原理，一定至少有两个数取自同一类，因此其中一个数是另一个数的倍数。]]></content>
      <categories>
        <category>算法与数据结构</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[硬币问题]]></title>
    <url>%2F2018%2F03%2F17%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E7%A1%AC%E5%B8%81%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[转自 动态规划之硬币组合问题 动态规划的本质是将原问题分解为同性质的若干相同子结构，在求解最优值的过程中将子结构的最优值记录到一个表中以避免有时会有大量的重复计算。 例如硬币组合问题，若求凑够11元的最少硬币数，可以先从凑够0元、1元、2元……的子结构开始分析。 假设d(i)为凑够i元所需最少硬币数，则 d(0) = 0 理所当然 d(1) = 1 要凑够1元，需要从面值小于等于1元的硬币中选择，目前只有面值为1元的硬币 此时d(1) = d(0) + 1 d(2) = d(2 - 1) + 1 = 2， 从面值小于等于2元的硬币中选择，符合要求的硬币面值为：1元。 此时d(2) = d(2-1) + 1 d(3) = d(3 - 3) + 1 = 1， 从面值小于等于3元的硬币中选择，符合要求的硬币面值为：1元，3元。 此时有有两种选择：是否选择含有面值3元的硬币 含有3元硬币：d(3) = d(3 - 3) + 1 = 1 不含3元硬币：d(3) = d(3 - 1) + 1 = d(2) + 1 = 3 自然是选择二者中较小值 依次类推… 就该问题总结一下，随着要凑够钱数的增加： 1、首先要知道所有不大于该钱数的面值; 2、对于每种面值的硬币，求出当选择一个该面值的硬币时所需的硬币数 当选择一个硬币后，所需硬币数+1，所要凑够的钱数=原所要凑的钱数-该硬币面值，所要凑够的钱数减少，求减少后要凑钱数最少所需硬币数，属于原问题的子结构，已求出解 3.在上述求出的结果集中，选择最小值，即为要凑够该钱数所需的最少硬币数 由此可以看出，每个问题的最优值都是借其子结构的最优值得到的。 而该算法的最小的子结构的最优解是已知的，即：当要凑钱数为0元时，最少需要0枚硬币。 利用这个最小的子结构，通过递推式便可求出所指定值凑够钱数的最优值 上面所提到的递推式，便是状态转移方程。利用已知状态，不断通过状态转移方程求解，便得到了最优值和最优解。 下面看一下硬币组合问题的数学描述： d(i)=min{ d(i-vj)+1 }，其中i-vj &gt;=0，vj表示第j个硬币的面值，i表示要凑够i元，d(i)表示凑够i元最少需要的硬币数。即： 123 0 i == 0 时min_coin_num(i) = &#123; min&#123; min_coin_num( i-coin_value(j) )+1 | i-coin_value(j)&gt;0&#125; coin_value(j)表示第j种硬币的面值 i &gt; 0 时 当总值total_value为i时， 对于所有的 coin_value(j) &lt; i的硬币j ,取min{ min_coin_num(i-coin_value(j)) } 代码在 dynamic_programming/coin.py]]></content>
      <categories>
        <category>算法与数据结构</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序]]></title>
    <url>%2F2018%2F03%2F17%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[堆排序 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#!/usr/bin/env python2# -*- coding: utf-8 -*-"""Created on Tue Mar 13 09:46:42 2018@author: david"""def sift_down(array, start, end): """ 调整成大顶堆，初始堆时，从下往上；交换堆顶与堆尾后，从上往下调整 :param array: 列表的引用 :param start: 父结点 :param end: 结束的下标 :return: 无 """ while True: # 当列表第一个是以下标0开始，结点下标为i,左孩子则为2*i+1,右孩子下标则为2*i+2; # 若下标以1开始，左孩子则为2*i,右孩子则为2*i+１ left_child = 2*start + 1 # 左孩子的结点下标 # 当结点的右孩子存在，且大于结点的左孩子时 if left_child &gt; end: break if left_child+1 &lt;= end and array[left_child+1] &gt; array[left_child]: left_child += 1 if array[left_child] &gt; array[start]: # 当左右孩子的最大值大于父结点时，则交换 array[left_child], array[start] = array[start], array[left_child] start = left_child # 交换之后以交换子结点为根的堆可能不是大顶堆，需重新调整 else: # 若父结点大于左右孩子，则退出循环 break print("&gt;&gt;", array)def heap_sort(array): # 堆排序 # 先初始化大顶堆 first = len(array)//2 -1 # 最后一个有孩子的节点(//表示取整的意思) # 第一个结点的下标为０，很多博客&amp;课本教材是从下标1开始，无所谓吧，你随意 for i in range(first, -1, -1): # 从最后一个有孩子的节点开始往上调整 print(array[i]) sift_down(array, i, len(array)-1) # 初始化大顶堆 print("初始化大顶堆结果:", array) # 交换堆顶与堆尾 for head_end in range(len(array)-1, 0, -1): # start stop step array[head_end], array[0] = array[0], array[head_end] # 交换堆顶与堆尾 sift_down(array, 0, head_end-1) # 堆长度减一(head_end-1)，再从上往下调整成大顶堆if __name__ == "__main__": array = [16, 7, 3, 20, 17, 8] print(array) heap_sort(array) print("堆排序最终结果:", array)]]></content>
      <categories>
        <category>算法与数据结构</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[分析+代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112'''Created on Oct 27, 2010Logistic Regression Working Module@author: Peter'''from numpy import *def loadDataSet(): dataMat = []; labelMat = [] fr = open('testSet.txt') for line in fr.readlines(): lineArr = line.strip().split() dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])]) labelMat.append(int(lineArr[2])) return dataMat,labelMatdef sigmoid(inX): return 1.0/(1+exp(-inX))def gradAscent(dataMatIn, classLabels): dataMatrix = mat(dataMatIn) #convert to NumPy matrix labelMat = mat(classLabels).transpose() #convert to NumPy matrix m,n = shape(dataMatrix) alpha = 0.001 maxCycles = 500 weights = ones((n,1)) for k in range(maxCycles): #heavy on matrix operations h = sigmoid(dataMatrix*weights) #matrix mult error = (labelMat - h) #vector subtraction weights = weights + alpha * dataMatrix.transpose()* error #matrix mult return weightsdef plotBestFit(weights): import matplotlib.pyplot as plt dataMat,labelMat=loadDataSet() dataArr = array(dataMat) n = shape(dataArr)[0] xcord1 = []; ycord1 = [] xcord2 = []; ycord2 = [] for i in range(n): if int(labelMat[i])== 1: xcord1.append(dataArr[i,1]); ycord1.append(dataArr[i,2]) else: xcord2.append(dataArr[i,1]); ycord2.append(dataArr[i,2]) fig = plt.figure() ax = fig.add_subplot(111) ax.scatter(xcord1, ycord1, s=30, c='red', marker='s') ax.scatter(xcord2, ycord2, s=30, c='green') x = arange(-3.0, 3.0, 0.1) y = (-weights[0]-weights[1]*x)/weights[2] ax.plot(x, y) plt.xlabel('X1'); plt.ylabel('X2'); plt.show()def stocGradAscent0(dataMatrix, classLabels): m,n = shape(dataMatrix) alpha = 0.01 weights = ones(n) #initialize to all ones for i in range(m): h = sigmoid(sum(dataMatrix[i]*weights)) error = classLabels[i] - h weights = weights + alpha * error * dataMatrix[i] return weightsdef stocGradAscent1(dataMatrix, classLabels, numIter=150): m,n = shape(dataMatrix) weights = ones(n) #initialize to all ones for j in range(numIter): dataIndex = range(m) for i in range(m): alpha = 4/(1.0+j+i)+0.0001 #apha decreases with iteration, does not randIndex = int(random.uniform(0,len(dataIndex)))#go to 0 because of the constant h = sigmoid(sum(dataMatrix[randIndex]*weights)) error = classLabels[randIndex] - h weights = weights + alpha * error * dataMatrix[randIndex] del(dataIndex[randIndex]) return weightsdef classifyVector(inX, weights): prob = sigmoid(sum(inX*weights)) if prob &gt; 0.5: return 1.0 else: return 0.0def colicTest(): frTrain = open('horseColicTraining.txt'); frTest = open('horseColicTest.txt') trainingSet = []; trainingLabels = [] for line in frTrain.readlines(): currLine = line.strip().split('\t') lineArr =[] for i in range(21): lineArr.append(float(currLine[i])) trainingSet.append(lineArr) trainingLabels.append(float(currLine[21])) trainWeights = stocGradAscent1(array(trainingSet), trainingLabels, 1000) errorCount = 0; numTestVec = 0.0 for line in frTest.readlines(): numTestVec += 1.0 currLine = line.strip().split('\t') lineArr =[] for i in range(21): lineArr.append(float(currLine[i])) if int(classifyVector(array(lineArr), trainWeights))!= int(currLine[21]): errorCount += 1 errorRate = (float(errorCount)/numTestVec) print "the error rate of this test is: %f" % errorRate return errorRatedef multiTest(): numTests = 10; errorSum=0.0 for k in range(numTests): errorSum += colicTest() print "after %d iterations the average error rate is: %f" % (numTests, errorSum/float(numTests)) 逻辑回归详解 对随机梯度下降算法，我们做两处改进来避免上述的波动问题： 1）在每次迭代时，调整更新步长alpha的值。随着迭代的进行，alpha越来越小，这会缓解系数的高频波动（也就是每次迭代系数改变得太大，跳的跨度太大）。当然了，为了避免alpha随着迭代不断减小到接近于0（这时候，系数几乎没有调整，那么迭代也没有意义了），我们约束alpha一定大于一个稍微大点的常数项，具体见代码。 2）每次迭代，改变样本的优化顺序。也就是随机选择样本来更新回归系数。这样做可以减少周期性的波动，因为样本顺序的改变，使得每次迭代不再形成周期性。 scikit-learn 逻辑回归类库使用小结]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[评价指标]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87%2F</url>
    <content type="text"><![CDATA[准确率Accuracy分类器正确分类的样本数与总样本数之比。 ROC AUC 相关(Relevant),正类 无关(NonRelevant),负类 被检索到(Retrieved) true positives(TP 正类判定为正类,例子中就是正确的判定”这位是女生”) false positives(FP 负类判定为正类,”存伪”,例子中就是分明是男生却判断为女生,当下伪娘横行,这个错常有人犯) 未被检索到(Not Retrieved) false negatives(FN 正类判定为负类,”去真”,例子中就是,分明是女生,这哥们却判断为男生—梁山伯同学犯的错就是这个) true negatives(TN 负类判定为负类,也就是一个男生被判断为男生,像我这样的纯爷们一准儿就会在此处) 假如某个班级有男生80人,女生20人,共计100人.目标是找出所有女生.现在某人挑选出50个人,其中20人是女生,另外还错误的把30个男生也当作女生挑选出来了.作为评估者的你需要来评估(evaluation)下他的工作 TP=20FP=30FN=0TN=50 考虑ROC曲线图中的四个点和一条线。第一个点，(0,1)，即FPR=0, TPR=1，这意味着FN（false negative）=0，并且FP（false positive）=0。 ROC曲线越接近左上角，该分类器的性能越好。 曲线就是一系列FPR和TPR的结果。就是将每次分类结果的（0,1）的点作为分类的阈值。 但是，ROC的曲线——如上面几位已经说过——有数据均衡的问题。在数据极度不平衡的情况下，譬如说1万封邮件中只有1封垃圾邮件，那么如果我挑出10封，50封，100，。。封垃圾邮件（假设全部包含真正的那封垃圾邮件），Recall都是100%，但是FPR分别是9/9999, 49/9999, 99/9999（数据都比较好看：FPR越低越好），而Precision却只有1/10，1/50， 1/100 （数据很差：Precision越高越好）。所以在数据非常不均衡的情况下，看ROC的AUC可能是看不出太多好坏的，而PR curve就要敏感的多。（不过真实世界中，垃圾邮件也许与你的有用的邮件一样多——甚至比有用的还更多。。。）作者：竹间智能 Emotibot链接：https://www.zhihu.com/question/30643044/answer/161955532来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 精确率PrecisionTP/(TP+FP) 检索到的结果中是正确的比例，比如检索出50个女生只有20个是正确的 召回率RecallTP/(TP+FN) 检索到的结果占应该检索结果的比例。比如检索的结果里面有20个女生，全班一共有20个女生，所以是100% F1精确值和召回率的调和均值 F_1 = \frac{2PR}{P+R} = \frac{2TP}{2TP + FP + FN}多分类的精确率和召回率http://blog.csdn.net/lanchunhui/article/details/51221729 把每个类别单独视为”正“，所有其它类型视为”负“ 就是看每行对角线的P和R 代码见multi_recall.py 1234567891011121314151617M = [ [14371, 6500, 9, 0, 0, 2, 316], [5700, 22205, 454, 20, 0, 11, 23], [0, 445, 3115, 71, 0, 11, 0], [0, 0, 160, 112, 0, 0, 0], [0, 888, 39, 2, 0, 0, 0], [0, 486, 1196, 30, 0, 74, 0], [1139, 35, 0, 0, 0, 0, 865]]n = len(M)for i in range(n): rowsum, colsum = sum(M[i]), sum(M[r][i] for r in range(n)) try: print 'precision: %s' % (M[i][i]/float(colsum)), 'recall: %s' % (M[i][i]/float(rowsum)) except ZeroDivisionError: print 'precision: %s' % 0, 'recall: %s' %0]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[知识点]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E7%9F%A5%E8%AF%86%E7%82%B9%2F</url>
    <content type="text"><![CDATA[特征选择经典算法LR线性回归模型的映射函数是一个线性方程。逻辑回归要解决分类问题，用logistic function代替线性方程，通过y的取值判断类别，y的取值是一个概率。 优点是，输出是0,1满足概率分布的要求。函数可微。 LR的L2正则 BFGS 贝叶斯在多项式模型中： 在多项式模型中， 设某文档d=(t1,t2,…,tk)，tk是该文档中出现过的单词，允许重复，则 先验概率P(c)= 类c下单词总数（包括重复的）/整个训练样本的单词总数 类条件概率P(tk|c)=(类c下单词tk在各个文档中出现过的次数之和+1)/(类c下单词总数+|V|) V是训练样本的单词表（即抽取单词，单词出现多次，只算一个），|V|则表示训练样本包含多少种单词。 P(tk|c)可以看作是单词tk在证明d属于类c上提供了多大的证据，而P(c)则可以认为是类别c在整体上占多大比例(有多大可能性)。 在伯努利模型中： P(c)= 类c下文件总数/整个训练样本的文件总数 P(tk|c)=(类c下包含单词tk的文件数+1)/(类c下文件总数+2) 平滑项是应对没有特征的情况。 朴素贝叶斯算法的12条建议 SVMSVM在哪个地方引入的核函数?如果用高斯核可以升到多少维? BP的推导反向传播的原理 随机森林解决决策树容易过拟合的缺点。采用多个决策树的投票机制来改善决策树。RF有多个决策树，不能用全样本去训练，要用到采样方法。 1、每棵树选择样本时，通过重采样产生n个样本 2、从m个特征中随机选择k个特征，构建决策树。 3、多数投票制预测 决策树IC3，用信息增益找分裂特征。 C4.5，用信息增益比找特征。 GBDT机器学习算法GBDT的面试要点总结-上篇 机器学习算法中GBDT和XGBOOST的区别有哪些？ 推荐系统稍微看看 k折交叉验证中k取值多少有什么关系, 和bias和variance有关系吗? 翻转二叉树 平衡二叉树 hadoop和spark的应用场景 样本倾斜的处理样本不均衡的情况下，用AUC会偏高。 分类算法的比较用于数据挖掘的分类算法有哪些，各有何优劣？ 预测的评价指标精确率、召回率、F1 值、ROC、AUC 各自的优缺点是什么？ ROC和AUC构建一个混淆矩阵 TP FP FN TN 精确率(tp+tn)/(tp+fp+fn+tn) 分类器对整个样本分类能力，正为正，负为负 准确率 TP/TP+FP 分类器判定为正例中，真正的正例样本比重。 召回率 tp/tp+fn 判定为正例在所有正例的比重 word2vec的原理 如何增量训练 是一个词转向量的工具，语料是收集的百万篇的url。 如何将词转成向量表达的？ 模型的目标函数是什么 L=\sum_{w \in C} \log p(w|Context(w)) 如何推导的？ ngram模型+对数似然函数 基于神经网络的模型是如何做的 输入层是(Context(w),w)的训练样本。输入的时候是n-1个词向量首尾拼接。（词向量还没训练呢是怎么得到的？在[0,1]之间随机取值初始化） 输入层到隐藏层用双曲正切函数作为激活函数。 隐藏层到输出层再用一个线性函数。这样输出的y只是一个普通向量，需要用softmax再做一个归一化处理。 有了目标函数后，如何构建CBOW网络结构？ 网络结构： 输入层：前后各c个词，共2c个词的词向量。 投影层：向量做sum再取平均。 输出层：输出到Huffman树。每个叶子节点是一个词典中的词。 与神经网络模型的区别： 1）前者是拼接，后者是累加。2）后者没有隐藏层。3）前者是线性结构；后者是树形结构。 如何利用Huffman树来定义目标函数？ 对于每个词，都存在一个路径，路径上存在分支，将每个分支看成一个二分类，每次分类产生一个概率。将这些概率连乘，就是目标函数。 公式见《NLP/Word2Vec原理》 权重初始化为0，词向量初始化为[0-1]的随机数。权重向量的长度就是所有词*词向量的长度。因为每层都有一个权重。 用了多线程方法加速训练。 为什么叫层次softmax？ 每层都是一个二分类问题 为什么同义词的向量也相近 我理解是上下文接近，对于CBOW，训练每个词后更新的是上下文，对于近义词往往有类似的上下文 如何计算距一个词最近的向量？ KD树搜索。（https://www.cnblogs.com/21207-iHome/p/6084670.html） 构建方法：任选一个特征（或者数据方差最大的特征，说明分散，越可能不属于同一个区间），以此为坐标轴划分，将最近的点落在坐标轴上。 KD树搜索 先从根节点往下找到叶子节点，再回溯，回溯到每层时要判断是否跟另一区间相交。 但是KD是递归查找，效率低。递归效率低是函数调用的开销导致的（函数调用需要准备资源），且有栈溢出的风险。 idistance https://en.m.wikipedia.org/wiki/IDistance 目前线上怎么做的 如何用word2vec计算句子相似度 词向量按tfidf权重求mean 也要分应用，需求是topic相关还是语义相关。比如我爱苹果，我不爱苹果，topic相似语义不相似。 如果从词的粒度比较，还要结合上下文，避免一词多义。 文本分类 可以直接用fasttext，也可以tfidf+svm 熵 随机游走算法 动态规划算法 异常检测算法 CTR预测如何预测CTR线上的效果线下的可以用log loss和AUC 这里要特别强调一下用线上的其它业务指标如点击率、营收、利润、eCPC等等是不能给出CTR预估效果评价的。这些业务指标，受到整个广告系统其它模块如bid optimization,budget pacing等和外部竞价环境的综合影响，它的变化或者AB test中观察到的不同，不是简单地由于CTR预估变化带来的。换句话说，如果上了一个新的CTR预估模型的实验，发现业务指标变好了，这不等于说CTR预估更准了。一个简单的例子：如果一个CTR预估模型给出的预估总是比上帝视角的完美预估在低CTR区域低估，在高CTR区域高估，那么假设bid是base_bid*pCTR的话，相比完美预估，这个模型会赢得更多的高CTR区域的竞价，输掉更多在低CTR区域的竞价，最后会观察到实验组的CTR反而比完美预估的试验组更高。作者：Jian Xu链接：https://www.zhihu.com/question/54009615/answer/137820154来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 为什么AUC可以评价CTR它和Wilcoxon-Mann-Witney Test是等价的[3]。而Wilcoxon-Mann-Witney Test就是测试任意给一个正类样本和一个负类样本，正类样本的score有多大的概率大于负类样本的score。 如何平衡样本？平衡后的数据不是真实分布 CTR评价CTR的评价用logloss和AUC。因为是概率输出，不方便用PR。 点击率各模型优缺点广告点击率模型中，LR, GBDT+LR, FM, DNN等模型的优点和缺点？实际效果如何? CTR预估中GBDT与LR融合方案 CTR点击率预估干货分享 Ad Click Prediction: a View from the Trenches LR 优点：实现简单 缺点：需要寻找特征。我们也使用了FTRL，但实践中它并不能非常有效的产生稀疏模型，如果模型非常大，会导致同步模型变慢，一样会严重影响效果。 点击率模型中特征如何选择广告点击率预估中的特征选择 不是看它分布均不均衡，而是看它符不符合原来的分布。如果符合原来的分布，那么训练误差最小化也就意味着整体分布误差的最小化，也就没有必要进行均衡。 如何降维，每个特征都对应的ctr，在构建模型时，根据ctr的区间将属于同一个区间的特征值作为一个特征；随着ctr区间越分越细来迭代。停止条件是不超过所有特征的一半或达到最小区间阈值。 投放中出现的问题spark数据倾斜https://www.cnblogs.com/hd-zg/p/6089220.html spark资源调优http://blog.csdn.net/u012102306/article/details/51637366 我们使用yarn作为资源管理集群。yarn集群管理器根据spark参数，在各个工作节点上，启动一定数量的Executor进程，每个进程有一定的内存和CPU Core。 在申请到了作业执行所需的资源之后，Driver进程就会开始调度和执行我们编写的作业代码了。Driver将代码拆分为多个stage，为每个stage创建一批task。将这些task分配到各个Executor执行。 SparkConf的一些参数： spark.sql.shuffle.partitions指partition的数量。SparkSQL在运行时，将一个查询任务分解成多个task，一个task就是一个partition。默认是200个partition，而如果实际集群只能并行3个task，则跑完200个partition要200/3=67次。 spark.network.timeout所有网络通信的超时时间，默认是120s mapreduce shuffle过程http://langyu.iteye.com/blog/992916 map过程，阶段，将输入文本做split，并转成格式，V的初始值是1 之后，partitioner可以根据key和value以及reduce的数量来决定map的输出放到那个reduce task。默认是key hash后对reduce取模。 接下来将output写入缓冲区，减少磁盘IO的影响。key，value，partitioner的结果都写入缓冲区。 当缓冲区内容达到一定比例，就调用单独线程溢写到磁盘。溢写前，如果设置了combiner，在这里就要做合并。当溢写启动后，需要对溢写内容的key做sort。 Merge。每次溢写生成一个文件，map task完成后内存缓冲区所有的数据也全部溢写生成一个文件。用Merge合并溢写文件。 reduce 执行之前，拉取每个job中每个map task的最终结果。从不同地方拉过来的做merge。作为reducer的输入，然后执行reducer，结果写入hdfs。 文本分类怎么优化特征 CTR预测怎么优化特征选特征，版位id，版位类型，地理位置，曝光时间，ua信息（操作系统，浏览器等），人群特征因为样本稀疏所有没有选 根据直接观察CTR，卡方检验，单特征AUC。 用互信息、卡方检验有没有用 没有CTR的新版位 word2vec新词 es java api常见操作，从hive导入，从文件导入，中文分词 user-gene，投放，tracking处理的流程图 domain黑名单 DFA算法过滤敏感词 二分查找 特征稀疏怎么做传统的CTR或推荐系统拥有高维特征和稀疏数据，转向深度学习如何 如果可用特征值太少，就丢弃；要么就设值为unknown；xgboost有稀疏感知算法 怎么想到age预测算法的 哪些媒体获取？电商，微信 audience的重合度，距离代表相关度 hive的存储，ORC格式，以前用SequenceFile pig，数据倾斜时的join用replicated，大表放左，其他放右。 动态规划 L1和L2有什么区别 乱序数组找中位数快排+二分。取partition后，去掉一半的数字。 直接拿一个素材的点击率当一个维度的特征？ 随机森林和GBDT的区别 如何提高召回率或精确率如何提高机器学习算法的召回率？（尤其在样本集不平衡时） 引擎流程用timeslot监控campaign参数变化，预测下一个slot。第一个slot内不投放。 1、计算的参数包括flowbiddingrate、利润区间。 fbr衡量擦camp能投放的量占总量的百分比。比如根据预算，剩余每个slot的预算/每个slot的最大预算；曝光就是每个slot剩余曝光量/最大曝光量。 2、然后预测ctr并计算松弛系数（实际-期望），判断是否可以投放。 3、计算竞价，计算初始值，对于CPM，根据投放和曝光速度进行调整。对于CPC，如果实际CTR较高，通过提高一些小CTR的出价来平滑。 4、通过版位历史价格计算预期利润，统计利润区间分布。根据fbr确定可投放的利润区间。 5、通过投放平衡率计算camp打分。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[知识图谱入门]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[知识图谱入门 知道RDF，OWL，SPARQL这些W3C技术堆栈，知道它们的长处和局限。会使用RDF数据库和推理机。 了解一点描述逻辑基础，知道描述逻辑和一阶逻辑的关系。知道模型论，不然完全没法理解RDF和OWL。 了解图灵机和基本的算法复杂性。知道什么是决策问题、可判定性、完备性和一致性、P、NP、NExpTime。 最好再知道一点逻辑程序（Logic Programming），涉猎一点答集程序（Answer Set Programming），知道LP和ASP的一些小工具。这些东西是规则引擎的核心。如果不满足于正则表达式和if-then-else，最好学一点这些。 从正则文法到自动机。不理解自动机很多高效的模式提取算法都理解不了。 熟悉常见的知识库，不必事事重新造轮子，如Freebase, Wikidata, Yago, DBPedia。 熟悉结构化数据建模的基本方法，如ER，面向对象，UML，脑图。 学会使用一些本体编辑器，如Protege。 熟悉任何一种关系数据库。会使用存储过程写递归查询。明白什么叫物化视图、传递闭包、推理闭包。 熟悉任何一种图数据库。明白图的局部索引和关系的全局索引的理论和实践性能差异。 熟悉词法分析的基本工具，如分词、词性标注 熟悉句法分析的基本工具，如成分分析、依存文法分析、深层文法分析 熟悉TFIDF、主题模型和分布式表示的基本概念和工具。知道怎么计算两个词的相似度、词和句子的关联度。 知道怎么做命名实体识别。知道一些常用的词表。知道怎么用规则做关系提取。 了解前人已经建好的各种Lexical数据库，如Wordnet, framenet, BabelNet, PropBank。熟悉一些常用的Corpus。 知道信息检索的基本原理。知道各种结构的索引的代价。 掌握Lucene或者Solr/Elasticsearch的使用。 知识融合把结构化数据、半结构化数据、非结构化数据的知识表达形式都统一成RDF的形式，便于存储和查询。具体的知识融合主要包括如下两种类型： 合并外部知识库： 数据层的融合、模式层的融合 开放数据集成框架：LDIF 合并关系型数据库：将关系型数据转换成RDF的格式，现有工具Triplify、 d2rServer 、OpenLink、 Virtuoso 、SparqlMap等 知识推理 jena是一个java 的API，用来支持语义网的有关应用，学习jena需要了解XML 、RDF、 Ontology、OWL等方面的知识。 RDFox是一个高度可扩展的内存RDF三重存储，支持共享内存并行数据推理。它是一个用C ++编写的跨平台软件，带有一个Java包装器，可以与任何基于Java的解决方案轻松集成]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[特征选择]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%2F</url>
    <content type="text"><![CDATA[转自知乎：特征工程到底是什么？ https://www.cnblogs.com/jasonfreak/p/5448385.html 代码在blogcodes/feature_selection.py 目录 1 特征工程是什么？2 数据预处理 2.1 无量纲化 2.1.1 标准化 2.1.2 区间缩放法 2.1.3 标准化与归一化的区别 2.2 对定量特征二值化 2.3 对定性特征哑编码 2.4 缺失值计算 2.5 数据变换3 特征选择 3.1 Filter 3.1.1 方差选择法 3.1.2 相关系数法 3.1.3 卡方检验 3.1.4 互信息法 3.2 Wrapper 3.2.1 递归特征消除法 3.3 Embedded 3.3.1 基于惩罚项的特征选择法 3.3.2 基于树模型的特征选择法4 降维 4.1 主成分分析法（PCA） 4.2 线性判别分析法（LDA）5 总结6 参考资料 本文中使用sklearn中的IRIS（鸢尾花）数据集来对特征处理功能进行说明。IRIS数据集由Fisher在1936年整理，包含4个特征（Sepal.Length（花萼长度）、Sepal.Width（花萼宽度）、Petal.Length（花瓣长度）、Petal.Width（花瓣宽度）），特征值都为正浮点数，单位为厘米。目标值为鸢尾花的分类（Iris Setosa（山鸢尾）、Iris Versicolour（杂色鸢尾），Iris Virginica（维吉尼亚鸢尾））。导入IRIS数据集的代码如下： 123from sklearn.datasets import load_iris# 返回的是一个Bunch，类似Dict，可以获取iris.data、iris.target等iris = load_iris() 数据预处理未处理的特征有如下问题，我们使用sklearn中的preproccessing库来进行数据预处理。 无量纲化无量纲化使不同规格的数据转换到同一规格。常见的无量纲化方法有标准化和区间缩放法。 标准化的前提是特征值服从正态分布，标准化后，其转换成标准正态分布。 区间缩放法利用了边界值信息，将特征的取值区间缩放到某个特定的范围，例如[0, 1]等。 标准化z-score。将原始数据归一化到均值为0，方差为1的数据集。 优点：当X的最大值和最小值未知，或孤立点左右了最大－最小规范化时， 该方法有用 1234from sklearn.preprocessing import StandardScaler #标准化，返回值为标准化后的数据StandardScaler().fit_transform(iris.data) 区间缩放归一化最大最小归一化。线性转换。 1234from sklearn.preprocessing import MinMaxScaler#区间缩放，返回值为缩放到[0, 1]区间的数据MinMaxScaler().fit_transform(iris.data) L2范数归一化 L2范数归一化就是向量中每个元素除以向量的L2范数 1234from sklearn.preprocessing import Normalizer#归一化，返回值为归一化后的数据Normalizer().fit_transform(iris.data) 什么情况需要主要看模型是否具有伸缩不变性。 有些模型在各个维度进行不均匀伸缩后，最优解与原来不等价，例如SVM。对于这样的模型，除非本来各维数据的分布范围就比较接近，否则必须进行标准化，以免模型参数被分布范围较大或较小的数据dominate。 标准化是依照特征矩阵的列处理数据 归一化是依照特征矩阵的行处理数据 有些模型在各个维度进行不均匀伸缩后，最优解与原来等价，例如logistic regression。对于这样的模型，是否标准化理论上不会改变最优解。但是，由于实际求解往往使用迭代算法，如果目标函数的形状太“扁”，迭代算法可能收敛得很慢甚至不收敛。所以对于具有伸缩不变性的模型，最好也进行数据标准化。 缩放的最主要优点是能够避免大数值区间的属性过分支配了小数值区间的属性。 另一个优点能避免计算过程中数值复杂度。因为关键值通常依赖特征向量的内积（inner products），例如，线性核和多项式核，属性的大数值可能会导致数值问题。我们推荐将每个属性线性缩放到区间[-1,+1]或者[0, 1]。 对定量特征二值化定量特征二值化的核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0，公式表达如下 1234from sklearn.preprocessing import Binarizer#二值化，阈值设置为3，返回值为二值化后的数据Binarizer(threshold=3).fit_transform(iris.data) 对定性特征哑编码1234from sklearn.preprocessing import OneHotEncoder#哑编码，对IRIS数据集的目标值，返回值为哑编码后的数据OneHotEncoder().fit_transform(iris.target.reshape((-1,1))) 缺失值计算默认用均值填充 1234567from numpy import vstack, array, nanfrom sklearn.preprocessing import Imputer#缺失值计算，返回值为计算缺失值后的数据#参数missing_value为缺失值的表示形式，默认为NaN#参数strategy为缺失值填充方式，默认为mean（均值）Imputer().fit_transform(vstack((array([nan, nan, nan, nan]), iris.data))) 数据变换常见的数据变换有基于多项式的、基于指数函数的、基于对数函数的。4个特征，度为2的多项式转换公式如下： 12345from sklearn.preprocessing import PolynomialFeatures#多项式转换#参数degree为度，默认值为2PolynomialFeatures().fit_transform(iris.data) 基于单变元函数的数据变换可以使用一个统一的方式完成，使用preproccessing库的FunctionTransformer对数据进行对数函数转换的代码如下： 123456from numpy import log1pfrom sklearn.preprocessing import FunctionTransformer#自定义转换函数为对数函数的数据变换#第一个参数是单变元函数FunctionTransformer(log1p).fit_transform(iris.data) 特征选择选有效的特征，一般从两个方面看： 1）特征是否发散。一个方差接近于0，也就是说样本在这个特征上基本没有差异，对于样本的区分就没什么用。 2）特征与目标的相关性。 根据特征选择的形式又可以将特征选择方法分为3种： Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。 Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。 Embedded：嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。 我们使用sklearn中的feature_selection库来进行特征选择。 Filter方差选择法选择方差大于阈值的特征。 12345from sklearn.feature_selection import VarianceThreshold#方差选择法，返回值为特征选择后的数据#参数threshold为方差的阈值VarianceThreshold(threshold=3).fit_transform(iris.data) 相关系数法缺点是对非线性关系不敏感。 计算各个特征对目标值的pearson相关系数 1234567from sklearn.feature_selection import SelectKBestfrom scipy.stats import pearsonr#选择K个最好的特征，返回选择特征后的数据#第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量，输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值。在此定义为计算相关系数#参数k为选择的特征个数SelectKBest(lambda X, Y: array(map(lambda x:pearsonr(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target) 要理解Pearson相关系数，首先要理解协方差（Covariance），协方差是一个反映两个随机变量相关程度的指标，如果一个变量跟随着另一个变量同时变大或者变小，那么这两个变量的协方差就是正值，反之相反，公式如下： Pearson相关系数公式如下： pearson是一个介于-1和1之间的值，当两个变量的线性关系增强时，相关系数趋于1或-1；当一个变量增大，另一个变量也增大时，表明它们之间是正相关的，相关系数大于0；如果一个变量增大，另一个变量却减小，表明它们之间是负相关的，相关系数小于0；如果相关系数等于0，表明它们之间不存在线性相关关系。 卡方检验方检验是检验定性自变量对定性因变量的相关性。 卡方检验原理及应用 衡量实际值和理论值的差异程度。卡方值越大，相关程度越大。 比如某个特征有3个取值，构建矩阵，看这3个特征对应的点击和不点击的值分别是多少，然后计算总的点击率，求出每个特征的理论点击和不点击的数量。然后计算卡方值，再查表。 因此卡方就可以用来降维，比如找相关程度最大的特征。 12345from sklearn.feature_selection import SelectKBestfrom sklearn.feature_selection import chi2#选择K个最好的特征，返回选择特征后的数据SelectKBest(chi2, k=2).fit_transform(iris.data, iris.target) 互信息法]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[样本不平衡带来的问题及如何解决]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%A0%B7%E6%9C%AC%E4%B8%8D%E5%B9%B3%E8%A1%A1%E5%B8%A6%E6%9D%A5%E7%9A%84%E9%97%AE%E9%A2%98%E5%8F%8A%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%2F</url>
    <content type="text"><![CDATA[不平衡数据下的机器学习方法简介 面向不平衡分类的逻辑回归算法 在样本分布及其不均匀的情况下,建议用PRC。。。可以看下这个精确率、召回率、F1 值、ROC、AUC 各自的优缺点是什么？ - 机器学习里面qian lv的回答 在分类中如何处理训练集中不平衡问题 确切的知道正样本但负样本不确定，且训练数据正负样本分布极不平衡问题求教？ 在分类中如何处理训练集中不平衡问题]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[模型的假设条件]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%81%87%E8%AE%BE%E6%9D%A1%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[作者：李韶华链接：https://www.zhihu.com/question/46301335/answer/112354887来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 理解模型的基本假设，看自己的数据是否符合这种假设。任何模型都是有某种假设的，如果数据不符合这种假设，就不太可能学出有意义的模型并用于预测。 比如LDA（主题模型），假设是在同样一批文档中经常共现的词，语义上往往是相关的。这种特性不仅在自然语言中成立，在一些领域，比如每个人经常访问的网址集合，可能也是成立的，所以LDA也可以拿过去用。但如果数据不符合这个特性，套用LDA就是没有意义的，比如每个球队里的队员，可能并没有因为属于一个球队而具有什么相似性。 再举个例子，CNN（卷积神经网络），它的基本假设是特征的不同维度之间有局部相关性，卷积操作可以抓住这只局部相关性，形成新的特征。比如自然语言里，有重复出现的bigram，或者图像里代表性的局部像素块。不满足这种局部相关性的数据，比如收到的邮件序列，这种局部相关性很弱，那用CNN就不能抓到有用的特征。 最后，高斯copula，在量化金融里曾被广泛使用，把债券之间非高斯的相关性用copula转化成高斯然后拟合。然而这个模型隐含的假设是这种相关性符合瘦尾分布(thin tailed distribution)，即罕见事件发生的概率非常非常低。这个不合理假设导致对黑天鹅事件概率严重低估，曾被视为2008年金融危机的根源之一。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[样本倾斜]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%A0%B7%E6%9C%AC%E5%80%BE%E6%96%9C%2F</url>
    <content type="text"><![CDATA[严重数据倾斜文本分类，比如正反比1:20～100，适合什么model，查准一般要做到多少可以上线？ - 竹间智能 Emotibot的回答 - 知乎 https://www.zhihu.com/question/59236897/answer/164500508 首先要明确对于precision和recall，我们的需求是怎样的 。对于数据极其不平衡的情况，precision和recall的trade-off尤其显著。通过under-sampling/over-sampling来配平正反例是可以提升recall，但是一定会出现大量的false positive。如果我们认为错杀的成本很高，可以适当地降低对于precision的要求。反之，如果我们追求precision，那么可以采用基于规则的方式，通过对关键词的特征进行过滤，当然这样recall就会很惨。这是一个必须接受的现实。如果将这两种策略结合，最起码可以做这样的尝试：对于高precision的分类器，采取比较高信心的策略，譬如探测出来就直接报告这个用户甚至屏蔽；对于高recall的分类器，可以采取一些warning的措施，不强制做影响用户的操作。 粗暴一点，随机森林+bootstrap效果不错。 或者如果倾斜非常大，可以考虑异常检测的方法。 过采样 SMOTE算法，插值 高维度不适宜SMOTE。因为高维空间数据倾向于接近互相正交，故两两不相近，所以效果不好。 欠采样再缩放/再平衡假设训练集是真是样本总体的无偏采样，观测几率就是真实几率。只要分类器的预测几率高于观测几率就应判定为正例，即 \frac {\tilde {y}} {1- \tilde{y}} = \frac y {1-y} × \frac {m^-} {m^+}使用机器学习处理分类问题时，若训练样本比较稀疏，可否向训练语料中增加人工构造样本，以提升模型泛化能力？ 如何解决机器学习中样本不均衡问题？]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[熵]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%86%B5%2F</url>
    <content type="text"><![CDATA[熵1、什么是信息量？假设X是一个离散型随机变量，其取值集合为X，概率分布函数为$p(x) = P(X=x), x \in X$，定义$X=x_0$的信息量为： I(x_0) = -log(p(x_0))可以理解为，一个事件发生的概率越大，携带的信息量越小。当$p(x_0)=1$时，信息量为0。 2、什么是熵？第一，假设存在一个随机变量，可以问一下自己当我们观测到该随机变量的一个样本时，我们可以接受到多少信息量呢？毫无疑问，当我们被告知一个极不可能发生的事情发生了，那我们就接收到了更多的信息；而当我们观测到一个非常常见的事情发生了，那么我们就接收到了相对较少的信息量。因此信息的量度应该依赖于概率分布，所以说熵的定义应该是概率的单调函数。 第二，假设两个随机变量和是相互独立的，那么分别观测两个变量得到的信息量应该和同时观测两个变量的信息量是相同的，即：$H(X+Y)=H(X)+H(Y)$ 。而从概率上来讲，两个独立随机变量就意味着$p(x,y)=p(x)p(y)$，所以此处可以得出结论熵的定义应该是概率的函数。因此一个随机变量的熵可以使用如下定义： 设$X \in {x_1,x_2,…,x_n} $ 为一个离散随机变量，其概率分布为$P(X=x_i)=p_i$。则X的熵为 H(X)=-\sum_{i=1}^np_i\log p_i其中，当$p_i=0$时，熵为0。 此处的负号仅仅是用来保证熵（即信息量）是正数或者为零。而函数基的选择是任意的（信息论中基常常选择为2，因此信息的单位为比特bits；而机器学习中基常常选择为自然常数，因此单位常常被称为nats）。 用熵来评价整个随机变量x平均的信息量，而平均最好的量度就是随机变量的期望。 熵的取值范围是 0 \leq H(X) \leq \log n两个随机变量一起发生的熵就是联合熵 3、条件熵设$Y \in {y_1,y_2,…,y_m}$为随机变量，在已知X的条件下，Y的条件熵（conditional entropy）为 H(Y|X) = \sum_{i=1}^n p(x_i)H(Y|X=x_i) = -\sum_{i=1}^np(x_i) \sum_{j=1}^m p(y_j|x_i)\log p(y_j|x_i)表示在已知X的条件下，Y的条件概率分布的熵对X的数学期望。 4、交叉熵例如： 箱子里面有小球任意个，但其中1/2是橙色球，1/4是紫色球，1/8是蓝色球及1/8是青色球。我从中拿出一个球，你猜我手中的球是什么颜色的？ 知道了每种颜色小球的比例，比如橙色占比二分之一，如果我猜橙色，很有可能第一次就猜中了。所以，根据策略2，1/2的概率是橙色球，小明需要猜一次，1/4的概率是紫色球，小明需要猜两次，1/8的概率是蓝色球，小明需要猜三次，1/8的概率是青色球，小明需要猜三次，所以小明预期的猜题次数为： H = 1/2 1 + 1/4 2 + 1/8 3 + 1/8 3= 1.75 针对概率为p的小球，需要猜球的次数$=log_2 \frac 1 p$ 。例如1/8是蓝色球，次数就是3。则预期的猜题次数就是熵。 因此，每个系统都会有一个真实的概率分布（真实分布）。根据真实分布，可以找到一个最优策略，以最小的代价消除系统的不确定性，这个不确定性的值就是熵（比如猜题次数，编码长度）。 如果小明不知道真实分布，认为小球的分布为（1/4，1/4，1/4，1/4），这个分布就是非真实分布。此时，小明猜中任何一种颜色的小球都需要猜两次，即1/2 2 + 1/4 2 + 1/8 2 + 1/8 2 = 2。 当我们使用非最优策略消除系统的不确定性，所需要付出的努力的大小我们该如何去衡量呢？ 这就需要引入交叉熵，其用来衡量在给定的真实分布下，使用非真实分布所指定的策略消除系统的不确定性所需要付出的努力的大小。 交叉熵的公式为： H = \sum_{i=1}^N p_i log_2 \frac 1 {q_i}其中，$p_i$是真实分布，$q_i$是非真实分布。 因此，交叉熵越低，这个策略就越好，最低的交叉熵也就是使用了真实分布所计算出来的信息熵，因为此时$p_i=q_i$ ，交叉熵 = 信息熵。 这也是为什么在机器学习中的分类算法中，我们总是最小化交叉熵，因为交叉熵越低，就证明由算法所产生的策略最接近最优策略，也间接证明我们算法所算出的非真实分布越接近真实分布。 5、相对熵(KL散度)最后，我们如何去衡量不同策略之间的差异呢？这就需要用到相对熵，其用来衡量两个取值为正的函数或概率分布之间的差异，即： KL(f(x) || g(x)) = \sum_{x \in X} f(x) * log_2 \frac {f(x)} {g(x)}现在，假设我们想知道某个策略和最优策略之间的差异，我们就可以用相对熵来衡量这两者之间的差异。即，相对熵 = 某个策略的交叉熵 - 信息熵（根据系统真实分布计算而得的信息熵，为最优策略），公式如下： KL(p || q) = H(p,q)-H(p) = \sum_{k=1}^N p_k log_2 \frac 1 {q_k} - \sum_{k=1}^N p_k log_2 \frac 1 {p_k} = \sum_{k=1}^N p_k log_2 \frac {p_k} {q_k}互信息是相对熵的特殊形式。如果变量不是独立的，可以通过考察联合概率分布和边缘概率分布乘积之间的相对熵，来判断它们是否接近于相对独立。此时，散度表示为 这被称为变量 x 和变量 y 之间的互信息( mutual information )。根据 Kullback-Leibler 散度的性质,我们看到 I[x, y] ≥ 0 ,当且仅当 x 和 y 相互独立时等号成立。经过推导，得互信息公式为 互信息不能归一化，对连续变量计算不方便（连续变量需要先离散化）。最大信息系数首先寻求一种最优离散化方式，然后把互信息取值转换成一种度量方式，取值区间在[0，1]。 6、总结熵：衡量不确定性的度量 联合熵：X、Y在一起时的不确定性度量 条件熵：X确定时，Y的不确定性度量。也就是在X发生的前提下，新发生Y带来的熵 交叉熵：衡量p和q的相似性，越小越相似 相对熵：p和q的不相似度量。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习模型的比较]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%AF%94%E8%BE%83%2F</url>
    <content type="text"><![CDATA[转自 用于数据挖掘的分类算法有哪些，各有何优劣？ - Jason Gu的回答 - 知乎https://www.zhihu.com/question/24169940/answer/26952728 机器学习经典算法优缺点总结 首先看训练集多大， 贝叶斯因为模型简单，所以高偏差低方差。数据量小的时候，贝叶斯更好，因为LR容易过拟合。随着训练集增大，LR能训练出更准确的模型。 偏差指因模型太简单带来的估计不准确的部分。方差指因模型太复杂带来的不确定性。 LR相对于贝叶斯，不需要考虑特征是否相关。更容易增量训练。 LR可解释好，某个特征的权重高，则对结果的影响也大。可以在线学习。 决策树容易过拟合，使用剪树枝，或者RF。 Linear SVM和LR的比较http://www.jishux.com/plus/view-615065-1.html 从模型解决问题的方式来看Linear SVM直观上是trade-off两个量 a large margin，就是两类之间可以画多宽的gap ；不妨说是正样本应该在分界平面向左gap/2（称正分界），负样本应该在分解平面向右gap/2（称负分界） L1 error penalty，对所有不满足上述条件的点做L1 penalty 给定一个数据集，一旦完成Linear SVM的求解，所有数据点可以被归成两类 一类是落在对应分界平面外并被正确分类的点，比如落在正分界左侧的正样本或落在负分界右侧的负样本 第二类是落在gap里或被错误分类的点。 假设一个数据集已经被Linear SVM求解，那么往这个数据集里面增加或者删除更多的一类点并不会改变重新求解的Linear SVM平面。不受数据分布的影响。 求解LR模型过程中，每一个数据点对分类平面都是有影响的，它的影响力远离它到分类平面的距离指数递减。换句话说，LR的解是受数据本身分布影响的。在实际应用中，如果数据维度很高，LR模型都会配合参数的L1 regularization。 两者的区别两个模型对数据和参数的敏感程度不同，Linear SVM比较依赖penalty的系数和数据表达空间的测度，而（带正则项的）LR比较依赖对参数做L1 regularization的系数。但是由于他们或多或少都是线性分类器，所以实际上对低维度数据overfitting的能力都比较有限，相比之下对高维度数据，LR的表现会更加稳定，为什么呢？因为Linear SVM在计算margin有多“宽”的时候是依赖数据表达上的距离测度的，换句话说如果这个测度不好（badly scaled，这种情况在高维数据尤为显著），所求得的所谓Large margin就没有意义了，这个问题即使换用kernel trick（比如用Gaussian kernel）也无法完全避免。所以使用Linear SVM之前一般都需要先对数据做normalization，而求解LR（without regularization）时则不需要或者结果不敏感。 Linear SVM和LR都是线性分类器Linear SVM不直接依赖数据分布，分类平面不受一类点影响；LR则受所有数据点的影响，如果数据不同类别strongly unbalance一般需要先对数据做balancing。Linear SVM依赖数据表达的距离测度，所以需要对数据先做normalization；LR不受其影响Linear SVM依赖penalty的系数，实验中需要做validationLinear SVM和LR的performance都会收到outlier的影响，其敏感程度而言，谁更好很难下明确结论。 balance的方法 调整正、负样本在求cost时的权重，比如按比例加大正样本cost的权重。然而deep learning的训练过程是on-line的因此你需要按照batch中正、负样本的比例调整。 做训练样本选取：如hard negative mining，只用负样本中的一部分。 做训练样本选取：如通过data augmentation扩大正样本数量。 过拟合方面 LR容易欠拟合，准确度低。 SVM不太容易过拟合：松弛因子+损失函数形式 注意SVM的求解方法叫拉格朗日乘子法，而对于均方误差的优化方法是最小二乘法。 方法的选择在Andrew NG的课里讲到过： 如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM 如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel 如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况 当你的数据非常非常非常非常非常大然后完全跑不动SVM的时候，跑LR。SVM适合于小样本学习。多大算是非常非常非常非常非常非常大？ 比如几个G，几万维特征，就勉强算大吧…而实际问题上几万个参数实在完全不算个事儿，太常见了。随随便便就得上spark。读一遍数据就老半天，一天能训练出来的模型就叫高效了。所以在新时代，LR其实反而比以前用的多了=. = 应用场景方面不同拟合程度，样本量， 距离测度，数据balance 模型简单易解释 如果数据特征维度高，svm要使用核函数来求解 Note：拉格朗日对偶没有改变最优解，但改变了算法复杂度：原问题—样本维度；对偶问题–样本数量。所以 线性分类&amp;&amp;样本维度&lt;样本数量：原问题求解（liblinear默认）； 非线性–升维—一般导致 样本维度&gt;样本数量：对偶问题求解 SVM适合处理什么样的数据？高维稀疏，样本少。【参数只与支持向量有关，数量少，所以需要的样本少，由于参数跟维度没有关系，所以可以处理高维问题】 机器学习算法选择机器学习算法小结与收割offer遇到的问题 随机森林平均来说最强，但也只在9.9%的数据集上拿到了第一，优点是鲜有短板。SVM的平均水平紧随其后，在10.7%的数据集上拿到第一。神经网络（13.2%）和boosting（~9%）表现不错。数据维度越高，随机森林就比AdaBoost强越多，但是整体不及SVM2。数据量越大，神经网络就越强。 K近邻典型的例子是KNN，它的思路就是——对于待判断的点，找到离它最近的几个数据点，根据它们的类型决定待判断点的类型。 它的特点是完全跟着数据走，没有数学模型可言。 随机森林RF与传统bagging的区别（1）样本采样：RF有放回选取和整体样本数目相同的样本，一般bagging用的样本&lt;总体样本数（2）特征采样：RF对特征进行采样，BAGGING用全部特征]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[文本分类]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[人群画像]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E4%BA%BA%E7%BE%A4%E7%94%BB%E5%83%8F%2F</url>
    <content type="text"><![CDATA[比你更了解你，浅谈用户画像 爱点击的性别预测模型 为什么用朴素贝叶斯？ 如何选择特征？ 去除覆盖率低的，去除 如何解决特征有依赖关系的问题？ 假设，对于同一个一级域名，下面的N级域名中男女分布比例在接近的合并为同一个特征。 训练集和测试集？ 线上效果 鼎盛时期，平均每个cookie有5个url]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[sklearn的一些总结]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2Fsklearn%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[转自 sklearn的一些总结]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CTR预测专题]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FCTR%E9%A2%84%E6%B5%8B%E4%B8%93%E9%A2%98%2F</url>
    <content type="text"><![CDATA[如何评价CTR预估效果？ 为什么LR可以用来做CTR预估？ 关于CTR预测的一个总结 为什么CTR预估使用AUC来评估模型？ 广告计算中的AUC和ROC曲线 常见计算广告点击率预估算法总结 关于贝叶斯平滑 http://blog.csdn.net/z363115269/article/details/78637702 http://blog.csdn.net/google19890102/article/details/50492787 http://blog.csdn.net/jinping_shi/article/details/78334362 http://blog.csdn.net/wwqwkg6e/article/details/55000216 贝叶斯平滑的思想是给CTR预设一个经验初始值，再通过当前的点击量和曝光量来修正这个初始值。如果某商品的点击量和曝光量都是0，那么该商品的CTR就是这个经验初始值；如果商品A和商品B的曝光量差别很大，那么可以通过这个经验初始值来修正，使得曝光量大的商品的权重增大。 贝叶斯平滑就是确定这个经验值的过程。贝叶斯平滑是基于贝叶斯统计推断的，因此经验值计算的过程依赖于数据的分布情况。 贝叶斯平滑的推导涉及贝叶斯参数估计 ctr能不能加入id类特征？ 这里没看懂？为什么ctr越高的分段上权重越大？ 假设一个最简单的问题，预估广告的点击率CTR。为了便于讨论，假设你只有一个特征，就是每次展现广告在过去一个时间窗内的历史点击率ctr，现在目标是预测下一次点击的ctr。简单起见，不妨假设系统中只有两条候选广告。 显而易见，预测分数是和ctr正相关的。如果你使用的是离散LR，那么在分段之后，显然ctr越高的分段上权重越大。这个模型实际跑起来就是最简单的“热门广告”的效果。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SVM]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FSVM%2F</url>
    <content type="text"><![CDATA[线性可分的推导http://blog.sina.com.cn/s/blog_4298002e010144k8.html 线性不可分的时候http://blog.csdn.net/american199062/article/details/51322852 松弛系数 允许错误的分类，但要付出代价。错分的苹果是大于1，在margin当中但分类正确的在0,1之间。 对于整体的惩罚力度，要另外使用一个参数C来衡量惩罚的程度。 通过核函数可以以低的计算复杂度构造更复杂的分类器，而不用在低维映射到高维。 SMO优化 https://www.cnblogs.com/pinard/p/6111471.html]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[BP神经网络]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FBP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[https://www.cnblogs.com/wjy-lulu/p/6511616.html https://www.cnblogs.com/biaoyu/archive/2015/06/20/4591304.html]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SVD]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FSVD%2F</url>
    <content type="text"><![CDATA[整理自： 奇异值分解(SVD)原理与在降维中的应用 奇异值分解(SVD) —- 线性变换几何意义 机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 特征值和特征分解 Ax=\lambda x其中A是一个n×n的矩阵，x是一个n维向量，则我们说λ是矩阵A的一个特征值，而x是矩阵A的特征值λ所对应的特征向量。 而从几何上看，A相当于对向量x进行了拉伸，λ是拉伸的尺度。 前提是A是一个对称矩阵。 对称矩阵 转置后与原矩阵相等。任意矩阵乘以它的转置也是对称矩阵。 特征分解时，A必须是方阵。 SVD之后，对于奇异值,它跟我们特征分解中的特征值类似，在奇异值矩阵中也是按照从大到小排列，而且奇异值的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上的比例。也就是说，我们也可以用最大的k个的奇异值和对应的左右奇异向量来近似描述矩阵。 A_{m \times n} = U_{m \times m}\Sigma_{m \times n} V^T_{n \times n} \approx U_{m \times k}\Sigma_{k \times k} V^T_{k \times n}这样，矩阵A就可以近似的表示为 （文本分析中） 三个矩阵有非常清楚的物理含义： 第一个矩阵X中的每一行表示意思相关的一类词，其中的每个非零元素表示这类词中每个词的重要性（或者说相关性），数值越大越相关。 第三个矩阵Y中的每一列表示同一主题一类文章，其中每个元素表示这类文章中每篇文章的相关性。 第二个矩阵B则表示类词和文章之间的相关性。因此，我们只要对关联矩阵A进行一次奇异值分解，我们就可以同时完成了近义词分类和文章的分类。（同时得到每类文章和每类词的相关性）。 SVD的性质 降维 对于奇异值,它跟我们特征分解中的特征值类似，在奇异值矩阵中也是按照从大到小排列，而且奇异值的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上的比例。也就是说，我们也可以用最大的k个的奇异值和对应的左右奇异向量来近似描述矩阵。也就是说： A_{m \times n} = U_{m \times m}\Sigma_{m \times n} V^T_{n \times n} \approx U_{m \times k}\Sigma_{k \times k} V^T_{k \times n}]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从内容-用户画像到如何做算法研发]]></title>
    <url>%2F2018%2F03%2F17%2F%E6%80%BB%E7%BB%93%E4%B8%8E%E6%80%9D%E8%80%83%2F%E4%BB%8E%E5%86%85%E5%AE%B9-%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F%E5%88%B0%E5%A6%82%E4%BD%95%E5%81%9A%E7%AE%97%E6%B3%95%E7%A0%94%E5%8F%91%2F</url>
    <content type="text"><![CDATA[转自http://blog.csdn.net/bitcarmanlee/article/details/77574371 要求以Spark的Mlib为载体，尽量所有人共用一个算法平台。这样做的好处是大家信息共享会更快，同一个平台也更好维护。比如，算法工程师写了一个巨牛逼的算法原型，然后他需要先给工程师讲懂这个算法，工程师看个人水平，先不说能否将算法实现，实现所花的时间，以及是否真的有时间和精力去帮着实现，实现的是不是有问题就是一个很大的问题了。来回一折腾，两个人都会比较累。 怎么才算对算法有了真正的理解。 首先我们看什么场景用什么算法，但实际用起来，效果并不是那么好。这个时候我们至少需要了解两方面： 算法的核心是什么，有什么潜在的需求？比如是不是对数据的分布做了什么假设么? 特征和数据集的情况是如何的？ 而且很多算法做了很多很粗暴的假设，这种假设会导致算法存在一些固有的问题，如果你不了解其内部的这些假设，你会以为这些是他的一个特性，其实是一个缺点。比如Gini Importance，如果你不去了解的内部思想，你在理解数据时，就会造成误解，导致错误的认为先被选中的特征是很重要的，而其余的特征是不重要的，但实际上这些特征对响应变量的作用确实非常接近的。 我有时候觉得，引用算法工程师最流行的一个话，就是tricky。 中文我不知道怎么翻译更合适，很多时候是需要悟性和对事物本质的了解，才能了解一个算法的，绝对不是靠几个公式就能搞定的。 协同算法是我们应用的比较广泛的一个算法。 但是我觉得协同不应该算是一个算法，而是一种模式。 我们常见的很多模型，最后都是协同模式。举个例子来说，是不是个A1用户推荐文章B1,我们可能是这么做的： 把用户用向量做表征，文章也是观察大量的用户A2,A3…AN 是不是有点击该B1使用逻辑回归/SVM等分类算法训练模型把A1,B1丢进模型，得到是否推荐。但事实上这套算法，用的就是协同。为啥的？本质上还是相近的用户做的选择互相推荐。]]></content>
      <categories>
        <category>总结与思考</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Word2Vec源码解读]]></title>
    <url>%2F2018%2F03%2F17%2FNLP%2FWord2vec-C%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[机器学习算法实现解析——word2vec源码解析 源码在/home/david/code/nlp/word2vec/word2vecC/word2vec.c 代码的主要工作包括： 预处理。变量声明，全局变量遍历； 构建词库。包括文本处理，以及是否需要有指定词库。 初始化网络结构。参数初始化，Huffman编码的生成。 多线程模型训练。 最终结果的处理。 以上的过程，可以用下图表示： 输入参数1234567891011121314151617181920212223242526-train text8 表示的是输入文件是text8-output vectors.bin 输出文件是vectors.bin-cbow 1 表示使用cbow模型，默认为Skip-Gram模型-size 200 每个单词的向量维度是200-window 8 训练的窗口大小为5就是考虑一个词前八个和后八个词语（实际代码中还有一个随机选窗口的过程，窗口大小小于等于8）-negative 0 使用ns的时候采样的样本数，默认0，通常是5-10-save-vocab 词汇表存储文件-read-vocab 词汇表加载文件-classes 输出单词类别数，默认为0，即不输出单词-hs 1不使用NEG方法，使用HS方法。--sample 亚采样拒绝概率的参数指的是采样的阈值，如果一个词语在训练样本中出现的频率越大，那么就越会被采样。-binary为1指的是结果二进制存储，为0是普通存储（普通存储的时候是可以打开看到词语和对应的向量的）-iter 15 迭代次数 全局变量int *vocab_hash 词在词库中的index，在构建词库时先初始化为-1 123456// 词的结构体struct vocab_word &#123; long long cn; // 出现的次数 int *point; // 从根结点到叶子节点的路径 char *word, *code, codelen;// 分别对应着词，Huffman编码，编码长度&#125;; vocab_word是词的结构体 vocab = (struct vocab_word *)calloc(vocab_max_size, sizeof(struct vocab_word)); vocab存储词 vocab_size ：词汇表的总量 syn0 ：上下文词 syn1 ：$\theta_{j-1}^w$ neu1 ：映射层的向量，就是输入层的向量之和 neu1e ：对应伪代码中的e 预处理 在预处理部分，对word2vec需要使用的参数进行初始化，在word2vec中是利用传入的方式对参数进行初始化的。 在预处理部分，实现了sigmoid函数值的近似计算。 如果每一次都请求计算sigmoid值，对性能将会有一定的影响，当sigmoid的值对精度的要求并不是非常严格时，可以采用近似计算。在word2vec中，将区间[−6,6]（设置的参数MAX_EXP为6）等距离划分成EXP_TABLE_SIZE等份，并将每个区间中的sigmoid值计算好存入到数组expTable中，需要使用时，直接从数组中查找。 1234567// 申请EXP_TABLE_SIZE+1个空间expTable = (real *)malloc((EXP_TABLE_SIZE + 1) * sizeof(real)); for (i = 0; i &lt; EXP_TABLE_SIZE; i++) &#123; expTable[i] = exp((i / (real)EXP_TABLE_SIZE * 2 - 1) * MAX_EXP); // 1/(1+e^6) ~ 1/(1+e^-6)即 0.01 ~ 1 的样子 expTable[i] = expTable[i] / (expTable[i] + 1); // Precompute f(x) = x / (x + 1) &#125; 注意：在上述代码中，作者使用的是小于EXP_TABLE_SIZE，实际的区间是[−6,6)。 构建词库在word2vec源码中，提供了两种构建词库的方法，分别为： 指定词库：ReadVocab()方法 从词的文本构建词库：LearnVocabFromTrainFile()方法 构建词库的过程在这里，我们以从词的文本构建词库为例。构建词库的过程如下所示： 在这部分中，最主要的工作是对文本进行处理，包括低频词的处理，hash表的处理等等。首先，会在词库中增加一个“&lt;/s&gt;”的词，同时，在读取文本的过程中，将换行符“\n”也表示成该该词 对词的哈希处理在存储词的过程中，同时保留这两个数组： 存储词的vocab 存储词的hash的vocab_hash 其中，在vocab中，存储的是词对应的结构体： 在vocab_hash中存储的是词在词库中的Index，vocab_hash的下标是词计算出的hash值。 在对词的处理过程中，主要包括： 计算词的hash值： 1234567// 取词的hash值int GetWordHash(char *word) &#123; unsigned long long a, hash = 0; for (a = 0; a &lt; strlen(word); a++) hash = hash * 257 + word[a]; hash = hash % vocab_hash_size; return hash;&#125; SearchVocab检索词是否存在。如不存在则返回-1，否则，返回该词在词库中的索引： 1234567while (1) &#123; if (vocab_hash[hash] == -1) return -1;// 不存在该词 //strcmp两个词相等，则返回0，所以要加上! if (!strcmp(word, vocab[vocab_hash[hash]].word)) return vocab_hash[hash];// 返回索引值 hash = (hash + 1) % vocab_hash_size;// 处理冲突&#125;return -1;// 不存在该词 在这个过程中，使用到了线性探测的开放定址法处理冲突，开放定址法就是一旦发生冲突，就去寻找下一个空的散列地址。 不存在，则插入新词。 对低频词的处理在循环读取每一个词的过程中，当出现“vocab_size &gt; vocab_hash_size * 0.7”时，需要对低频词进行处理。其中，vocab_size表示的是目前词库中词的个数，vocab_hash_size表示的是初始设定的hash表的大小。 ReduceVocab() 在处理低频词的过程中，通过参数“min_reduce”来控制，若词出现的次数小于等于该值时，则从词库中删除该词。 123456for (a = 0; a &lt; vocab_size; a++) if (vocab[a].cn &gt; min_reduce) &#123; vocab[b].cn = vocab[a].cn; vocab[b].word = vocab[a].word; b++; &#125; else free(vocab[a].word); vocab_size = b;// 删减后词的个数 在删除了低频词后，需要重新对词库中的词进行hash值的计算。 123456789for (a = 0; a &lt; vocab_hash_size; a++) vocab_hash[a] = -1;for (a = 0; a &lt; vocab_size; a++) &#123; // Hash will be re-computed, as it is not actual hash = GetWordHash(vocab[a].word); while (vocab_hash[hash] != -1) hash = (hash + 1) % vocab_hash_size; vocab_hash[hash] = a;&#125;fflush(stdout);min_reduce++; 根据词频对词库中的词排序基于以上的过程，程序已经将词从文件中提取出来，并存入到指定的词库中（vocab数组），接下来，需要根据每一个词的词频对词库中的词按照词频从大到小排序，其基本过程在函数SortVocab中，排序过程为 1qsort(&amp;vocab[1], vocab_size - 1, sizeof(struct vocab_word), VocabCompare); 原 型:void qsort(void base, int nelem, int width, int (fcmp)(const void ,const void )); 功 能: 使用快速排序例程进行排序 参 数： 1 待排序数组首地址 2 数组中待排序元素数量 3 各元素的占用空间大小 4 指向函数的指针，用于确定排序的顺序 说 明：qsort函数是ANSI C标准中提供的，其声明在stdlib.h文件中，是根据二分法写的，其时间复杂度为n*log(n)。 qsort要求提供的函数是需要自己定义的一个比较函数，比较函数使得qsort通用性更好。有了比较函数qsort可以实现对数组、字符串、结构体等结构进行升序或降序排序。如int cmp(const void a, const void b)中有两个元素作为参数（参数的格式不能变的。）返回一个int值，如果比较函数返回大于0，qsort就认为a &gt; b，返回小于0,qsort就认为a &lt; b。qsort知道元素的大小了，就可以把大的放前面去。如果你的比较函数返回本来应该是1的（即a &gt; b），而却返回-1（小于0的数），那么qsort认为a &lt; b，就把b放在前面去，但实际上是a &gt; b的，所以就造成了降序排序的差别了。简单来说，比较函数的作用就是给qsort指明元素的大小事怎么比较的。 保持字符“&lt; \s&gt;”在最开始的位置。排序后，根据“min_count”对低频词进行处理，与上述一样，再对剩下的词重新计算hash值。 至此，整个对词的处理过程就已经结束了。接下来，将是对网络结构的处理和词向量的训练。 初始化网络结构有了以上的对词的处理，就已经处理好了所有的训练样本，此时，便可以开始网络结构的初始化和接下来的网络训练。网络的初始化的过程在InitNet()函数中完成。 初始化网络参数在初始化的过程中，主要的参数包括词向量的初始化和映射层到输出层的权重的初始化，如下图所示： 词向量的初始化：为每个词分配空间，大小是vocab_size*layer1_size。 初始化的时候要分配所有词*词向量长度的空间？为何要这么大？12// layer1_size是词向量的长度a = posix_memalign((void **)&amp;syn0, 128, (long long)vocab_size * layer1_size * sizeof(real)); 1234&gt; int posix_memalign (void **memptr,&gt; size_t alignment,&gt; size_t size);&gt; &gt; 调用posix_memalign( )成功时会返回size字节的动态内存，并且这块内存的地址是alignment的倍数。参数alignment必须是2的幂，还是void指针的大小的倍数。返回的内存块的地址放在了memptr里面，函数返回值是0。 CBOW网络有两种可选的算法：层次Softmax和Negative Sampling。在输入参数时选择任意一种。 12345678910111213141516// 层次softmax的结构 if (hs) &#123; // 映射层到输出层之间的权重，就是Huffman树的非叶子结点的向量θ a = posix_memalign((void **)&amp;syn1, 128, (long long)vocab_size * layer1_size * sizeof(real)); if (syn1 == NULL) &#123;printf("Memory allocation failed\n"); exit(1);&#125; for (a = 0; a &lt; vocab_size; a++) for (b = 0; b &lt; layer1_size; b++) syn1[a * layer1_size + b] = 0;// 权重初始化为0 &#125; // 负采样的结构 if (negative&gt;0) &#123; a = posix_memalign((void **)&amp;syn1neg, 128, (long long)vocab_size * layer1_size * sizeof(real)); if (syn1neg == NULL) &#123;printf("Memory allocation failed\n"); exit(1);&#125; for (a = 0; a &lt; vocab_size; a++) for (b = 0; b &lt; layer1_size; b++) syn1neg[a * layer1_size + b] = 0; &#125; 在初始化的过程中，映射层到输出层的权重都初始化为0，而对于每一个词向量的初始化，作者的初始化方法如下代码所示： 12345678// 随机初始化 for (a = 0; a &lt; vocab_size; a++) for (b = 0; b &lt; layer1_size; b++) &#123; next_random = next_random * (unsigned long long)25214903917 + 11; // 1、与：相当于将数控制在一定范围内 // 2、0xFFFF：65536 // 3、/65536：[0,1]之间 syn0[a * layer1_size + b] = (((next_random &amp; 0xFFFF) / (real)65536) - 0.5) / layer1_size;// 初始化词向量 &#125; 首先，生成一个很大的next_random的数，通过与“0xFFFF”进行与运算截断，再除以65536得到[0,1]之间的数，最终，得到的初始化的向量的范围为：[−0.5/m,0.5/m]，其中，m为词向量的长度。 3.2、Huffman树的构建在层次Softmax中需要使用到Huffman树以及Huffman编码，因此，在网络结构的初始化过程中，也需要初始化Huffman树。在生成Huffman树的过程中，首先定义了3个长度为vocab_size*2+1的数组： 1234// 申请2倍的词的空间long long *count = (long long *)calloc(vocab_size * 2 + 1, sizeof(long long));long long *binary = (long long *)calloc(vocab_size * 2 + 1, sizeof(long long));long long *parent_node = (long long *)calloc(vocab_size * 2 + 1, sizeof(long long)); 其中，count数组中前vocab_size存储的是每一个词的对应的词频，词频是从高到低排序。后面的vocab_size先初始化为很大的数。 123// 分成两半进行初始化for (a = 0; a &lt; vocab_size; a++) count[a] = vocab[a].cn;// 前半部分初始化为每个词出现的次数for (a = vocab_size; a &lt; vocab_size * 2; a++) count[a] = 1e15;// 后半部分初始化为一个固定的常数 构建Huffman树的过程如下所示 首先，设置两个指针pos1和pos2，分别指向最后一个词和最后一个词的后一位 12345// 两个指针：// pos1指向前半截的尾部// pos2指向后半截的开始pos1 = vocab_size - 1;pos2 = vocab_size; 从两个指针所指的数中选择出最小的值，记为min1i， 如pos1所指的值最小，此时，将pos1左移，再比较pos1和pos2所指的数，选择出最小的值，记为min2i，将他们的和存储到pos2所指的位置。并将此时pos2所指的位置设置为min1i和min2i的父节点，同时，记min2i所指的位置的编码为1（这里令右子树的编码为1）。 如pos2所指的值小，此时，将pos2右移，再比较pos1和pos2，选出最小的值，记为min2i， 12345678910111213141516171819202122232425262728293031323334353637// Following algorithm constructs the Huffman tree by adding one node at a time // 每次增加一个节点，构建Huffman树 for (a = 0; a &lt; vocab_size - 1; a++) &#123; // First, find two smallest nodes &apos;min1, min2&apos; // 选择最小的节点min1 if (pos1 &gt;= 0) &#123; if (count[pos1] &lt; count[pos2]) &#123; min1i = pos1; pos1--; &#125; else &#123; min1i = pos2; pos2++; &#125; &#125; else &#123; min1i = pos2; pos2++; &#125; // 选择最小的节点min2 if (pos1 &gt;= 0) &#123; if (count[pos1] &lt; count[pos2]) &#123; min2i = pos1; pos1--; &#125; else &#123; min2i = pos2; pos2++; &#125; &#125; else &#123; min2i = pos2; pos2++; &#125; count[vocab_size + a] = count[min1i] + count[min2i]; // 设置父节点 parent_node[min1i] = vocab_size + a; parent_node[min2i] = vocab_size + a; binary[min2i] = 1;// 设置一个子树的编码为1 &#125; 构建好Huffman树后，此时，需要根据构建好的Huffman树生成对应节点的Huffman编码。假设，上述的数据生成的最终的Huffman树为： 此时，count数组，binary数组和parent_node数组分别为： 在生成Huffman编码的过程中，针对每一个词（词都在叶子节点上），从叶子节点开始，将编码存入到code数组中，如对于上图中的“R”节点来说，其code数组为{1,0}，再对其反转便是Huffman编码： 12345678910111213141516171819// Now assign binary code to each vocabulary word // 为每一个词分配二进制编码，即Huffman编码 for (a = 0; a &lt; vocab_size; a++) &#123;// 针对每一个词 b = a; i = 0; while (1) &#123; code[i] = binary[b];// 找到当前的节点的编码 point[i] = b;// 记录从叶子节点到根结点的序列 i++; b = parent_node[b];// 找到当前节点的父节点 if (b == vocab_size * 2 - 2) break;// 已经找到了根结点，根节点是没有编码的 &#125; vocab[a].codelen = i;// 词的编码长度 vocab[a].point[0] = vocab_size - 2;// 根结点 for (b = 0; b &lt; i; b++) &#123; vocab[a].code[i - b - 1] = code[b];// 编码的反转 vocab[a].point[i - b] = point[b] - vocab_size;// 记录的是从根结点到叶子节点的路径 &#125; &#125; 3.3、负样本选中表的初始化（自己没看） 如果是采用负采样的方法，此时还需要初始化每个词被选中的概率。在所有的词构成的词典中，每一个词出现的频率有高有低，我们希望，对于那些高频的词，被选中成为负样本的概率要大点，同时，对于那些出现频率比较低的词，我们希望其被选中成为负样本的频率低点。这个原理于“轮盘赌”的策略一致（详细可以参见“优化算法——遗传算法”）。在程序中，实现这部分功能的代码为： 12345678910111213141516171819// 生成负采样的概率表void InitUnigramTable() &#123; int a, i; double train_words_pow = 0; double d1, power = 0.75; table = (int *)malloc(table_size * sizeof(int));// int --&gt; int for (a = 0; a &lt; vocab_size; a++) train_words_pow += pow(vocab[a].cn, power); // 类似轮盘赌生成每个词的概率 i = 0; d1 = pow(vocab[i].cn, power) / train_words_pow; for (a = 0; a &lt; table_size; a++) &#123; table[a] = i; if (a / (double)table_size &gt; d1) &#123; i++; d1 += pow(vocab[i].cn, power) / train_words_pow; &#125; if (i &gt;= vocab_size) i = vocab_size - 1; &#125;&#125; 在实现的过程中，没有直接使用每一个词的频率，而是使用了词的0.75次方。 4、多线程模型训练以上的各个部分是为训练词向量做准备，即准备训练数据，构建训练模型。在上述的初始化完成后，接下来就是根据不同的方法对模型进行训练，在实现的过程中，作者使用了多线程的方法对其进行训练。 4.1、多线程的处理为了能够对文本进行加速训练，在实现的过程中，作者使用了多线程的方法，TrainModelThread() 对每一个线程上分配指定大小的文件： 12// 利用多线程对训练文件划分，每个线程训练一部分的数据fseek(fi, file_size / (long long)num_threads * (long long)id, SEEK_SET); 这个过程可以通过下图简单的描述： 在实现多线程的过程中，作者并没有加锁的操作，而是对模型参数和词向量的修改可以任意执行，这一点类似于基于随机梯度的方法，训练的过程与训练样本的训练是没有关系的，这样可以大大加快对词向量的训练。抛开多线程的部分，在每一个线程内执行的是对模型和词向量的训练。 作者在实现的过程中，主要实现了两个模型，即CBOW模型和Skip-gram模型，在每个模型中，又分别使用到了两种不同的训练方法，即层次Softmax和Negative Sampling方法。 4.2、CBOW模型4.2.1、从输入层到映射层首先找到每个词对应的词向量，并将这些词的词向量相加，程序代码如下所示： 12345678910111213141516// in -&gt; hiddencw = 0;//b是随机生成的0到window-1，相当于左右各看window-b/2个词for (a = b; a &lt; window * 2 + 1 - b; a++) if (a != window) &#123; //sentence_position 单词在句子中的位置 c = sentence_position - window + a; // 判断c是否越界 if (c &lt; 0) continue; if (c &gt;= sentence_length) continue; // 找到c对应的索引 last_word = sen[c]; if (last_word == -1) continue; // neu1就是隐藏层向量，也就是上下文对应vector的和 for (c = 0; c &lt; layer1_size; c++) neu1[c] += syn0[c + last_word * layer1_size]; cw++;&#125; 当累加完窗口内的所有的词向量的之后，存储在映射层neu1中，并取平均，程序代码如下所示： 1for (c = 0; c &lt; layer1_size; c++) neu1[c] /= cw; 当取得了映射层的结果后，此时就需要使用Hierarchical Softmax或者Negative Sampling对模型进行训练。 4.2.2、Hierarchical Softmax1234567891011121314151617181920if (hs) for (d = 0; d &lt; vocab[word].codelen; d++) &#123; f = 0; // point存储了从该词的叶子结点的编号到root的序号，这些序号可以对应到syn1的位置，也就是参数向量的位置 l2 = vocab[word].point[d] * layer1_size; // Propagate hidden -&gt; output // q=sigma(x*theta) for (c = 0; c &lt; layer1_size; c++) f += neu1[c] * syn1[c + l2]; if (f &lt;= -MAX_EXP) continue; else if (f &gt;= MAX_EXP) continue; // 查表得知sigma的值，省去计算的时间 else f = expTable[(int)((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))]; // g = eta(1-d-q) g = (1 - vocab[word].code[d] - f) * alpha; // Propagate errors output -&gt; hidden // e = e + g*theta for (c = 0; c &lt; layer1_size; c++) neu1e[c] += g * syn1[c + l2]; // Learn weights hidden -&gt; output // theta = theta + g*x for (c = 0; c &lt; layer1_size; c++) syn1[c + l2] += g * neu1[c]; &#125; 接下来更新Context(w) 123456789// hidden -&gt; in for (a = b; a &lt; window * 2 + 1 - b; a++) if (a != window) &#123; c = sentence_position - window + a; if (c &lt; 0) continue; if (c &gt;= sentence_length) continue; last_word = sen[c]; if (last_word == -1) continue; for (c = 0; c &lt; layer1_size; c++) syn0[c + last_word * layer1_size] += neu1e[c]; &#125; Word2Vec为什么快 用查表代替计算sigmoid 相对于神经网络的结构，去掉了隐藏层 增量训练从搜索引擎爬包含新词的文本，加上一个小语料，训练一个w2v模型。 对于每个新词，找出小模型中最接近的10个词，以及每个词与新词的相似度打分score。 再从大模型中找出每个词的词向量，每个维度乘以小模型中的score，最多叠加5个。再对每个维度取加权平均。 最后转成单位向量。 参考 【1】http://blog.csdn.net/google19890102/article/details/51887344]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Word2Vec原理]]></title>
    <url>%2F2018%2F03%2F17%2FNLP%2FWord2Vec%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[参考 《word2vec数学原理.pdf》 词向量基础​ 用词向量来表示词并不是word2vec的首创，在很久之前就出现了。最早的词向量是很冗长的，它使用是词向量维度大小为整个词汇表的大小，对于每个具体的词汇表中的词，将对应的位置置为1。比如我们有下面的5个词组成的词汇表，词”Queen”的序号为2， 那么它的词向量就是(0,1,0,0,0)。同样的道理，词”Woman”的词向量就是(0,0,0,1,0)。这种词向量的编码方式我们一般叫做1-of-N representation或者one hot representation. ​ One hot representation用来表示词向量非常简单，但是却有很多问题。最大的问题是我们的词汇表一般都非常大，比如达到百万级别，这样每个词都用百万维的向量来表示简直是内存的灾难。这样的向量其实除了一个位置是1，其余的位置全部都是0，表达的效率不高，能不能把词向量的维度变小呢？ ​ Dristributed representation可以解决One hot representation的问题，它的思路是通过训练，将每个词都映射到一个较短的词向量上来。所有的这些词向量就构成了向量空间，进而可以用普通的统计学的方法来研究词与词之间的关系。这个较短的词向量维度是多大呢？这个一般需要我们在训练时自己来指定。 ​ 比如下图我们将词汇表里的词用”Royalty”,”Masculinity”, “Femininity”和”Age”4个维度来表示，King这个词对应的词向量可能是(0.99,0.99,0.05,0.7)(0.99,0.99,0.05,0.7)。当然在实际情况中，我们并不能对词向量的每个维度做一个很好的解释。 ​ 有了用Dristributed representation表示的较短的词向量，我们就可以较容易的分析词之间的关系了，比如我们将词的维度降维到2维，有一个有趣的研究表明，用下图的词向量表示我们的词时，我们可以发现： \vec {King} - \vec {Man} + \vec {Woman} = \vec {Queen} 可见我们只要得到了词汇表里所有词对应的词向量，那么我们就可以做很多有趣的事情了。不过，怎么训练得到合适的词向量呢？一个很常见的方法是使用神经网络语言模型。 2. CBOW与Skip-Gram用于神经网络语言模型​ 在word2vec出现之前，已经有用神经网络DNN来用训练词向量进而处理词与词之间的关系了。采用的方法一般是一个三层的神经网络结构（当然也可以多层），分为输入层，隐藏层和输出层(softmax层)。 ​ 这个模型是如何定义数据的输入和输出呢？一般分为CBOW（Continuous Bag-of-Words） 与Skip-Gram两种模型。 2.1 CBOW上下文相关的词对应的词向量，而输出就是这特定的一个词的词向量。比如下面这段话，我们的上下文大小取值为4，特定的这个词是”Learning”，也就是我们需要的输出词向量,上下文对应的词有8个，前后各4个，这8个词是我们模型的输入。由于CBOW使用的是词袋模型，因此这8个词都是平等的，也就是不考虑他们和我们关注的词之间的距离大小，只要在我们上下文之内即可。 在这个CBOW神经网络模型中，输入层有8个神经元（8个词向量），输出层有词汇表D大小的神经元。 目标函数通常为 L=\sum_{w \in C} \log p(w|Context(w)) 为什么是这个形式？ 根据n-gram模型和对数最大似然，得到这个目标函数。 包括四个层：输入、投影、隐藏、输出。 对于语料C中的任意一个词w，将Context(w)设为取前面的n-1个词，这样二元对(Context(w),w)就是一个训练样本。 投影层向量$x_w$的构造是，将输入层的n-1个词向量按顺序首尾相接的拼起来，长度就是m(n-1)了（每个词向量的长度是m）。 从而 z_w = tanh(W{x_w}+p) \\ y_w=Uz_w+q其中，tanh是双曲正切函数，用来做隐藏层的激活函数。 经过上面两步计算得到的$yw=(y{w,1},y{w,2},…,y{w,N})^T$是一个长度为N的向量，其分量不能代表概率。如果想要$y_{w,i}$表示当上下文为Context(w)时下一次为词典D中第i个词的概率，则还需要做一个softmax归一化，之后得到 p(w|Context(w))=\frac {e^{y_{w,i_w}}} {\sum_{i=1}^N e^{y_{w,i_w}}}其中$i_w$表示词w在词典D中的索引。 与n-gram相比，神经概率语言模型的优势是： 1、词语之间的相似性可以通过词向量来体现。 1）神经网络模型通过上下文来预测，那么相似的上下文的词的词向量也是相似的； 2）概率函数关于词向量是光滑的，即词向量的一个小变化对概率的影响也是一个小变化。 2、词向量自带平滑功能（因为$p(w|Context(w)) \in (0,1)$不会为零）。 2.2 Skip-Gram与CBOW相反，输入是一个特定向量，输出是特定词对应的上下文词向量。 3、基于Hierarchical Softmax的模型3.1 CBOW模型网络的构建 输入层是上下文的若干个词的词向量 投影层就是将这些词向量直接相加。 层次Softmax的基本思想就是： 对于词典D中的任意词w，Huffman树中必存在一条从根结点到词w对应结点的路径$p^w$，且这条路径是唯一的。路径$p^w$上存在$l^w-1$个分支，将每个分支看做是一个二分类，每一次分类就产生一个概率，将这些概率连乘起来，就是所需的$p(w|Context(w))$ 。 条件概率连乘的公示可以写为 p(w|Context(w))=\prod_{j=2}^{l^w}p(d_j^w|x_w,\theta_{j-1}^w)其中， p(d_j^w|x_w,\theta_{j-1}^w)=[\sigma(x_w^T\theta_{j-1}^w)]^{1-d_j^w} \cdot [1-\sigma(x_w^T\theta_{j-1}^w)]^{d_j^w} \tag{3-1}这里$\sigma(xw^T\theta{j-1}^w)$表示分到正类的概率。 将3-1代入对数似然函数，得到 L=\sum_{w \in C} \sum_{j=2}^{l^w} \{ (1-d_j^w)\log [\sigma(x_w^T\theta_{j-1}^w)] + d_j^w \log[1-\sigma(x_w^T\theta_{j-1}^w)] \}其中，令 L(w,j)=(1-d_j^w)\log [\sigma(x_w^T\theta_{j-1}^w)] + d_j^w \log[1-\sigma(x_w^T\theta_{j-1}^w)]至此，已经推导出对数似然函数，这就是CBOW模型的目标函数。 参数$\theta$是怎样的矩阵？ 我理解，任何用树的多个二分类问题，目标函数都可以表示成这种形式。 为了使该目标函数最大化，word2vec采用的是随机梯度上升法。每取一个样本，计算梯度再刷新所有的参数。推导出更新公式为 \theta_{j-1}^w = \theta_{j-1}^w + \eta [1-d_j^w-\sigma(x_w^T\theta_{j-1}^w)]x_w^T同样的 \frac {\partial L(w,j)} {\partial x_w}=[1-d_j^w-\sigma(x_w^T\theta_{j-1}^w)]\theta_{j-1}^w我们的最终目的是要求词典D中每个词的词向量，而这里的$x_w$表示的是Context(w)所有词向量的累加和，那么如何利用偏导对$v(\tilde w), w \in Context(\tilde w)$进行更新呢，word2vec算法中直接取 v(\tilde w) = v(\tilde w)+\eta \sum_{j=2}^{l^w} \frac {\partial L(w,j)} {\partial x_w}即把后面增量的值贡献到Context(w)的每一个词的词向量上。 伪代码如下： 1234567891011for j=huffman code:&#123; 1)q=sigma(x*theta) 2)g=eta(1-d-q) 3)e=e+g*theta 4)theta=theta+g*x&#125;for w in Context(w):&#123; v(w) = v(w) + e&#125; 注意3、 4两步不能颠倒位置，要先计算出e，再更新θ。 4、基于Negative Sampling的模型NEG不使用Huffman树，而是利用随机负采样，能大幅度提高性能。 前面的原理差不多，也是用梯度下降，关键在于，h-softmax是通过Huffman树的路径长度来进行迭代和更新参数；NEG是通过找出负采样来迭代。 4.1 负采样算法词典D中的词在语料C中出现的次数有高有低，对于那些高频词，被选为负采样的概率就应该比较大，反之较小。这是我们对采样过程的一个大致要求，本质上就是一个带权采样问题。 设词典D中的每个词w对应一条线段l(w)，则线段的长度可以表示为 len(w)=\frac {count(w)} {\sum_{u \in D} count(u)}也就是计算词频再归一化。现在将这些线段首尾相连拼接在一起，形成一个长度为1的单位线段。如果随机往这个单位线段上打点，则其中越长的线段命中概率越大。 通过这些线段得到一个非等距剖分（假设分成N个区间），再定义一个等距剖分（假设分成M个区间），$M &gt;&gt; N$ 。 建立如下的映射关系 Table(i) = w_k, \ where\ m_i \in I_k,\ i=1,2,...,M-1那么，采样就是每次生成一个[1,M-1]之间的整数r，Table(r)就是一个样本。如果负采样的时候选到自己，就跳过再选。 Word2Vec与神经网络模型的区别 NN的输入是类似n-gram，取前N-1个词，w2v取前后各n-1个词 NN多了隐层，输入到隐层用双曲正切当激活函数。 NN的输入词向量是首尾拼接，W2V是加总。这样当窗口中向量不足时，也不需要补。 NN的输出是一个长度为N的向量，就是整个词汇表的长度，然后再做一个softmax归一化，得到给定上下文时下一个词恰好为词汇表的某个词的概率。 疑问1）在这里非叶子结点对应的那些向量就可以扮演参数的角色？ 就是说可以把$\theta$当做待求的参数，这样就可以用sigmoid来求分类的概率。 参考 【1】word2vec原理(一) CBOW与Skip-Gram模型基础]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[iau的ES使用场景]]></title>
    <url>%2F2018%2F03%2F17%2FElastic%20Search%2Fiau%E7%9A%84ES%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF%2F</url>
    <content type="text"><![CDATA[比较brand和competitor的交叉人群 分别算出brand和competitor的人群，然后再计算包含brand和competitor的人群，就得出交叉人群。 比较brand和competitor的距离 就是比较brand和competitor的每个产品和对方关键词的相关度。]]></content>
      <categories>
        <category>Elastic Search</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[NL工具]]></title>
    <url>%2F2018%2F03%2F17%2FNLP%2FNLP%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[NLP工具Word2Vec-Java加载模型及词向量计算hdfs版 /pig-ext-lite/src/main/java/com/buzzinate/pig/util/WordVec.java 单机版 /persona/src/main/java/com/iclick/persona/nlp/word2vec/W2VContral.java 词向量聚类/home/david/gitlab/user-gene/domain-classify-model/src/main/java/com/buzzinate/domain/classifier/DomainClassifier.java 123456789101112131415161718192021/** * 通过keams得到中心点向量 * * @param wordVec * @return */ public ArrayList&lt;float[]&gt; getCenterVecByKeams(ArrayList&lt;float[]&gt; wordVec) &#123; ArrayList&lt;float[]&gt; centerVec = new ArrayList&lt;float[]&gt;(); List&lt;Classes&gt; cls = null; logger.info(&quot;before keams： &quot; + wordVec.size()); System.out.println(&quot;before keams： &quot; + wordVec.size()); cls = KMeansClustering.getClusteringResult(wordVec, wordVec.subList(0, Math.min(100, wordVec.size() - 1))); for (Classes cl : cls) &#123; if (!Float.isNaN(WVUtils.toFloat(cl.getCenter())[0])) centerVec.add(WVUtils.toFloat(cl.getCenter())); &#125; logger.info(&quot;after keams： &quot; + centerVec.size()); System.out.println(&quot;after keams： &quot; + centerVec.size()); return centerVec;&#125; 通过向量找最近的词Ansj-seg添加词到自定义词典user-gene中用的是某一版ansj的基础上修改的，使用方法是 12Value value = new Value(newWord, new String[] &#123; &quot;userDefine&quot;, &quot;1000&quot; &#125;); Library.insertWord(UserDefineRecognition.FOREST, value); 知乎找到的方法是https://www.zhihu.com/question/32226656 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152作者：ansj链接：https://www.zhihu.com/question/32226656/answer/113724991来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。package org.ansj;import java.util.List;import org.ansj.app.summary.SummaryComputer;import org.ansj.domain.Result;import org.ansj.domain.Term;import org.ansj.library.UserDefineLibrary;import org.ansj.splitWord.analysis.ToAnalysis;import org.nlpcn.commons.lang.tire.domain.Forest;import org.nlpcn.commons.lang.tire.domain.Value;import org.nlpcn.commons.lang.tire.library.Library;public class Test &#123; public static void main(String[] args) throws Exception &#123; // 构造一个用户词典 Forest forest = Library.makeForest(&quot;library/default.dic&quot;); forest = new Forest(); // 增加新词,中间按照&apos;\t&apos;隔开 UserDefineLibrary.insertWord(&quot;ansj中文分词&quot;, &quot;userDefine&quot;, 1000); Result terms = ToAnalysis.parse(&quot;我觉得Ansj中文分词是一个不错的系统!我是王婆!&quot;); System.out.println(&quot;增加新词例子:&quot; + terms); // 删除词语,只能删除.用户自定义的词典. UserDefineLibrary.removeWord(&quot;ansj中文分词&quot;); terms = ToAnalysis.parse(&quot;我觉得ansj中文分词是一个不错的系统!我是王婆!&quot;); System.out.println(&quot;删除用户自定义词典例子:&quot; + terms); // 歧义词 Value value = new Value(&quot;济南下车&quot;, &quot;济南&quot;, &quot;n&quot;, &quot;下车&quot;, &quot;v&quot;); System.out.println(ToAnalysis.parse(&quot;我经济南下车到广州.中国经济南下势头迅猛!&quot;)); Library.insertWord(UserDefineLibrary.ambiguityForest, value); System.out.println(ToAnalysis.parse(&quot;我经济南下车到广州.中国经济南下势头迅猛!&quot;)); // 多用户词典 String str = &quot;神探夏洛克这部电影作者.是一个dota迷&quot;; System.out.println(ToAnalysis.parse(str)); // 两个词汇 神探夏洛克 douta迷 Forest dic1 = new Forest(); Library.insertWord(dic1, new Value(&quot;神探夏洛克&quot;, &quot;define&quot;, &quot;1000&quot;)); Forest dic2 = new Forest(); Library.insertWord(dic2, new Value(&quot;dota迷&quot;, &quot;define&quot;, &quot;1000&quot;)); System.out.println(ToAnalysis.parse(str, dic1, dic2)); &#125;&#125; 如果在spark下面用要修改 Hanlp官方文档http://hanlp.linrunsoft.com/doc/_build/html/index.html https://github.com/hankcs/HanLP/releases HanLP中的数据分为词典和模型，其中词典是词法分析必需的，模型是句法分析必需的。 1234data│├─dictionary└─model 配置文件的作用是告诉HanLP数据包的位置，只需修改第一行 1root=usr/home/HanLP/ 为data的父目录即可 一开始用1.2.9版本，一直提示找不到HanLP.properties，后来换了1.3.2，默认是项目根目录下的data目录。 4.1 分词 标准分词 1list&lt;Term&gt; termlist = StandardTokenizer.segment(&quot;XXXX&quot;); HanLP中有一系列“开箱即用”的静态分词器，以Tokenizer结尾。 HanLP.segment其实是对StandardTokenizer.segment的包装。 分词结果包含词性，每个词性的意思请查阅《HanLP词性标注集》。 算法详解：《词图的生成》 词性标注（Part-of-Speech tagging 或POS tagging)：又称词类标注或者简称标注，是指为分词结果中的每个单词标注一个正确的词性的程序，也即确定每个词是名词、动词、形容词或其他词性的过程。在汉语中，词性标注比较简单，因为汉语词汇词性多变的情况比较少见，大多词语只有一个词性，或者出现频次最高的词性远远高于第二位的词性。据说，只需选取最高频词性，即可实现80%准确率的中文词性标注程序。 利用HMM即可实现更高准确率的词性标注。 去掉停用词、标点的分词 1list&lt;Term&gt; termlist = NotionalTokenizer.segment()]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ES的analyzer]]></title>
    <url>%2F2018%2F03%2F17%2FElastic%20Search%2FES%E7%9A%84analyzer%2F</url>
    <content type="text"><![CDATA[转自ElasticSearch 解析机制常见用法库 之 analyzer常用用法 单词或文档先经过Character Filters；Character Filters的作用就是对文本进行一个预处理，例如把文本中所有“&amp;”换成“and”，把“?”去掉等等操作。 ​ 之后就进入了十分重要的tokenizers模块了，Tokenizers的作用是进行分词，例如，“tom is a good doctor .”。经过Character Filters去掉句号“.”（假设）后，分词器Tokenizers会将这个文本分出很多词来：“tom”、“is”、“a”、“good”、“doctor”。 ​ 经过分词之后的集合，最后会进入Token Filter词单元模块进行处理，此模块的作用是对已经分词后的集合(tokens)单元再进行操作，例如把“tom”再次拆分成“t”、“o”、“m”等操作。最后得出来的结果集合，就是最终的集合。 ​ 所以整个流程是：单词 ====》Character Filter 预处理 =====》tokenizer分词 ====》 token filter对分词进行再处理。 ​ 到此为止，Analyzer是什么鬼？它干什么呢？ ​ Analyzer是由一个tokenizer、零到多个Token Filter、还有零到多个CharFilters构成的，也就是说一个Analyzer就是一个完整的解析模块。 ​ 下面，着重介绍一下常用的Analyzer、Tokenizer、Token filter、Character Filter： Standard Analyzer 一个“standard”标准类型的 analyzer 就是由 标准分词 “Standard Tokenizer”和标准分词过滤器“Standard Token Filter”、小写字母转换分词过滤“Lower case Token Filter”、还有“Stop Token Filter”过滤构成的 以下是一个standard类型 设置说明stopwords 一个用于初始化stop filter的需要stop 单词的列表.默认为空 。 max_token_length 最大的token集合,即经过tokenizer过后得到的结果集的最大值。如果token的长度超过了设置的长度，将会继续分，默认255 Stop Analyzer 一个stop类型的analyzer是由 Lower case Tokenizer 和 Stop Token Filter构成的。 以下是一个stop analyzer可以设置的属性: 设置 说明 stopwords 一个用于初始化stop filter的需要stop 单词的列表.默认单词是英语 stopwords_path 一个有stopwords 配置文件的路径(一个和config文件相关的路径或者URL) 用“stopwords: _none_ ”来定义一个空的stopword列表 Simple Analyzer 一个simple类型的analyzer是由lower case Tokenizer构成的，具体信息可以查看此Tokenzier Whitespace Analyzer 一个whitespace类型的analyzer是由Whitespace Tokenizer构成，请具体查看Whitespace Tokenizer Keyword Analyzer 一个keyword类型的analyzer，它的Tokenizer将整块的数据作为一个小Token（即经过Tokenizer过滤后的数据），这对于像“邮政编码”、“id”等数据非常有用。注意：当使用并定义这种analyzer的时候，单纯的将fieled 设置为“not_analyzed”可能会更有意义。 Pattern Analyzer 一个pattern类型的analyzer可以通过正则表达式将文本分成”terms”(经过token Filter 后得到的东西 )。接受如下设置: 一个 pattern analyzer 可以做如下的属性设置: lowercase terms是否是小写. 默认为 true 小写. pattern 正则表达式的pattern, 默认是 \W+. flags 正则表达式的flags. stopwords 一个用于初始化stop filter的需要stop 单词的列表.默认单词是空的列表 Snowball Analyzer 一个snowball类型的analyzer是由standard tokenizer和standard filter、lowercase filter、stop filter、snowball filter这四个filter构成的。 snowball analyzer 在Lucene中通常是不推荐使用的。 Language Analyzers 一个用于解析特殊语言文本的analyzer集合。（ arabic,armenian, basque, brazilian, bulgarian, catalan, cjk, czech, danish, dutch, english, finnish, french,galician, german, greek, hindi, hungarian, indonesian, irish, italian, latvian, lithuanian, norwegian,persian, portuguese, romanian, russian, sorani, spanish, swedish, turkish, thai.）可惜没有中文。不予考虑 Custom Analyzer 简而言之，是自定义的analyzer。允许多个零到多个tokenizer，零到多个 Char Filters. custom analyzer 的名字不能以 “_”开头. The following are settings that can be set for a custom analyzer type: Setting Description tokenizer 通用的或者注册的tokenizer. filter 通用的或者注册的 token filters. char_filter 通用的或者注册的 character filters. position_increment_gap 距离查询时，最大允许查询的距离，默认是100 自定义的模板： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051index : analysis : analyzer : myAnalyzer2 : type : custom tokenizer : myTokenizer1 filter : [myTokenFilter1, myTokenFilter2] char_filter : [my_html] position_increment_gap: 256 tokenizer : myTokenizer1 : type : standard max_token_length : 900 filter : myTokenFilter1 : type : stop stopwords : [stop1, stop2, stop3, stop4] myTokenFilter2 : type : length min : 0 max : 2000 char_filter : my_html : type : html_strip escaped_tags : [xxx, yyy] read_ahead : 1024]]></content>
      <categories>
        <category>Elastic Search</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[fastText]]></title>
    <url>%2F2018%2F02%2F18%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2Ffasttext%2F</url>
    <content type="text"><![CDATA[fastText 源码分析 fastText原理和实践 带监督的文本分类算法FastText]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>分类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐系统]]></title>
    <url>%2F2018%2F02%2F11%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[推荐算法概述根据数据源的不同分为 1）基于人口的统计学推荐 2）基于内容的推荐Content-based 比如说电影推荐中，基于电影的内容，推荐相似的 3）基于协同过滤的推荐Collaborative Filtering-based 然后CF再分User-Based、Item-Based、Model-Based User-Based、Item-Based都是将用户所有输入载入内存进行运算，又合称为Memory-Based Model-Based包括Aspect Model，pLSA，LDA，聚类，SVD，Matrix Factorization等。 SVD和矩阵分解算法SVD是矩阵分解的一种，不过两种方法在推荐系统中的用法不一样。 基于SVD的推荐系统对A做奇异值分解后，取奇异值矩阵的前N个对角值，相应的取U和V的行和列，再重新拼成A2。 此时的A2相当于A的有损压缩。 分析得知，U矩阵和V矩阵可以近似来代表A矩阵，换据话说就是将A矩阵压缩成U矩阵和V矩阵，至于压缩比例得看当时对S矩阵取前k个数的k值是多少。 SVD在推荐系统中的应用 代码见svd_recommendation.py 假设行是user，列是item，SVD分解后， U的行和user的行数一致，代表user的主题分布 V的列和item的列数一致，代表item的主题分布 S是奇异值，从中选择k 该计算的含义不明白 基于矩阵分解的推荐系统koren获得netflix grand prize时关于矩阵分解的的论文 Matrix Factorization Techniques For Recommender Systems https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf 矩阵分解在推荐系统中的应用：NMF和经典SVD实战 论文Factorization meets the neighborhood a multifaceted collaborative filtering model http://vdisk.weibo.com/s/khQ1v Collaborative Filtering with Temporal Dynamics http://vdisk.weibo.com/s/khQ9o 其他推荐系统的论文 作者：严林链接：https://www.zhihu.com/question/25566638/answer/37455091来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 The Wisdom of The Few 豆瓣阿稳在介绍豆瓣猜的时候极力推荐过这篇论文，豆瓣猜也充分应用了这篇论文中提出的算法； Restricted Boltzmann Machines for Collaborative Filtering 目前Netflix使用的主要推荐算法之一； Factorization Meets the Neighborhood: a Multifaceted Collaborative Filtering Model 这个无需强调重要性，LFM几乎应用到了每一个商业推荐系统中； Collaborative Filtering with Temporal Dynamics 加入时间因素的SVD++模型，曾在Netflix Prize中大放溢彩的算法模型； Context-Aware Recommender Systems 基于上下文的推荐模型，现在不论是工业界还是学术界都非常火的一个topic； Toward the next generation of recommender systems 对下一代推荐系统的一个综述； Item-Based Collaborative Filtering Recommendation Algorithms 基于物品的协同过滤，Amazon等电商网站的主力模型算法之一； Information Seeking-Convergence of Search, Recommendations and Advertising 搜索、推荐和广告的大融合也是未来推荐系统的发展趋势之一； Ad Click Prediction: a View from the Trenches 可以对推荐结果做CTR预测排序； Performance of Recommender Algorithm on top-n Recommendation Task TopN预测的一个综合评测，TopN现在是推荐系统的主流话题，可以全部实现这篇文章中提到的算法大概对TopN有个体会； http://dsec.pku.edu.cn/~jinlong/publication/wjlthesis.pdf 北大一博士对Netflix Prize算法的研究做的毕业论文，这篇论文本身对业界影响不大，但是Netflix Prize中运用到的算法极大地推动了推荐系统的发展 Hulu视频推荐算法干货：从相关性到 RNN，一家线上“租碟店”的视频推荐算法演进 http://blog.csdn.net/dzjx2eotaa24adr/article/details/79060659 推荐系统的模型： 利用（Exploitation）： 计算要给用户推荐什么，比较确定的兴趣。 货架：从协同滤波到深度神经网络 自动播放：RNN 探索（Exploration）： 探索用户的新兴趣，防止出现一样的推荐。 Adaptive：在线学习 Diversity：行列式点过程 推荐系统的优化目标，如果从用户的偏好来讲，我们是希望提供给用户一个非常健康的组合，既符合他们的口味，又有足够的多样性 。 最后从算法或者数学抽象来讲，我们就把这个问题抽象成为“怎样最大化用户的观看时长”。包括 点击进去看一个视频之后，你看了多长时间？这个跟视频本身的长度有关，跟你的完成度有关，我们叫playback_duration。 有效点击eCTR。（推荐系统优劣的评价） 整个内容的曝光量。（推荐系统的作用！） 把这三项乘在一起的话，就变成了每个用户的观看时长。 我们最终想要最大化的是eCTR和曝光量的乘积。 视频推荐系统的框架 1）Data。最底下一层是基础数据。 2）Feature。第二层是特征，特征可以分为用户的特征和内容的特征。 3）Model，分为Exploitation、Exploration。 Exploitation包括： a、Relevant相关度， b、Transparent透明度（对产品经理还是对最终用户是不是可解释） c、Contextual上下文（时间、设备、地点这些上下文信息） Exploration包括 a、Coverage范围，对新内容我们有没有给足够的展现 b、Serendipity惊喜，有没有给用户惊喜、发现用户隐藏的一些兴趣。 c、Adapive适应性，当用户兴趣发生改变的时候，我们是不是非常快地适应了用户这种兴趣改变。 d、Diverse多样性，我们给出的这个套餐是不是组合了很多不同的类别，而不是非常单调、单一的。 4）Application。这些应用主要是服务用户在四个阶段的行为。 a、Onboarding：新用户，刚刚订阅我们的服务的时候，还在一个初始的、给我们一些信号的阶段。我们让用户选择他最喜欢的内容频道，然后用户会告诉我们他喜欢体育、喜剧或者动作片。基于这些大类，我们可以给他做一个冷启动，给他第一屏的推荐结果。 b、Convert-to-Pay：在一个新用户进来之后，我们大概有七天的时间把他转化为一个付费用户。在这个阶段，我们需要快速地探索用户的各种需求，让他体会到我们的服务非常有价值，那么他才愿意买单。 c、Retention留存：到了第三个阶段，就是用户已经是一个付费用户，那我们就需要留住他，所以就是不断的去给他更多的、他之前可能没有看过，但是和之前看过的很相关的内容。 d、Monetization变现：广告变现。一个已经付费的用户，已经留下来的用户，我们怎么样用广告把他的流量变现。 视频推荐算法Exploitation基于用户行为和side information做货架场景的排序和自动播放的这种持续预测。 相似性算法1）Relevant的角度，还是user-based+item-based 基于存储的Memory-based：item-based CF 基于模型的Model-based：矩阵分解和神经网络。神经网络现在也有基于RBM的，还有Embedding-based Neural Network。 Hulu经历了三代相关性算法的演进，第一代是item-based CF，第二代是基于矩阵分解，现在我们正在开发的第三代是基于Embedding-based Neural Network。从Netflix公开的文件来看，它主要使用的是SVD和RBM的方法。 a、第一代，协同滤波 创建一个相关度矩阵。这个矩阵是很稀疏的，大约有百分之七八十的数据是实际上是零。 b、第二代，矩阵分解 矩阵分解为了解决这个稀疏性的问题，使用了线性代数里面的一个特性，就是一个低秩矩阵，可以用两个相对低维度矩阵的乘积来表示。 相关度矩阵（评分矩阵）是低秩的，可以用一个P和Q的乘积来表示，P就是所有用户的特征。 SVD待看 c、第三代，深度学习 把原来矩阵分解里面代数运算的步骤，用一个前向神经网络来替换。好处是， 一方面非线性前向网络允许一些非线性映射，可以有更好的表达能力去model一个更复杂的分布。 另外可以直接把关于用户的处行为外的所有信息，用一个矢量feed到神经网络中去。 对内容我们也可以做相应的处理，就是把元数据，比如说导演、演员信息用一个向量来表示，然后把它feed到神经网络里面去。 相似性算法应用场景 对应这两种不同的场景，其实需要不同的相关运算。 一个是所谓的货架场景，就是给一个网格里面按照相关性做了排序，然后希望用户点越靠上越靠左的这些内容。CF比较适合货架场景的召回和排序。 一个是自动播放的场景，就是播完一个内容之后，我们会自动地开始下一个我们觉得用户最可能看的内容。用RNN。 用反向传播的方法去训练一个RNN模型，来预测用户在网络上的下一个行为。 如何评估： 仿真测试的方法就是当用户看完剧A/B/C之后，我们假装不知道这个用户接下来看了哪一个，然后基于时间序列的建模，来算出一个最可能看的剧，它可能是当前这一剧的下一集，或者是跳到另外一个剧D或者是另外一个剧E。根据RNN模型，我们找到最有可能的下一个剧，然后和用户实际看的下一个剧之间做比对，这是离线的一种评估方案。 Exploration自适应为了解决用户兴趣的时变以及新内容的冷启动，我们采用一种叫做多臂老虎机（MAB）的模型。 原理 一个赌徒可以在不同的时间选择不同的摇臂，每个摇臂会给这个赌徒不同的赢率。如果赌徒每次都选择摇臂1的话，有可能不是最优的，因为可能另外一个摇臂的反馈更好。 在推荐系统中，每个摇臂就是推荐用户的一个剧，算法就是赌徒，根据一些策略选择推荐哪个剧。每个摇臂的奖励就是这个用户是否点击和观看了。 LinUCB算法。会根据当前推的结果，来实时更新对每个摇臂的点击率的预测。 在线上部署LinUCB的算法，有一个线上更新提取特征以及模型运算的过程，以及一个线下根据之前模型采集到的信号去更新模型参数的过程。 在我们这个实验里面，可能对大家比较有参考意义的就是我们发现的LinUCB的一些特征，其中包括用户当前看剧的完成度，就是他看到了第几集、是不是看到了高潮部分还是快要结束的部分。完成度是一个很重要的特征维度。然后就是上次给用户曝光这个剧的时间和现在之间的时间间隔，以及它历史上的点击率，还有这个剧的一些元数据信息，它在外面的流行程度，以及根据刚才讲的协同滤波的方法，得到的用户和这个剧之间的相关性。 多样性 为什么要关注多样性？ 因为用户的兴趣爱好可能不是单峰分布的。有可能用户有多个兴趣爱好，其中有非常突出的一个，就是这个比较高的右边的峰。但是还有一个比较低的，就是右边这张图里面的靠左的这个峰值。用多样性来考虑用户的多个爱好。不是只推荐他最喜欢的。 传统上使用启发式的方法，它会在多样性和相关性之间用一个加权平均的方法来获得一个总体的优化目标，然后两两之间比较当前推荐的差异性，然后试图最大化这个总的平衡了之后的优化目标，用穷举的方法。 我们在现有的启发式的搜索基础上，采用了一些不同的代数模型，就是把两两之间比较不相似性改变成为用一个多边形的体积来量化我们给出的不同物品之间的差异性。把每个物品看作一个多维空间里的向量，然后用这些向量总体张成的一个多边形的体积来度量这个集合的差异性。 推荐的应用场景生成推荐的理由比如说，当我们推荐《终结者2》，我们说是由于你历史上看过《终结者1》，这时候就比完全没有任何原因的推荐显得更加顺理成章。如何构建一个推荐的理由？我们可以用刚才很简单的模板，就是因为你历史上看过和它相关的一个剧。 但是如果我们想做得更加人性化、更加自然，我们要用一种知识图谱的方法。在知识图谱里面构建内容，用户的群组，相关性的信息，以及一些统计信息，包括这个剧的流行程度，它在外面的排名。我们用一种N元组的方法来记录这个知识图谱。 推理规则基于这个知识图谱，我们可以设定一些推理规则，手工建立规则。 每一条规则其实对应某一种经典算法，比如第一条规则，就是如果用户喜欢电视剧，一是由于他曾经看过电视剧，二是电视剧2和1非常相近，这就是item-based CF逻辑的一种表达方式。类似的话，我们还可以把user-based CF也用一条规则来表达。比如说这个地方列出的第二条规则，就是如果用户属于某一个群组，而这个群组里面60%的人都看过剧1，那就说明当前这个用户也可能会喜欢看剧1。 语音对话推荐然后当用户对当前的推荐不满的时候，他也可以用自然语言来告诉我们，为什么他不喜欢这个剧，以及他想要换另外一个什么样的剧。大家知道PC时代，鼠标和键盘是最流行的交互方式，到了移动时代，触屏变成了手机上的最流行的交互方式。随着物联网的发展，我们认为语音会成为下一代的交互方式。 怎么识别用户的兴趣是否改变呢？ 其实对于多臂老虎机的问题模型来讲，认为用户的兴趣是一个可以实时更新的参数。 知识图谱是怎么建立和生成的？ 一方面是从第三方采买的，有专门的构建知识图谱的厂商，他们会做数据清洗爬取。另外一部分是我们从内容提供商那里获得的一些元数据信息。 延伸：bandit问题]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MCMC]]></title>
    <url>%2F2018%2F02%2F11%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FEM%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>MCMC</tag>
        <tag>随机游走</tag>
        <tag>PyMC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MCMC]]></title>
    <url>%2F2018%2F02%2F11%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FMCMC%2F</url>
    <content type="text"><![CDATA[MCMC马尔可夫链蒙特卡洛(MCMC)采样 Metropolis-Hasting 算法 &amp; 图上的Metropolis-Hasting Random Walk (MHRW) Markov Chain Monte Carlo，用于模拟采样的算法。 像Word2Vec中的负采样算法（Negative Sampling），适合离散的概率分布，知道每个值的概率。 而连续分布（概率密度函数表示的）就不行了。 知乎问题：为什么要使用MCMC方法？ 为何不用等距采样替换MCMC MCMC的应用是和”维数灾难”有关的。MCMC应用的概率模型，其参数维数往往巨大，但每个参数的支撑集非常小。比如一些NLP问题的参数只取{0,1}，但维数往往达到几千甚至上万左右 参考 马尔可夫链蒙特卡洛(MCMC)采样 延伸：word2vec里面也涉及到采样：。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>MCMC</tag>
        <tag>随机游走</tag>
        <tag>PyMC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Anaconda操作]]></title>
    <url>%2F2018%2F02%2F11%2F%E5%B7%A5%E5%85%B7%E5%92%8C%E7%8E%AF%E5%A2%83%2FAnaconda%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[安装环境使用清华大学的镜像 https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/ 切换版本目前我装的是anaconda2的版本，若要使用py3，先安装python3 1conda create -n py3 python=3 在py3下安装spyder 1conda install -n py3 spyder 要启动py3的spyder，在切换到py3环境后，直接执行spyder即可 再安装jupyter 1conda install -n py3 jupyter 当切换至py3的环境 1source activate py3]]></content>
      <categories>
        <category>工具和环境</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RNN]]></title>
    <url>%2F2018%2F02%2F07%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2FRNN%2F</url>
    <content type="text"><![CDATA[RNN教程 The Unreasonable Effectiveness of Recurrent Neural Networks RNN Sequences Vanilla Neural Networks和CNN网络的限制是，输入和输出都是fixed-sized vector。 char-rnn尝试 char-rnn torch-rnnIt uses Adam for optimization and hard-codes the RNN/LSTM forward/backward passes for space/time efficiency. This also avoids headaches with cloning models in this repo. In other words, torch-rnn should be the default char-rnn implemention to use now instead of the one in this code base. 安装环境安装lua char-rnn-tensorflow代码：https://github.com/sherjilozair/char-rnn-tensorflow.git 代码解析：https://www.cnblogs.com/edwardbi/p/5573951.html 1、读取数据源 除了txt格式的数据源，还需要 12vocab_file = os.path.join(data_dir, &quot;vocab.pkl&quot;)tensor_file = os.path.join(data_dir, &quot;data.npy&quot;) 这两个可以通过输入文本生成，生成的方法是 12345678910111213141516171819def preprocess(self, input_file, vocab_file, tensor_file): with codecs.open(input_file, "r", encoding=self.encoding) as f: data = f.read() # 统计每个字符的出现次数，返回一个dict counter = collections.Counter(data) # 对dict的items，根据value排序，降序排列 count_pairs = sorted(counter.items(), key=lambda x: -x[1]) # 对列表解压，key和value分别归为一个元组，chars就是所有按降序排列的字符 self.chars, _ = zip(*count_pairs) # 词典的大小 self.vocab_size = len(self.chars) # 用zip转为多个二元组的列表，然后再转为dict self.vocab = dict(zip(self.chars, range(len(self.chars)))) with open(vocab_file, 'wb') as f: cPickle.dump(self.chars, f) # 对data中的每个字符，执行vocab.get，得到词典中的序号，然后转成list（python2中map直接返回list，python3中map返回generator；最后再转为array self.tensor = np.array(list(map(self.vocab.get, data))) # 保存，二进制格式 np.save(tensor_file, self.tensor) 接下来是create_batches batch_size和seq_length默认是50 1234567891011121314151617181920def create_batches(self): self.num_batches = int(self.tensor.size / (self.batch_size * self.seq_length)) # When the data (tensor) is too small, # let's give them a better error message if self.num_batches == 0: assert False, "Not enough data. Make seq_length and batch_size small." # 截取tensor到合适的长度 self.tensor = self.tensor[:self.num_batches * self.batch_size * self.seq_length] xdata = self.tensor ydata = np.copy(self.tensor) ydata[:-1] = xdata[1:] ydata[-1] = xdata[0] self.x_batches = np.split(xdata.reshape(self.batch_size, -1), self.num_batches, 1) self.y_batches = np.split(ydata.reshape(self.batch_size, -1), self.num_batches, 1)]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习随笔]]></title>
    <url>%2F2018%2F01%2F30%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9A%8F%E7%AC%94%2F</url>
    <content type="text"><![CDATA[根据数据采样来估计概率分布，往往可以用极大似然估计法。这种做法需要假定参数符合一个先验分布。贝叶斯分类用的就是这个思路。 机器学习实践中学到的最重要的内容]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[符号约定]]></title>
    <url>%2F2018%2F01%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E7%AC%A6%E5%8F%B7%E7%BA%A6%E5%AE%9A%2F</url>
    <content type="text"><![CDATA[大写字母表示随机变量，小写字母表示某个随机变量具体的取值，如$X=x$ 用$P(X)$表示随机变量X的概率分布，用$P(X,Y)$表示X和Y的联合概率分布，用$P(Y|X)$表示已知X时Y的条件概率分布。 用$p(X=x)$表示X某个取值的概率，在不引起混淆的情况下，$p(X=x)=p(x)$。 用$p(x,y)$表示联合概率，用$p(y|x)$表示条件概率。 m组训练样本的表示 \{(x^{(1)},y^{(1)}) ,(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)}) \}其中，$x^{(i)} \in \mathbb{R}^{n} $，n表示特征向量x的维度，$y^{(i)} \in {0,1}$。 令$X={ x^{(1)},x^{(2)},…,x^{(m)} }$，$Y={ y^{(1)},y^{(2)},…,y^{(m)} }$]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[xgboost]]></title>
    <url>%2F2018%2F01%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2Fxgboost%2F</url>
    <content type="text"><![CDATA[xgboost入门与实战（实战调参篇） 标签： xgboostpythonkaggle机器学习 Python XGBoost算法代码实现和筛选特征应用 xgboost+LR做ctr预测]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>xgboost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据预处理]]></title>
    <url>%2F2018%2F01%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[MNIST使用和处理1234567891011121314151617181920import numpy as npimport pandas as pdimport xgboost as xgbfrom sklearn.cross_validation import train_test_split#记录程序运行时间import time start_time = time.time()#读入数据train = pd.read_csv("Digit_Recognizer/train.csv")tests = pd.read_csv("Digit_Recognizer/test.csv") #用sklearn.cross_validation进行训练数据集划分，这里训练集和交叉验证集比例为7：3，可以自己根据需要设置train_xy,val = train_test_split(train, test_size = 0.3,random_state=1)y = train_xy.labelX = train_xy.drop(['label'],axis=1)val_y = val.labelval_X = val.drop(['label'],axis=1)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>预处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kaggle经典问题]]></title>
    <url>%2F2018%2F01%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FKaggle%E7%BB%8F%E5%85%B8%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[Digit Recognizerhttps://www.kaggle.com/c/digit-recognizer]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Kaggle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python容器]]></title>
    <url>%2F2018%2F01%2F12%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FPYTHON%2FPython%E5%AE%B9%E5%99%A8%2F</url>
    <content type="text"><![CDATA[列表[]列表是可变的，这是它区别于字符串和元组的最重要的特点，一句话概括即：列表可以修改，而字符串和元组不能。 创建直接创建 12list1 = [&apos;a&apos;,&apos;b&apos;]list2 = [1,2] list函数创建 12list3 = list(&quot;hello&quot;)print list3 输出 [‘h’, ‘e’, ‘l’, ‘l’, ‘o’] 1lista = [0] * 6 过滤1[elem for elem in li if len(elem) &gt; 1] 划分1234567a=[1,2,3,4]#不包括1a[:1]#输出[1]#包括2a[2:]#输出[3、4] 把列表中某个值划分出去123if featureVec[axis] == value: reducedFeatVec = featureVec[:axis] reducedFeatVec.extend(featureVec[axis+1:]) 二维列表12345dataSet = [[1, 1, &apos;yes&apos;], [1, 1, &apos;yes&apos;], [1, 0, &apos;no&apos;], [0, 1, &apos;no&apos;], [0, 1, &apos;no&apos;]] 定义一个5×4的都是0的二维数组 1c=[[0 for i in range(4)] for j in range(5)] 合并1234circle_file = glob.glob(os.path.join(self.resource_dir, &apos;circle/*.png&apos;))table_file = glob.glob(os.path.join(self.resource_dir, &apos;table/*.png&apos;)) # 直接相加 self.jump_file = [cv2.imread(name, 0) for name in circle_file + table_file] generator转list1234import jieba# jieba的cut返回的是一个generatora = jieba.cut(&apos;我喜欢吃土豆&apos;)b = list(a) 列表扩展的两种方式123456789a=[1,2,3]b=[4,5,6]a.append(b)[1,2,3,[4,5,6]]a.extend(b)[1,2,3,4,5,6] 元组()元组与列表一样，也是一种序列，唯一不同的是元组不能被修改（字符串其实也有这种特点）。 创建123456t1=1,2,3t2="jeffreyzhao","cnblogs"t3=(1,2,3,4)t4=()t5=(1,)print t1,t2,t3,t4,t5 输出： (1, 2, 3) (‘jeffreyzhao’, ‘cnblogs’) (1, 2, 3, 4) () (1,) 从上面我们可以分析得出： a、用逗号分隔一些值，元组自动创建完成； b、元组大部分时候是通过圆括号括起来的； c、空元组可以用没有包含内容的圆括号来表示； d、只含一个值的元组，必须加个逗号（,）； list转元组tuple函数和序列的list函数几乎一样：以一个序列作为参数并把它转换为元组。如果参数就是元组，那么该参数就会原样返回 12345678t1=tuple([1,2,3])t2=tuple(&quot;jeff&quot;)t3=tuple((1,2,3))print t1print t2print t3t4=tuple(123)print t4 输出： (1, 2, 3)(‘j’, ‘e’, ‘f’, ‘f’)(1, 2, 3) t4=tuple(123)TypeError: ‘int’ object is not iterable 词典{}12345678prices = &#123; &apos;A&apos;:123, &apos;B&apos;:450.1, &apos;C&apos;:12, &apos;E&apos;:444,&#125;prices[&apos;A&apos;] dict创建词典123456789&gt;&gt;&gt;dict() # 创建空字典&#123;&#125;&gt;&gt;&gt; dict(a='a', b='b', t='t') # 传入关键字&#123;'a': 'a', 'b': 'b', 't': 't'&#125;&gt;&gt;&gt; dict(zip(['one', 'two', 'three'], [1, 2, 3])) # 映射函数方式来构造字典&#123;'three': 3, 'two': 2, 'one': 1&#125; &gt;&gt;&gt; dict([('one', 1), ('two', 2), ('three', 3)]) # 可迭代对象方式来构造字典&#123;'three': 3, 'two': 2, 'one': 1&#125;&gt;&gt;&gt; 转list1li = dict.items() 结果类似于 1[(u&apos;11&apos;, 50808340), (u&apos;1101&apos;, 9842378)] 排序转为list后再排序 12 判断词典是否包含某个元素1234labelCount=&#123;&#125;for feature in dataSet: label = feature[-1] if label not in labelCount[label]: labelCount[label] = 0 词典的遍历iteritems 12345678910sentences = [&quot;我喜欢吃土豆&quot;,&quot;土豆是个百搭的东西&quot;,&quot;我不喜欢今天雾霾的北京&quot;]words = []for doc in sentences: words.append(list(jieba.cut(doc)))dic = corpora.Dictionary(words)for word,index in dic.token2id.iteritems(): print word + &apos;, index: &apos; + str(index) 在3.x 里 用 items()替换iteritems() 增加元素1234567891011121314#比如有个词典action = &#123; &quot;_index&quot;: elastic_urls_index, &quot;_type&quot;: doc_type_name, &quot;_id&quot;: data[0], &quot;_source&quot;: &#123; &quot;iclick_id&quot;: data[0], &quot;onsite_id&quot;: data[1], &quot;create_time&quot;: self.today_2 &#125;&#125;#要增加元素data[&apos;_soupyrce&apos;][&apos;age&apos;] = &apos;aa&apos; 提取文本的高频词1234567891011121314documents = ["Human machine interface for lab abc computer applications", "A survey of user opinion of computer system response time"]stoplist = set('for in and'.split())texts = [ [word for word in document.lower().split() if word not in stoplist ] for document in documents]from collections import defaultdictfrequency = defaultdict(int)for text in texts: for word in text: frequency[word]+=1texts = [ [word for word in text if frequency[word]&gt;1] for text in texts ] 映射mapping集合set定义1aaa = set() 增加1aaa.add(1) 判断是否在集合1if 1 in aaa: 数组转集合12a = [11,22,33,44,11,22] b = set(a) 通过set去除停用词123456documents = [&quot;Human machine interface for lab abc computer applications&quot;, &quot;A survey of user opinion of computer system response time&quot;]stoplist = set(&apos;for in and&apos;.split())texts = [ [word for word in document.lower().split() if word not in stoplist ] for document in documents] set增加数据123vocabSet = set([])for document in dataSet: vocabSet = vocabSet | set(document)]]></content>
      <categories>
        <category>PYTHON</category>
      </categories>
      <tags>
        <tag>容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[吴恩达深度学习笔记]]></title>
    <url>%2F2018%2F01%2F12%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Numpy用法sigmoid12def sigmoid(x): s = 1/(1+ np.exp(-x)) img2Vec-reshape123def image2vector(image): v = image.reshape((image.shape[0]*image.shape[1]*image.shape[2]),1) return v normalize行归一化：一行的每个列的值，除以该列的L2范数。 1234def normalizeRows(x): x_norm = np.linalg.norm(x,axis=1,keepdims=True) x = x/x_norm return x softmax normalize 1234def softmax(x): x_exp = np.exp(x) x_sum = np.sum(x_exp, axis=1, keepdims=True) s = x_exp/x_sum 实现损失函数L1损失函数 L_1(\hat y,y)=\sum_{i=0}^m |\hat y^{(i)}-y^{(i)}|123def L1(yhat, y): loss = np.sum(abs(yhat-y)) return loss L2损失函数 L_2(\hat y, y)=\sum_{i=0}^m (\hat y^{(i)}-y^{(i)})^2123def L2(yhat, y): loss = np.dot(y-yhat,y-yhat) return loss LR图像分类代码见 blogcodes/deeplearning_ai/lr_image.py 吴恩达课程的数据集在blogcodes/deeplearning_ai/datasets和blogcodes/deeplearning_ai/lr_utils.py 1234567import numpy as npimport matplotlib.pyplot as pltimport h5pyimport scipyfrom PIL import Imagefrom scipy import ndimagefrom lr_utils import load_dataset 数据预处理包括加载数据、flatten到二维数组（原来是四维数组），归一化 加载数据 1train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset() train_set_x_orig是shape为(m_train, num_px, num_px, 3)的numpy-array，m_train是训练样本数。 ​ 为了便于训练，将其reshapre为(num_px*num_px*3, 1)，这样每列就代表一个图像，一共有m_train列。 123# 写成train_set_x_orig.reshape(-1,train_set_x_orig.shape[0])就可以了，不知为何要反过来再加上Ttrain_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0],-1).Ttest_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0],-1).T 再将矩阵的元素归一化到[0-1]的范围，每个元素/255即可 12train_set_x = train_set_x_flatten/255test_set_x = test_set_x_flatten/255 LR的主要公式为 Building the Neural Network构建一个神经网络的主要步骤是： 1、Define the model structure ( usually such as number of input features) 2、Initialize the model’s parameters 3、Loop: - Calculate current loss ( forward propagation) Calculate current gradient ( backward porpagation) Update parameters ( gradient descent) Initializing Parameterswill creates a vector of zeros of shape (dim, 1) for w and initializes b to 0. 1234567def initialize_with_zeros(dim): w = np.zeros((dim, 1)) b = 0 assert(w.shape == (dim ,1)) assert(isinstance(b, float) or isinstance(b, int)) return w,b the shape of w should be (num_px*num_px*3, 1) Forward and backward propagation 之前的LR没有考虑b，只有对w的梯度下降优化 123456789101112131415161718192021222324# 3个样本，每个样本2个特征，X的每行是一个特征，每列是一个样本# w=(2,1), X=(2,3), Y=(1,3)w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]),np.array([[1,0,1]])#my codedef forward_propagation(w, b, X, Y): A = sigmoid(np.dot(w.T, X) + b) # 错误的写法 J = -1/m * np.sum( np.dot(Y, np.log(A)) + np.dot((1-Y),np.log(1-A)) ) dw = 1/m * np.dot(X, np.dot(A-Y).T) db = 1/m * np.sum(A-Y) # correct codedef forward_propagation(w, b, X, Y): # m是样本个数，3 m = X.shape[1] A = sigmoid(np.dot(w.T, X) + b) cost = - ( np.dot(Y, np.log(A.T)) + np.dot(np.log(1-A),(1-Y).T) )/m dw = np.dot(X, (A-Y).T) / m db = np.sum(A-Y) / m cost = np.squeeze(cost) grads = &#123;"dw": dw, "db", db&#125; return grads, cost 1、损失函数写成矩阵运算的形式，否则dot会报错。 2、漏定义了m，m为样本个数 3、将cost变为数字而不是矩阵 4、dw和db放到dict中，方便读取 Optimizationupdate parameters using gradient descent. The goal of optimization function is to learn w and b by minimizing J. For a parameter $\theta$ , the update rule is \theta = \theta - \alpha \ d\theta1234567891011121314151617181920def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False): costs = [] for i in range(num_iterations): grads, cost = forward_propagation(w, b, X, Y) dw = grads[&quot;dw&quot;] db = grads[&quot;db&quot;] w = w - learning_rate * dw b = b - learning_rate * db if i % 100 == 0: costs.append(cost) print(cost) params = &#123; &quot;w&quot;:w, &quot;b&quot;:b &#125; return params 例子中给的方法，是批量梯度下降，每次循环都要计算所有样本 PredictWe will use w and b to predict the labels for a dataset X. 1234567891011121314151617181920# my codedef predict(w, b, X): Y = sigmoid(np.dot(w.T, X) + b) return Y # correct codedef predict(w, b, X): m = X.shape[1] Y_prediction = np.zeros((1,m)) A = sigmoid(np.dot(w.T, X) + b) for i in range(A.shape[1]): if A[0][i] &lt;= 0.5: A[0][i] = 0 else: A[0][i] = 1 Y_prediction = A return Y_prediction 1、我求出来的只是sigmoid的输出，还要转化成分类。 2、先定义一个空白的Y，将sigmoid的输出根据值转换为0和1，然后赋值给Y。 Merge All Functions into a Model]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python编码问题]]></title>
    <url>%2F2017%2F11%2F09%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FPYTHON%2FPython%E7%BC%96%E7%A0%81%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[困扰了很长时间的问题，找到一篇解释不错的文章，转载并整理之。 ‘ascii’ codec can’t decode byte 0xe4 in position 0: ordinal not in range(128) python的编码是unicode -&gt; str，解码是str -&gt; unicode 关于文件开头的”编码指示”，也就是-*- coding: -*-这个语句。Python 默认脚本文件都是 UTF-8 编码的，当文件中有非 UTF-8 编码范围内的字符的时候就要使用”编码指示”来修正. 关于 sys.defaultencoding，这个在解码没有明确指明解码方式的时候使用。比如我有如下代码： 1234#! /usr/bin/env python # -*- coding: utf-8 -*- s = &apos;中文&apos; # 注意这里的 str 是 str 类型的，而不是 unicode s.encode(&apos;gb18030&apos;) 这句代码将 s 重新编码为gb18030的格式，即进行unicode -&gt; str的转换。因为 s 本身就是 str类型的，因此 Python 会自动的先将 s 解码为unicode，然后再编码成 gb18030。因为解码是python自动进行的，我们没有指明解码方式，python 就会使用sys.defaultencoding指明的方式来解码。很多情况下 sys.defaultencoding 是ANSCII，如果 s 不是这个类型就会出错。 拿上面的情况来说，我的 sys.defaultencoding是anscii，而 s 的编码方式和文件的编码方式一致，是 utf8 的，所以出错了:UnicodeDecodeError: &#39;ascii&#39; codec can&#39;t decode byte 0xe4 in position 0: ordinal not in range(128) Unicode和UTF8的区别 unicode指的是万国码，是一种“字码表”。而utf-8是这种字码表储存的编码方法。unicode不一定要由utf-8这种方式编成bytecode储存，也可以使用utf-16,utf-7等其他方式。目前大多都以utf-8的方式来变成bytecode。 python中字符串类型分为byte string 和unicode string两种。 如果在python文件中指定编码方式为utf-8(#coding=utf-8)，那么所有带中文的字符串都会被认为是utf-8编码的byte string（例如：mystr=”你好”），但是在函数中所产生的字符串则被认为是unicode string问题就出在这边，unicode string 和byte string是不可以混合使用的，一旦混合使用了，就会产生这样的错误。例如： 1self.response.out.write(&quot;你好&quot;+self.request.get(&quot;argu&quot;)) ​ 以下有两个解决方法： 第一种,是明确的指示出 s 的编码方式 1234#! /usr/bin/env python # -*- coding: utf-8 -*- s = &apos;中文&apos; s.decode(&apos;utf-8&apos;).encode(&apos;gb18030&apos;) 第二种,更改sys.defaultencoding为文件的编码方式 12345#! /usr/bin/env python # -*- coding: utf-8 -*- import sys #要重新载入sys。因为 Python 初始化后会删除 sys.setdefaultencoding 这个方 法reload(sys) sys.setdefaultencoding(&apos;utf-8&apos;)]]></content>
      <categories>
        <category>PYTHON</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>编码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树]]></title>
    <url>%2F2017%2F09%2F11%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[1 决策树公式符号 \begin{align} &Ent(X)\ \ \ 熵 \\ &Gain(X)\ \ \ 信息增益\\ &Gini(X)\ \ \ 基尼指数\\ &D\ \ \ 训练集\\ &A\ \ \ 训练集的某个特征\\ &N\ \ \ 特征A的类别总数\\ &K\ \ \ 标签分类的数量\\ \end{align}1.1 关键步骤-python实现创建决策树分支的createBranch()伪代码函数 123456789检查数据集中每个子项是否属于同一个分类： IF YES return 类标签； ELSE 寻找划分数据集的最好特征； 划分数据集； 创建分支节点； for 每个划分的子集 递归调用createBranch()并增加返回结果到分支节点中 return 分支节点 对label的分类计算熵 12345678910111213141516def calcEnt(dataSet): labelNum = len(dataSet) ent = 0.0 #定义字典存放每个类别的count统计 labelCounts = &#123;&#125; #统计每个label的个数 for featureVec in dataSet: #最后一列是label label = featureVec[-1] if label not in labelCounts.keys(): labelCounts[label] = 0 labelCounts[label] += 1 #计算概率以及熵 for key in labelCounts: prob = float(labelCounts[key]) / labelNum ent -= prob * log(2, prob) return ent 对数据集进行划分 12345678def splitDataSet(dataSet, axis, value): subDataSet = [] for featureVec in dataSet: if featureVec[axis] == value: reducedFeatVec = featureVec[:axis] reducedFeatVec.extend(featureVec[axis+1:]) resDataSet.append(reducedFeatVec) return subDataSet 选出最好的数据集划分方式 信息增益 熵的定义是 Ent(X) = -\sum_{i=1}^{n}p(x_i)log_2p(x_i)n是类别总数。 条件熵$Ent(Y|X)$表示在已知X的条件下Y的不确定性，定义为给定X时Y的条件概率分布的熵对X的期望 Ent(Y|X)=\sum_{i=1}^np_iEnt(Y|X=x_i)对于训练集D以及其中的特征A，熵就是 Ent(D) = -\sum_{k=1}^K \frac {|C_k|}{|D|} log_2\frac{|C_k|}{|D|}其中，K是标签分类的数量，$C_k$是每个分类的样本数 条件熵就是 \begin{aligned} Ent(D|A) &=\sum_{i=1}^N\frac{|D_i|}{|D|}Ent(D_i) \\ &=\sum_{i=1}^N\frac{|D_i|}{|D|}(-\sum_{k=1}^K \frac {|D_{ik}|}{|D_i|} log_2\frac{|D_{ik}|}{|D_i|}) \end{aligned}其中，N是特征A的类别总数，$D_i$是特征A的每种类别的数量。 信息增益就是两者之差 Gain(D,A)=Ent(D)-Ent(D|A)信息增益也称为互信息。 找出信息增益最大的来划分数据集 12345678910111213141516171819202122def chooseBestFeature(dataSet): #feature数量，最后一列是label numFeature = len(dataSet[0]-1) bestInfoGain = 0.0 bestFeature = -1 #先计算熵 baseEntropy = calcEnt(dataSet) for i in range(numFeature): #首先需要知道该特征有几个值 uniqueValue = set([sample[i] for sample in dataSet]) #用set去重是最快方法 newEntropy = 0.0 #对于每个特征，计算条件熵 for value in uniqueValue: #用这个特征划分数据集 subDataSet = splitDataSet(dataSet, i, value) newEntropy += calcEnt(subDataSet) #计算信息增益 infoGain = baseEntropy-newEntropy if infoGain &gt; bestInfoGain: bestInfoGain = infoGain bestFeature = ireturn bestFeature 如果所有特征都处理过了，但是类标签依然不是唯一的，用投票决定 1234567def majorityCnt(classList): classCount=&#123;&#125; for vote in classList: if vote not in classCount.keys() classCount[vote] = 0 classCount[vote] += 1 sortedClassCount = sorted(classCount, key = operator.itemgetter(1), reverse = True) return sortedClassCount[0][0] 1.2 ID3算法与上面的步骤类似。但是ID3只有树的生成，容易过拟合。 1.3 C4.5算法与ID3相比，C4.5用信息增益比来选择特征。 信息增益比 在面对类别比较少的离散数据时，两者差不多。但如果面对连续的数据（如体重、身高、年龄、距离等），或者每列数据没有明显的类别之分（最极端的例子的该列所有数据都独一无二）。 那么根据信息增益公式，$Ent(D)$不变，当数据独一无二时， Ent(D|A)=\sum_{i=1}^n \frac {1}{n}Ent(D_i)这样$Ent(D|A)$最小，程序会倾向于这种划分，导致划分效果差。 信息增益比的公式为 Gain_R(D,A)=\frac {Gain(D,A)}{Ent(D)}可以理解成对分支数目的惩罚项。 1.5 CART算法CART是分类与回归树，由特征选择、树的生成和剪枝组成。 CART是在给定输入变量X条件下输出随机变量Y的条件概率分布的方法。CART假设决策树是二叉树，内部结点特征的取值为是和否，约定左是右否。 决策树等价于递归的二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定的条件下输出的条件概率分布。 1.5.1 CART的生成 递归构建二叉树的过程。回归树用最小二乘，分类树用基尼指数。 1）回归树 2）分类树 假设有K个类，样本点属于第k类的概率是$p_k$，则基尼指数定义为 Gini(p) = \sum_{k=1}^K p_k(1-p_k) = 1-\sum_{k=1}^K p_k^2=1-\sum_{k=1}^K(\frac{|C_k|}{|D|})^2如果是两分类问题，则概率分布的基尼指数为 Gini(P)=2p(1-p)若样本集合D根据特征A是否取某一值a被划分为$D_1$和$D_2$两部分，即 D_1=\{(x,y)\in D | A(x)=a\}, D_2=D-D_1则在特征A的条件下，集合D的基尼指数为 Gini(D,A)=\frac {|D_1|}{|D|}Gini(D_1)+\frac {|D_2|}{|D|}Gini(D_2)Gini越大，样本集合的不确定性越大，与熵相似。 算法过程 12345678输入：训练集D，停止条件输出：CART决策树从根结点递归对每个结点进行以下操作，构建二叉树：1）对每个特征和可能的取值a，根据A=a的为是或否，将D分割成D1和D2，计算基尼指数2）选出基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。3）对两个子结点递归调用#1、#2，直至满足停止条件4）生成CART决策树 3）CART剪枝 1.6 决策树的剪枝 如何判断剪枝后泛华性能提升？ 用留出法。将一部分训练集作为验证集。 剪枝是为了解决过拟合。通过极小化决策树整体的损失函数来实现。设树T的叶结点个数为$|T|$，t是T的叶结点，该叶结点有$Nt$个样本点，其中k类的样本点有$N{tk}$个，则损失函数定义为 C_{\alpha}(T) = \sum_{t=1}^T N_tEnt_t(T) + \alpha|T|由于 Ent_t(T) = - \sum_{k=1}^K \frac {N_{tk}}{N_t} log_2\frac {N_{tk}}{N_t}则令 C(T) = - \sum_{t=1}^T\sum_{k=1}^KN_{tk}log_2\frac {N_{tk}}{N_t}于是 C_\alpha(T) = C(T) +\alpha|T|这里，$C(T)$表示训练数据的预测误差，$|T|$表示模型复杂度，$\alpha$控制两者影响，较大时选择较简单的树，反之亦然，等于0时就不考虑模型复杂度。 两种剪枝思路 预剪枝（Pre-Pruning） 构造的同时剪枝。比如设一个阈值，熵减小的数量小于这个阈值，即使还可以继续降低熵，也停止继续创建分支。 有些分支虽然当前划分时性能下降，但后续划分有可能又会提高，仅根据当前验证集来判断是否要继续划分，往往会导致欠拟合。 后剪枝（Post-Pruning） 三种主要方法 1）REP错误率降低剪枝 简单粗暴，对每个非叶结点的子树，用其替换一个叶结点，类别用子树覆盖训练样本中类最多的代替。这样产生的简化树再跟原树比较在测试数据集中的效果。若错误更少就替换。算法以Bottom-up的方式遍历所有的子树，直到没有任何改进时，终止。 2）PEP悲观剪枝 1.7 连续和缺失值处理1）连续值离散化 jueceshu最简单的策略是二分法，也是C4.5采用的机制。 对于连续属性a，可以考察包含n-1个元素的候选划分点集合 T_a=\left \{ \frac {a^i+a^{i+1}} {2} | 1 \leqslant i \leqslant n-1 \right \}即把区间的中位点作为候选划分点，然后像离散值那样考察划分点，再选出最优的划分点。 2）缺失值 考虑：①如何在属性值缺失的情况下进行划分属性选择？②给定划分属性，若样本在该属性的值缺失，如何划分？ 靠权重。在判定划分时，权重相等，用已知的样本来划分属性。对于每个划分属性，若属性缺失，将缺失的记录根据属性的每个划分所占比例作为权重，分到属性的每个子结点中。 1.8 多变量决策树非子结点不再针对某个属性，而是多个属性的线性组合。即，每个非子结点都是一个线性分类器。 2、随机森林随机森林如何随机的，特征也随机，样本也随机 3、GBDT决策树是否应该用one-hot编码 参考 统计学习方法 决策树的剪枝问题]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>决策树</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[贝叶斯分类器]]></title>
    <url>%2F2017%2F09%2F11%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8%2F</url>
    <content type="text"><![CDATA[半朴素贝叶斯分类器AODE 比如西瓜书提到的TAN。每个属性依赖的另外的属性由最大带权生成树来确定。 （1）先求每个属性之间的互信息来作为他们之间的权值。 （2）构件完全图。权重是刚才求得的互信息。然后用最大带权生成树算法求得此图的最大带权的生成树。 （3）找一个根变量，然后依次将图变为有向图。 （4）添加类别y到每个属性的的有向边。 贝叶斯判定准则（Bayes decision rule）：为最小化总体风险，只需在每个样本上选择那个能使条件风险$R(c|x)$最小的类别标记。 我定义了一个条件风险和一个判断分类的准则，如果这个准则能使条件风险最小，也就能使总体风险最小。 朴素贝叶斯假设数据是独立分布的，即属性条件独立性假设 （attribute conditional independence assumption）。对已知类别，假定所有属性相互独立。这一假设使朴素贝叶斯变得简单，但会牺牲一定的分类准确度。 然后，如果属性中有连续值的属性，在计算概率时又假定数据符合正态分布。 贝叶斯分类器有两种实现方式， 基于伯努利模型，不考虑样本中特征出现的次数，只考虑出不出现，相当于假设每个特征是同等权重的。 基于多项式模型，也考虑出现次数 机器学习实战的代码 从文档中创建词典 12345def createVocabList(dataSet): vocabSet = set([]) #create empty set for document in dataSet: vocabSet = vocabSet | set(document) #union of the two sets return list(vocabSet) 给定一个词典和输入文档，如果某个词出现，就给词典的下标置1，就是创建词袋 1234567def setOfWords2Vec(vocabList, inputSet): returnVec = [0]*len(vocabList) for word in inputSet: if word in vocabList: returnVec[vocabList.index(word)] = 1 else: print "the word: %s is not in my Vocabulary!" % word return returnVec 训练朴素贝叶斯分类器 书中这里出现了错误！ http://blog.csdn.net/lming_08/article/details/37542331 1234567891011121314151617181920def trainNB0(trainMatrix,trainCategory): numTrainDocs = len(trainMatrix) numWords = len(trainMatrix[0]) # 计算所有样本为第一类的概率p(c)，分类是0和1所以就直接相加 pAbusive = sum(trainCategory)/float(numTrainDocs) # 防止最后某一个的概率是0 p0Num = ones(numWords); p1Num = ones(numWords) #change to ones() p0Denom = 2.0; p1Denom = 2.0 #change to 2.0，这里代表类别数 for i in range(numTrainDocs): if trainCategory[i] == 1: # 矩阵粒度的加法，对应位置相加 p1Num += trainMatrix[i] # 这里为何是所有特征相加，回去再看看 p1Denom += sum(trainMatrix[i]) else: p0Num += trainMatrix[i] p0Denom += sum(trainMatrix[i]) p1Vect = log(p1Num/p1Denom) #change to log() p0Vect = log(p0Num/p0Denom) #change to log() return p0Vect,p1Vect,pAbusive 预测 12345678def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1): # 之前用了log，于是概率相乘就转化为相加，最后再加上p(c) p1 = sum(vec2Classify * p1Vec) + log(pClass1) #element-wise mult p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1) if p1 &gt; p0: return 1 else: return 0 高斯、伯努利、多项式高斯模型假设连续变量符合高斯分布， 多项式模型常用于文本分类，特征是单词，值是单词的出现次数。 多项式模型在训练一个数据集结束后可以继续训练其他数据集而无需将两个数据集放在一起进行训练。在sklearn中，MultinomialNB()类的partial_fit()方法可以进行这种训练。这种方式特别适合于训练集大到内存无法一次性放入的情况。 伯努利模型每个特征的取值是bool型，即true或false]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>贝叶斯</tag>
        <tag>西瓜书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（转）LDA]]></title>
    <url>%2F2017%2F08%2F13%2FNLP%2FGensim-LDA%2F</url>
    <content type="text"><![CDATA[原理gensim-LDAhttp://blog.csdn.net/whzhcahzxh/article/details/17528261 引用gensim包，gensim包中引用corpora,models, similarities，分别做语料库建立，模型库和相似度比较库 from gensim import corpora, models, similarities import jieba 1、分词123456sentences = [&quot;我喜欢吃土豆&quot;,&quot;土豆是个百搭的东西&quot;,&quot;我不喜欢今天雾霾的北京&quot;]words=[]for doc in sentences:# 结巴分词返回的是一个generator，要用list()转成list words.append(list(jieba.cut(doc)))print words 输出： [[u’\u6211’, u’\u559c\u6b22’, u’\u5403’, u’\u571f\u8c46’], [u’\u571f\u8c46’, u’\u662f’, u’\u4e2a’, u’\u767e’, u’\u642d’, u’\u7684’, u’\u4e1c\u897f’], [u’\u6211’, u’\u4e0d’, u’\u559c\u6b22’, u’\u4eca\u5929’, u’\u96fe’, u’\u973e’, u’\u7684’, u’\u5317\u4eac’]] 此时输出的格式为unicode，不影响后期运算，因此我保留不变，如果想看分词结果可以用循环输出jieba分词结果 2、分词结果构造词典12345dic = corpora.Dictionary(words)# 词袋中的所有词print dic# 每个词和编号print dic.token2id 输出： Dictionary(15 unique tokens: [u’\u973e’, u’\u5403’, u’\u5317\u4eac’, u’\u7684’, u’\u4e1c\u897f’]…) {u’\u973e’: 14, u’\u5403’: 0, u’\u5317\u4eac’: 12, u’\u7684’: 9, u’\u4e1c\u897f’: 4, u’\u4e2a’: 5, u’\u642d’: 6, u’\u662f’: 7, u’\u6211’: 3, u’\u559c\u6b22’: 1, u’\u4eca\u5929’: 11, u’\u571f\u8c46’: 2, u’\u4e0d’: 10, u’\u96fe’: 13, u’\u767e’: 8} 为方便看数据： 12for word,index in dic.token2id.iteritems(): print word +&quot; 编号为:&quot;+ str(index) 输出： 北京 编号为:12搭 编号为:6的 编号为:9喜欢 编号为:1不 编号为:10东西 编号为:4土豆 编号为:2霾 编号为:14是 编号为:7个 编号为:5雾 编号为:13百 编号为:8今天 编号为:11我 编号为:3吃 编号为:0 3、生成语料库词袋模型 12corpus = [dic.doc2bow(text) for text in words]print corpus 输出： [[(0, 1), (1, 1), (2, 1), (3, 1)], [(2, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1)], [(1, 1), (3, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1)]] 4、TFIDF变换通过语料库得到tfidf的值，由tfidf来描述句子 123456789#通过语料库得到tfidf模型tfidf = models.TfidfModel(corpus)vec = [(0, 1), (4, 1)]print tfidf[vec]#对corpus的每个文档中的每个词计算tfidf，得到的结果是，每个文档的每个词都是一个元组，包括id和tfidf值corpus_tfidf = tfidf[corpus]for doc in corpus_tfidf: print doc 输出： [(0, 0.7071067811865475), (4, 0.7071067811865475)][(0, 0.8425587958192721), (1, 0.3109633824035548), (2, 0.3109633824035548), (3, 0.3109633824035548)][(2, 0.16073253746956623), (4, 0.4355066251613605), (5, 0.4355066251613605), (6, 0.4355066251613605), (7, 0.4355066251613605), (8, 0.4355066251613605), (9, 0.16073253746956623)][(1, 0.1586956620869655), (3, 0.1586956620869655), (9, 0.1586956620869655), (10, 0.42998768831312806), (11, 0.42998768831312806), (12, 0.42998768831312806), (13, 0.42998768831312806), (14, 0.42998768831312806)] vec是查询文本向量，比较vec和训练中的三句话相似度 123index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features=14)sims = index[tfidf[vec]]print list(enumerate(sims)) 输出： [(0, 0.59577906), (1, 0.30794966), (2, 0.0)] 表示和第1句话相似度为59.578%，和第二句话的相似度位30.79%，第三句没有相似度， 我们看看vec这句话是什么：0为吃，4为东西，所以vec这句话可以是[“吃东西”]或者[“东西吃”] 而第一句话”我喜欢吃土豆”,”土豆是个百搭的东西”明显有相似度，而第三句话”我不喜欢今天雾霾的北京”，相似度几乎为0，至于为什么第一句比第二句更相似，就需要考虑TfIdf document representation和cosine similarity measure了 回到tfidf转换，接着训练LSI模型，假定三句话属于2个主题， 1234lsi = models.LsiModel(corpus_tfidf, id2word=dic, num_topics=2)lsiout=lsi.print_topics(2)print lsiout[0]print lsiout[1] 输出： 0.532“吃” + 0.290“喜欢” + 0.290“我” + 0.258“土豆” + 0.253“霾” + 0.253“雾” + 0.253“北京” + 0.253“今天” + 0.253“不” + 0.166“东西”0.393“百” + 0.393“搭” + 0.393“东西” + 0.393“是” + 0.393“个” + -0.184“霾” + -0.184“雾” + -0.184“北京” + -0.184“今天” + -0.184“不” 这就是基于SVD建立的两个主题模型内容 将文章投影到主题空间中 123corpus_lsi = lsi[corpus_tfidf]for doc in corpus_lsi: print doc 输出： [(0, -0.70861576320682107), (1, 0.1431958007198823)][(0, -0.42764142348481798), (1, -0.88527674470703799)][(0, -0.66124862582594512), (1, 0.4190711252114323)] 因此第一三两句和主题一相似，第二句和主题二相似 同理做个LDA 1234567lda = models.LdaModel(corpus_tfidf, id2word=dic, num_topics=2)ldaOut=lda.print_topics(2)print ldaOut[0]print ldaOut[1]corpus_lda = lda[corpus_tfidf]for doc in corpus_lda: print doc 得到的结果每次都变，给一次的输出： 0.077吃 + 0.075北京 + 0.075雾 + 0.074今天 + 0.073不 + 0.072霾 + 0.070喜欢 + 0.068我 + 0.062的 + 0.061土豆0.091吃 + 0.073搭 + 0.073土豆 + 0.073个 + 0.073是 + 0.072百 + 0.071东西 + 0.066我 + 0.065喜欢 + 0.059霾[(0, 0.31271095988105352), (1, 0.68728904011894654)][(0, 0.19957991735916861), (1, 0.80042008264083142)][(0, 0.80940337254233863), (1, 0.19059662745766134)] 第一二句和主题二相似，第三句和主题一相似 结论和LSI不一样，我估计这和样本数目太少，区别度不高有关，毕竟让我来区分把第一句和哪一句分在一个主题，我也不确定 输入一句话，查询属于LSI得到的哪个主题类型，先变成词袋模型，然后查询LSI： 12345query = "雾霾"query_bow = dic.doc2bow(list(jieba.cut(query)))print query_bowquery_lsi = lsi[query_bow]print query_lsi 输出: [(13, 1), (14, 1)][(0, 0.50670602027401368), (1, -0.3678056037187441)] 与第一个主题相似 比较和第几句话相似，用LSI得到的索引接着做，并排序输出 12345index = similarities.MatrixSimilarity(lsi[corpus])sims = index[query_lsi]print list(enumerate(sims))sort_sims = sorted(enumerate(sims), key=lambda item: -item[1])print sort_sims 输出： [(0, 0.90161765), (1, -0.10271341), (2, 0.99058259)][(2, 0.99058259), (0, 0.90161765), (1, -0.10271341)] 可见和第二句话相似度很高，因为只有第二句话出现了雾霾两个词，可是惊讶的是和第一句话的相似度也很高，这得益于LSI模型的算法：在A和C共现，B和C共现的同时，可以找到A和B的相似度 代码位于blogcodes/gensim_lda.py]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>gensim</tag>
        <tag>LDA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Gensim-Tutorials]]></title>
    <url>%2F2017%2F08%2F12%2FNLP%2FGensim-Tutorials%2F</url>
    <content type="text"><![CDATA[http://radimrehurek.com/gensim/tutorial.html Gensim 使用Python标准logging模块来记录log，使用方法是 12import logginglogging.basicConfig(format=&apos;%(asctime)s : %(levelname)s : %(message)s&apos;, level=logging.INFO)]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>gensim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLP代码片段]]></title>
    <url>%2F2017%2F08%2F12%2FNLP%2FNLP%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5%2F</url>
    <content type="text"><![CDATA[根据标点拆分句子[AtomSplit.java](../../../../gitlab/user-gene/nlp/src/main/java/com/buzzinate/nlp/segment/AtomSplit.java) 12345678910111213141516171819202122 public static List&lt;String&gt; splitSentences(String text) throws IOException &#123; List&lt;String&gt; result = new ArrayList&lt;String&gt;(); int last = 0; for (Term term: Segment.split(text, ToAnalysis.USE_USER_DEFINE)) &#123; if (sentenceNatures.contains(term.getNatrue().natureStr) &amp;&amp; !isWhiteSpace(term.getName())) &#123; if (term.getOffe() &gt; last) &#123; String snippet = text.substring(last, term.getOffe()).trim(); if (snippet.length() &gt; 0) result.add(snippet); &#125; last = term.getOffe() + term.getName().length(); &#125; &#125; if (text.length() &gt; last) &#123; String snippet = text.substring(last, text.length()).trim(); if (snippet.length() &gt; 0) result.add(snippet); &#125; return result; &#125;private static boolean isWhiteSpace(String term) &#123; return term.length() == 1 &amp;&amp; (Character.isWhitespace(term.charAt(0)) || term.charAt(0) == '-'); &#125; 给每个字符标记类型[AtomSplit.java](../../../../gitlab/user-gene/nlp/src/main/java/com/buzzinate/nlp/segment/AtomSplit.java) 123456789101112131415161718192021222324252627282930/** * 给每个字符标记类型，如 * [2011(AT_NUM), -(AT_PUNC), 34(AT_NUM), -(AT_PUNC), 43(AT_NUM), (AT_PUNC), 为(AT_CHINESE), 中(AT_CHINESE), 国(AT_CHINESE)] * @param text * @return */public static List&lt;Atom&gt; split(String text) &#123; List&lt;Atom&gt; result = new ArrayList&lt;Atom&gt;(); int last = 0; AtomType t = AtomType.AT_LETTER; for (int i = 0; i &lt; text.length(); i++) &#123; char ch = text.charAt(i); if (TextUtil.isAlphaOrDigit(ch) || ch == '\'' || ch == '.') &#123; if (i == last) &#123; t = AtomType.AT_LETTER; if (Character.isDigit(ch)) t = AtomType.AT_NUM; &#125; &#125; else if (Character.isLetter(ch)) &#123; if (i &gt; last) result.add(new Atom(text.substring(last, i), t)); result.add(new Atom(text.substring(i, i+1), AtomType.AT_CHINESE)); last = i + 1; &#125; else &#123; if (i &gt; last) result.add(new Atom(text.substring(last, i), t)); if (t != AtomType.AT_LETTER || !isConnectChar(ch)) result.add(new Atom(text.substring(i, i+1), AtomType.AT_PUNC)); last = i + 1; &#125; &#125; if (text.length() &gt; last) result.add(new Atom(text.substring(last, text.length()), t)); return result;&#125; 判断字符串的语言有两个开源的项目可以使用。一个是Apache Tika，一个是language-detection。language-detection是google Code上开源的一个语言检测软件包，不折不扣的日货，但使用起来非常方便，其project链接如下：http://code.google.com/p/language-detection。基本上，你只需要引用langdetect.jar和其依赖的jsonic-1.3.0.jar（也是日货）即可 /rocket-iaudience-api/src/main/java/com/iclick/rocket/iaudience/api/common/LanguageDetectUtil.java]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[spark笔记]]></title>
    <url>%2F2017%2F07%2F12%2Fhadoop-spark%2Fspark%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[spark框架 spark集群的节点包括 Driver、Master、Worker、Executor 算圆周率pi1234567891011121314151617import org.apache.spark.SparkConf;import org.apache.spark.SparkContext;case class PerTypeson[T,S](var name:T,var age:S) &#123;&#125;object SparkTest&#123; def main(args: Array[String]) &#123; val conf = new SparkConf().setAppName("Spark Pi").setMaster("local") //关键 val sc = new SparkContext(conf) val NUM_SAMPLES=100000 val count=sc.parallelize(1 to NUM_SAMPLES).map&#123;i=&gt; val x=Math.random() val y=Math.random() if(x*x+y*y&lt;1)1 else 0 &#125;.reduce(_+_) println("pi is rougly"+4.0*count/NUM_SAMPLES) &#125;&#125; 读取外部文件和链接数据库（用spark 1.6的版本） 12345678910111213141516171819202122232425262728import org.apache.spark.SparkConf;import org.apache.spark.SparkContextimport org.apache.spark.ml.classification.LogisticRegressionimport org.apache.spark.sql.SQLContextimport scala.collection.mutable.Mapcase class Person(var name:String)object SparkTest&#123; def main(args: Array[String]) &#123; // val conf = new SparkConf().setAppName("Spark Pi").setMaster("spark://hadoop:7070") //关键 val conf = new SparkConf().setAppName("Spark Pi").setMaster("local") //关键 val sc = new SparkContext(conf)// val textfile=sc.textFile("C:\\Users\\Administrator\\Desktop\\分词.txt")// textfile.collect().foreach(println) val sqlContext=new SQLContext(sc) val df=sqlContext.read.json("F:\\people.json") df.cache() println(df.select("age").show()) df.registerTempTable("df1") println(sqlContext.sql("select * from df1 where age=19")) val map=Map("url" -&gt; "jdbc:mysql://localhost:3306/test", "user"-&gt;"root","password"-&gt;"") map+=("dbtable" -&gt;"class") "dbtable" -&gt; "SELECT * FROM iteblog" val jdbc=sqlContext.read.format("jdbc").options(map).load() println(jdbc.show(1))// val lr = new LogisticRegression().setMaxIter(10) &#125;&#125; 创建DataFrame并简单操作DataFrame123456789101112131415161718192021222324252627282930313233343536import org.apache.spark.SparkConf;import org.apache.spark.SparkContextimport org.apache.spark.ml.classification.LogisticRegressionimport org.apache.spark.sql.SQLContextimport scala.collection.mutable.Mapcase class Person(var name:String)case class Employee(age: Int, name: String)object SparkTest&#123; def main(args: Array[String]) &#123; // val conf = new SparkConf().setAppName("Spark Pi").setMaster("spark://hadoop:7070") //关键 val conf = new SparkConf().setAppName("Spark Pi").setMaster("local") //关键 val sc = new SparkContext(conf) val sqlContext=new SQLContext(sc) //第一种方式就创建DataFrame，读取外部文件 println("第一种方式就创建DataFrame，读取外部文件") val textfile=sc.textFile("C:\\Users\\Administrator\\Desktop\\分词.txt") val df_person=textfile.map(x=&gt;Person(x)) val df_test=sqlContext.createDataFrame(df_person).withColumnRenamed("name","anmoyi") println(df_test.filter(df_test("anmoyi").contains("使用")).count()) //第二种方式创建DataFrame println("第二种方式创建DataFrame，通过List和case类的方式创建") val listOfEmployee=List(Employee(1,"zhou"),Employee(1,"zhou"),Employee(2,"mei"),Employee(3,"xu")) val emFrame=sqlContext.createDataFrame(listOfEmployee) println(emFrame.show()) emFrame.registerTempTable("employeeTable") val sortedByNameEmployees = sqlContext.sql("select * from employeeTable order by name desc") println(sortedByNameEmployees.show()) println(emFrame.groupBy("age").count().show()) println(emFrame.select(emFrame("name"),emFrame("age"),(emFrame("age")+1).as("age1")).show()) println(sortedByNameEmployees.show()) //第三种方式通过TupleN来创建DataFrame println("第三种方式通过TupleN，元祖的方式来创建DataFrame") val mobiles=sqlContext.createDataFrame(Seq((1,"Android"), (2, "iPhone"))).toDF("age","mobile") println(mobiles.show()) &#125;&#125; Spark中统计相关的东西spark shell中增加依赖包 bin/spark-shell --packages com.databricks:spark-csv_2.10:1.0.3 12345678910111213141516171819import org.apache.spark.SparkConfimport org.apache.spark.SparkContextimport org.apache.spark.sql.SQLContextimport org.apache.spark.sql.functions._ //包含了常见的统计函数和数学函数import org.apache.log4j.&#123;Level, Logger&#125;//import com.databricks.spark.csv._object Test &#123; def main(args: Array[String]):Unit=&#123;// 屏蔽不必要的日志显示在终端上Logger.getLogger("org.apache.spark").setLevel(Level.WARN)Logger.getLogger("org.eclipse.jetty.server").setLevel(Level.OFF) val conf = new SparkConf().setAppName("stastic").setMaster("local") //关键 val sc = new SparkContext(conf) val sqlContext=new SQLContext(sc) import sqlContext.implicits._ //用于隐式转化，可以由RDD直接转换为DataFrame val df = sc.parallelize(0 until 10).toDF("id").withColumn("rand1", rand(10)) .withColumn("rand2", rand(seed=27)).withColumn("rand3",rand(20)) println(df.columns) println(df.describe().show()) Spark中的多对多JOIN如果存在多对多的情况下，则是以乘法得到最后结果，并不是以某列多的情况 12345678910111213141516171819202122val df2=sc.parallelize(0 until 6).toDF("id").withColumn("age",rand(10))println(df.join(df2,df("id")===df2("id"),"left").show()) //左链接println(df.join(df2,df("id")===df2("id"),"right").show()) //右链接println(df.join(df2,df("id")===df2("id"),"outer").show()) //全链接println(df.join(df2,df("id")===df2("id"),"inner").show()) //inner 链接df.join(df2, $"df1Key" === $"df2Key")df.join(df2).where($"df1Key" === $"df2Key")df.join(df2, Seq("user_id", "user_name")) println("统计函数开始") println(df.groupBy($"id").agg(Map( "rand1" -&gt; "avg", "rand2" -&gt; "max", "rand3" -&gt; "min" )).show()) println(df.drop("rand1").show()) println(df.stat.corr("rand1","rand2")) println(df.stat.cov("rand1", "rand2")) val df1=sqlContext.createDataFrame(Seq((1, 1), (1, 2), (2, 1), (2, 1), (2, 3), (3, 2), (3, 3))).toDF("key", "value") println(df1.stat.crosstab("key","value").show()) &#125;&#125; pyspark12345678from pyspark import SparkContextsc = SparkContext("local", "Simple App")rdd = sc.parallelize([1,2,3])rdd.collect()#[1, 2, 3]rdd1 = rdd.map(lambda x : x+1)rdd1.collect()#[2, 3, 4] spark作业提交以WordCount为例说明RDD从转换到作业提交的过程 1sc.textFile("/User/david/key.txt").flatMap(line=&gt;line.split(" ")).map(word=&gt;(word,1)).reduceByKey(_+_) 步骤1：val rawFile = sc.textFile(&quot;/User/david/key.txt&quot;) textFile先生成HadoopRDD，然后再通过map操作生成MappedRDD。在spark-shell中可以看到 123scala&gt; val rawFile = sc.textFile(&quot;/User/david/key.txt&quot;)rawFile: org.apache.spark.rdd.RDD[String] = /User/david/key.txt MapPartitionsRDD[3] at textFile at &lt;console&gt;:271.6.3版本变成了MapPartitionsRDD 步骤2： 1val splittedText = rawFile.flatMap(line=&gt;line.split(&quot; &quot;)) flatMap将原来的MappedRDD转换为FlatMappedRDD。 步骤3 1val wordCount = splittedText.map(word=&gt;(word,1)) 步骤4：reduceByKey 作业执行在任务提交中主要涉及Driver和Executor两个节点。 Driver可以理解为我们自己编写的程序。主要解决 RDD依赖性分析，以生成DAG 根据RDD DAG将Job分割为多个stage Stage确认后，生成相应的task，分发到Executor执行。 Executor：在每个WorkerNode上为某应用启动的一个进程，是一个执行task的容器。 另外 Job：包含很多task的并行计算，可以认为是Spark RDD 里面的action,每个action的计算会生成一个job。 用户提交的Job会提交给DAGScheduler，Job会被分解成Stage和Task。 Stage： 一个Job会被拆分为多组Task，每组任务被称为一个Stage就像Map Stage， Reduce Stage。 Stage的划分在RDD的论文中有详细的介绍，简单的说是以shuffle和result这两种类型来划分。在Spark中有两类task，一类是shuffleMapTask，一类是resultTask，第一类task的输出是shuffle所需数据，第二类task的输出是result，stage的划分也以此为依据，shuffle之前的所有变换是一个stage，shuffle之后的操作是另一个stage。比如 rdd.parallize(1 to 10).foreach(println) 这个操作没有shuffle，直接就输出了，那么只有它的task是resultTask，stage也只有一个；如果是rdd.map(x =&gt; (x, 1)).reduceByKey( + ).foreach(println), 这个job因为有reduce，所以有一个shuffle过程，那么reduceByKey之前的是一个stage，执行shuffleMapTask，输出shuffle所需的数据，reduceByKey到最后是一个stage，直接就输出结果了。如果job中有多次shuffle，那么每个shuffle之前都是一个stage。 Task 即 stage 下的一个任务执行单元，一般来说，一个 rdd 有多少个 partition，就会有多少个 task，因为每一个 task 只是处理一个 partition 上的数据. 依赖性分析和stage划分RDD之间的依赖分为窄依赖和宽依赖。 窄依赖是指父RDD所有输出都会被执行的子RDD消费，也就是输出路径固定。例如如下的Transformation： map、flatMap、filter、sample 宽依赖是指父RDD输出会由不同子RDD消费，输出路径不固定。例如： sortByKey、reduceByKey、groupByKey、cogroupByKey、join、cartensian 调度器（Scheduler）会计算RDD之间的依赖关系，将窄依赖的RDD归并到同一个stage，而宽依赖则作为划分不同Stage的判断标准。 RDD API合集Spark JAVA RDD API 最全合集整理 Spark API 详解/大白话解释 之 map、mapPartitions、mapValues、mapWith、flatMap、flatMapWith、flatMapValues Spark API 详解/大白话解释 之 RDD、partition、count、collect flatMap和mapSpark之中map与flatMap的区别 map的作用就是对rdd之中的元素进行逐一进行函数操作映射为另外一个rdd。 flatMap的操作是将函数应用于rdd之中的每一个元素，将返回的迭代器的所有内容构成新的rdd。通常用来切分单词。 传递给flatMap的函数返回的类型是一个可迭代的类型（例如list）。 map会返回多个数组对象，flatmap返回一个 map函数会对每一条输入进行指定的操作，然后为每一条输入返回一个对象；而flatMap函数则是两个操作的集合——正是“先映射后扁平化”： 操作1：同map函数一样：对每一条输入进行指定的操作，然后为每一条输入返回一个对象 操作2：最后将所有对象合并为一个对象 reduce和reduceByKey转自https://blog.csdn.net/guotong1988/article/details/50555671 reduce reduce将RDD中元素前两个传给输入函数，产生一个新的return值，新产生的return值与RDD中下一个元素（第三个元素）组成两个元素，再被传给输入函数，直到最后只有一个值为止。 12val c = sc.parallelize(1 to 10)c.reduce((x, y) =&gt; x + y)//结果55 具体过程，RDD有1 2 3 4 5 6 7 8 9 10个元素，1+2=33+3=66+4=1010+5=1515+6=2121+7=2828+8=3636+9=4545+10=55 reduceByKey reduceByKey就是对元素为KV对的RDD中Key相同的元素的Value进行binary_function的reduce操作，因此，Key相同的多个元素的值被reduce为一个值，然后与原RDD中的Key组成一个新的KV对。 12val a = sc.parallelize(List((1,2),(1,3),(3,4),(3,6)))a.reduceByKey((x,y) =&gt; x + y).collect 结果 Array((1,5), (3,10))]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark-sql笔记]]></title>
    <url>%2F2017%2F07%2F12%2Fhadoop-spark%2Fspark-sql%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[spark sql可以用基本的SQL语法，或者hiveQL。Spark SQL也能从hive中读数据。 Data Frame的来源可以是：结构化数据、hive表、外部数据库或者RDD Starting Point: SQLContextThe entry point into all functionality in Spark SQL is the SQLContext class, or one of its descendants. To create a basic SQLContext, all you need is a SparkContext. 12345val sc: SparkContext // An existing SparkContext.val sqlContext = new org.apache.spark.sql.SQLContext(sc)// this is used to implicitly convert an RDD to a DataFrame.import sqlContext.implicits._ 在SQLContext的基础上，也要创建一个HiveContext，可以用Hive的udf，也能读hive表。HiveContext是一个独立的包，不需要安装hive Creating DataFramesWith a SQLContext, applications can create DataFrames from an existing RDD, from a Hive table, or from data sources. As an example, the following creates a DataFrame based on the content of a JSON file: 1234567val sc: SparkContext // An existing SparkContext.val sqlContext = new org.apache.spark.sql.SQLContext(sc)val df = sqlContext.read.json("examples/src/main/resources/people.json")// Displays the content of the DataFrame to stdoutdf.show() Creating Datasets123456789101112131415161718import org.apache.spark.sql.SQLContext......// Encoders for most common types are automatically provided by importing sqlContext.implicits._val sqlContext = SQLContext.getOrCreate(sc)import sqlContext.implicits._val ds = Seq(1, 2, 3).toDS()ds.map(_ + 1).collect() // Returns: Array(2, 3, 4)// Encoders are also created for case classes.case class Person(name: String, age: Long)val ds = Seq(Person("Andy", 32)).toDS()// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name.val path = "examples/src/main/resources/people.json"val people = sqlContext.read.json(path).as[Person] 在DataFrame中创建表并查询12345678val sqlContext = SQLContext.getOrCreate(sc)import sqlContext.implicits._//把andy和32匹配到name和age列。//先创建一个DataFrame，再注册为tableval ds = sc.parallelize(Seq(("Andy", 32))).toDF("name","age") ds.registerTempTable("ds1")sqlContext.sql("select * from ds1") sparkSQL链接GP1、maven中增加包一开始试过8.2的包就不行12345&lt;dependency&gt; &lt;groupId&gt;postgresql&lt;/groupId&gt; &lt;artifactId&gt;postgresql&lt;/artifactId&gt; &lt;version&gt;8.3-606.jdbc3&lt;/version&gt;&lt;/dependency&gt; 2、连接jdbc12345val jdbcDF = sqlContext.read.format("jdbc").options( Map("url" -&gt; "jdbc:postgresql://10.1.1.230:5432/xmo_dw", "user"-&gt;"david_xu", "password"-&gt;"w7dtfxHD", "dbtable" -&gt; "(select * from xmo_dw.bshare_blacklist_tagid) as aa")).load().show() 在spark sql命令行中测试连接/usr/lib/spark/bin/spark-sql —jars /home/david/jars/postgresql-8.3-606.jdbc4.jar 12345678910CREATE TEMPORARY TABLE temp_imageviewsUSING org.apache.spark.sql.jdbcOPTIONS ( driver "org.postgresql.Driver", url "jdbc:postgresql://10.1.1.230:5432/xmo_dw", user "david_xu", password "w7dtfxHD", dbtable "(select date_i,opxpid::text from xmo_dw.imageviews where date_i=20160512 limit 20000000) as aa", numPartitions "6"); 1234567891011121314cd /usr/lib/spark/bin//usr/lib/spark/bin/spark-sql --executor-memory 100gadd jar /home/wilson/sparksql/postgresql-8.3-606.jdbc3.jar;set spark.sql.shuffle.partitions=20; CREATE TEMPORARY TABLE temp_imageviewsUSING org.apache.spark.sql.jdbcOPTIONS ( driver "org.postgresql.Driver", url "jdbc:postgresql://10.1.1.230:5432/xmo_dw", user "david_xu", password "w7dtfxHD", dbtable "(select timeslot,record_server,referring_site ,opxsid from xmo_dw.imageviews where date_i=20160515 limit 5000000) as aa", numPartitions "6"); spark sql读取HDFS建表12345678910111213/usr/lib/spark/bin/spark-sql -e "CREATE TEMPORARY TABLE rtbreq_hourUSING org.apache.spark.sql.bytesOPTIONS ( paths '$&#123;rtbreq_path&#125;');create TEMPORARY TABLE rtbreq_tanx_hourUSING org.apache.spark.sql.bytesOPTIONS( paths '$&#123;rtbreq_tanx_path&#125;');select ip,count(1) cnt from (select ip from rtbreq_hour union all select ip from rtbreq_tanx_hour) a group by ip having cnt &gt; 30 order by cnt desc;select bxid,count(1) cnt from (select bxid from rtbreq_hour union all select bxid from rtbreq_tanx_hour) a group by bxid having cnt &gt; 1000 order by cnt desc;" &gt; rtbreq/$day/$hour.txt]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
      <tags>
        <tag>spark-sql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark笔记-操作elastic search]]></title>
    <url>%2F2017%2F07%2F12%2Fhadoop-spark%2Fspark%E7%AC%94%E8%AE%B0-%E6%93%8D%E4%BD%9Celastic-search%2F</url>
    <content type="text"><![CDATA[最简单的例子1、在pom.xml中增加12345&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-spark_2.10&lt;/artifactId&gt; &lt;version&gt;2.2.1&lt;/version&gt;&lt;/dependency&gt; 2、在spark的main中导入org.elasticsearch.spark包12...import org.elasticsearch.spark._ 3、在spark的conf中增加如下配置1set(&quot;es.index.auto.create&quot;, &quot;true&quot;).set(&quot;es.nodes&quot;, &quot;192.168.37.129&quot;).set(&quot;es.port&quot;,&quot;9200&quot;) 其中，es.nodes是ElasticSearch的host 4、简单的写法如下1234567val conf = ...val sc = new SparkContext(conf) val numbers = Map("one" -&gt; 1, "two" -&gt; 2, "three" -&gt; 3)val airports = Map("arrival" -&gt; "Otopeni", "SFO" -&gt; "San Fran")sc.makeRDD(Seq(numbers, airports)).saveToEs("spark/docs") 也可以用case class来写1234567case class Trip(departrue: String, arrival: String) val upTrip = Trip("OTF", "SFO")val downTrip = Trip("MUC", "OTP")val rdd = sc.makeRDD(Seq(upTrip, downTrip))EsSpark.saveToEs(rdd, "spark/docs") 5、在Elastic Search的Sense中查询1GET /spark/docs/_search 返回123456789101112131415161718192021222324252627282930313233343536&#123; &quot;took&quot;: 4, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;spark&quot;, &quot;_type&quot;: &quot;docs&quot;, &quot;_id&quot;: &quot;AVSkEdTv9l_YEZuMmxgt&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;arrival&quot;: &quot;Otopeni&quot;, &quot;SFO&quot;: &quot;San Fran&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;spark&quot;, &quot;_type&quot;: &quot;docs&quot;, &quot;_id&quot;: &quot;AVSkEdZp9l_YEZuMmxgu&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;one&quot;: 1, &quot;three&quot;: 3, &quot;two&quot;: 2 &#125; &#125; ] &#125;&#125; 测试通过 与spark streaming结合123456789101112131415// 创建 StreamingContext，5秒一个批次val ssc = new StreamingContext(sc, Seconds(3))val lines = ssc.socketTextStream("192.168.37.129", 9999)// 对每一行数据执行 Split 操作val words = lines.flatMap(_.split(" "))// 统计 word 的数量val pairs = words.map(word =&gt; (word, 1))pairs.foreachRDD&#123;x =&gt;x.saveToEs("spark/words")&#125;ssc.start()ssc.awaitTermination() pairs是一个DSteamRDD，通过foreachRDD来遍历其中的每个RDD，对于每个RDD，可以saveToEs]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
      <tags>
        <tag>elastic search</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用scikit-learn生成测试数据集]]></title>
    <url>%2F2017%2F07%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E7%94%A8scikit-learn%E7%94%9F%E6%88%90%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE%E9%9B%86%2F</url>
    <content type="text"><![CDATA[https://www.jiqizhixin.com/articles/2018-02-05-2 分类测试问题将看三个分类问题：blobs、moons 和 circles。 线性分类 make_blobs() 函数可被用于生成具有高斯分布的 blobs 点。你可以控制生成 blobs 的数量，生成样本的数量以及一系列其他属性。考虑到 blobs 的线性可分性质，该问题也适用于线性分类问题。 下面的例子是一个多类分类预测问题，它生成了一个具有三个 blobs 的 2D 样本数据集。每个数据有两个输入和 0、1 或 2 个类的值。 1234567891011121314from sklearn.datasets.samples_generator import make_blobsfrom matplotlib import pyplotfrom pandas import DataFrame# generate 2d classification datasetX, y = make_blobs(n_samples=100, centers=3, n_features=2)# dict中定义三个key，分别是坐标和label，再通过dict创建DataFramedf = DataFrame(dict(x=X[:,0], y=X[:,1], label=y))colors = &#123;0:'red', 1:'blue', 2:'green'&#125;fig, ax = pyplot.subplots()#groupby可以通过传入需要分组的参数实现对数据的分组grouped = df.groupby('label')for key, group in grouped: group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])pyplot.show() 非线性分类make_moons() 函数用于二进制分类并且将生成一个漩涡模式，或者两个 moons。你可以控制 moon 形状中的噪声量，以及要生产的样本数量。 这个测试问题适用于能够学习非线性类边界的算法。下面的例子生成了一个中等噪音的 moon 数据集。 12345678910111213from sklearn.datasets import make_moonsfrom matplotlib import pyplotfrom pandas import DataFrame# generate 2d classification datasetX, y = make_moons(n_samples=100, noise=0.1)# scatter plot, dots colored by class valuedf = DataFrame(dict(x=X[:,0], y=X[:,1], label=y))colors = &#123;0:'red', 1:'blue'&#125;fig, ax = pyplot.subplots()grouped = df.groupby('label')for key, group in grouped: group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])pyplot.show() make_circles() 函数生成一个数据集落入同心圆的二进制分类问题。再一次地，与 moons 测试问题一样，你可以控制形状中的噪声量。该测试问题适用于可以学习复杂的非线性流行的算法。下面的例子中生成了一个具有一定噪音的 circles 数据集。 12345678910111213from sklearn.datasets import make_circlesfrom matplotlib import pyplotfrom pandas import DataFrame# generate 2d classification datasetX, y = make_circles(n_samples=100, noise=0.05)# scatter plot, dots colored by class valuedf = DataFrame(dict(x=X[:,0], y=X[:,1], label=y))colors = &#123;0:'red', 1:'blue'&#125;fig, ax = pyplot.subplots()grouped = df.groupby('label')for key, group in grouped: group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])pyplot.show() 回归测试问题线性回归make_regression() 函数将创建一个输入和输出具有线性关系的数据集。你可以配置样本数量，输入特征数量，噪声级别等等。该数据集适用于可以学习线性回归函数的算法。 下面的例子将生成 100 个示例，他们具有适度的噪声，都有一个输入特征和一个输出特征。 1234567from sklearn.datasets import make_regressionfrom matplotlib import pyplot# generate regression datasetX, y = make_regression(n_samples=100, n_features=1, noise=0.1)# plot regression datasetpyplot.scatter(X,y)pyplot.show() 通过这些测试集可以： 比较算法。选择一个测试问题，并比较该问题的一系列算法并汇报性能。 放大问题。选择一个测试问题并探索将其放大，用级数法来可视化结果，也可以探索一个特定算法模型技能和问题规模。 代码见blogcodes\sclearn_testDataset.py]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
        <tag>机器之心</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[presto笔记]]></title>
    <url>%2F2017%2F07%2F12%2Fhadoop-spark%2Fpresto%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[增加kafka配置1、在/opt/presto-server-0.152/etc/catalog/增加文件kafka.properties，内容是 12345connector.name=kafkakafka.table-names=showup,click kafka.nodes=10.11.10.33:9092#kafka.hide-internal-columns-hidden=falsekafka.default-schema=rawdata 其中， kafka.table-names 跟topic名称相同，如果topic是带前缀的，比如rawdata.showup，那么schema就是rawdata。 kafka.hide-internal-columns-hidden 建表后有一系列内置column，默认这些是隐藏的，设为false使其显示。 kafka.default-schema 如果topic没有前缀，默认的schema是default，可以用该参数修改默认schema名称。 2、在etc的config.propreties中的datasources增加kafka 3、增加topic描述文件 放在etc/kafka目录中，以.json结尾，文件名和表名最好一致。例如： 12345678910111213141516171819202122232425&#123; &quot;tableName&quot;: &quot;click_dis&quot;, &quot;schemaName&quot;: &quot;rawdata&quot;, &quot;topicName&quot;: &quot;click_dis&quot;, &quot;key&quot;: &#123; &quot;dataFormat&quot;: &quot;raw&quot;, &quot;fields&quot;: [ &#123; &quot;name&quot;: &quot;kafka_key&quot;, &quot;type&quot;: &quot;VARCHAR&quot;, &quot;hidden&quot;: &quot;false&quot; &#125; ] &#125;, &quot;message&quot;: &#123; &quot;dataFormat&quot;: &quot;json&quot;, &quot;fields&quot;: [ &#123; &quot;name&quot;: &quot;dt_i&quot;, &quot;mapping&quot;: &quot;dt_i&quot;, &quot;type&quot;: &quot;BIGINT&quot; &#125; ] &#125;&#125; 4、重启presto服务器 1/opt/presto-server-0.152/bin/launcher restart 5、连接服务器测试 1/opt/jdk1.8.0_102/bin/java -jar /opt/presto-server-0.152/presto-cli --server 10.11.10.33:8082 --catalog kafka --schema rawdata 1select * from click limit 10; 其中内置column的意思是： Column name Type Description _partition_id BIGINT 包含这行数据的kafka partition的id _partition_offset BIGINT kafka partition的offset _segment_start BIGINT 在该segment中的最小offset _segment_end BIGINT 在该segment中的最大offset _segment_count BIGINT 对于一个未压缩的topic，_segment_start + _segment_count is equal to _partition_offset _message_corrupt BOOLEAN 为TRUE就说明解码器不能解析message _message VARCHAR UTF-8编码的string，只对text的topic有效 _message_length BIGINT message长度 _key_corrupt BOOLEAN 为TRUE就说明解码器不能解析key _key VARCHAR UTF-8编码的string _key_length BIGINT key的长度 查询key 1select count(1) from showup_dis where kafka_key like &apos;20161009%&apos;;]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
      <tags>
        <tag>presto</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单聚类算法]]></title>
    <url>%2F2017%2F07%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E7%AE%80%E5%8D%95%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[canopy 12345678while D is not empty select element d from D to initialize canopy c remove d from D Loop through remaining elements in D if distance between d_i and c &lt; T1 : add element to the canopy c if distance between d_i and c &lt; T2 : remove element from D end add canopy c to the list of canopies C]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（转）HDFS基本文件常用命令]]></title>
    <url>%2F2017%2F07%2F12%2Fhadoop-spark%2F%EF%BC%88%E8%BD%AC%EF%BC%89HDFS%E5%9F%BA%E6%9C%AC%E6%96%87%E4%BB%B6%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[-ls path列出path目录下的内容，包括文件名，权限，所有者，大小和修改时间。 -lsr path与ls相似，但递归地显示子目录下的内容。 -du path显示path下所有文件磁盘使用情况下，用字节大小表示，文件名用完整的HDFS协议前缀表示。 -dus path与-du相似，但它还显示全部文件或目录磁盘使用情况 -mv src dest在HDFS中，将文件或目录从HDFS的源路径移动到目标路径。 -cp src dest在HDFS中，将src文件或目录复制到dest。 –rm path删除一个文件或目录 –rmr path删除一个文件或递归删除目录注意：这里的mv cp操作的源路径和目的路径都是在HDFS中的路径文件 –put localSrc dest将本地文件或目录localSrc上传到HDFS中的dest路径。 –copyFromLocal localSrc dest与-put命令相同 –moveFromLocal localSrc dest将文件或目录从localSrc上传到HDFS中的dest目录，再删除本地文件或目录localSrc。12 –get [-crc] src localDest将文件或目录从HDFS中的src拷贝到本地文件系统localDest。13 –getmerge src localDest [addnl]将在HDFS中满足路径src的文件合并到本地文件系统的一个文件localDest中。14 –cat filename显示文件内容到标准输出上。 -copyToLocal [-crc] src localDest与-get命令相同。16 -moveToLocal [-crc] src localDest与-get命令相似，但拷贝结束后，删除HDFS上原文件。17 -mkdir path在HDFS中创建一个名为path的目录，如果它的上级目录不存在，也会被创建，如同linux中的mkidr –p。18 -setrep [-R] [-w] rep path设置目标文件的复制数。19 -touchz path创建一个文件。时间戳为当前时间，如果文件本就存在就失败，除非原文件长充为0。20 -test –[ezd] path如果路径(path)存在，返回1，长度为0(zero)，或是一个目录(directory)。21 –stat [format] path显示文件所占块数(%b)，文件名(%n)，块大小(%n)，复制数(%r)，修改时间(%y%Y)。22 –tail [-f] file显示文件最后的1KB内容到标准输出。23 –chmod [-R] [owner][:[group]] path…递归修改时带上-R参数，mode是一个3位的8进制数，或是[augo]+/-{rwxX}。24 –chgrp [-R] group设置文件或目录的所有组，递归修改目录时用-R参数。25 –help cmd显示cmd命令的使用信息，你需要把命令的“-”去掉复制到本地hadoop fs -copyToLocal /tmp/admaster_xid_15-05-08/part-r-00298 /home/david/查看文件夹大小hadoop fs -du /shortdata/xmo_info | awk ‘{ sum=$1 ;dir2=$3 ; hum[10243]=”Gb”;hum[10242]=”Mb”;hum[1024]=”Kb”; for (x=1024**3; x&gt;=1024; x/=1024){ if (sum&gt;=x) { printf “%.2f %s \t %s\n”,sum/x,hum[x],dir2;break } }}’]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
      <tags>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala笔记]]></title>
    <url>%2F2017%2F07%2F12%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FSCALA%2Fscala%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[optionOption[A] 是一个类型为 A 的可选值的容器： 如果值存在， Option[A] 就是一个 Some[A] ，如果不存在， Option[A] 就是对象 None 。 Option类型的值通常作为Scala集合类型（List,Map等）操作的返回类型。比如Map的get方法： 12345678scala&gt; val capitals = Map("France"-&gt;"Paris", "Japan"-&gt;"Tokyo", "China"-&gt;"Beijing")capitals: scala.collection.immutable.Map[String,String] = Map(France -&gt; Paris, Japan -&gt; Tokyo, China -&gt; Beijing)scala&gt; capitals get "France"res0: Option[String] = Some(Paris)scala&gt; capitals get "North Pole"res1: Option[String] = None Option有两个子类别，Some和None。当程序回传Some的时候，代表这个函式成功地给了你一个String，而你可以透过get()函数拿到那个String，如果程序返回的是None，则代表没有字符串可以给你。 在返回None，也就是没有String给你的时候，如果你还硬要调用get()来取得 String 的话，Scala一样是会抛出一个NoSuchElementException异常给你的。 我们也可以选用另外一个方法，getOrElse。这个方法在这个Option是Some的实例时返回对应的值，而在是None的实例时返回传入的参数。换句话说，传入getOrElse的参数实际上是默认返回值。 12345678910111213141516scala&gt; capitals get "North Pole" getwarning: there was one feature warning; re-run with -feature for detailsjava.util.NoSuchElementException: None.get at scala.None$.get(Option.scala:347) at scala.None$.get(Option.scala:345) ... 33 elidedscala&gt; capitals get "France" getwarning: there was one feature warning; re-run with -feature for detailsres3: String = Parisscala&gt; (capitals get "North Pole") getOrElse "Oops"res7: String = Oopsscala&gt; capitals get "France" getOrElse "Oops"res8: String = Paris 提示：Scala程序使用Option非常频繁，在Java中使用null来表示空值，代码中很多地方都要添加null关键字检测，不然很容易出现NullPointException。因此Java程序需要关心那些变量可能是null,而这些变量出现null的可能性很低，但一但出现，很难查出为什么出现NullPointerException。Scala的Option类型可以避免这种情况，因此Scala应用推荐使用Option类型来代表一些可选值。使用Option类型，读者一眼就可以看出这种类型的值可能为None。文／JasonDing（简书作者）原文链接：http://www.jianshu.com/p/95896d06a94d 详解Option[T]在Scala里Option[T]实际上是一个容器，就像数组或是List一样，你可以把他看成是一个可能有零到一个元素的List。当你的Option里面有东西的时候，这个List的长度是1（也就是 Some），而当你的Option里没有东西的时候，它的长度是0（也就是 None）。 case class例如： 12345abstract class Exprcase class Var(name:String) extends Exprcase class Number(num:Double) extends Exprcase class UnOp(operator:String, arg:Expr) extends Exprcase class BinOp(operator:String,left:Expr,right:Expr) extends Expr 首先，编译器为case class生成一个同名的对象构造器（Factory Method），也就是你可以使用 Var(“x”) 来创建一个类的实例，而无需使用new Var(“x”). 12scala&gt; val x = Var(&quot;x&quot;)x: Var = Var(x) 其次，Scala编译器为case class的构造函数的参数创建以参数名为名称的属性，比如Val的类的参数name:String 可以直接通过 .name访问，比如： 12scala&gt; x.nameres1: String = x 第三，编译器为case class 构造了更自然的toString，hashCode和equals实现，它们会递归打印，比较case class的参数属性。比如： 12345678scala&gt; val op=BinOp(&quot;+&quot;,Number(1),x)op: BinOp = BinOp(+,Number(1.0),Var(x))scala&gt; println(op)BinOp(+,Number(1.0),Var(x))scala&gt; op.right == Var(&quot;x&quot;)res3: Boolean = true 数据类型double转int 12val a=1.0a.toInt 类型强转，用asInstanceOf 12345//java中StructObjectInspector inspector = (StructObjectInspector)reader.getObjectInspector();//scala中val inspector = reader.getObjectInspector().asInstanceOf[StructObjectInspector] 文件读写读文件 1234567891011import scala.io.Sourceobject Test &#123; def main(args: Array[String]) &#123; println(&quot;Following is the content read:&quot; ) Source.fromFile(&quot;test.txt&quot; ).foreach&#123; print &#125; &#125;&#125; 写文件123val writer = new PrintWriter(new File(&quot;ctrout.dat&quot;))writer.println(p.toString() + &quot;,&quot; + log.get.predictionCtr.toString() + &quot;,&quot; + log.get.isClick)writer.close() 初始化空对象123456var qc_hk : QueueCollection = null try &#123; qc_hk = new QueueCollection(config.getProperty(&quot;data.dir&quot;, &quot;/var/lib/queues_hk&quot;)) &#125; catch &#123; case t: Throwable =&gt; t.printStackTrace() &#125; 集合数组定义数组 123456789val greetStrings = new Array[String](3) greetStrings(0) = &quot;Hello&quot; greetStrings(1) = &quot;, &quot; greetStrings(2) = &quot;world!\n&quot; for (i &lt;- 0 to 2) print(greetStrings(i)) 取数组下标greetStrings(0) = ‘Hello’ 定义2val numNames = Array(‘1’, ‘2’ , ‘3’)实际是创造并返回新数组的apply工厂方法。apply有个不定个数的参数。完整写法是val numNames = Array.apply (‘1’, ‘2’ , ‘3’) 12//新建一个固定长度的数组val data = Array.ofDim[T](max) 1234//取数组中不为空的并转为listdef list(): List[T] = &#123; data.filter(x =&gt; x != null).toList &#125; 12345// 重复多次计算的结果生成一个数组Array.fill(3)&#123; math.random &#125;res3: Array[Double] = Array(0.365461167592537, 1.550395944913685E-4, 0.7907242137333306)val queues = Array.fill(4)(new java.util.LinkedList[QueueInfo]) 列表List创建了不能改变。val list1 = List(1,2,3) 列表的叠加 123 val oneTwo = List(1,2) val threeFour = List(3,4) val oneFour = oneTwo:::threeFour 将新元素组合到现有列表的前端 12 val oneTwo = List(1,2) val zeroTwo = 1 :: oneTwo 在 1 :: oneTwo中，::是右操作数oneTwo的方法，1是方法的传入参数，因此也可写成oneTwo.::(1) list的一些方法计算长度为4的元素个数list.count(s =&gt; s.length == 4) 判断元素是否在listlist.exists(s =&gt; s == ‘until’) 返回长度为4的元素组成的新列表list.filter(s =&gt; s.length == 4) 判断是否列表所有元素都以1结尾list.forall(s =&gt; s.endsWith(‘1’)) 打印数组list.foreach(print) 列表每个元素加上字符的新列表list.map(s =&gt; s + ‘y’) 用列表元素组成字符串list.mkStirng(“, “) 按照第一个元素的小写字母排序list.sort((s,t) =&gt; s.charAt(0).toLowerCase &lt; t.charAr(0).toLowerCase) 元组元组不可变，但可以包含不同类型的元素 定义val pair = (99, ‘tuple’)用下划线访问元素pair._1pair._2 列表可以用list(0)，因为列表的apply方法始终返回同样类型。另外，元组_N的索引从1开始。因为对拥有静态类型元素的其他语言，入Haskell和ML，从1开始是传统设定。 集（set）和映射（map）scala的API包含set的基本特质（trait），特质相当于接口。Scala还提供了两个子特质，可变set和不可变set。 比如图中的HashSet，各有一个可变或不可变的类型。 定义 12var jetset = Set(&quot;1&quot;, &quot;2&quot;) //不可变setjetset += &quot;3&quot; //成为可变set 不可变set用+=，会产生一个新的可变set，+=在这里的完整写法是jetset = jetset + “3” 这里jetset默认是不可变set，因此+=操作会需要对jetset重新赋值，要用var 12345import scala.collection.mutable.Setval movieSet = Set(&quot;Hi&quot;, &quot;df&quot;)movieSet += &quot;Shet&quot; 这里显式指定了是可变set，所以新增元素就不需要重新赋值，用val就可以 定义 123456import scala.collection.mutable.Mapval treaMap = Map[Int, String]()treaMap += (1-&gt; &quot;Go&quot;)println(treaMap(1)) //这里的1是key map的遍历注意这个case 12var a:Map[String,Int]=Map("k1"-&gt;1,"k2"-&gt;2)a.foreach&#123;case (e,i) =&gt; println(e,i)&#125; 零碎Unit：在scala任何的函数、表达式、方法都有返回值（有些情况类似与java的void，所以scala创立了unit这个标识符来表示特殊的返回值） 12val (codeMap, codeLength) = featureCodeMap.get(featureFlag).getfeatureCodeMap.get(featureFlag).get出来是一个tuple (Map[String, Int], Int) 定义空的scala对象 123var buff: BufferedWriter = nullbuff = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(existedSlotTagsPath)))buff = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(featureCodeMapPath))) SeqSeq 是列表，适合存有序重复数据，进行快速插入/删除元素等场景Set 是集合，适合存无序非重复数据，进行快速查找海量元素的等场景 本地文件读取123import scala.io.Sourceval s = Source.fromFile(&quot;D:\\code\\Data\\rtb.BJ2.2016052513_7.log&quot;).getLines().foreach &#123; x =&gt; println(x.toString()) &#125; 找出文本中最长的一行 123val longestLine = lines.reduceLeft ( (a,b) =&gt; if (a.length &gt; b.length ) then a else b) Array数组的初始化 1234567891011121314//长度为10的整数数组，所有元素初始化为0 val numArr = new Array[Int](10) //长度为10的字符串数组，所有元素初始化为null val numArr = new Array[String](10) //长度为2的数组，数据类型自动推断出来，已经提供初始值就不需要new关键字 val s = Array(&quot;cai&quot;,&quot;yong&quot;) //通过ArrayName(index)访问数组元素和更改数组元素 val s = Array(&quot;cai&quot;,&quot;yong&quot;) println(s(0)) s(0) = &quot;haha&quot; println(s(0)) 1234//n.length是0val n = Array[Double](100)//n.length是100val n = new Array[Double](100) Listlist转array list遍历 ListBuffer1234val instance: ListBuffer[Int]=ListBuffer()ListBuffer.result = ListBuffer.toList match12345678910def main(args: Array[String]) &#123; //通过模式匹配进行条件判断 val test1: String = "1" val result1 = test1 match &#123; case "1" =&gt; &#123; "one" &#125; case "2" =&gt; "two" case _ =&gt; "other" &#125; 1234567val test2: Int = 1; val result2 = test2 match &#123; case i if i + 1 == 2 =&gt; "one" case i if i + 1 == 3 =&gt; "tow" case _ =&gt; "error" &#125; println(result2) 线上执行1234567891011#!/bin/shexport JAVA_HOME=/opt/jdk1.7.0_67export PATH=$PATH:$JAVA_HOME/binP_HOME="/home/xmo/buzzads-bidding-model-test"CLASSPATH=$&#123;P_HOME&#125;/conf:$&#123;P_HOME&#125;/buzzads-bidding-model-local.jarfor f in $&#123;P_HOME&#125;/lib/*.jar; do CLASSPATH=$&#123;CLASSPATH&#125;:$fdonejava -server -Dfile.encoding=UTF-8 -Xms1G -Xmx1G -XX:+UseConcMarkSweepGC -XX:+CMSIncrementalMode -XX:+CMSIncrementalPacing -XX:CMSIncrementalDutyCycleMin=0 -XX:CMSIncrementalDutyCycle=10 -XX:MaxNewSize=1024M -XX:MaxPermSize=256M -XX:+DisableExplicitGC -cp $CLASSPATH com.buzzinate.bidding.main.Main &gt; $&#123;P_HOME&#125;/train.log 随机数12345import java.util.Randomval random = new Random(System.currentTimeMillis)if (random.nextDouble() &lt; 0.00001) &#123; trait类似于接口。使用的时候 12//with后面的就是traitval titleCnt = new HashMap[String, Int] with HashMapUtil.IntHashMap[String] HashTable一个应用是定义一个可以插入或累加的方法 1234567891011121314151617181920212223242526272829303132//定义是/home/david/gitlab/user-gene/nlp/src/main/scala/com/buzzinate/keywords/util/HashMapUtil.scalaobject HashMapUtil &#123; trait IntHashMap[A] extends HashTable[A, DefaultEntry[A, Int]] &#123; def adjustOrPut(key: A, incr: Int, value: Int): Int = &#123; val e = findEntry(key) if (e == null) &#123; addEntry(new DefaultEntry[A, Int](key, value)) 0 &#125; else &#123; val old = e.value e.value += incr old &#125; &#125; def putMax(key: A, value: Int): Int = &#123; val e = findEntry(key) if (e == null) &#123; addEntry(new DefaultEntry[A, Int](key, value)) 0 &#125; else &#123; val old = e.value if (value &gt; e.value) e.value = value old &#125; &#125; &#125;&#125;//使用时val titleCnt = new HashMap[String, Int] with HashMapUtil.IntHashMap[String]titleCnt.adjustOrPut(te.extract(rawTitle).trim, 1, 1) case class让编译器可以自动生成一些方法，如apply、copy、equals等，当希望设计一个类只是用来作为数据载体时，case class是一个ERR Client sent AUTH, but no password is set很好的选择。 RDD操作配置123val conf = new SparkConf().setAppName("wordSegname").setMaster("local[4]"). set("spark.sql.shuffle.partitions","10").set("spark.network.timeout","30s") val sc = new SparkContext(conf) local[4]是指在本地运行，用4核CPU。 spark.sql.shuffle.partitions是指partition的数量。SparkSQL在运行时，将一个查询任务分解成多个task，一个task就是一个partition。默认是200个partition，而如果实际集群只能并行3个task，则跑完200个partition要200/3=67次。 spark.network.timeout是指所有网络通信的超时时间，默认是120s 读文件12345678//读取本地文件val path = &quot;file:///home/david/get_keyword_hash.txt&quot;val word1 = sc.textFile(path).map &#123; x =&gt; val x_filter = x.replaceAll(&quot;\\p&#123;Punct&#125;&quot;, &quot; &quot;).replaceAll(&quot;\\pP&quot;, &quot; &quot;).replaceAll(&quot;[&quot; + AtomsUitl.stopwords + &quot;]&quot;, &quot; &quot;) .replaceAll(&quot; &quot;, &quot; &quot;).replaceAll(&quot;\\p&#123;Blank&#125;&quot;, &quot; &quot;).replaceAll(&quot;\\p&#123;Space&#125;&quot;, &quot; &quot;).replaceAll(&quot;\\p&#123;Cntrl&#125;&quot;, &quot; &quot;) x_filter &#125; 上文中，是将所有的特殊字符都用空格代替 AtomsUitl.stopwords停用词是”的很了么呢是嘛个都也比还这于不与才上用就好在和对挺去后没说” sc.textFile读取文件后，生成一个RDD，以行为单位，所以后面的map是对每行的操作 打印出所有的元素，用 word1.foreach { x =&gt; println(x) } 12345678val word_document = word1.zipWithIndex.filter &#123; x =&gt; !StringUtils.isBlank(x._1) &#125;.flatMap &#123; x =&gt; val arr = ArrayBuffer[(String, Int)]() val line = x._1.split(&quot; &quot;) for (i &lt;- line) &#123; arr += ((i, x._2.toInt)) &#125; arr &#125;.map &#123; x =&gt; (x._1.trim, x._2) &#125;.filter(x =&gt; !StringUtils.isBlank(x._1)) zipWithIndex该函数将RDD中的元素和这个元素在RDD中的ID（索引号）组合成键/值对。 例如： 1234567&gt; scala&gt; var rdd2 = sc.makeRDD(Seq(&quot;A&quot;,&quot;B&quot;,&quot;R&quot;,&quot;D&quot;,&quot;F&quot;),2)&gt; rdd2: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[34] at makeRDD at :21&gt; &gt; scala&gt; rdd2.zipWithIndex().collect&gt; res27: Array[(String, Long)] = Array((A,0), (B,1), (R,2), (D,3), (F,4))&gt;&gt; filter 过滤的结果还是RDD flatMap和Map Spark 中 map函数会对每一条输入进行指定的操作，然后为每一条输入返回一个对象； 而flatMap函数则是两个操作的集合——正是“先映射后扁平化”： 操作1：同map函数一样：对每一条输入进行指定的操作，然后为每一条输入返回一个对象 操作2：最后将所有对象合并为一个对象 获取目录下所有文件路径123456789101112import java.io.PrintWriterimport java.io.Fileimport scala.reflect.io.Directoryobject Test &#123; def main(args: Array[String]): Unit = &#123; val dir = new File(&quot;F:\\joke\\DCIM\\299MEDIA&quot;) val children = dir.listFiles() for ( d &lt;- children) println(d) &#125;&#125; 结果为123456F:\joke\DCIM\299MEDIA\YDXJ0580.THMF:\joke\DCIM\299MEDIA\YDXJ0580_thm.mp4F:\joke\DCIM\299MEDIA\YDXJ0581.THMF:\joke\DCIM\299MEDIA\YDXJ0581_thm.mp4F:\joke\DCIM\299MEDIA\YDXJ0582.THMF:\joke\DCIM\299MEDIA\YDXJ0582_thm.mp4 位运算除以212// 二进制向右移动一位10 &gt;&gt;&gt; 1 得到5 翻倍12// 二进制向左移动一位10 &lt;&lt; 1 得到20]]></content>
      <categories>
        <category>SCALA</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark笔记-local开发]]></title>
    <url>%2F2017%2F07%2F12%2Fhadoop-spark%2Fspark%E7%AC%94%E8%AE%B0-local%E5%BC%80%E5%8F%91%2F</url>
    <content type="text"><![CDATA[mac安装spark环境https://blog.csdn.net/python_tty/article/details/72820469 安装scala2.10.7 下载后放到david/david/opt/scala 修改.bash_profile 12export SCALA_HOME=&quot;/Users/david/david/opt/scala-2.10.7&quot;export PATH=&quot;$SCALA_HOME/bin:$PATH&quot; 下载sparkhttp://spark.apache.org/downloads.html 下载的是1.6.3的版本 加到环境变量 12export SPARK_HOME=&quot;/Users/david/david/opt/spark-1.6.3-bin-hadoop2.6&quot;export PATH=&quot;$SPARK_HOME/bin:$PATH&quot; 允许ssh启动spark-shell进入到spark安装目录的sbin/目录下，执行 ./start-all 启动spark 进入到spark安装目录的bin/目录下，执行 ./spark-shell，看spark 是否安装成功 spark开发环境一、环境 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt; &lt;version&gt;1.6.1&lt;/version&gt; &lt;/dependency&gt; 二、编写local的spark程序1234567891011121314import org.apache.log4j.&#123; Level, Logger &#125;import org.apache.spark.&#123; SparkConf, SparkContext &#125;def main(args: Array[String]): Unit = &#123; //关闭一些不必要的日志 Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.WARN) Logger.getLogger(&quot;org.eclipse.jetty.server&quot;).setLevel(Level.OFF) val conf = new SparkConf().setAppName(&quot;wordSegname&quot;).setMaster(&quot;local[4]&quot;). set(&quot;spark.sql.shuffle.partitions&quot;,&quot;10&quot;).set(&quot;spark.network.timeout&quot;,&quot;30s&quot;) .set(&quot;spark.shuffle.compress&quot;,&quot;true&quot;).set(&quot;spark.shuffle.spill.compress&quot;,&quot;true&quot;) .set(&quot;spark.shuffle.manager&quot;,&quot;sort&quot;) val sc = new SparkContext(conf) &#125; 程序示例[WordTest.scala](../../../../code/spark/spark-buzzads/src/main/scala/com/iclick/word_segmentation/WordTest.scala) spark数据操作sparkRDD创建RDD1、数据集合 12 var data = Array(1,2,3,4,5,6,7,8,9) var disData = sc.parallelize(data,3) 创建一个RDD，包括1-9，分在3个分区 2、外部数据源 1textFile(path:String, minPartitions:Int) //第一个指定路径，第二个指定分区 RDD转换1、map，对每个元素执行一个指定函数产生新的RDD12val rdd1 = sc.parallelize(1 to 9, 3)val rdd2 = rdd1.map(x =&gt; x*2) RDD actionreducecollect 所有元素以array形式返回countfirsttake返回数据集中前N个元素takeSample 返回随机元素takeOrderedsaveAdTextFilecountByKeyforeach 错误记录启动报错1234567891011121314151617181920Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: io/netty/util/NettyRuntime at io.netty.channel.MultithreadEventLoopGroup.&lt;clinit&gt;(MultithreadEventLoopGroup.java:39) at org.apache.spark.network.util.NettyUtils.createEventLoop(NettyUtils.java:56) at org.apache.spark.network.client.TransportClientFactory.&lt;init&gt;(TransportClientFactory.java:104) at org.apache.spark.network.TransportContext.createClientFactory(TransportContext.java:88) at org.apache.spark.rpc.netty.NettyRpcEnv.&lt;init&gt;(NettyRpcEnv.scala:77) at org.apache.spark.rpc.netty.NettyRpcEnvFactory.create(NettyRpcEnv.scala:447) at org.apache.spark.rpc.RpcEnv$.create(RpcEnv.scala:53) at org.apache.spark.SparkEnv$.create(SparkEnv.scala:253) at org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:193) at org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:288) at org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:457) at com.iclick.word_segmentation.WordTest$.main(WordTest.scala:30) at com.iclick.word_segmentation.WordTest.main(WordTest.scala)Caused by: java.lang.ClassNotFoundException: io.netty.util.NettyRuntime at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ... 13 more 执行spark报错12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989918/03/19 16:52:25 ERROR sparkDriverActorSystem-akka.actor.default-dispatcher-6 ActorSystemImpl (Slf4jLogger.scala:66): Uncaught fatal error from thread [sparkDriverActorSystem-akka.remote.default-remote-dispatcher-8] shutting down ActorSystem [sparkDriverActorSystem]java.lang.NoClassDefFoundError: org/jboss/netty/channel/ChannelHandler at java.lang.Class.getDeclaredConstructors0(Native Method) at java.lang.Class.privateGetDeclaredConstructors(Class.java:2671) at java.lang.Class.getConstructor0(Class.java:3075) at java.lang.Class.getDeclaredConstructor(Class.java:2178) at akka.actor.ReflectiveDynamicAccess$$anonfun$createInstanceFor$2.apply(DynamicAccess.scala:76) at scala.util.Try$.apply(Try.scala:161) at akka.actor.ReflectiveDynamicAccess.createInstanceFor(DynamicAccess.scala:73) at akka.actor.ReflectiveDynamicAccess$$anonfun$createInstanceFor$3.apply(DynamicAccess.scala:84) at akka.actor.ReflectiveDynamicAccess$$anonfun$createInstanceFor$3.apply(DynamicAccess.scala:84) at scala.util.Success.flatMap(Try.scala:200) at akka.actor.ReflectiveDynamicAccess.createInstanceFor(DynamicAccess.scala:84) at akka.remote.EndpointManager$$anonfun$9.apply(Remoting.scala:711) at akka.remote.EndpointManager$$anonfun$9.apply(Remoting.scala:703) at scala.collection.TraversableLike$WithFilter$$anonfun$map$2.apply(TraversableLike.scala:722) at scala.collection.Iterator$class.foreach(Iterator.scala:727) at scala.collection.AbstractIterator.foreach(Iterator.scala:1157) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike$WithFilter.map(TraversableLike.scala:721) at akka.remote.EndpointManager.akka$remote$EndpointManager$$listens(Remoting.scala:703) at akka.remote.EndpointManager$$anonfun$receive$2.applyOrElse(Remoting.scala:491) at akka.actor.Actor$class.aroundReceive(Actor.scala:467) at akka.remote.EndpointManager.aroundReceive(Remoting.scala:394) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) at akka.actor.ActorCell.invoke(ActorCell.scala:487) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238) at akka.dispatch.Mailbox.run(Mailbox.scala:220) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)Caused by: java.lang.ClassNotFoundException: org.jboss.netty.channel.ChannelHandler at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ... 32 more18/03/19 16:52:25 INFO sparkDriverActorSystem-akka.actor.default-dispatcher-6 RemoteActorRefProvider$RemotingTerminator (Slf4jLogger.scala:74): Shutting down remote daemon.18/03/19 16:52:25 INFO sparkDriverActorSystem-akka.actor.default-dispatcher-6 RemoteActorRefProvider$RemotingTerminator (Slf4jLogger.scala:74): Remote daemon shut down; proceeding with flushing remote transports.18/03/19 16:52:25 ERROR sparkDriverActorSystem-akka.actor.default-dispatcher-6 Remoting (Slf4jLogger.scala:65): Remoting system has been terminated abrubtly. Attempting to shut down transports18/03/19 16:52:25 INFO sparkDriverActorSystem-akka.actor.default-dispatcher-2 RemoteActorRefProvider$RemotingTerminator (Slf4jLogger.scala:74): Remoting shut down.18/03/19 16:52:35 ERROR main SparkContext (Logging.scala:95): Error initializing SparkContext.java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds] at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219) at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223) at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107) at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53) at scala.concurrent.Await$.result(package.scala:107) at akka.remote.Remoting.start(Remoting.scala:179) at akka.remote.RemoteActorRefProvider.init(RemoteActorRefProvider.scala:184) at akka.actor.ActorSystemImpl.liftedTree2$1(ActorSystem.scala:620) at akka.actor.ActorSystemImpl._start$lzycompute(ActorSystem.scala:617) at akka.actor.ActorSystemImpl._start(ActorSystem.scala:617) at akka.actor.ActorSystemImpl.start(ActorSystem.scala:634) at akka.actor.ActorSystem$.apply(ActorSystem.scala:142) at akka.actor.ActorSystem$.apply(ActorSystem.scala:119) at org.apache.spark.util.AkkaUtils$.org$apache$spark$util$AkkaUtils$$doCreateActorSystem(AkkaUtils.scala:121) at org.apache.spark.util.AkkaUtils$$anonfun$1.apply(AkkaUtils.scala:53) at org.apache.spark.util.AkkaUtils$$anonfun$1.apply(AkkaUtils.scala:52) at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1988) at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141) at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1979) at org.apache.spark.util.AkkaUtils$.createActorSystem(AkkaUtils.scala:55) at org.apache.spark.SparkEnv$.create(SparkEnv.scala:266) at org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:193) at org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:288) at org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:457) at com.iclick.spark.buzzads.stats.Initial$.main(Initial.scala:20) at com.iclick.spark.buzzads.stats.Initial.main(Initial.scala)Exception in thread &quot;main&quot; java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds] at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219) at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223) at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107) at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53) at scala.concurrent.Await$.result(package.scala:107) at akka.remote.Remoting.start(Remoting.scala:179) at akka.remote.RemoteActorRefProvider.init(RemoteActorRefProvider.scala:184) at akka.actor.ActorSystemImpl.liftedTree2$1(ActorSystem.scala:620) at akka.actor.ActorSystemImpl._start$lzycompute(ActorSystem.scala:617) at akka.actor.ActorSystemImpl._start(ActorSystem.scala:617) at akka.actor.ActorSystemImpl.start(ActorSystem.scala:634) at akka.actor.ActorSystem$.apply(ActorSystem.scala:142) at akka.actor.ActorSystem$.apply(ActorSystem.scala:119) at org.apache.spark.util.AkkaUtils$.org$apache$spark$util$AkkaUtils$$doCreateActorSystem(AkkaUtils.scala:121) at org.apache.spark.util.AkkaUtils$$anonfun$1.apply(AkkaUtils.scala:53) at org.apache.spark.util.AkkaUtils$$anonfun$1.apply(AkkaUtils.scala:52) at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1988) at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141) at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1979) at org.apache.spark.util.AkkaUtils$.createActorSystem(AkkaUtils.scala:55) at org.apache.spark.SparkEnv$.create(SparkEnv.scala:266) at org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:193) at org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:288) at org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:457) at com.iclick.spark.buzzads.stats.Initial$.main(Initial.scala:20) at com.iclick.spark.buzzads.stats.Initial.main(Initial.scala) 读本地文件报错1234567Exception in thread &quot;main&quot; java.lang.VerifyError: class com.fasterxml.jackson.module.scala.ser.ScalaIteratorSerializer overrides final method withResolved.(Lcom/fasterxml/jackson/databind/BeanProperty;Lcom/fasterxml/jackson/databind/jsontype/TypeSerializer;Lcom/fasterxml/jackson/databind/JsonSerializer;)Lcom/fasterxml/jackson/databind/ser/std/AsArraySerializerBase; at org.apache.spark.rdd.RDDOperationScope$.&lt;init&gt;(RDDOperationScope.scala:81) at org.apache.spark.rdd.RDDOperationScope$.&lt;clinit&gt;(RDDOperationScope.scala) at org.apache.spark.SparkContext.withScope(SparkContext.scala:714) at org.apache.spark.SparkContext.textFile(SparkContext.scala:830) at com.iclick.word_segmentation.WordTest$.main(WordTest.scala:40) at com.iclick.word_segmentation.WordTest.main(WordTest.scala) 在spark版本是 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.10&lt;/artifactId&gt; &lt;version&gt;1.6.1&lt;/version&gt; &lt;/dependency&gt; 必须要要需要引入jackson的scala版本，这里的jackson-module-scala_2.10原来我用的2.4.4还依旧报错，改为2.7.3就OK了。 1234567891011121314151617181920&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.module&lt;/groupId&gt; &lt;artifactId&gt;jackson-module-scala_2.10&lt;/artifactId&gt; &lt;version&gt;2.7.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.module&lt;/groupId&gt; &lt;artifactId&gt;jackson-module-jaxb-annotations&lt;/artifactId&gt; &lt;version&gt;2.7.4&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.7.4&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-annotations&lt;/artifactId&gt; &lt;version&gt;2.7.4&lt;/version&gt; &lt;/dependency&gt;]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试刷题]]></title>
    <url>%2F2017%2F07%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E9%9D%A2%E8%AF%95%E5%88%B7%E9%A2%98%2F</url>
    <content type="text"><![CDATA[那些深度学习《面试》你可能需要知道的 如何准备机器学习工程师的面试 ？ 七月在线实验室—-BAT机器学习面试题 如何准备机器学习工程师的面试 ？ 读完这21个机器学习面试问题和答案，入职率提升99% 国内互联网公司机器学习数据挖掘类的职位面试主要考察什么方面的东西？ ….等等 如何判断一个而链表中是否有环？给定一棵二叉查找树中的两个元素，求它们的最近公共祖先。给一个栈排序基于比较的排序算法的时间复杂度是什么？证明？如何求一个带权图中两个结点直接按的最短路径？如果有些权值是负的怎么办？求一个字符串中所有的回文子串。对这些问题你都要能够推导你的解法的时间和空间复杂度（大 O 表示法），并且尽量用最低的复杂度解决。只有通过大量的练习才能将这些不同类型的问题烂熟于胸，从而在面试中迅速地给出一个高效的解法。常用的算法面试准备平台有 InterviewBit、LeetCode、Interview Cake、Pramp、http://interviewing.io 等。 概率论和统计典型问题 给出一个群体中男性和女性各自的平均身高，求整个群体的平均身高。一次调查表明意大利三分之一的汽车都是法拉利，并且在那之中一半的车都是红色的。如果你在意大利的街头看到一辆红色的汽车驶来，请问它是法拉利的可能性有多大？你试图找出在自己的网站上放置版头的最佳方案。变量包括版头的尺寸（大、中、小）以及放置的位置（顶部、中间、底部）。假定需要 95% 的置信水平，请问你至少需要多少次访问和点击来确定某个方案比其他的组合都要好？很多机器学习算法都以概率论和统计作为理论基础。对于这些基础知识有清晰的概念是极为重要的。当然同时你也要能够将这些抽象的概念与现实联系起来。数据建模和评估典型问题 一位农民想搞明白是什么因素影响了他的牛奶产量。他记录了每天的气温（30 - 40 度）、湿度（60 - 90%）、饲料消耗（2000 - 2500 千克）以及牛奶产量（500 - 1000 升）。假设问题是要预测每天的牛奶产量，你会如何处理数据并建立模型？这是一个什么类型的机器学习问题？你的公司在开发一个面部表情识别系统。这个系统接受 1920 x 1080 的图片作为输入，并告诉用户图片中的人脸处于以下哪种情绪状态：平常、高兴、悲伤、愤怒和恐惧。当图片中没有人脸时系统要能够分辨这种情况。这是一个什么类型的机器学习问题？如果每个像素点由 3 个值来表示（RGB），那么输入数据的原始维度有多大？有办法降维吗？如何对系统的输出进行编码？为什么？过去几个世纪的气象数据展现出一种循环的气温模式：一会升高一会下降。对于这样的数据（一个年平均气温的序列），你会如何建模并预测未来 5 年的平均气温？你在一家在线新闻网站工作，需要从各处收集文本，并将不同来源的内容聚集成一篇报道。你会如何设计这样一个系统？会用到哪些机器学习技术？应用机器学习算法和库 你用一个给定的数据集训练一个单隐层的神经网络，发现网络的权值在训练中强烈地震荡（有时在负值和正值之间变化）。为了解决这个问题你需要调整哪个参数？支持向量机的训练在本质上是在最优化哪个值？LASSO 回归用 L1-norm 作为惩罚项，而岭回归（Ridge Regression）则使用 L2-norm 作为惩罚项。这两者哪个更有可能得到一个稀疏（某些项的系数为 0）的模型？在用反向传播法训练一个 10 层的神经网络时，你发现前 3 层的权值完全没有变化，而 4 ~ 6 层的权值则变化得非常慢。这是为什么？如何解决？你手上有一个关于小麦产出的数据集，包括年降雨量 R、平均海拔 A 以及小麦产量 O。你经过初步分析认为产量跟年降雨量的平方以及平均海报的对数之间存在关系，即：O = β_0 + β_1 x R^2 + β_2 x log(A)。能用线性回归求出系数 β 吗？你可以通过像 Kaggle 比赛那样的数据科学和机器学习挑战来了解各种各样的问题和它们之间的细微差别。多多参加这些比赛，并尝试应用不同的机器学习模型。软件工程和系统设计典型问题 你有一个电商网站，当用户点击一个商品打开详情页面时，你想基于商品特征和用户的购买历史为用户推荐 5 个其他的商品显示在页面的底部。你需要哪些服务和数据表来实现这个功能？请写一个查询语句或一段过程式代码来返回所要推荐的 5 个商品。对于 YouTube 那样的在线视频网站，你会收集哪些数据来衡量用户的参与度和视频的人气度？一个简单的垃圾邮件检测系统是这样的：它每次处理一封邮件，统计不同单词的出现频率（Term frequency），并将这些频率与之前已经被标注为垃圾 / 正常邮件的那些频率进行比较。现在需要对这系统进行拓展来处理海量的邮件流量，请设计一个 Map-Reduce 方案在一个集群上部署这个系统。你要生成一个实时的热力图，来展示用户正在浏览和点击一个网页的哪些部分。在客户端和服务端分别需要哪些组件 / 服务 / API 来实现这个功能？ 机器学习岗位面试问题汇总 之 集成学习]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka笔记]]></title>
    <url>%2F2017%2F07%2F12%2Fhadoop-spark%2Fkafka%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[多个消费者读取一个topic，多次消费不同消费者设置不同的groupid 12val kafkaParm = Map("metadata.broker.list" -&gt; "localhost:9092","auto.offset.reset" -&gt; "smallest", "group.id" -&gt; "davidtopi1c1") 每个消费者不会重复消费数据1234 kafkaStrem.foreachRDD&#123; rdd=&gt; km.updateZKOffsets(rdd)&#125;]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集成学习]]></title>
    <url>%2F2017%2F07%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[个体学习器一般是弱学习器。 弱学习器是指泛华性能略优于随机猜测的学习器，例如二分上略高于50%的学习器。 要获得好的集成，个体学习器应“好而不同”，即个体学习器要有一定的准确性和多样性（学习器之间有差异）。 理论上，假设个体学习器的误差是相互独立，那么随着学习器数量增大，集成的错误率将指数下降，最终趋向于零。 但实际上不可能相互独立。且准确性和多样性本身就是矛盾的，追求准确性就要牺牲多样性。所以如何产生并结合“好而不同”的学习器，是集成学习研究的核心。 根据集成的方式不同， 1）个体学习器存在强依赖性，必须串行生成，如Boosting； 2）个体学习器间不存在强依赖关系，可同时并行生成，如Bagging和随机森林。 1、Boosting工作机制： 先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使前一个做错的样本在后续得到更多关注，然后基于调整后的样本分布来训练下一个基学习器；重复迭代s达到T。 代表是AdaBoost. 1.1 算法描述假设$y_i \in {-1,+1}$ AdsBoost有多种推广方式，一般的是基于加性模型（additive model），即用基学习器的线性组合 H(x) = \sum_{t=1}^T a_th_t(x)来最小化指数损失函数 l_{exp}(H|D) =E_{x \sim D}[e^{-f(x)H(x)}]求偏导 \frac {\partial l_{exp(H|D)}} {\partial H(x)} =2、Bagging 3、随机森林]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>西瓜书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[广告反作弊]]></title>
    <url>%2F2017%2F07%2F12%2F%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A%2F%E5%B9%BF%E5%91%8A%E5%8F%8D%E4%BD%9C%E5%BC%8A%2F</url>
    <content type="text"><![CDATA[秒针发布过《互联网广告反作弊技术白皮书》 腾讯灯塔联手秒针、AdMaster发布 腾讯《2017广告反欺诈白皮书》日均校验40亿广告请求，识别的作弊比率在15%左右。部分行业和campaign中，作弊率有60%。 欺诈手段 虚假流量。 某APP在地推时，新增用户暴涨。95%的新增用户的共同特征： ROM编译机名称一致；指令逃逸差异数与正常用户不一致；CPU结构为X86，为PC机模拟器；文件系统类型的差异度与正常用户不一致。 某APP的新增用户中，工作室批量刷量的特征有： 安装时间具有明显的规律；手机APP安装数量一致；明显的地域集中性。 2.2 黑产技术 1、广告作弊类型分类 1）模拟器刷量：电脑模拟器刷量、手机软件模拟刷量、脚本刷量（录制行为跑循环任务）。 2）真假机用户：储备大量手机或者sim卡，利用数据线push命令到手机，手机执行命令。 3）静默安装（真机真用户假行为）。人工方式或网络传播方式将木马/具有再分发能力的应用植入到用户手机，形成僵尸网络，在用户无感知的情况下，完成App的下载、激活和删除等一系列操作。 4）羊毛党（真机真用户真行为假动机）：登录一次就删除应用、使用时长极短、留存率极低。在大部份的情况下，这种用户对业务的健康发展并无太大价值。 5）广告素材、篇幅偷换（不可见）。“1 像素广告”指在用户的手机屏幕上只展示1个像素大小的广告。这种广告，用户看不见，但统计工具可以统计到，仍然会作为曝光广告与广告主结算，给广告主带来经济损失。 6）以次充好（不匹配）。部分媒体会将广告主原本定向的一线城市用户偷偷换成二三线城市用户，达到以次充好的目的。 应对方法2.3 反作弊技术 1）用户群体数据检测。低阶技术，常见方式有： a.看留存率。真实的用户的留存曲线是一条平滑的指数衰减曲线，如果发现留存曲线存在陡升陡降的异常波动，则判断刷量者干预了数据。 b.看用户终端、网络信息。如根据经验分析渠道新增用户或者启动用户的设备排名;2G、3G、4G的使用比例分布是否正常等。 c.看用户注册信息。比如说注册昵称的分布和规律等。 以上操作均效率低下。 2）用户行为特征分析 a.单个指标。与黑IP库进行比对，是否为黑名单IP、是否为代理IP;与IMEI库进行对比，是否为为黑IMEI; b.群体指标。用户的IP、IMEI、机型、OS、位置信息、运营商、接入方式的分布是否符合先验数据的分布 c.设备一致性验证。CPU、制造商、MAC地址、IMEI、机型、操作系统的一致性验证。 简单粗暴，没有黑白转换机制，误判率高。 这些方法容易被刷量者利用。在某电商专业Android app游戏激活、注册、留存、付费、应用市场好评平台上，买主只需要很小的代价，即可刷出完全符合正常用户规律的留存率、IP分布机型分布使用时长等。 3）终端特征分析+云端交叉验证 “查”模型负责找寻黑产界的新型作弊方式，提升整体模型的覆盖率 “杀”模型负责准确识别恶意份子 “验”模型通过多业务交叉验证，负责保证“查”、“杀”模型的准确率 终端识别模块（灯塔SDK稽核模块）:该模块主要是采用机器学习算法选取系统中所有可用的信息作为特征，然后对这些特征进行运算得到该设备的指纹，可以有效识别手机模拟器、修改系统参数等行为。 基于规则的识别模块（业务自有模块）:该模块一般是通过业务经验及对历史可疑渠道的总结形成的反作弊规则，可以理解为多维组合规则，一般需根据业务成本、对渠道的容忍度设置关键变量的阈值。 基于数据挖掘的识别模块（灯塔云端模块）:该模块主要从硬件信息、用户活跃、用户行为进行多维度、多业务交叉验证，分别计算每个维度下面的不同特征值，结合决策树、LR、贝叶斯网络等多种算法进行精准的定位。分类 为了增强识别的准确性和稳定性，模块之间、模块内部均采用集成学习方法的思想，其核心思想是在模块内对同一个训练集训练不同的分类器，然后把这些分类器结合起来构成一个最终的分类器，而每一个模块可以针对不同的作弊手段进行识别，再把模块与模块结合，才能识别所有的作弊手段。 admaster《广告反欺诈研究报告》2016今年 1 月 29 日和 3 月 2 日,宝洁公司首席品牌官 Marc Pritchard 分别在美国互动广告局(InteractiveAdvertising Bureau, IAB)和美国广告主协会(Association of National Advertisers, ANA)两个年度重要峰会上进行主题发言,针对数字广告透明度和可见性标准的言论引发了全球营销圈和数字行业的热议。宝洁呼吁业界在四个方面采取行动: 数字广告采纳一套统一的可见性测量标准; 贯彻第三方测评机构 的验证审核; 提倡全面透明的代理公司合同机制; 预防广告欺诈。 AdMaster先后推出了 BlueAir、定投识别 (VOA)、监播实录 (SNAP) 等技术产品,逐步建立了“全方位、深层次、多角度”的广告反欺诈解决方案。尤其在程序化购买中,AdMaster 在投放前预判 (Pre-bid),即事前广告反欺诈技术。 BlueAir可以对智能电视IP、地域、频次以及User Agent等维度的异常流量进行甄别，将行为逻辑上不正常的设备加入黑名单，从而保障广告投放的安全。同时，与海信、康佳、创维、欢网等硬件厂家共同建立智能电视设备白名单，以便从设备维度进一步甄别流量真实性。 定投识别 监播实录 投放前预判。在流量请求、广告未展现时,根据历史流量质量进行排查,从而提前避免广告在无效流量上的投放。 无效流量类型1、广告可见性问题引发的低质量流量 目前媒体的环境导致广告不易可见。AdMaster 在 AdServing 广告投放管理技术上能够实现广告可见性的预估判断。在多种广告形式的后测方面,通过独创的模型评估广告可见性表现。 2、机器人无效流量（Non-Human Traffic） 从最初的 cookie 和 IP不变的前提下,反复刷新页面和点击广告,造成广告曝光和点击的增加,到通过木马或者恶意程序控制海量人肉刷机、伪造大量 IP 与设备信息进行模拟访问、或将 IP 和 cookie、User Agent 一起进行轮替的流量造假方式,都属于机器人无效流量。 BlueAir 广告反欺诈技术能够结合历史异常数据,能够在流量请求、广告未展现时,即根据历史流量质量判断此流量的质量,在投放前通过 Pre-bid 判断出流量的异常,杜绝流量造假现象发生。 3、视频类无效流量 (1)针对剧目投偏现象,AdMaster 定投识别 (VOA) 功能通过 Referrer/ 剧目 ID 等方式解析 广告曝光时所播放剧目名称,并与广告主定投的剧目内容进行匹配。在移动端定投评估中,高诚信度的视频媒体也提供高度配合。剧目投偏比例,一目了然。 (2)针对时有发生的曝光代码调用,但是素材未正确展示的广告欺诈现象,AdMaster 利用监播实录 (SNAP)功能,采用类似于“神秘访客”概念的方法从海量抽样监测,将视频内容播放前的所有贴片内容录制下来,并通过图像识别与适当的素材进行对比,判断素材是否被正确展示,以及是否按照要求展示。 4、智能电视无效流量 支持智能电视广告的监测模式一般有 3 种 : 分别是第三方 SDK 监测、C2S 和 S2S 两种 API。前两种相比 S2S 更为安全,也更容易监测流量异常情况,可以说 C2S API 是智能电视广告监测安全的起点和基础(S2S API 传输方式目前很难识别无效流量)。 inmobi《移动广告反作弊白皮书》没技术 反作弊措施 1）剔除自动流量。 识别机器人脚本，分析展示和点击的质量。 2）数据信号双重检测。 将媒体共享的人群信息和从SDK收集的信息比对，对所有无效信息定位和删除。 [部分有关 广告联盟作弊 与反作弊资料收集]（http://cwiki.apachecn.org/pages/viewpage.action?pageId=4882639） roadmap中，AD fraud的业界措施 监控广告投放的效果； 保存曝光设备的ip 记录用户在landing页的行为 网页分析 基于数据挖掘的识别模块（灯塔云端模块）:该模块主要从硬件信息、用户活跃、用户行为进行多维度、多业务交叉验证，分别计算每个维度下面的不同特征值，结合决策树、LR、贝叶斯网络等多种算法进行精准的定位。为了增强识别的准确性和稳定性，模块之间、模块内部均采用集成学习方法的思想，其核心思想是在模块内对同一个训练集训练不同的分类器，然后把这些分类器结合起来构成一个最终的分类器，而每一个模块可以针对不同的作弊手段进行识别，再把模块与模块结合，才能识别所有的作弊手段。 这个方法未来肯定要实现的，大致是通过集成学习的方式，综合硬件信息，网页特征，用户行为 最初我们只有url信息，提取url的特征判断投放的网页是否安全； 后面对每条 前期，通过分析url内容，判断网页是否安全，是否为虚假网页，网页内容与广告品牌是否有冲突。 后面，分析曝光的详细信息，包括设备信息，用户特征，判断是否为作弊流量；分析网页元素和广告位置，判断广告可见性。 再后面，在对url和黑名单有一定积累的基础上，在投放或竞价前检测投放环境，主动识别和屏蔽非安全流量]]></content>
      <categories>
        <category>计算广告</category>
      </categories>
      <tags>
        <tag>反作弊</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pig笔记]]></title>
    <url>%2F2017%2F07%2F12%2Fhadoop-spark%2Fpig%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[JOIN的优化1、Replicated Join ​ 当进行Join的一个表比较大，而其他的表都很小(能够放入内存)时，replicated join会非常高效。 ​ 在Join时使用 Using ‘replicated’语句来触发Replicated Join，大表放置在最左端，其余小表(可以有多个)放置在右端。 ​ 为了防止replicated join应用于大表的连接关系，pig会在这个连接关系复制的字节大小比pig.join.replicated.max.bytes属性的值大会失败 (default = 1GB)。 2、Skewed Join ​ 当进行Join的两个表的内连接时，一个表数据记录针对key的分布极其不均衡的时候使用，如果多于两个连接，要自己拆分成多个双表的连接。 ​ pig.skewedjoin.reduce.memusage属性的值指定了reduce可以占用堆内存的百分数，低的分数可以让pig执行更多的reducer，但是增加了复制的成本。性能好的范围值在0.1到0.4，但是这仅仅是一个范围。这个值取决于这个操作的可用的堆内存和这个输入的行数和倾斜。默认值是0.5。 ​ Skewed Join并没有专注于解决或者说的平衡这种不均匀的数据分布在reducer，而是确保这个Join连接能够完成而不是失败，但是会慢。他会增加5%的时间用于计算这个连接操作。 3、Merge Join ​ 当进行Join的两个表都已经用Join的键进行了排序，可以使用Merge Join。 可以在Join时使用Using ‘merge’语句来触发Merge Join，需要创建索引的表放置在右端。 另外，在进行Join之前，首先过滤掉key为Null的数据记录可以减少Join的数据量。 读取HDFS文件 比如下面加载某个模型到pig 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748private static Map&lt;String, InfoPredictor&gt; cache = new ConcurrentHashMap&lt;String, InfoPredictor&gt;(); private static AtomicBoolean isLoading = new AtomicBoolean(false); public static InfoPredictor getInfoPredictor(String modelFile) throws IOException, InterruptedException &#123; InfoPredictor model = cache.get(modelFile); if (model == null) &#123; // 保证在多线程下只执行一次 if(isLoading.compareAndSet(false, true)) &#123; model = load(modelFile);/* InfoPredict.loadDomainModel(modelFile);*/ cache.put(modelFile, model); &#125; else &#123; long maxLoadingTimeSecs = 120l; long loadingTimeSecs = 0l; while(null == cache.get(modelFile)) &#123; Thread.currentThread().sleep(5000l); loadingTimeSecs += 5; if(loadingTimeSecs &gt; maxLoadingTimeSecs) &#123; throw new IOException("try loading KNNSearcher model out times"); &#125; &#125; model = cache.get(modelFile); &#125; &#125; return model; &#125; public static InfoPredictor load(String modelFile) throws IOException &#123; FSDataInputStream fin = null; // pig下加载hdfs配置 Configuration conf = UDFContext.getUDFContext().getJobConf(); String hdfsPath = conf.get("fs.defaultFS") + modelFile; FileSystem fs = FileSystem.get(URI.create(hdfsPath), conf); fin = fs.open(new Path(hdfsPath)); ScoreIndex sIndex = new ScoreIndex(); BufferedReader in = null; in = new BufferedReader(new InputStreamReader(fin)); String line = ""; while((line = in.readLine()) != null)&#123; String[] scoreArr = line.split(","); for (int i = 0; i &lt; scoreArr.length; i++) &#123; sIndex.putScore(i + 1, (float)(Float.valueOf(scoreArr[i])/* * CorrectValue[i]*/)); &#125; &#125; sIndex.sortScore(); InfoPredict.setsIndex(sIndex); return new InfoPredictor(); &#125; UDF返回tupleUDF返回DataBag123456789101112131415161718192021222324252627282930313233public class searchPerKeywords extends EvalFunc&lt;DataBag&gt; &#123; private static TupleFactory mTupleFactory = TupleFactory.getInstance(); private static BagFactory mBagFactory = BagFactory.getInstance(); @Override public DataBag exec(Tuple input) throws IOException &#123; String keywordsFile = input.get(0).toString(); String modelFile = input.get(1).toString(); Integer featureLength = Integer.valueOf(input.get(2).toString()); Integer filtLevel = Integer.valueOf(input.get(3).toString()); String compressedFloatVec = input.get(4).toString(); if(StringUtils.isEmpty(compressedFloatVec)) &#123; return null; &#125; DataBag bag = mBagFactory.newDefaultBag(); try &#123; float[] vecs = WVUtils.String2floatArray(compressedFloatVec); KeyWordSearcher searcher = KeyWordSearcher.getSearcher(keywordsFile, modelFile); for(String matchedWord : searcher.MatchedWords(WVUtils.splitFeature(vecs, featureLength), filtLevel)) &#123; bag.add(mTupleFactory.newTuple(matchedWord)); &#125; &#125; catch (Exception e) &#123; return null; &#125; if(bag.size() == 0) &#123; return null; &#125; return bag; &#125; &#125; UDF返回tupleUDF的写法 123456789101112131415161718192021222324252627282930public Tuple exec(Tuple input) throws IOException &#123; if (input == null || input.size() != 1) &#123; return null; &#125; WebPageInfo nlp = null; String nlpPageInfo = (String) input.get(0); Tuple tup = mTupleFactory.newTuple(); if (StringUtils.isBlank(nlpPageInfo)) &#123; return null; &#125; try &#123; nlp = JSON.parseObject(nlpPageInfo, WebPageInfo.class); &#125; catch (Exception e) &#123; return null; &#125; tup.append(nlp.content); if (filter.isContaintSensitiveWord(nlp.content, 2)) &#123; tup.append("-1"); &#125; else &#123; tup.append("1"); &#125; return tup; &#125; pig的写法DetectUnsafeUrl.pig 1url_classify = FOREACH url_page_info GENERATE FLATTEN(DetectUnsafeUrl(info)) as (content, score); pig问题filter为null的报错1Exception while executing (Name: url_safety: Filter[bag] - scope-10 Operator Key: scope-10): org.apache.pig.backend.executionengine.ExecException: ERROR 0: Exception while executing [POUserFunc (Name: POUserFunc(com.buzzinate.pig.udf.webdata.DetectUnsafeUrl)[chararray] - scope-7 Operator Key: scope-7) children: null at []]: java.lang.NullPointerException 因为在udf中加了一段抛出异常 12345try &#123; nlp = JSON.parseObject(nlpPageInfo, WebPageInfo.class); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; 当有返回异常的时候，pig的filter就不能对返回结果做过滤了 1url_safety = FILTER url_classify BY score == &apos;-1&apos;; 这里就会报错。把抛出异常改为返回一个空字符串或者null就可以了]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
      <tags>
        <tag>pig</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive笔记-orc格式读写]]></title>
    <url>%2F2017%2F07%2F12%2Fhadoop-spark%2Fhive%E7%AC%94%E8%AE%B0-orc%E6%A0%BC%E5%BC%8F%E8%AF%BB%E5%86%99%2F</url>
    <content type="text"><![CDATA[需要引入的包： 123456789101112131415&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-core&lt;/artifactId&gt; &lt;version&gt;2.6.0-mr1-cdh5.9.1&lt;/version&gt; &lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-common&lt;/artifactId&gt; &lt;version&gt;1.1.0-cdh5.9.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-exec&lt;/artifactId&gt; &lt;version&gt;1.1.0-cdh5.9.1&lt;/version&gt; &lt;/dependency&gt; 1234567891011121314151617181920212223242526272829val fs = FileSystem.get(conf);val prop = Config.getConfig("config.properties")println(sdf.format(new Date))val readerOpts = OrcFile.readerOptions(conf)val reader = OrcFile.createReader(new Path(iaxReqPath+"/ds=17-08-21/20170821000000antispam_2529853576425433.orc"), readerOpts)val inspector = reader.getObjectInspector().asInstanceOf[StructObjectInspector]val count = reader.getNumberOfRowsinfo("the count is: " + count.toString())val fields = inspector.getAllStructFieldRefs()fields.foreach &#123; x =&gt; println(x.getFieldObjectInspector.getCategory) &#125;val records = reader.rows()var n=0val loop = new Breaksloop.breakable(&#123; while(records.hasNext)&#123; if (n&gt;5) loop.break val row = records.next(null) val valueList = inspector.getStructFieldsDataAsList(row) info(valueList.get(10).toString) info(row.toString()) n = n+1 &#125;&#125;)]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>orc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[优先级队列]]></title>
    <url>%2F2017%2F07%2F12%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E4%BC%98%E5%85%88%E7%BA%A7%E9%98%9F%E5%88%97%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122class PriorityQueue[E](top: Int) &#123; import PriorityQueue.Entry private var nsize = 0 private val heap = Array.ofDim[Entry[E]](top+1) /** * Adds an Object to a PriorityQueue in log(size) time. It returns the * object (if any) that was dropped off the heap because it was full. This * can be the given parameter (in case it is smaller than the full heap's * minimum, and couldn't be added), or another object that was previously * the smallest value in the heap and now has been replaced by a larger one, * or null if the queue wasn't yet full with maxSize elements. */ def add(key: Double, value: E): Option[Entry[E]] = &#123; if (nsize &lt; top) &#123; nsize += 1 heap(nsize) = Entry(key, value) upHeap None &#125; else if (nsize &gt; 0 &amp;&amp; key &gt;= heap(1).key) &#123; val old = heap(1) heap(1) = Entry(key, value) updateTop Some(old) &#125; else Some(Entry(key, value)) &#125; def values(): List[E] = &#123; val entries = Array.ofDim[Entry[E]](nsize) System.arraycopy(heap, 1, entries, 0, nsize) entries sortBy(-_.key) map(_.value) toList &#125; /** * Removes and returns the least element of the PriorityQueue in log(size) * time. */ def pop(): Option[Entry[E]] = &#123; if (nsize &gt; 0) &#123; val result = heap(1) heap(1) = heap(nsize) heap(nsize) = null nsize -= 1 downHeap Some(result) &#125; else None &#125; /** Returns the number of elements currently stored in the PriorityQueue. */ def size(): Int = nsize /** * Should be called when the Object at top changes values. Still log(n) * worst case, but it's at least twice as fast to * * &lt;pre&gt; * pq.top().change(); * pq.updateTop(); * &lt;/pre&gt; * * instead of * * &lt;pre&gt; * o = pq.pop(); * o.change(); * pq.push(o); * &lt;/pre&gt; * * @return the new 'top' element. */ def updateTop(): Entry[E] = &#123; downHeap heap(1) &#125; private def upHeap(): Unit = &#123; var i = nsize val node = heap(i) var j = i &gt;&gt;&gt; 1 while (j &gt; 0 &amp;&amp; node.key &lt; heap(j).key) &#123; heap(i) = heap(j) i = j j = j &gt;&gt;&gt; 1 &#125; heap(i) = node &#125; private def downHeap(): Unit = &#123; var i = 1 val node = heap(i) var j = i &lt;&lt; 1 var k = j + 1 if (k &lt;= nsize &amp;&amp; heap(k).key &lt; heap(j).key) &#123; j = k &#125; while (j &lt;= nsize &amp;&amp; heap(j).key &lt; node.key) &#123; heap(i) = heap(j) i = j j = i &lt;&lt; 1 k = j + 1 if (k &lt;= nsize &amp;&amp; heap(k).key &lt; heap(j).key) &#123; j = k &#125; &#125; heap(i) = node &#125;&#125;object PriorityQueue &#123; case class Entry[E](key: Double, value: E) def main(args: Array[String]) &#123; val pq = new PriorityQueue[(Int, Double)](4) val ds = Array[Double](0.1, 0.2, 0.7, 0.2, 0.2, 0.3, 0.3, 0.5, 0.5, 0.5) for (i &lt;- 0 until ds.length) &#123; val e = pq.add(ds(i), (i, ds(i))) println((i, ds(i)) + " =&gt; " + e) &#125; println(pq.values) &#125;&#125;]]></content>
      <categories>
        <category>算法与数据结构</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记-最大熵]]></title>
    <url>%2F2017%2F07%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%9C%80%E5%A4%A7%E7%86%B5%2F</url>
    <content type="text"><![CDATA[1、最大熵原理日常生活中，很多事情的发生表现出一定的随机性，试验的结果往往是不确定的，也不知道这个随机现象所服从的概率分布。最大熵的实质就是，在已知部分知识的前提下，关于未知分布最合理的推断就是符合已知知识最不确定或者最随机的推断。任何其他的选择都意味着我们增加了其他的约束和假设。 将最大熵应用到分类，就是最大熵模型。给定一个训练集： T = \{ (x_1,y_1), (x_2,y_2),..., (x_N,y_N)\}其中$x_i \in X$是输入，$y_i \in Y$是输出，X和Y表示输入和输出空间。N为样本数。目标是，利用最大熵原理选出一个最好的分类模型，即对于任意给定的输入$x \in X$，可以以概率$p(y|x)$输出$y \in Y$ 。 按照最大熵原理，应该优先保证模型满足已知的所有约束。思路是，从训练数据T中抽取若干有用的特征，要求这些特征在T上关于经验分布$\tilde{p}(x,y)$的数学期望与它们在模型中关于$p(x,y)$的数学期望相等。这样，一个特征就是一个约束了。 这里就涉及到，特征如何刻画？经验分布如何表示？ 2、特征函数假设通过特征选择，抽取若干特征。特征通常由特征函数来表示。例如 f(x,y) =\left\{\begin{matrix} \begin{aligned} & 1，若x,y满足某个事实 \\ & 0，否则 \end{aligned} \end{matrix}\right.这里的特征不是指输入的某个特征，而是指输入和输出共同的特征。 例如，假设我们需要判断“打”是动词还是量词，已知的训练数据有 (x1,y1)=(一打火柴，量词); (x2,y2)=(三打啤酒，量词); (x3,y3)=(打电话，动词); (x4,y4)=(打篮球，动词); 通过观察，发现“打”前面是数字时，是量词，“打”后面是名词时，是动词。这就是从训练数据中提取的两个特征，可分别用特征函数表示为 3、经验分布经验（概率）分布就是通过对训练集T进行统计得到的分布，用$\tilde p$表示。这里列举两个经验分布 \tilde p(x,y) = \frac {count(x,y)} {N} , \tilde p(x)=\frac {count(x)} {N}其中，count表示出现的次数。 4、约束条件对于任意一个特征函数f，$E{\tilde p}f$ 表示f在训练数据T上关于$\tilde p(x,y)$的数学期望， $E{p}f$ 表示f在训练数据T上关于$p(x,y)$的数学期望。按照期望的定义，我们有 E_{\tilde p}f=\sum_{x,y}\tilde p(x,y)f(x,y) E_{ p}f=\sum_{x,y} p(x,y)f(x,y)其中，p(x,y)是未知的，而建模的目标是生成$p(y|x)$，因此，根据Bayes定理，$p(x,y)=p(x)p(y|x)$。在样本数量足够的条件下，$p(x)$可以用$\tilde p(x)$近似表示。这样 E_{ p}f=\sum_{x,y} \tilde p(x)p(y|x)f(x,y)对于概率分布$p(y|x)$，我们希望特征f的期望值应该和从训练集中得到的特征期望值是一致的，因此，增加约束 E_{ p}f=E_{\tilde p}f假设我们从训练集中抽取了n个特征，相应的，便有n个特征函数$f_i(i=1,2,…,n)$以及n个约束条件 C_i:E_{ p}(f_i)=E_{\tilde p}(f_i) \tag {3-1} 关于约束条件的几何解释 （a）：P是所有可能的概率空间，此时没有约束条件，所有的概率模型$p(y|x)$都是允许的； （b）：增加了一个线性约束条件$C_1$，此时，目标分布$p(y|x)$只能落在由$C_1$定义的线段上； （c）：在（b）的基础上增加了另一个约束条件$C_2$ ，且$C_1 \cap C_2 \neq \varnothing$。此时，目标分布只能落在交点上，即被唯一确定； （d）：在（b）基础上增加了另一个约束$C_3$，且$C_1 \cap C_2 = \varnothing$，此时不存在能够同时满足$C_1$和$C_3$的$p(y|x)$。 利用（3-1）定义的约束条件，我们定义P的一个子空间 C=\{p \in P | E_p(f_i)={\tau}_i, i=1,2,...,n\}5、最大熵模型由于我们的目标是获得一个条件分布，因此这里也采用相应的条件熵 H(p(y|x))=-\sum_{x,y} \tilde p(x)p(y|x)\log p(y|x)可以看出这里也是用$\tilde p(x)$来近似$p(x)$。以下将$H(p(y|x))$简记为$H(p)$。至此，可以给出最大熵模型的完整描述。 对于给定的训练集T，特征函数$f_i(x,y), i=1,2,…n$，最大熵模型就是求解 \underset {p \in C} {max} \ \ H(p) = \begin{pmatrix} -\sum_{x,y} \tilde p(x)p(y|x)\log p(y|x) \end{pmatrix}, \\ s.t. \sum_y p(y|x)=1 \tag {5-1} \\ s.t. \ C=\{p \in P | E_p(f_i)={\tau}_i, i=1,2,...,n\}其中的s.t.是为了保证$p(y|x)$是一个（合法的）条件概率分布。 等价于一个求极小值问题 \underset {p \in C} {min} \ \ -H(p) = \begin{pmatrix} \sum_{x,y} \tilde p(x)p(y|x)\log p(y|x) \end{pmatrix}, \\ s.t. \sum_y p(y|x)=1 \tag {5-2} \\ s.t. \ C=\{p \in P | E_p(f_i)={\tau}_i, i=1,2,...,n\}6、模型求解对于5-1的求解，主要思路和步骤如下： 利用Lagrange乘子将最大熵模型由一个带约束的最优化问题转为无约束的最优化问题，这是一个极小极大问题（min max）。 利用对偶问题等价性，转化为求解上一步得到的极大/极小问题的对偶问题，也是一个极大极小问题。 6.1 原始问题和对偶问题根据（5-2），引入拉格朗日乘子$\lambda=(\lambda_0,\lambda_1,…,\lambda_n)^T$，定义拉格朗日函数 L(p,\lambda) = -H(p) + \lambda_0(1-\sum_y p(y|x))+\sum_{i=1}^n\lambda_i(\tau_i-E_p(f_i)) \tag{6-1}利用对偶性，求解（6-1）的原始问题表示为： \underset {p \in C} {min}\ \underset {\lambda} {max}\ L(p,\lambda) \tag{6-2}对偶问题为： \underset {\lambda} {max}\ \underset {p \in C} {min}\ L(p,\lambda) \tag{6-3}由于$H(p)$是关于p的凸函数，因此要求解最大熵模型，只需求解对偶问题（6-3）即可。 6.1.1 指数形式的解首先求解内部的极小问题。由于$\underset {p \in C} {min}\ L(p,\lambda)$是关于$\lambda$的函数，将其记做： \Psi (\lambda) =\underset {p \in C} {min}\ L(p,\lambda) = L(p_{\lambda}, \lambda) \tag {6-4}其中 p_{\lambda}=\underset {p \in C} {argmin}\ L(p,\lambda)=p_{\lambda}(y|x) \tag {6-5}根据拉格朗日乘子法，求$L(p,\lambda)$对$p(y|x)$的偏导，得（求解过程略）： p_{\lambda}=\frac {1} {Z_{\lambda}(x)} \ \exp(\sum_{i=1}^n \lambda_i f_i(x,y)) \tag{6-6}其中， Z_{\lambda}(x)=\sum_y \exp(\sum_{i=1}^n \lambda_i f_i(x,y)) \tag{6-7}称为规范化因子（normalizing factor）。注意，此时已经没有$\lambda_0$了。 由（6-6）定义的$p_{\lambda}$就是最大熵模型的解，它具有指数形式。其中，$\lambda_i$就是特征$f_i$的权重，越大表示特征越重要。 6.1.2 最大似然估计得到对偶问题的内层极小值问题的解之后，接着求解外层的极大值问题$\underset {\lambda} {max} \ \Psi(\lambda)$。 设其解为 \lambda^* = \underset {\lambda} {argmax} \ \Psi(\lambda) \tag{6-8}则最大熵模型的解为 p^*=p_{\lambda^*} \tag{6-9}根据推导，最大化$\Psi(\lambda)$与最大似然估计是等价的！ 7、最优化方法通用的方法有梯度下降，拟牛顿法等，最大熵模型有两个量身定做的方法：通用迭代尺度法（Generalized Iterative Scaling，GIS）和改进的迭代尺度法（Impoved Iterative Scaling，IIS）。 7.1 GIS算法 算法1： S1：初始化参数，令$\lambda=0$ S2：计算$E_{\tilde p}(f_i),\ i=1,2,…,n$ S3：执行一次迭代，对参数做一次刷新。 ​ 计算$E{p{\lambda}}(f_i)$ ​ FOR i=1,2,…,n DO { ​ $\lambdai\ += \ \eta \log\frac {E{\tilde p}(fi)} {E{p_{\lambda}}(f_i)}$ ​ } S4：检查是否收敛，若未收敛则继续S3 其中，$\eta$是学习率，在实际中取$\frac {1} {C}$，$$，表示训练数据中包含特征最多的那个样本所包含的特征个数。 \Delta\lambda_i=\eta \log\frac {E_{\tilde p}(f_i)} {E_{p_{\lambda}}(f_i)}是校正量。 每次迭代，先用当前的权重估算每个特征$fi$在训练数据中的概率分布的期望，然后逐个与相应的经验分布的期望比较，其偏差程度通过$\log\frac {E{\tilde p}(fi)} {E{p_{\lambda}}(f_i)}$来进行刻画。 收敛条件就是当两次迭代的$\lambda$在一个较小的范围。 GIS每次迭代时间很长，不太稳定，容易溢出，一般不会使用。 7.2 IIS算法与GIS的不同主要在$\Delta\lambda_i$的计算上。IIS通过求解方程 \sum_{x,y} \tilde p(x)p(y|x)f_i(x,y)\exp(\Delta\lambda_i\sum_{i=1}^nf_i(x,y))=\tilde p(f_i)1）若$\sum{i=1}^nf_i(x,y)$为常数，即对任意样本(x,y)，都有$\sum{i=1}^nf_i(x,y)=C$，则 \Delta\lambda_i=\frac {1} {C} \log\frac {E_{\tilde p}(f_i)} {E_{p_{\lambda}}(f_i)}此时，IIS可以看做是GIS的一种推广。 2）若$\sum_{i=1}^nf_i(x,y)$不是常数，则需要通过数值方式来求解$\Delta\lambda_i$，如牛顿法。 8、优缺点优点是：在建模时，只需要集中精力选取特征，不需要花费精力考虑如何使用这些特征，可以灵活使用不同类型的特征。 缺点是计算量大。 参考 【1】 最大熵学习笔记 【2】统计学习方法]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>最大熵</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python笔记]]></title>
    <url>%2F2017%2F07%2F12%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FPYTHON%2Fpython%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[json操作1.1 json转字符串1result = json.loads(s) 1.2 遍历json的key将json当做dict，用dict的方法遍历 12for k,v in result.items(): print result[k] 1.3 读json文件1234567import jsonf = open(&quot;/home/david/keywordjson&quot;)result = json.load(f,encoding=&apos;utf-8&apos;)for k,v in result.items(): print result[k] 文件操作2.1 读CSV文件csv文件的格式如 12id,click,hour,C1,C2,C3,C4,C5123,1,14091123,a,b,c,d,e 读取的方式为： 12345from csv import DictReader# t是每行的index，row是每行的具体数据，dict型for t, row in enumerate(DictReader(open(path))): ID = row[&apos;id&apos;] 2.2 按行读普通文件12345678910for line in open('../result/ctrout-featurelist-1-10.dat'): #要去掉换行符 arr = line.strip('\n').split(',') if len(arr) &gt; 0: try: prectr.append(float(arr[0])) onlinectr.append(float(arr[1])) isclick.append(1 if arr[2]=='true' else 0) except ValueError: continue 上述方法不够严谨，为防止读取时出错，应该用 1234567file = open(filePath) try: tempSet = set() for line in file: tempSet.add(line) finally: file.close() 内置函数3.1 hash返回对象的hash值，返回的哈希值是使用一个整数表示，通常使用在字典里，以便实现快速查询键值。参数object输入是数字类型时，是根据数值来计算的，比如1和1.0计算出来是一样的哈希值，因此说这个函数是不区分不同的数值类型。 日期操作12345678910111213141516171819202122232425import datetimedstr = '20170228'd2 = datetime.datetime.strptime(dstr,'%Y%m%d') + datetime.timedelta(days=1)d2str = d2.strftime('%Y%m%d')结果是20170301获取当前日期并格式化import timetime.strftime('%Y%m%d',time.localtime(time.time()))time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time()))日期加减#datetime.timedelta(days, seconds, microseconds)d1 = datetime.date.today()加一天：d2 = d1 + datetime.timedelta(1)減一天：d2 = d1 + datetime.timedelta(-1)当天日期的格式化datetime.date.today().strftime('%Y%m%d') 字符串字符串拼接1234runToday=date.today().strftime(&apos;%y-%m-%d&apos;)b=time.strptime(runToday,&apos;%y-%m-%d&apos;)btime = datetime(*b[:3])&quot;,&quot;.join([&quot;/shortdata/persona/xid_present_info/&quot; + (btime - timedelta(x)).strftime(&apos;%y-%m-%d&apos;) for x in range(2,13)]) 1&apos;/shortdata/persona/xid_present_info/17-05-08,/shortdata/persona/xid_present_info/17-05-07,/shortdata/persona/xid_present_info/17-05-06,/shortdata/persona/xid_present_info/17-05-05,/shortdata/persona/xid_present_info/17-05-04,/shortdata/persona/xid_present_info/17-05-03,/shortdata/persona/xid_present_info/17-05-02,/shortdata/persona/xid_present_info/17-05-01,/shortdata/persona/xid_present_info/17-04-30,/shortdata/persona/xid_present_info/17-04-29,/shortdata/persona/xid_present_info/17-04-28&apos; 字符串切割数字除法后转float12# 加个点即可scale = state.shape[1] / 720. 循环12345678documents = ["Human machine interface for lab abc computer applications", "A survey of user opinion of computer system response time", "The EPS user interface management system", "System and human system engineering testing of EPS"]stoplist = set('for a of the and to in'.split())texts = [[word for word in document.lower().split() if word not in stoplist] for document in documents] hdfs操作https://snakebite.readthedocs.io/en/latest/client.html 代码规范定义类123456class model_evaluation(object): def __init__(self, prectrArr, onlinectrArr, isclickArr): self.prectrArr = prectrArr self.onlinectrArr = onlinectrArr self.isclickArr = isclickArr main函数123456def main(argv=None): if argv == None: argv = sys.argv if __name__ == &quot;__main__&quot;: main() nohup执行nohup python -u download_bing_api.py &gt;&gt; result.log &amp; 可以输出print的内容 max初级技巧 12tmp = max(1,2,4)print(tmp) 1234#可迭代对象a = [1, 2, 3, 4, 5, 6]tmp = max(a)print(tmp) 中级技巧：key属性的使用 当key参数不为空时，就以key的函数对象为判断的标准。如果我们想找出一组数中绝对值最大的数，就可以配合lamda先进行处理，再找出最大值 123a = [-9, -8, 1, 3, -4, 6]tmp = max(a, key=lambda x: abs(x))print(tmp) 高级技巧：找出字典中值最大的那组数据 如果有一组商品，其名称和价格都存在一个字典中，可以用下面的方法快速找到价格最贵的那组商品： 12345678910prices = &#123; &apos;A&apos;:123, &apos;B&apos;:450.1, &apos;C&apos;:12, &apos;E&apos;:444,&#125;# 在对字典进行数据操作的时候，默认只会处理key，而不是value# 先使用zip把字典的keys和values翻转过来，再用max取出值最大的那组数据max_prices = max(zip(prices.values(), prices.keys()))print(max_prices) # (450.1, &apos;B&apos;) 画图画圆 12345678import numpy as np import matplotlib.pyplot as plt from matplotlib.patches import Polygon import matplotlib.patches as mpatches for i in range(0,r,600): plt.plot(np.cos(t)*i, np.sin(t)*i)plt.show() 去掉坐标轴 123ax=plt.subplot(111)ax.set_xticks([]) ax.set_yticks([]) 进程subprocess运行python的时候，我们都是在创建并运行一个进程。像Linux进程那样，一个进程可以fork一个子进程，并让这个子进程exec另外一个程序。在Python中，我们通过标准库中的subprocess包来fork一个子进程，并运行一个外部的程序。 如 1cat = subprocess.Popen([&quot;hadoop&quot;, &quot;fs&quot;, &quot;-du&quot;, abspath], stdout=subprocess.PIPE) Popen对象创建后，主程序不会自动等待子进程完成。我们必须调用对象的wait()方法，父进程才会等待 (也就是阻塞block)，举例： 123&gt;&gt;&gt; import subprocess&gt;&gt;&gt; child = subprocess.Popen([&apos;ping&apos;,&apos;-c&apos;,&apos;4&apos;,&apos;blog.linuxeye.com&apos;])&gt;&gt;&gt; print &apos;parent process&apos; 从运行结果中看到，父进程在开启子进程之后并没有等待child的完成，而是直接运行print。对比等待的情况: 1234&gt;&gt;&gt; import subprocess&gt;&gt;&gt; child = subprocess.Popen(&apos;ping -c4 blog.linuxeye.com&apos;,shell=True)&gt;&gt;&gt; child.wait()&gt;&gt;&gt; print &apos;parent process&apos; 从运行结果中看到，父进程在开启子进程之后并等待child的完成后，再运行print。此外，你还可以在父进程中对子进程进行其它操作，比如我们上面例子中的child对象: 1234child.poll() # 检查子进程状态child.kill() # 终止子进程child.send_signal() # 向子进程发送信号child.terminate() # 终止子进程 子进程的PID存储在child.pid，子进程的标准输入、标准输出和标准错误如下属性分别表示: 123child.stdinchild.stdoutchild.stderr Python模块安装方法一、方法1： 单文件模块直接把文件拷贝到 $python_dir/Lib 二、方法2： 多文件模块，带setup.py 下载模块包，进行解压，进入模块文件夹，执行：python setup.py install 三、 方法3：easy_install 方式 先下载ez_setup.py,运行python ez_setup 进行easy_install工具的安装，之后就可以使用easy_install进行安装package了。easy_install packageNameeasy_install package.egg 四、 方法4：pip 方式 先进行pip工具的安裝：easy_install pip（pip 可以通过easy_install 安裝，而且也会装到 Scripts 文件夹下。） 安裝：pip install PackageName 更新：pip install -U PackageName 移除：pip uninstall PackageName 搜索：pip search PackageName 帮助：pip help 输出日志123456789101112131415161718192021logger = logging.getLogger("eventToKafka")logger.setLevel(logging.DEBUG)# 建立一个filehandler来把日志记录在文件里，级别为debug以上fh = logging.FileHandler("/home/hdbatch/fh.log")fh.setLevel(logginng.DEBUG)# 建立一个streamhandler来把日志打在CMD窗口上，级别为error以上ch = logging.StreamHandler()ch.setLevel(logginng.ERROR)# 设置日志格式formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")ch.setFormatter(formatter)fh.setFormatter(formatter)#将相应的handler添加在logger对象中logger.addHandler(ch)logger.addHandler(fh)# 开始打日志logger.debug("debug message")logger.info("info message")logger.warn("warn message")logger.error("error message")logger.critical("critical message") 输出到excel1234567891011121314151617181920212223242526import xlwtfrom datetime import datetime style0 = xlwt.easyxf(&apos;font: name Times New Roman, color-index red, bold on&apos;,num_format_str=&apos;#,##0.00&apos;)style1 = xlwt.easyxf(num_format_str=&apos;D-MMM-YY&apos;) wb = xlwt.Workbook(encoding=&apos;utf-8&apos;)ws = wb.add_sheet(&apos;A Test Sheet&apos;)ln = 1for line in open(&apos;/home/david/taxo.txt&apos;): arr = line.strip(&apos;\n&apos;).split(&apos;|&apos;) if len(arr) &gt; 0: try: print arr[0] ws.write(ln,0,arr[0]) print arr[1] ws.write(ln,1,arr[1]) print arr[2] ws.write(ln,2,arr[2]) ln += 1 except ValueError: print ValueError.message continue wb.save(&apos;/home/david/example.xls&apos;) latin1转utf8很多语料库下载后都是latin1编码，是乱码 判断变量是否为None123456789三种主要的写法有：第一种：if X is None;第二种：if not X；当X为None, False, 空字符串&quot;&quot;, 0, 空列表[], 空字典&#123;&#125;, 空元组()这些时，not X为真，即无法分辨出他们之间的不同。第三种：if not X is None; 在Python中，None、空列表[]、空字典{}、空元组()、0等一系列代表空和无的对象会被转换成False。除此之外的其它对象都会被转化成True。 在命令if not 1中，1便会转换为bool类型的True。not是逻辑运算符非，not 1则恒为False。因此if语句if not 1之下的语句，永远不会执行。 对比：foo is None 和 foo == None 示例： 123456789&gt;&gt;&gt; class Foo(object): def __eq__(self, other): return True&gt;&gt;&gt; f = Foo()&gt;&gt;&gt; f == NoneTrue&gt;&gt;&gt; f is NoneFalse 输入参数判断12345678import argparseparser = argparse.ArgumentParser()parser.add_argument(&quot;-v&quot;, &quot;--verbosity&quot;, help=&quot;increase output verbosity&quot;)args = parser.parse_args()if args.verbosity: print &quot;verbosity turned on&quot; 一种是通过一个-来指定的短参数，如-h； 一种是通过--来指定的长参数，如--help 这两种方式可以同存，也可以只存在一个。通过解析后，其值保存在args.verbosity变量中用法如下： 1234567891011121314151617yarving@yarving-VirtualBox /tmp $ python prog.py -v 1verbosity turned onyarving@yarving-VirtualBox /tmp $ python prog.py --verbosity 1verbosity turned onyarving@yarving-VirtualBox /tmp $ python prog.py -h usage: prog.py [-h] [-v VERBOSITY]optional arguments: -h, --help show this help message and exit -v VERBOSITY, --verbosity VERBOSITY increase output verbosityyarving@yarving-VirtualBox /tmp $ python prog.py -v usage: prog.py [-h] [-v VERBOSITY]prog.py: error: argument -v/--verbosity: expected one argument 或者这样使用 12345678910parser = argparse.ArgumentParser() parser.add_argument('--phone', default='Android', choices=['Android', 'IOS'], type=str, help='mobile phone OS') parser.add_argument('--sensitivity', default=2.045, type=float, help='constant for press time') parser.add_argument('--serverURL', default='http://localhost:8100', type=str, help='ServerURL for wda Client') parser.add_argument('--resource', default='resource', type=str, help='resource dir') parser.add_argument('--debug', default=None, type=str, help='debug mode, specify a directory for storing log files.') args = parser.parse_args() # print(args) AI = WechatAutoJump(args.phone, args.sensitivity, args.serverURL, args.debug, args.resource) type定义了输入的数据类型，如果不符合会报错 后面也可以给args增加新的参数 1args.vocab_size=50 搜索指定目录下的文件123import glob# 搜索目录下的所有png文件，返回的是所有路径的列表glob.glob(os.path.join(self.resource_dir, &apos;circle/*.png&apos;)) 复制文件12import shutilshutil.copyfile(&apos;state.png&apos;, os.path.join(self.debug, &apos;state_&#123;:03d&#125;.png&apos;.format(self.step))) codecs普通的open打开会有编码问题，用codecs.open打开后，再写入数据就不会有问题 https://www.cnblogs.com/buptldf/p/4805879.html 这种方法可以指定一个编码打开文件，使用这个方法打开的文件读取返回的将是unicode。写入时，如果参数 是unicode，则使用open()时指定的编码进行编码后写入；如果是str，则先根据源代码文件声明的字符编码，解码成unicode后再进行前述 操作。相对内置的open()来说，这个方法比较不容易在编码上出现问题。 12with codecs.open(input_file, &quot;r&quot;, encoding=self.encoding) as f: data = f.read() 这里的read就是读取整个文本，换行符为\r\n with12with codecs.open(input_file, &quot;r&quot;, encoding=self.encoding) as f: data = f.read() 使用with后不管with中的代码出现什么错误，都会进行对当前对象进行清理工作。 例如file的file.close()方法，无论with中出现任何错误，都会执行file.close（）方法 with语句类似 try : except: finally: 的功能：但是with语句更简洁。而且更安全。代码量更少。 collectionscollections模块自Python 2.4版本开始被引入，包含了dict、set、list、tuple以外的一些特殊的容器类型，分别是： OrderedDict类：排序字典，是字典的子类。引入自2.7。 namedtuple()函数：命名元组，是一个工厂函数。引入自2.6。 Counter类：为hashable对象计数，是字典的子类。引入自2.7。 deque：双向队列。引入自2.4。 defaultdict：使用工厂函数创建字典，使不用考虑缺失的字典键。引入自2.5。 CounterCounter类的目的是用来跟踪值出现的次数。它是一个无序的容器类型，以字典的键值对形式存储，其中元素作为key，其计数作为value。计数值可以是任意的Interger（包括0和负数）。Counter类和其他语言的bags或multisets很相似。 1c = collections.Counter(&apos;sdfsadfag&apos;) Counter({‘a’: 2, ‘s’: 2, ‘d’: 2, ‘f’: 2, ‘g’: 1}) counter对象的常用操作 123456789sum(c.values()) # 所有计数的总数c.clear() # 重置Counter对象，注意不是删除list(c) # 将c中的键转为列表set(c) # 将c中的键转为setdict(c) # 将c中的键值对转为字典c.items() # 转为(elem, cnt)格式的列表Counter(dict(list_of_pairs)) # 从(elem, cnt)格式的列表转换为Counter类对象c.most_common()[:-n:-1] # 取出计数最少的n-1个元素c += Counter() # 移除0和负值 sorted默认是升序 1234567&gt;&gt;&gt; L=[(&apos;b&apos;,2),(&apos;a&apos;,1),(&apos;c&apos;,3),(&apos;d&apos;,4)]&gt;&gt;&gt; sorted(L, cmp=lambda x,y:cmp(x[1],y[1])) # 利用cmp函数[(&apos;a&apos;, 1), (&apos;b&apos;, 2), (&apos;c&apos;, 3), (&apos;d&apos;, 4)]&gt;&gt;&gt; sorted(L, key=lambda x:x[1]) # 利用key[(&apos;a&apos;, 1), (&apos;b&apos;, 2), (&apos;c&apos;, 3), (&apos;d&apos;, 4)]# 变成降序sorted(L, key=lambda x:-x[1]) zip123456789&gt;&gt;&gt; a = [1,2,3]&gt;&gt;&gt; b = [4,5,6]&gt;&gt;&gt; c = [4,5,6,7,8]&gt;&gt;&gt; zipped = zip(a,b) # 打包为元组的列表[(1, 4), (2, 5), (3, 6)]&gt;&gt;&gt; zip(a,c) # 元素个数与最短的列表一致[(1, 4), (2, 5), (3, 6)]&gt;&gt;&gt; zip(*zipped) # 与 zip 相反，可理解为解压，返回二维矩阵式[(1, 2, 3), (4, 5, 6)] 持久化http://blog.csdn.net/bh20077/article/details/6070278 如果希望透明地存储 Python 对象，而不丢失其身份和类型等信息，则需要某种形式的对象序列化：它是一个将任意复杂的对象转成对象的文本或二进制表示的过程。同样，必须能够将对象经过序列化后的形式恢复到原有的对象。在 Python 中，这种序列化过程称为 pickle，可以将对象 pickle 成字符串、磁盘上的文件或者任何类似于文件的对象，也可以将这些字符串、文件或任何类似于文件的对象 unpickle 成原来的对象。 12345678# 写入# 这里的chars是字符的列表with open(vocab_file, 'wb') as f: cPickle.dump(chars, f)# 读取with open(vocab_file, 'rb') as f: self.chars = cPickle.load(f) mapmap() 会根据提供的函数对指定序列做映射。 第一个参数 function 以参数序列中的每一个元素调用 function 函数，返回包含每次 function 函数返回值的新列表。 1234567891011&gt;&gt;&gt;def square(x) : # 计算平方数... return x ** 2... &gt;&gt;&gt; map(square, [1,2,3,4,5]) # 计算列表各个元素的平方[1, 4, 9, 16, 25]&gt;&gt;&gt; map(lambda x: x ** 2, [1, 2, 3, 4, 5]) # 使用 lambda 匿名函数[1, 4, 9, 16, 25] # 提供了两个列表，对相同位置的列表数据进行相加&gt;&gt;&gt; map(lambda x, y: x + y, [1, 3, 5, 7, 9], [2, 4, 6, 8, 10])[3, 7, 11, 15, 19] 记录shell执行的结果如果用os.system执行shell命令，不会保存执行的结果，要保存结果，可以用 12345678city_list = [] cmd_out = subprocess.Popen( " hadoop fs -ls /shortdata/mobile_poi_raw/day_id=%s|awk -F'city=' 'NF==2&#123;print $2&#125;'" % (self.today), stdout=subprocess.PIPE, shell=True) for line in cmd_out.stdout: line = line.strip() if len(line) &gt; 0: city_list.append(line) 其中， shell=True如果 args 是字符串，它将作为命令行字符串通过shell 执行．如果是一个序列， 它的第一个条目将作为命令行字符串，后面的条目作为附加的shell参数。 stdout=subprocess.PIPE如果stdout=PIPE,这个属性是个文件对象，提供子进程的输出，否则它是None 这样cmd_out就可以保存shell的结果 统计程序的执行时间（python3）1234567x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]### VECTORIZED DOT PRODUCT OF VECTORS ###tic = time.process_time()dot = np.dot(x1,x2)toc = time.process_time()print ("dot = " + str(dot) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms") pass如果初始化的时候不需要什么，可以写pass当占位符。 1234class NaiveBayes(object): def __init__(self): pass]]></content>
      <categories>
        <category>PYTHON</category>
      </categories>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive笔记]]></title>
    <url>%2F2017%2F07%2F12%2Fhadoop-spark%2Fhive%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[查询时如何去掉重复数据假设数据为 1234name adx tran_id cost tsck 5 125.168.10.0 33.00 1407234660ck 5 187.18.99.00 33.32 1407234661ck 5 125.168.10.0 33.24 1407234661 1select * from (select *,row_number() over (partition by tran_id order by timestamp asc) num from table) t where t.num=1; 附上：ROW_NUMBER() OVER函数的基本用法 语法：ROW_NUMBER() OVER(PARTITION BY COLUMN ORDER BY COLUMN) 简单的说row_number()从1开始，为每一条分组记录返回一个数字，这里的ROW_NUMBER() OVER (ORDER BY xlh DESC) 是先把xlh列降序，再为降序以后的没条xlh记录返回一个序号。示例： 123456&gt; xlh row_num &gt; 1700 1 &gt; 1500 2 &gt; 1085 3 &gt; 710 4 &gt; &gt; row_number() OVER (PARTITION BY COL1 ORDER BY COL2) 表示根据COL1分组，在分组内部根据 COL2排序，而此函数计算的值就表示每组内部排序后的顺序编号（组内连续的唯一的) 查询ES表报错1Failed with exception java.io.IOException:org.elasticsearch.hadoop.rest.EsHadoopInvalidRequest: The number of slices [1126] is too large. It must be less than [1024]. This limit can be set by changing the [index.max_slices_per_scroll] index level settin 修改es的设置 1234567891011PUT /megacorp/_settings&#123; &quot;index&quot;: &#123; &quot;max_slices_per_scroll&quot; : 1126 &#125;&#125; 切换队列1set mapred.job.queue.name=data; sqoop切换队列是 1-D mapred.job.queue.name=data]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最长公共子串]]></title>
    <url>%2F2017%2F07%2F12%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E4%B8%B2%2F</url>
    <content type="text"><![CDATA[找出最长子串，需要知道子串的起始位置和子串的长度。 因此，维护一个二维数组，存放两个字符串的字符关系，再创建两个变量存放index和maxLen。 123456789101112131415161718192021222324252627282930313233public static String findLcsNew(String str1, String str2) &#123; int len1, len2; len1 = str1.length(); len2 = str2.length(); int maxLen = 0, index = 0; int[][] arr = new int[len1 + 1][len2 + 1]; for (int i = 0; i &lt; len1; i++) &#123; for (int j = 0; j &lt; len2; j++) &#123; if ( i == 0 || j == 0 ) &#123; // 第一行和第一列都是0 arr[i][j] = 0; &#125; else &#123; // 该字符和上一个字符均相等时 if (str1.charAt(i) == str2.charAt(j) &amp;&amp; str1.charAt(i-1) == str2.charAt(j-1)) &#123; arr[i][j] = arr[i-1][j-1] + 1; &#125; else &#123; arr[i][j] = 0; &#125; &#125; if (arr[i][j] &gt; maxLen) &#123; maxLen = arr[i][j]; index = i; &#125; &#125; &#125; String newStr = str1.substring(index - maxLen, index + 1); return newStr; &#125; 也可以用一维数组实现，同时可以记录多个相同长度的最长子串 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public static String findLcs(String str1, String str2) &#123; StringBuffer buff = new StringBuffer(); int i, j; int len1, len2; len1 = str1.length(); len2 = str2.length(); int maxLen = len1 &gt; len2 ? len1 : len2; int[] max = new int[maxLen]; int[] maxIndex = new int[maxLen]; int[] c = new int[maxLen]; for (i = 0; i &lt; len2; i++) &#123; for (j = len1 - 1; j &gt;= 0; j--) &#123; if (str2.charAt(i) == str1.charAt(j)) &#123; if ((i == 0) || (j == 0)) c[j] = 1; else c[j] = c[j - 1] + 1; &#125; else &#123; c[j] = 0; &#125; if (c[j] &gt; max[0]) &#123; // 如果是大于那暂时只有一个是最长的,而且要把后面的清0; max[0] = c[j]; maxIndex[0] = j; for (int k = 1; k &lt; maxLen; k++) &#123; max[k] = 0; maxIndex[k] = 0; &#125; &#125; else if (c[j] == max[0]) &#123; // 有多个是相同长度的子串 for (int k = 1; k &lt; maxLen; k++) &#123; if (max[k] == 0) &#123; max[k] = c[j]; maxIndex[k] = j; break; // 在后面加一个就要退出循环了 &#125; &#125; &#125; &#125; &#125; for (j = 0; j &lt; maxLen; j++) &#123; if (max[j] &gt; 0) &#123; for (i = maxIndex[j] - max[j] + 1; i &lt;= maxIndex[j]; i++) buff.append(str1.charAt(i)); &#125; &#125; return buff.toString();&#125;]]></content>
      <categories>
        <category>算法与数据结构</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vim命令]]></title>
    <url>%2F2017%2F07%2F12%2F%E5%B7%A5%E5%85%B7%E5%92%8C%E7%8E%AF%E5%A2%83%2Fvim%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[查找并删除:g/要删除的内容/d 查找 /要查找的 按n就是下一个 删除一行 dd 查找匹配的个数 :%s/refering_site/&amp;/gn 或 :%s/17779//gn :%s/“opxcreativeid”:16650//gn 查找并替换%s/源字符串/目的字符串/g 如 :%s/\/home\/weinan/\/opt\/pig_home\/bshare_etl/g :%s/gpadmin/gpxmo/g :%s/\t-1/\t1/g :%s/“//g :%s/16-06-28/16-06-29/g 多行变1行大写V选中行+shift J 替换每行的行首、行尾12:1,$ s/^/HELLO/g:1,$ s/$/WORLD/g sh文件的编码转成unix查看用:set fileformat 修改用:set fileformat=unix 编码从latin1转成utf8:e ++enc=cp936:set fileencoding=utf-8]]></content>
      <categories>
        <category>工具和环境</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scp免密码登录]]></title>
    <url>%2F2017%2F07%2F12%2F%E5%B7%A5%E5%85%B7%E5%92%8C%E7%8E%AF%E5%A2%83%2Fscp%E5%85%8D%E5%AF%86%E7%A0%81%E7%99%BB%E5%BD%95%2F</url>
    <content type="text"><![CDATA[假设A要免密码传输文件到B 在A上创建秘钥 1ssh-keygen -t rsa 拷贝id_rsa.pub到B的.ssh，并改名为authorized_keys 注意要修改权限 A机器 .ssh目录，以及/home/当前用户 需要700权限，参考以下操作调整 sudo chmod 700 ~/.ssh sudo chmod 700 /home/当前用户 B机器 .ssh目录下的authorized_keys文件需要600或644权限，参考以下操作调整 sudo chmod 600 ~/.ssh/authorized_keys]]></content>
      <categories>
        <category>工具和环境</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark笔记-新词发现]]></title>
    <url>%2F2017%2F07%2F12%2Fhadoop-spark%2Fspark%E7%AC%94%E8%AE%B0-%E6%96%B0%E8%AF%8D%E5%8F%91%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[12val conf = new SparkConf().setAppName("wordSegname").setMaster("local[4]").set("spark.sql.shuffle.partitions","10").set("spark.network.timeout","30s") local[4]是指在本地运行，用4核CPU。 spark.sql.shuffle.partitions是指partition的数量。SparkSQL在运行时，将一个查询任务分解成多个task，一个task就是一个partition。默认是200个partition，而如果实际集群只能并行3个task，则跑完200个partition要200/3=67次。 spark.network.timeout是指所有网络通信的超时时间，默认是120s 123456789val word1=sc.textFile(path).map&#123;x=&gt; val x_filter=x.replaceAll("\p&#123;Punct&#125;", " ").replaceAll("\pP", " ") .replaceAll(" ", " ").replaceAll("[" + AtomsUitl.stopwords + "]", " ").replaceAll("\p&#123;Blank&#125;", " ").replaceAll("\p&#123;Space&#125;", " ").replaceAll("\p&#123;Cntrl&#125;", " ") x_filter &#125; replaceAll中是正则表达式。上文中，是将所有的特殊字符都用空格代替 AtomsUitl.stopwords停用词是”的很了么呢是嘛个都也比还这于不与才上用就好在和对挺去后没说” sc.textFile读取文件后，生成一个RDD，以行为单位，所以后面的map是对每行的操作 打印出所有的元素，用 word1.foreach { x =&gt; println(x) } 12345678val word_document = word1.zipWithIndex.filter &#123; x =&gt; !StringUtils.isBlank(x._1) &#125;.flatMap &#123; x =&gt; val arr = ArrayBuffer(String, Int) val line = x._1.split(" ") //对于每一行，都用空格分割 for (i &lt;- line) &#123; arr += ((i, x._2.toInt)) //分割后，每一个tuple加到数组中 &#125; arr &#125;.map &#123; x =&gt; (x.1.trim, x.2) &#125;.filter(x =&gt; !StringUtils.isBlank(x._1)) zipWithIndex用带有index的来压缩RDD，索引从0开始 word1.zipWithIndex.foreach { x =&gt; println(x) } (ab ef,0) (cd,1) 上述代码得到的结果是 (ab,0) (ef,0) (cd,1) 12345678val wordleft = word.map(x =&gt; AtomsUitl.reverse(x)).map &#123; x =&gt; "" + x + "" &#125;.flatMap &#123; x =&gt; var arr = ArrayBufferString for (y &lt;- 1 to AtomsUitl.len(x) - 2) &#123; arr += AtomsUitl.substring(x, y, Math.min(maxLen + y, AtomsUitl.len(x))) &#125; arr&#125;.sortBy(x =&gt; x) 将每个句子倒序排列，提取每个子集 今$ 四期星天今 处言语然自 天今$ 星天今$ 期星天今$ 然自$ 理处言语然 自$ 言语然自 语然自$ $ 12345678val wordleft_caculate = wordleft.map &#123; s =&gt; val first = AtomsUitl.substring(s, 0, 1).toString (first, s) &#125;.groupBy(f =&gt; f._1).map &#123; x =&gt; x._2 &#125;wordleft_caculate.foreach&#123;x=&gt; println(x.iterator.next())&#125; groupBy之后得到 (期, CompactBuffer((期,期星天今$))等 这个是Iterable，可迭代的。可以转换为一个迭代器x.iterator.next()。迭代出来就是 (期,期星天今$) 等]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>新词发现</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux命令]]></title>
    <url>%2F2017%2F07%2F12%2F%E5%B7%A5%E5%85%B7%E5%92%8C%E7%8E%AF%E5%A2%83%2Flinux%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[各种压缩解压缩rar 解压缩 unrar e XXX.rar 7z 7za x word2vec_c_from_weixin.7z -r -o./ 注意-o和后面的路径之间没有空格 查看gz压缩文件行数 zcat *.gz | wc -l tar 压缩 tar -czvf tracking_ingest_BJIDCnopig.tar.gz tracking_ingest_BJIDCnopig 解压缩 tar -zxvf greenplum-db-4.2.3.1.tar -C /usr/local/ gz gunzip -c gp_dump_1_1_20160806093243.gz &gt; gp_dump_1_1_20160806093243 zip unzip archive_name.zip rar 安装：sudo apt install unrar卸载：sudo apt-get remove unrar rar常用命令主要有:e 将文件解压到当前目录 例:rar e test.rar​ 注:用e解压的话，不仅原来的file1.txt和file2.txt被解压到当前目录，就连dir1里面的所有文件​ 也被解压到当前目录下，不能保持压缩前的目录结构，如果想保持压缩前的目录结构，用x解压x 带路径解压文档中内容到当前目录 例:rar x test.rar​ 这样解压的话，dir1就会保持原来的目录结构 查看OS版本lsb_release -a 看某个端口是否在使用netstat -tunlp |grep 9527 统计指定文件的大小du -c -h adgroup.BJ1.20160623* 看linux某个软件的版本rpm -qa | grep mapr 看文件的指定行 sed -n ‘5,10p’ filename 这样你就可以只查看文件的第5行到第10行。 文件链接ln -s greenplum-db-4.2.3.1/ greenplum-db 停止crontab服务这个命令在red hat当中常用,有的linux发行版本中没有这个命令.$ service crond start //启动服务$ service crond stop //关闭服务$ service crond restart //重启服务 2.linux发行版本没有service这个命令时：/etc/init.d/cron stop/etc/init.d/cron start cronjob路径 /var/spool/cron/ 查看已安装版本号sudo apt-get install apt-show-versions 用apt-show-versions查看 若查看单个软件包的版本 apt-show-versions –p 查看可升级的软件包 apt-show-versions –u centos中查看已安装的包 1yum list installed |grep mysql 查看可以安装的包 yum install mysql mysql-server mysql-devel 指定文件大小总和du -m 201604 | awk ‘{sum += $1}; END{print sum}’ 查看目录结构tree -a 文件夹搜索文件find . -name “*.py” dpkgdpkg命令常用格式如下：sudo dpkg -I iptux.deb#查看iptux.deb软件包的详细信息，包括软件名称、版本以及大小等（其中-I等价于—info）sudo dpkg -c iptux.deb#查看iptux.deb软件包中包含的文件结构（其中-c等价于—contents）sudo dpkg -i iptux.deb#安装iptux.deb软件包（其中-i等价于—install）sudo dpkg -l iptux#查看iptux软件包的信息（软件名称可通过dpkg -I命令查看，其中-l等价于—list）sudo dpkg -L iptux#查看iptux软件包安装的所有文件（软件名称可通过dpkg -I命令查看，其中-L等价于—listfiles）sudo dpkg -s iptux#查看iptux软件包的详细信息（软件名称可通过dpkg -I命令查看，其中-s等价于—status）sudo dpkg -r iptux#卸载iptux软件包（软件名称可通过dpkg -I命令查看，其中-r等价于—remove） #清空文件 > filename #linux 匹配tab ctrl+M+tab #在行首添加字符 sed ‘s/^/HEAD&amp;/g’ test.file #在行尾添加字符 sed ‘s/$/&amp;TAIL/g’ test.file 查找文件夹最近修改的文件查找最近30分钟修改的当前目录下的.php文件 1find . -name &apos;*.php&apos; -mmin -30 查找最近24小时修改的当前目录下的.php文件 1find . -name &apos;*.php&apos; -mtime 0 查找最近24小时修改的当前目录下的.php文件，并列出详细信息 1find . -name &apos;*.inc&apos; -mtime 0 -ls 查找当前目录下，最近24-48小时修改过的常规文件。 1find . -type f -mtime 1 查找当前目录下，最近1天前修改过的常规文件。 1find . -type f -mtime +1 删除 find . -name “*.pyc” | xargs rm ping端口telnet 1.1.1.1 8080 文件去重sort -k2n file | uniq &gt; a.out 当file中的重复行不再一起的时候，uniq没法删除所有的重复行。经过排序后，所有相同的行都在相邻，因此uniq可以正常删除重复行。 统计文件夹大小并排序du -sh * | sort -rn | head -5]]></content>
      <categories>
        <category>工具和环境</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac技巧]]></title>
    <url>%2F2017%2F07%2F12%2F%E5%B7%A5%E5%85%B7%E5%92%8C%E7%8E%AF%E5%A2%83%2FMac%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[卸载jdk9安装jdk8brew默认安装的jdk9带来了一些麻烦， 比如mvn java和scala混编项目会报错 1error: error while loading package, Missing dependency &apos;object java.lang.Object in compiler mirror&apos;, required by /Users/david/david/.m2/repository/org/scala-lang/scala-library/2.10.5/scala-library-2.10.5.jar(scala/package.class) 于是打算卸载并换回jdk8 安装Java： 1brew cask install java //命令安装的是最新的Java9，我需要的是Java8…… 卸载Java9: 12345678ls /Library/Java/JavaVirtualMachines/ //查看jdk版本//卸载sudo rm -rf /Library/Java/JavaVirtualMachines/jdk-9.0.4.jdk //此处9.0.1即是上一步查看到的版本号sudo rm -fr /Library/Internet\ Plug-Ins/JavaAppletPlugin.plugin sudo rm -fr /Library/PreferencePanes/JavaControlPanel.prefPane sudo rm -fr ~/Library/Application\ Support/Java 安装Java8: 12brew tap caskroom/versionsbrew cask install java8 安装eclipse： 1brew cask install eclipse-ide 配置Java环境变量： 12345678910111213141516/usr/libexec/java_home //定位JAVA_HOME位置 Matching Java Virtual Machines (1): 1.8.0_121, x86_64: &quot;Java SE 8&quot; /Library/Java/JavaVirtualMachines/jdk1.8.0_152.jdk/Contents/Homevi ~/.bash_profile //编辑profile文件//按i键，输入以下代码JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_152.jdk/Contents/HomePATH=$JAVA_HOME/bin:$PATHCLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar//然后按esc键，输入 :wq 保存并退出profile文件。 之后执行： 123source ~/.base_profile echo ~/.base_profile 查看环境变量是否配置成功： 123$ echo $JAVA_HOME/Library/Java/JavaVirtualMachines/jdk1.8.0_152.jdk/Contents/Home Finder复制路径https://www.jianshu.com/p/757f9ffc5acf]]></content>
      <categories>
        <category>工具和环境</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[网页关键词提取]]></title>
    <url>%2F2017%2F07%2F12%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E7%BD%91%E9%A1%B5%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96%2F</url>
    <content type="text"><![CDATA[jsoup解析html的DOM1Document doc =Jsoup.connect(url).userAgent(&quot;Mozilla&quot;).get(); 提取网页getPageDetail获取网页提取的结果，返回WebPageInfo类，该类包括 12345678910public String domain;public String url;public String rawTitle;public String title;public String content;public String summary;public HashMap&lt;String, String&gt; meta;public HashMap&lt;String, List&lt;String&gt;&gt; calculation;public long freq = 1;public String createTime; meta_desc，来自网页meta的description元素 1meta.put(&quot;description&quot;, meta_desc); meta_keywords，来自网页meta的keywords 1meta.put(&quot;keywords&quot;, meta_keywords); 标题rawTitle=doc.title() title，通过ExtractUtil.extractTitle(doc.body(), rawTitle)进一步抽取。目的是去掉标题中的无关信息，如网站信息等 1234567891011121314151617181920212223def extractTitle(root: Element, rawTitle:String): String = &#123; if(StringUtils.isBlank(rawTitle)) return ""; val titleCnt = new HashMap[String, Int] with HashMapUtil.IntHashMap[String] titleCnt.adjustOrPut(te.extract(rawTitle).trim, 1, 1) titleCnt.adjustOrPut(te.extractFirst(rawTitle).trim, 1, 1) val tq = new PriorityQueue[String](2) extractTitle0(root, rawTitle, 1, tq) for (candidate &lt;- tq.values) titleCnt.adjustOrPut(candidate.trim, 1, 1) var maxCnt = 0 var title = rawTitle titleCnt.foreach &#123; case (candidate, cnt) =&gt; if (maxCnt &lt; cnt) &#123; maxCnt = cnt title = candidate &#125; if (maxCnt == cnt &amp;&amp; candidate.length &gt; title.length) title = candidate &#125; title &#125; te.extract， 首先要split。通过判断unicode字符的类别（Unicode字符类）来分割标题 1234567891011121314151617181920for (int i = 0; i &lt; title.length(); i++) &#123; char ch = title.charAt(i); int type = Character.getType(ch); // 标点，前引号 if (type == Character.INITIAL_QUOTE_PUNCTUATION) quoteCnt++; // 标点，开始 if (type == Character.START_PUNCTUATION) quoteCnt++; if (quoteCnt == 0 &amp;&amp; !lastLetter &amp;&amp; !lastDigit &amp;&amp; splitChars.contains(ch)) &#123; parts.add(title.substring(last, i)); last = i + 1; &#125; // 标点，后引号 if (type == Character.FINAL_QUOTE_PUNCTUATION) quoteCnt--; // 标点，结束 if (type == Character.END_PUNCTUATION) quoteCnt--; if (ch &gt;= 'A' &amp;&amp; ch &lt;= 'Z' || ch &gt;= 'a' &amp;&amp; ch &lt;= 'z') lastLetter = true; else lastLetter = false; if (Character.isDigit(ch)) lastDigit = true; else if (!splitChars.contains(ch)) lastDigit = false; &#125; 然后提取最长的part。提取的原则是， 1、如果出现不重要的字符前缀后缀ignoreSuffixes、ignorePrefixes，降低part的长度 2、第一个part的长度翻倍，可能是考虑到真的标题往往出现在第一块，如 清润饮食“熄灭”冬季之火 - 素食 - 大渡网-佛教资讯，生活，人文，心灵感悟，佛艺时尚杂志，佛教音乐，佛教常识，佛教视频 从草根到精英——大陆网络民族主义流变-观点评论-时事评论-四月网-青年思想门户-M4.CN 1234567891011121314151617181920212223242526272829private static List&lt;String&gt; ignoreSuffixes = Arrays.asList("频道", "站", "网", "报", "集", "公司", ".com", ".cn", "平台", "门户", "博客", "精选", "博客精选"); private static List&lt;String&gt; ignorePrefixes = Arrays.asList("Powered by"); private static HashSet&lt;Character&gt; splitChars = new HashSet&lt;Character&gt;(Arrays.asList('|', '_', '-', '—', '－', '&lt;', '&gt;', '«', '»'));private String getLongestPart(List&lt;String&gt; parts) &#123; double longestNumWords = 0; String longestPart = ""; for (int i = 0; i &lt; parts.size(); i++) &#123; String p = parts.get(i).trim(); int ignoreCount = 0; for (String is: ignoreSuffixes) if (p.toLowerCase().endsWith(is)) ignoreCount++; for (String ip: ignorePrefixes) if (p.toLowerCase().startsWith(ip)) ignoreCount++; int colonCnt = StringUtils.countMatches(p, ","); if (colonCnt &gt; 0) ignoreCount += colonCnt - 1; colonCnt = StringUtils.countMatches(p, "，"); if (colonCnt &gt; 0) ignoreCount += colonCnt - 1; double numWords = TextUtil.countNumWords(p); numWords = numWords / (1 + 2 * ignoreCount); if (i == 0) numWords = numWords * 2; if (numWords &gt; longestNumWords) &#123; longestNumWords = numWords; longestPart = p; &#125; &#125; if (longestPart.length() == 0) return ""; else return longestPart.trim(); &#125; extractTitle0(root, rawTitle, 1, tq) 传入root和刚才提取的rawTitle，递归遍历root的各个head元素，h，title，每种赋值不同权重。再寻找与rawTitle的最长公共子串。 12345678910111213141516171819202122232425private def extractTitle0(node: Node, title: String, weight: Double, tq: PriorityQueue[String], depth: Int = 0): Unit = &#123; node match &#123; case textNode: TextNode =&gt; &#123; val text = textNode.text.trim if (text.length &gt; 0) &#123; val lcs = TextUtil.findLcs(title, text) val nwords = TextUtil.countNumWords(lcs) val pos = title.indexOf(lcs) if (pos != -1 &amp;&amp; nwords &gt; 0) &#123; tq.add(nwords * weight / (1 + math.log(2 + pos)), lcs) &#125; &#125; &#125; case e: Element =&gt; &#123; var w = weight if (e.tagName.startsWith("h")) w = w * 1.2 if (e.tagName == "a") w = w / 2 if (e.className.contains("title")) w = w * 1.5 if (e.tagName != "title" &amp;&amp; !isNegativeBlock(e.className + " " + e.id) &amp;&amp; depth &lt; Extract_STOP_DEPTH) &#123; for (n &lt;- e.childNodes.asScala) extractTitle0(n, title, w, tq, depth + 1) &#125; &#125; case _ =&gt; &#123;&#125; &#125; &#125; 通过以上方法提取出各种title后，选出出现频率最高的作为最终的title。 content123456789101112131415161718def extractContent(url: String, doc: Document): List[String] = &#123; val rawTitle = doc.title if(doc.body == null) return null ExtractUtil.cleanup(doc.body) val title = ExtractUtil.extractTitle(doc.body, rawTitle) val metaKeywords = ExtractUtil.extractMeta(doc, "keywords") val blocks = ExtractUtil.extractBlocks(doc, title) map &#123; block =&gt; SnippetBlock(block.snippets map &#123; snippet =&gt; TextProcess.normalize(urlReg.matcher(snippet).replaceAll("")) &#125;, block.score, block.isArticle, block.imgs) &#125; val normalTitle = TextUtil.fillText(title) val normalRawTitle = TextUtil.fillText(doc.title) val allsnippets = blocks.filter(_.isArticle).flatMap &#123; b =&gt; b.snippets &#125; return allsnippets; &#125; 清洗docExtractUtil.cleanup(doc.body) 1234567def cleanup(root: Element): Unit = &#123; val cleanNodes = for &#123; e &lt;- root.getAllElements.asScala if INVALID_TAGS.contains(e.tagName) || e.attr("style").contains("display:none") &#125; yield e for (cn &lt;- cleanNodes) cn.remove &#125; 提取title、metakeywords提取blocks1234567def extractBlocks(doc: Document, title: String): List[SnippetBlock] = &#123; val blocks = new ListBuffer[BlockDetail] val bd = new BlockDetailBuffer extractBlocks(doc.body, blocks, bd) if (bd.isDefined) blocks += bd.result calcScore(title, blocks.result filterNot(b =&gt; hasICP(b))) ++ List(SnippetBlock(List(extractMeta(doc, "keywords")), 1d, true, List()), SnippetBlock(List(extractMeta(doc, "description")), 0d, false, List())) &#125; 1234567891011121314151617181920212223private def extractBlocks(root: Node, blocks: ListBuffer[BlockDetail], bd: BlockDetailBuffer, inLink: Boolean = false, depth: Int = 0): Unit = &#123; root match &#123; case tn: TextNode =&gt; &#123; val text = StringUtils.replace(tn.text, "\u00a0", " ").trim if (text.length &gt; 0) bd.add(text, inLink) &#125; case e: Element =&gt; &#123; val isLink = inLink || (e.tagName == "a") if(depth &lt; Extract_STOP_DEPTH)&#123; e.childNodes.asScala foreach &#123; c =&gt; extractBlocks(c, blocks, bd, isLink, depth + 1) &#125; &#125; if (e.tagName == "img" || e.tagName == "embed") &#123; bd.addImg(e) &#125; if (e.isBlock) &#123; if (bd.isDefined) blocks += bd.result bd.clear &#125; &#125; case _ =&gt; &#123;&#125; &#125;&#125; 提取每个TextNode的文本，放到BlockDetailBuffer中。将每个BlockDetailBuffer的内容放到BlockDetail的list blocks中。 过滤掉包含icp备或icp证的文本，再对所有的blocks计算打分calcScore 最后提取所有是文本的snippet，作为content 提取keywords同样是先clean，提取title、metaKeyword， 再提取blocks 12345678910111213val blocks:List[SnippetBlock] = ExtractUtil.extractBlocks(doc, title).map &#123; block =&gt; &#123; SnippetBlock(block.snippets map &#123; snippet =&gt; TextProcess.normalize(urlReg.matcher(snippet).replaceAll("")) &#125;, block.score, block.isArticle, block.imgs) val temp = block.snippets map &#123; snippet =&gt; TextProcess.normalize(urlReg.matcher(snippet).replaceAll("")) &#125; maxLen += temp.map(AtomSplit.count(_)).sum val retVal:SnippetBlock = if(maxLen &lt; MAX_CONTENT_LENGTH || maxflag)&#123;SnippetBlock(temp, block.score, block.isArticle, block.imgs)&#125; else null if(maxLen &gt; MAX_CONTENT_LENGTH)&#123; maxflag = false &#125; retVal &#125; &#125;.filter( _ != null) dlg再提取dlg 1val dlg = DlgExtractor.extract(normalTitle, TextUtil.fillText(doc.title), blocks, 6)/*.filter(_._2 &gt; 1.0)*/ 分词后，计算每个词的权重， First of all, for any web page, we can use jsoup to obtain the Document Object Model (DOM) , which can access all the HTML elements of it. After that，we clean up HTML elements by drop some invalid or useless tags, such as the tags with “display:none” property. At last, We extract the Content and Keywords of HTML. For extracting Content, we iterate through the DOM tree to find all TextNode elements, extract the text and take them as the Snippets. Then we calculate the scores of all Snippets, and get the available Snippets as Content. For extracing Keywords, besides the Snippets from TextNode elements, we also collect the title, keywords and description from tag, store them as Blocks. For every Block, we segment words to generate the corpus by ansj_seg, and calculate the weight of every word using TFIDF. Finally, we get the TOP 10 words as Keywords of web page. 我们解析了10万左右的网页，根据解析的网页content打上safe和unsafe的label，后期我们会对safe和unsafe进一步细分。 训练过程：我们载入所有含标签的训练样本，由于fasttext提供了适用于各种语言的Word2Vec预向量集，将网页内容转为词向量，通过fasttext训练出模型并保存到本地。 预测过程：载入模型到内存，当输入一个网页的content后，转为词向量，根据模型给出safe或unsafe的分类结果。 We have analyzed some 100 thousand web pages, classified text in categories, such as safe and unsafe by content of these web pages, and we will extent more categories in future. In order to train the text classifier model, we load all samples containing a training sentence per line along with the labels, and transfer all words to vectors using pre-trained word vectors model published by fastText. Then we use the code from Github to run the training program. Once the model was trained, we save it on disk as a file. When input a content of web page, we transfer it to word vectors and run the prediction program, as a result we get the category of this web page.]]></content>
      <categories>
        <category>算法与数据结构</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[查找树]]></title>
    <url>%2F2017%2F07%2F12%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F%E6%9F%A5%E6%89%BE%E6%A0%91%2F</url>
    <content type="text"><![CDATA[动态查找树主要有：二叉查找树（Binary Search Tree），平衡二叉查找树（Balanced Binary Search Tree），红黑树 (Red-Black Tree )，B-tree/B+-tree/ B*-tree (B~Tree)。前三者是典型的二叉查找树结构，其查找的时间复杂度*O(log2N)*与树的深度相关，那么降低树的深度自然对查找效率是有所提高的；还有一个实际问题：就是大规模数据存储中，实现索引查询这样一个实际背景下，树节点存储的元素数量是有限的（如果元素数量非常多的话，查找就退化成节点内部的线性查找了），这样导致二叉查找树结构由于树的深度过大而造成磁盘I/O读写过于频繁，进而导致查询效率低下（为什么会出现这种情况，待会在外部存储器-磁盘中有所解释），那么如何减少树的深度（当然是不能减少查询的数据量），一个基本的想法就是：采用多叉树结构（由于树节点元素数量是有限的，自然该节点的子树数量也就是有限的）。 这样我们就提出了一个新的查找树结构——多路查找树。根据平衡二叉树的启发，自然就想到平衡多路查找树结构，也就是B树 1、B树1.1 原理二叉搜索树： 所有非叶子结点至多拥有两个儿子（ Left 和 Right ）； 所有结点存储一个关键字； 非叶子结点的左指针指向小于其关键字的子树，右指针指向大于其关键字的子树； 1.2 搜索方法 B 树的搜索，从根结点开始，如果查询的关键字与结点的关键字相等，那么就命中；否则，如果查询关键字比 结点关键字小，就进入左儿子；如果比结点关键字大，就进入右儿子；如果左儿子或右儿子的指针为空，则报告找不到相应的关键字； 如果 B 树的所有非叶子结点的左右子树的 结点数目均保持差不多（平衡），那么 B 树的搜索性能逼近二分查找；但它比连续内存空间的二分查找的优点是，改变 B 树结构（插入与删除结点）不需要移动大段的内存数据，甚至通常是常数开销；如 但 B 树在经过多次插入与删除后，有可能导致不同的结构： 右边也是一个 B 树，但 它的搜索性能已经是线性的了；同样的关键字集合有可能导致不同的树结构索引；所以，使用 B 树还 要考虑尽可能让 B 树保持左图的结构，和避免右图的结构，也就是所谓的“平衡”问题； 实际使用的 B 树都是在原 B 树的基 础上加上平衡算法，即“平衡二叉树”；如何保持 B 树结点分布均匀的平衡算法是平衡二叉树的 关键；平衡算法是一种在 B 树中插入和删除结点的策略； 2、B-树1.1 原理是一种多路搜索树（并不是二叉的）： ​ 1. 定义任意非叶子结点最多只有 M 个儿 子；且 M&gt;2 ； ​ 2. 根结点的儿子数为 [2, M] ； ​ 3. 除根结点以外的非叶子结点的儿子数为 [M/2, M] ； ​ 4. 每个结点存放至少 M/2-1 （取 上整）和至多 M-1 个关键字；（至少 2 个关键 字） ​ 5. 非叶子结点的关键字个数 = 指向儿 子的指针个数 -1 ； ​ 6. 非叶子结点的关键字： K[1], K[2], …, K[M-1] ；且 K[i] &lt; K[i+1] ； ​ 7. 非叶子结点的指针： P[1], P[2], …, P[M] ；其中 P[1] 指向关键字小于 K[1] 的子树， P[M] 指向关键字大于K[M-1] 的子树，其它 P[i] 指 向关键字属于 (K[i-1], K[i]) 的子树； ​ 8. 所有叶子结点位于同一层； 如：（ M=3 ） B- 树的特性： ​ 1. 关键字集合分布在整颗树中； ​ 2. 任何一个关键字出现且只出现在一个结点中； ​ 3. 搜索有可能在非叶子结点结束； ​ 4. 其搜索性能等价于在关键字全集内做一次二分查找； ​ 5. 自动层次控制； ​ 由于限制了除根结点以外的非叶子结点，至少含有 M/2 个儿子，确保了结点的至少利用率，其最底搜索性能为： 其中， M 为设定的非叶子结点最多子树个 数， N 为关键字总数； ​ 所以 B- 树的性能总是等价于二分查找 （与 M 值无关），也就没有 B 树平衡 的问题； ​ 由于 M/2 的限制，在插入结点时，如果 结点已满，需要将结点分裂为两个各占 M/2 的结点；删除结点时，需将两个不足 M/2 的 兄弟结点合并； 1.2 方法参考 BTree,B-Tree,B+Tree,B*Tree都是什么 B-tree/B+tree/B*tree]]></content>
      <categories>
        <category>算法与数据结构</category>
      </categories>
      <tags>
        <tag>树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 配置和使用]]></title>
    <url>%2F2017%2F07%2F12%2F%E5%B7%A5%E5%85%B7%E5%92%8C%E7%8E%AF%E5%A2%83%2FDocker-%E9%85%8D%E7%BD%AE%E5%92%8C%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[0、基本概念镜像 Docker镜像通过镜像ID进行识别。镜像ID是一个64字符的十六进制的字符串。但是当我们运行镜像时，通常我们不会使用镜像ID来引用镜像，而是使用镜像名来引用。要列出本地所有有效的镜像，可以使用命令 $ docker images 镜像可以发布为不同版本，这种机制我们称之为标签（Tag）。 容器 Docker容器可以使用命令创建： $ docker run imagename 它会在所有的镜像层之上增加一个可写层。这个可写层有运行在CPU上的进程，而且有两个不同的状态：运行态（Running）和退出态（Exited）。这就是Docker容器。当我们使用docker run启动容器，Docker容器就进入运行态，当我们停止Docker容器时，它就进入退出态。 对容器的变更是写入到容器的文件系统的，而不是写入到Docker镜像中的。 用同一个镜像启动多个Docker容器，这些容器启动后都是活动的，彼此还是相互隔离的。我们对其中一个容器所做的变更只会局限于那个容器本身。 如果对容器的底层镜像进行修改，那么当前正在运行的容器是不受影响的，不会发生自动更新现象。 1、配置1.1 安装12$ sudo apt-get update$ sudo apt-get install docker 编辑/etc/default/docker增加如下配置，自定义docker镜像存放路径 1DOCKER_OPTS=&quot;--graph=/home/david/opt/docker&quot; sudo service docker start #启动sudo service docker stop #关闭sudo service docker restart #重启 1.2 pull一个镜像查看镜像：此时应该没有镜像docker images 下载镜像 sudo docker pull ubuntu 1.3 创建容器docker ps命令 123$ sudo docker ps #列出当前所有正在运行的container$ sudo docker ps -l #列出最近一次启动的，且正在运行的container$ sudo docker ps -a #列出所有的container 启动容器，并且进入到Ubuntu容器的bash命令 1$ sudo docker run -itv /home/david/docker:/home/davida/gitlab ubuntu /bin/bash 其中， 1.4 提交容器退出容器，使用 docker commit 命令来提交更新后的副本。 1$ sudo docker commit -m 'xmo run evn' -a 'david' bde4f2f2db5f ubuntu-ruby:v1 其中，-m 来指定提交的说明信息，跟我们使用的版本控制工具一样；-a 可以指定更新的作者信息；之后是用来创建镜像的容器的ID；最后指定目标镜像的仓库名和 tag 信息。创建成功后会返回这个镜像的 ID 信息。v1是tag（版本号） 1$ sudo docker inspect ubuntu-ruby 查看详细信息 1.5 Dockerfile提交容器Dockerfile的指令是忽略大小写的，建议使用大写，使用 # 作为注释，每一行只支持一条指令，每条指令可以携带多个参数。Dockerfile的指令根据作用可以分为两种，构建指令和设置指令。构建指令用于构建image，其指定的操作不会在运行image的容器上执行；设置指令用于设置image的属性，其指定的操作将在运行image的容器中执行。 1）FROM 基础image 基础image可以是官方远程仓库中的，也可以位于本地仓库。 1FROM &lt;image&gt; 或者 1FROM &lt;image&gt;:&lt;tag&gt; 制定某个tag版本 1.4 用SSH访问容器先安装ssh 12apt-get updateapt-get install openssh-server 需要修改/etc/ssh/sshd_config文件中内容 1234RSAAuthentication yes #启用 RSA 认证PubkeyAuthentication yes #启用公钥私钥配对认证方式AuthorizedKeysFile .ssh/authorized_keys #公钥文件路径（和上面生成的文件同）PermitRootLogin yes #root能使用ssh登录 重启sshd服务 12/etc/init.d/ssh stop/etc/init.d/ssh start 2、常用命令docker start和docker run 从图片可以看出，docker run 命令先是利用镜像创建了一个容器，然后运行这个容器。 docker run命令类似于git pull命令。git pull命令就是git fetch 和 git merge两个命令的组合，同样的，docker run就是docker create和docker start两个命令的组合。 docker psdocker ps 命令会列出所有运行中的容器。这隐藏了非运行态容器的存在，如果想要找出这些容器，我们需要使用docker ps -a docker imagesdocker images命令会列出了所有顶层（top-level）镜像。实际上，在这里我们没有办法区分一个镜像和一个只读层，所以我们提出了top-level 镜像。只有创建容器时使用的镜像或者是直接pull下来的镜像能被称为顶层（top-level）镜像，并且每一个顶层镜像下面都隐藏了多个镜像层。 docker images -a docker images –a命令列出了所有的镜像，也可以说是列出了所有的可读层。如果你想要查看某一个image-id下的所有层，可以使用docker history来查看。 docker stop docker stop命令会向运行中的容器发送一个SIGTERM的信号，然后停止所有的进程。 docker commit docker commit命令将容器的可读写层转换为一个只读层，这样就把一个容器转换成了不可变的镜像。 dockesr exec docker exec 命令会在运行中的容器执行一个新进程。 docker save docker save命令会创建一个镜像的压缩文件，这个文件能够在另外一个主机的Docker上使用。和export命令不同，这个命令为每一个层都保存了它们的元数据。这个命令只能对镜像生效。 docker export docker export命令创建一个tar文件，并且移除了元数据和不必要的层，将多个层整合成了一个层，只保存了当前统一视角看到的内容（expoxt后 的容器再import到Docker中，通过docker images –tree命令只能看到一个镜像；而save后的镜像则不同，它能够看到这个镜像的历史镜像）。 参考Ubuntu 15.04下安装Docker Docker的镜像和容器的区别 Docker容器和镜像区别 Docker学习笔记（3）— 如何使用Dockerfile构建镜像]]></content>
      <categories>
        <category>工具和环境</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CNN]]></title>
    <url>%2F2017%2F07%2F12%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2FCNN%2F</url>
    <content type="text"><![CDATA[卷积核也称为滤波器。 权重共享：卷积核的权重（矩阵的值）对于不同位置的所有输入都是相同的。 卷积操作的意义例如，有整体边缘滤波器Ke，横向边缘滤波器Kh，纵向边缘滤波器Kv。 K_e=\begin{bmatrix} 0 & -4 & 0\\ -4 & 16 & -4\\ 0 & -4 & 0 \end{bmatrix}\ \ K_h=\begin{bmatrix} 1 & 2 & 1\\ 0 & 0 & 0\\ -1 & -2 & -1 \end{bmatrix}\ \ K_v=\begin{bmatrix} 1 & 0 & -1\\ 2 & 0 & -2\\ 1 & 0 & -1 \end{bmatrix}若某像素位于物体边缘，则周边像素与该像素会有明显差异，用Ke可以放大边缘和周边的差异，起到边缘检测的作用。同理，Kh、Kv可以保留横向、纵向的边缘信息。 池化层也叫汇合层。通常操作有平均值池化（average-pooling）和最大值汇合（max-pooling）。与卷积核操作不同，池化层不包含需要学习的参数。仅指定汇合类型，核大小（kernel size）和步长（stride）。 汇合的结果相对输入降小了，是一种降采样（down-sampling）操作。也可以看成是一个用p范数（p-norm）作为非线性映射的卷积操作。当p趋于正无穷时就是最大值汇合。 汇合层的引入是仿照人的视觉系统对视觉输入对象进行降维和抽象。作用有： 1）特征不变性（feature invariant）。使模型更关注是否存在某些特征而不是特征具体的位置。 2）特征降维。 3）一定程度上防止过拟合。 全连接层fully connected layers 参考： 【1】解析卷积神经网络.pdf]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elastic search笔记]]></title>
    <url>%2F2017%2F07%2F12%2FElastic%20Search%2Felastic-search%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[过滤字段中为空的通过filter中的exists，如 12345678910111213141516171819GET iclick_persona/iclick/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123;&quot;term&quot;: &#123; &quot;province&quot;: &#123; &quot;value&quot;: &quot;CN_IN_SG&quot; &#125; &#125;&#125; ], &quot;filter&quot;: &#123; &quot;exists&quot;: &#123; &quot;field&quot;: &quot;articles.titles&quot; &#125; &#125; &#125; &#125;&#125; 大索引按天拆开好像不能直接从A索引中抽取一部分到B索引 python从文件插入到ES12345678910111213141516171819202122232425def set_pois_data(es, input_file, index_name=&quot;pois&quot;, doc_type_name=&quot;iclick&quot;): #读入数据 fp = open(input_file) #创建ACTIONS ACTIONS = [] for line in fp: fields = line.strip(&apos;\n&apos;).split(&quot;\t&quot;) name = fields[0] location = fields[2] + &quot;,&quot; + fields[3] action = &#123; &quot;_index&quot;: index_name, &quot;_type&quot;: doc_type_name, &quot;_source&quot;: &#123; &quot;name&quot; : name, &quot;pois&quot; : location, &#125; &#125; #print action ACTIONS.append(action) # 批量处理 success, _ = bulk(es, ACTIONS, index=index_name, raise_on_error=True) print(&apos;Performed %d actions&apos; % success) 复制index的数据到另一个12345678import pyesconn = pyes.es.ES(&quot;http://10.xx.xx.xx:8305/&quot;)search = pyes.query.MatchAllQuery().search(bulk_read=1000)hits = conn.search(search, &apos;store_v1&apos;, &apos;client&apos;, scan=True, scroll=&quot;30m&quot;, model=lambda _,hit: hit)for hit in hits: #print hit conn.index(hit[&apos;_source&apos;], &apos;store_v2&apos;, &apos;client&apos;, hit[&apos;_id&apos;], bulk=True)conn.flush() 统计数组的元素个数的sum、avg1234567891011121314151617181920212223242526GET iclick_persona/iclick/_search?size=0&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123;&quot;term&quot;: &#123; &quot;device&quot;: &#123; &quot;value&quot;: &quot;PC&quot; &#125; &#125;&#125; ], &quot;filter&quot;: [ &#123;&quot;script&quot;:&#123;&quot;script&quot;:&quot;doc[&apos;articles.domains&apos;].values.length&gt;1&quot;&#125;&#125; ] &#125; &#125;, &quot;aggs&quot;: &#123; &quot;NAME&quot;: &#123; &quot;avg&quot;: &#123; &quot;script&quot;: &#123; &quot;inline&quot;: &quot;doc[&apos;articles.domains&apos;].values.length&quot; &#125; &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>Elastic Search</category>
      </categories>
      <tags>
        <tag>elastic search</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow环境搭建]]></title>
    <url>%2F2017%2F07%2F12%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2FTensor%20Flow%2F</url>
    <content type="text"><![CDATA[1 环境1.1 Ubuntu14.04官网：https://www.tensorflow.org/ 新建一个conda环境 123conda create -n tensor python=2.7source activate tensorconda install -c conda-forge tensorflow tensorflow 自动安装的是0.12.1 1.2 windowshttp://blog.csdn.net/neilron/article/details/51387161 需要用到docker 1、安装Docker Toolbox 文档在Docker Windows文档 安装到这一步 1docker pull gcr.io/tensorflow/tensorflow 报错 12Using default tag: latest An error occurred trying to connect: Post http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.24/images/create?fromImage=gcr.io%2Ftensorflow%2Ftensorflow&amp;tag=latest: open //./pipe/docker_engine: The system cannot find the file specified. 换官方的方法 1docker run -it -p 8888:8888 gcr.io/tensorflow/tensorflow 报错 1the input device is not a TTY. If you are using mintty, try prefixing the command with &apos;winpty&apos; 查到说 1The &quot;-t&quot; tells docker to configure the tty, which won&apos;t work if you don&apos;t have a tty and try to attach to the container (default when you don&apos;t do a &quot;-d&quot;). 于是去掉-t，执行 1docker run -i b.gcr.io/tensorflow/tensorflow 又回到原来的找不到文件的错误 采用Docker Quickstart Terminal来执行上面任务，成功。 执行 1docker run -i b.gcr.io/tensorflow/tensorflow 进入docker环境 1.3 Ubuntu 2 入门http://blog.csdn.net/zhongxon/article/details/54709347 1、准备 代码在https://github.com/martin-gorner/tensorflow-mnist-tutorial 本地在/media/david/project/gitlab/tensorflow-mnist-tutorial 2、代码mnist_1.0_softmax.py 12345678# input X: 28x28 grayscale images, the first dimension (None) will index the images in the mini-batchX = tf.placeholder(tf.float32, [None, 28, 28, 1])# correct answers will go hereY_ = tf.placeholder(tf.float32, [None, 10])# weights W[784, 10] 784=28*28W = tf.Variable(tf.zeros([784, 10]))# biases b[10]b = tf.Variable(tf.zeros([10])) X、Y_是占位符。 X训练期间填充实际数据的参数，通常是训练图像。持有训练图像的张量的形式是 [None, 28, 28, 1]，其中 28, 28, 1: 图像是 28x28 每像素 x 1（灰度）。最后一个数字对于彩色图像是 3 但在这里并非是必须的。 None: 这是代表图像在小批量（mini-batch）中的数量。在训练时可以得到。 Y_是标签 12345678# 网络模型XX = tf.reshape(X, [-1, 784])Y = tf.nn.softmax(tf.matmul(XX, W) + b)# 计算交叉熵cross_entropy = -tf.reduce_mean(Y_ * tf.log(Y)) * 1000.0 # 评估模型的精度correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) tf.reshape 命令将我们的 28×28 的图像转化成 784 个像素的单向量。 tf.reduce_sum是对向量的所有元素求和。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（转）卷积神经网络简介]]></title>
    <url>%2F2017%2F07%2F12%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F%E4%B8%93%E6%A0%8F%20%7C%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650723520&amp;idx=4&amp;sn=8ee14dd052766ca3e0afa60dcbb65b2d&amp;chksm=871b10beb06c99a81ef547319637a177142d33a40da5a85024fc6a3b623d60d3a7ac22e3efc3&amp;scene=21#wechat_redirect 2017-02-21 机器之心 转自知乎 作者：张觉非 阿里巴巴 友盟+ 机器之心经作者授权转载 链接：https://zhuanlan.zhihu.com/p/25249694 一、卷积 我们在 2 维上说话。有两个的函数 f(x,y) 和 g(x,y)。f 和 g 的卷积就是一个新的 的函数。通过下式得到： 这式子的含义是：遍览从负无穷到正无穷的全部 s 和 t 值，把 g 在 (x-s,y-t) 位置上的值乘上 f 在 (s,t) 位置上的值之后「加和」（积分意义上的加和）到一起，就是 c 在 (x,y) 上的值。说白了卷积就是一种「加权求和」。以 (x,y) 为中心，把 g 距离中心 (-s,-t) 位置上的值乘上 f 在 (s,t) 的值，最后加到一起。把卷积公式写成离散形式就更清楚了： 如果 G 表示一幅 100 x 100 大小的灰度图像，G(x,y) 取值 [0,255] 区间内的整数，是图像在 (x,y) 的灰度值。范围外的位置上的 G 值全取 0。令 F 在 s 和 t 取 {-1,0,1} 的时候有值，其他位置全是 0。F 可以看作是一个 3 x 3 的网格。如下图： 图 1 G 每个小格子里的值就是图像在 (x,y) 的灰度值。F 每个小格子里的值就是 F 在 (s,t) 的值。 图 2 如上图所示，将 F 的中心 (0,0) 对准 G 的 (5,6)。把 F 和 G 对应的 9 个位置上各自的函数值相乘，再将 9 个乘积加在一起，就得到了卷积值 C(5,6)。对 G 的每一个位置求 C 值，就得到了一幅新的图像。其中有两个问题： 如果 F 的所有值之和不等于 1.0，则 C 值有可能不落在 [0,255] 区间内，那就不是一个合法的图像灰度值。所以如果需要让结果是一幅图像，就得将 F 归一化——令它的所有位置之和等于 1.0 ； 对于 G 边缘上的点，有可能它的周围位置超出了图像边缘。此时可以把图像边缘之外的值当做 0。或者只计算其周围都不超边缘的点的 C。这样计算出来的图像就比原图像小一些。在上例中是小了一圈，如果 F 覆盖范围更大，那么小的圈数更多。 上述操作其实就是对数字图像进行离散卷积操作，又叫滤波。F 称作卷积核或滤波器。不同的滤波器起不同的作用。想象一下，如果 F 的大小是 3 x 3，每个格子里的值都是 1/9。那么滤波就相当于对原图像每一个点计算它周围 3 x 3 范围内 9 个图像点的灰度平均值。这应该是一种模糊。看看效果： 图 3 左图是 lena 灰度原图。中图用 3 x 3 值都为 1/9 的滤波器去滤，得到一个轻微模糊的图像。模糊程度不高是因为滤波器覆盖范围小。右图选取了 9 x 9 值为 1/81 的滤波器，模糊效果就较明显了。滤波器还有许多其他用处。例如下面这个滤波器： 尝试用它来滤 lena 图。注意该滤波器没有归一化（和不是 1.0），故滤出来的值可能不在 [0,255] 之内。通过减去最小值、除以最大／最小值之差、再乘以 255 并取整，把结果值归一到 [0,255] 之内，使之成为一幅灰度图像。 图 4 该滤波器把图像的边缘检测出来了。它就是 Sobel 算子。图像模糊、边缘检测等等都是人们设计出来的、有专门用途的滤波器。如果搞一个 9 x 9 的随机滤波器，会是什么效果呢？ 图 5 如上图，效果也类似于模糊。因为把一个像素点的值用它周围 9 x 9 范围的值随机加权求和，相当于「捣浆糊」。但可以看出模糊得并不润滑。 这时我们不禁要想，如果不是由人来设计一个滤波器，而是从一个随机滤波器开始，根据某种目标、用某种方法去逐渐调整它，直到它接近我们想要的样子，可行么？这就是卷积神经网络（Convolutional Neural Network, CNN）的思想了。可调整的滤波器是 CNN 的「卷积」那部分；如何调整滤波器则是 CNN 的「神经网络」那部分。 二、神经网络 人工神经网络（Neural Network, NN）作为一个计算模型，其历史甚至要早于计算机。W.S. McCulloch 和 W. Pitts 在四十年代就提出了人工神经元模型。但是单个人工神经元甚至无法计算异或。人工智能领域的巨擘马文. 明斯基认为这个计算模型是没有前途的。在那时人们已经认识到将多个人工神经元连接成网络就能克服无法计算异或的问题，但是当时没有找到多层人工神经网络的训练方法，以至于人工神经网络模型被压抑多年。直到人们找到了多层人工神经网络的训练方法，人工神经网络才迎来了辉煌。 人工神经元就是用一个数学模型简单模拟人的神经细胞。人的神经细胞有多个树突和一个伸长的轴突。一个神经元的轴突连接到其他神经元的树突，并向其传导神经脉冲。一个神经元会根据来自它的若干树突的信号决定是否从其轴突向其他神经元发出神经脉冲。 图 6 一个人工神经元就是对生物神经元的数学建模。见下图。 图 7 是人工神经元的输入。a 是人工神经元的输出。人工神经元将输入 加权求和后再加上偏置值 b，最后再施加一个函数 f，即： 上式最后是这个式子的向量形式。P 是输入向量，W 是权值向量，b 是偏置值标量。f 称为「激活函数」。激活函数可以采用多种形式。例如 Sigmoid 函数： 这是单个人工神经元的定义。人工神经网络就是把这样的人工神经元互联成一个网络：一个神经元的输出作为另一个神经元的输入。神经网络可以有多种多样的拓扑结构。其中最简单的就是「多层全连接前向神经网络」。它的输入连接到网络第一层的每个神经元。前一层的每个神经元的输出连接到下一层每个神经元的输入。最后一层神经元的输出就是整个神经网络的输出。 如下图，是一个三层神经网络。它接受 10 个输入，也就是一个 10 元向量。第一层和第二层各有 12 个神经元。最后一层有 6 个神经元。就是说这个神经网络输出一个 6 元向量。 图 8 整个神经网络的计算可以用矩阵式给出。我们给出人工神经网络单层的式子。每层的神经元个数不一样，输入／输出维度也就不一样，计算式中的矩阵和向量的行列数也就不一样，但形式是一致的。假设我们考虑的这一层是第 i 层。它接受 m 个输入，拥有 n 个神经元（n 个输出），那么这一层的计算如下式所示： 上标 i 表示第 i 层。是输出向量，n 元，因为第 i 层有 n 个神经元。第 i 层的输入，即第 i-1 层的输出，是 m 元向量。权值矩阵 W 是 n x m 矩阵：n 个神经元，每个神经元有 m 个权值。W 乘以第 i-1 层输出的 m 向量，得到一个 n 向量，加上 n 元偏置向量 b，再对结果的每一个元素施以激活函数 f，最终得到第 i 层的 n 元输出向量。 若不嫌繁琐，可以将第 i-1 层的输出也展开，最终能写出一个巨大的式子。它就是整个全连接前向神经网络的计算式。可以看出整个神经网络其实就是一个向量到向量的函数。至于它是什么函数，就取决于网络拓扑结构和每一个神经元的权值和偏置值。如果随机给出权值和偏置值，那么这个神经网络是无用的。我们想要的是有用的神经网络。它应该表现出我们想要的行为。 要达到这个目的，首先准备一个从目标函数采样的包含若干「输入－输出对儿」的集合——训练集。把训练集的输入送给神经网络，得到的输出肯定不是正确的输出。因为一开始这个神经网络的行为是随机的。 把一个训练样本输入给神经网络，计算输出与正确输出的（向量）差的模平方（自己与自己的内积）。再把全部 n 个样本的差的模平方求平均，得到 e ： e 称为均方误差 mse。e 越小则神经网络的输出与正确输出越接近。神经网络的行为就与想要的行为越接近。 目标是使 e 变小。在这里 e 可以看做是全体权值和偏置值的一个函数。这就成为了一个无约束优化问题。如果能找到一个全局最小点，e 值在可接受的范围内，就可以认为这个神经网络训练好了。它能够很好地拟合目标函数。这里待优化的函数也可以是 mse 外的其他函数，统称 Cost Function，都可以用 e 表示。 经典的神经网络的训练算法是反向传播算法（Back Propagation, BP）。BP 算法属于优化理论中的梯度下降法（Gradient Descend）。将误差 e 作为全部权值和全部偏置值的函数。算法的目的是在自变量空间内找到 e 的全局极小点。 首先随机初始化全体权值和全体偏置值，之后在自变量空间中沿误差函数 e 在该点的梯度方向的反方向（该方向上方向导数最小，函数值下降最快）前进一个步长。步长称为学习速率（Learning Rate, LR）。如此反复迭代，最终（至少是期望）解运动到误差曲面的全局最小点。 下图是用 matlab 训练一个极简单的神经网络。它只有单输入单输出。输入层有两个神经元，输出层有一个神经元。整个网络有 4 个权值加 3 个偏置。图中展示了固定其他权值，只把第一层第一个神经元的权值和偏置做自变量时候的 e 曲面，以及随着算法迭代，解的运动轨迹。 图 9 最终算法没有收敛到全局最优解（红 +）。但是解已经运动到了一个峡谷的底部。由于底部过于平缓，解「走不动」了。所得解比最优也差不到哪去。 对于一个稍复杂的神经网络，e 对权值和偏置值的函数将是一个非常复杂的函数。求梯度需要计算该函数对每一个权值和偏置值的偏导数。所幸的是，每一个权值或偏置值的偏导数公式不会因为这个权值或偏置值距离输出层越远而越复杂。计算过程中有一个中间量，每层的权值和偏置值的偏导数都可根据后一层的以统一形式计算出来。每层再把计算过程中产生的传递给前一层。这就是「反向传播」名称的由来——沿着反向向前传。这与计算网络输出时，计算结果向后传相反。如此可逐层计算出全部权值和偏置值的偏导数，得到梯度。具体推导这里不给出了，可以参考［1］第八章和［2］第十一章。正是反向传播能够让我们训练神经网络「深处」的参数，这就是「Deep Learning」的含义。 梯度下降法有很多变体。通过调整学习速率 LR 可以提高收敛速度；通过增加冲量可以避免解陷入局部最优点。还可以每一次不计算全部样本的 e，而是随机取一部分样本，根据它们的 e 更新权值。这样可以减少计算量。梯度下降是基于误差函数的一阶性质。还有其他方法基于二阶性质进行优化，比如共轭法、牛顿法等等。优化作为一门应用数学学科，是机器学习的一个重要理论基础，在理论和实现上均有众多结论和方法。参考［1］。 三、卷积神经网络 现在把卷积滤波器和神经网络两个思想结合起来。卷积滤波器无非就是一套权值。而神经网络也可以有（除全连接外的）其它拓扑结构。可以构造如下图所示意的神经网络： 图 10 该神经网络接受个输入，产生个输出。图中左边的平面包含 n x n 个格子，每个格子中是一个 [0,255] 的整数值。它就是输入图像，也是这个神经网络的输入。右边的平面也是 n x n 个格子，每个格子是一个神经元。每个神经元根据二维位置关系连接到输入上它周围 3 x 3 范围内的值。每个连接有一个权值。所有神经元都如此连接（图中只画了一个，出了输入图像边缘的连接就认为连接到常数 0）。右边层的个神经元的输出就是该神经网络的输出。 这个网络有两点与全连接神经网络不同。首先它不是全连接的。右层的神经元并非连接上全部输入，而是只连接了一部分。这里的一部分就是输入图像的一个局部区域。我们常听说 CNN 能够把握图像局部特征、AlphaGO 从棋局局部状态提取信息等等，就是这个意思。这样一来权值少了很多，因为连接就少了。权值其实还更少，因为每一个神经元的 9 个权值都是和其他神经元共享的。全部个神经元都用这共同的一组 9 个权值，并且不要偏置值。那么这个神经网络其实一共只有 9 个参数需要调整。 看了第一节的同学们都看出来了，这个神经网络不就是一个卷积滤波器么？只不过卷积核的参数未定，需要我们去训练——它是一个「可训练滤波器」。这个神经网络就已经是一个拓扑结构特别简单的 CNN 了。 试着用 Sobel 算子滤出来的图片作为目标值去训练这个神经网络。给网络的输入是灰度 lena 图，正确输出是经过 Sobel 算子滤波的 lena 图，见图 4。这唯一的一对输入输出图片就构成了训练集。网络权值随机初始化，训练 2000 轮。如下图： 图 11 从左上到右下依次为：初始随机滤波器输出、每个 200 轮训练后的滤波器输出（10 幅）、最后一幅是 Sobel 算子的输出，也就是用作训练的目标图像。可以看到经过最初 200 轮后，神经网络的输出就已经和 Sobel 算子的输出看不出什么差别了。后面那些轮的输出基本一样。输入与输出的均方误差 mse 随着训练轮次的变化如下图： 图 12 1500 轮过后，mse 基本就是 0 了。训练完成后网络的权值是： 与 Sobel 算子比较一下： 注意训练出来的滤波器负数列在右侧而不是左侧。因为用 Sobel 算子算卷积的时候也许库函数（scipy.ndimage.filters.convolve）是把滤波器「反着扣上去」的。这并不重要。关键是一正列、一负列，中间零值列。正／负列值之比近似 1:2:1。它就是近似的 Sobel 算子。我们以训练神经网络的方式把一个随机滤波器训练成了 Sobel 算子。这就是优化的魔力。AlphaGO 之神奇的核心也在于此——优化。 在 CNN 中，这样的滤波器层叫做卷积层。一个卷积层可以有多个滤波器，每一个叫做一个 channel，或者叫做一个 feature map。可以给卷积层的输出施加某个激活函数：Sigmoid 、Tanh 等等。激活函数也构成 CNN 的一层——激活层，这样的层没有可训练的参数。 还有一种层叫做 Pooling 层（采样层）。它也没有参数，起到降维的作用。将输入切分成不重叠的一些 n x n 区域。每一个区域就包含个值。从这个值计算出一个值。计算方法可以是求平均、取最大 max 等等。假设 n=2，那么 4 个输入变成一个输出。输出图像就是输入图像的 1/4 大小。若把 2 维的层展平成一维向量，后面可再连接一个全连接前向神经网络。 通过把这些组件进行组合就得到了一个 CNN。它直接以原始图像为输入，以最终的回归或分类问题的结论为输出，内部兼有滤波图像处理和函数拟合，所有参数放在一起训练。这就是卷积神经网络。 四、举个栗子 手写数字识别。数据集中一共有 42000 个 28 x 28 的手写数字灰度图片。十个数字（0～9）的样本数量大致相等。下图展示其中一部分（前 100 个）： 图 13 将样本集合的 75% 用作训练，剩下的 25% 用作测试。构造一个结构如下图的 CNN ： 图 14 该 CNN 共有 8 层（不包括输入层）。它接受 784 元向量作为输入，就是一幅 28 x 28 的灰度图片。这里没有将图片变形成 28 x 28 再输入，因为在 CNN 的第一层放了一个 reshape 层，它将 784 元的输入向量变形成 1 x 28 x 28 的阵列。最开始那个 1 x 表示只有一个 channel，因为这是灰度图像，并没有 RGB 三个 channel。 接下来放一个卷积层。它包含 32 个滤波器，所以它的输出维度是 32 x 28 x 28。32 个滤波器搞出来 32 幅图像（channel），每个都是 28 x 28 大小。后面又是一个 32 个滤波器的卷积层，输出维度也是 32 x 28 x 28。 后面接上一个 Pooling 层，降降维。一个 2 x 2 的取平均值 Pooling 层，把输出维度减小了一半：32 x 14 x 14。接着是一个展平层，没有运算也没有参数，只变化一下数据形状：把 32 x 14 x 14 展平成了 6272 元向量。 该 6272 元向量送给后面一个三层的全连接神经网络。该网络的神经元个数是 1000 x 1000 x 10。两个隐藏层各有 1000 个神经元，最后的输出层有 10 个神经元，代表 10 个数字。假如第六个输出为 1，其余输出为 0，就表示网络判定这个手写数字为「5」（数字「0」占第一个输出，所以「5」占第六个输出）。数字「5」就编码成了： 训练集和测试集的数字标签都这么编码（one-hot 编码）。 全连接神经网络这部分的激活函数都采用了 Sigmoid。这出于我一个过时且肤浅的理解：用「弯弯绕」较多的 Sigmoid 给网络贡献非线性。实际上当代深度学习从生物神经的行为中得到启发，设计了其它一些表现优异的激活函数，比如单边线性 Relu。 误差函数采用均方误差 mse。优化算法采用 rmsprop，这是梯度下降的一个变体。它动态调整学习速率 LR。训练过程持续 10 轮。注意这里 10 轮不是指当前解在解空间只运动 10 步。一轮是指全部 31500 个训练样本都送进网络迭代一次。每次权值更新以 32 个样本为一个 batch 提交给算法。下图展示了随着训练，mse 的下降情况： 图 15 下图是分类正确率随着训练的变化情况： 图16 该 CNN 在测试集上的正确率（accuracy）是 96.7%，各数字的准确率 / 召回率 / f1-score 如下： 该 CNN 对测试集 10 种数字分类的混淆矩阵为： 图17 训练完成神经网络后，最有趣的是将其内部权值以某种方式展现出来。看着那些神秘的、不明所以的连接强度最后竟产生表观上有意义的行为，不由让我们联想起大脑中的神经元连接竟构成了我们的记忆、人格、情感 … 引人遐思。 在 CNN 上就更适合做这种事情。因为卷积层训练出来的是滤波器。用这些滤波器把输入图像滤一滤，看看 CNN 到底「看到」了什么。下图用第一、二卷积层的 32 个滤波器滤了图 13 第 8 行第 8 列的那个手写数字「6」。32 个 channel 显示如下： 图 18 图 19 其中有些把边缘高亮（输出值较大），有些把「6」的圈圈高亮，等等。这些就是 CNN 第一步滤波后「看到」的信息。再经过后面的各神经层，抽象程度逐层提高，它就这样「认出」了手写数字。 最后把代码附上。CNN 使用的是 keras 库。数据集来自 kaggle ：https://www.kaggle.com/c/digit-recognizer/data。 import pandas as pd from keras.models import Sequential from keras.layers import Dense, Flatten, Reshape, AveragePooling2D, Convolution2D from keras.utils.np_utils import to_categorical from keras.utils.visualize_util import plot from keras.callbacks import Callback from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report, accuracy_score, confusion_matrix class LossHistory(Callback): def init(self): Callback.init(self) self.losses = [] self.accuracies = [] def on_train_begin(self, logs=None): pass def on_batch_end(self, batch, logs=None): self.losses.append(logs.get(‘loss’)) self.accuracies.append(logs.get(‘acc’)) history = LossHistory() data = pd.read_csv(“train.csv”) digits = data[data.columns.values[1:]].values labels = data.label.values train_digits, test_digits, train_labels, test_labels = train_test_split(digits, labels) train_labels_one_hot = to_categorical(train_labels) test_labels_one_hot = to_categorical(test_labels) model = Sequential() model.add(Reshape(target_shape=(1, 28, 28), input_shape=(784,))) model.add(Convolution2D(nb_filter=32, nb_row=3, nb_col=3, dim_ordering=”th”, border_mode=”same”, bias=False, init=”uniform”)) model.add(Convolution2D(nb_filter=32, nb_row=3, nb_col=3, dim_ordering=”th”, border_mode=”same”, bias=False, init=”uniform”)) model.add(AveragePooling2D(pool_size=(2, 2), dim_ordering=”th”)) model.add(Flatten()) model.add(Dense(output_dim=1000, activation=”sigmoid”)) model.add(Dense(output_dim=1000, activation=”sigmoid”)) model.add(Dense(output_dim=10, activation=”sigmoid”)) with open(“digits_model.json”, “w”) as f: f.write(model.to_json()) plot(model, to_file=”digits_model.png”, show_shapes=True) model.compile(loss=”mse”, optimizer=”rmsprop”, metrics=[“accuracy”]) model.fit(train_digits, train_labels_one_hot, batch_size=32, nb_epoch=10, callbacks=[history]) model.save_weights(“digits_model_weights.hdf5”) predict_labels = model.predict_classes(test_digits) print(classification_report(test_labels, predict_labels)) print(accuracy_score(test_labels, predict_labels)) print(confusion_matrix(test_labels, predict_labels)) 五、参考书目 ［1］《最优化导论》（美）Edwin K. P. Chong（美）Stanislaw H. Zak ［2］《神经网络设计》（美）Martin T.Hagan（美）Howard B.Demuth（美）Mark Beale ©本文为机器之心转载文章，转载请联系本公众号获得授权。 ✄———————————————————————— 加入机器之心（全职记者/实习生）：hr@jiqizhixin.com 投稿或寻求报道：editor@jiqizhixin.com 广告&amp;商务合作：bd@jiqizhixin.com 微信扫一扫关注该公众号]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（转）入门 | 一文概览深度学习中的卷积结构]]></title>
    <url>%2F2017%2F07%2F12%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F%E5%85%A5%E9%97%A8%20%7C%20%E4%B8%80%E6%96%87%E6%A6%82%E8%A7%88%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%8D%B7%E7%A7%AF%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650731362&amp;idx=4&amp;sn=d163dd19d806f84928cacae8e92f5114&amp;chksm=871b371cb06cbe0a738122035ff03c67eb4be20730722fd8a6f36e8b116b5501e1124bff02fe&amp;scene=21#wechat_redirect 2017-09-27 机器之心 选自Medium 作者：Paul-Louis Prove 机器之心编译 参与：路雪、李亚洲 本文对三种不同的卷积进行了介绍，同时讲解了各自的优点，对初学者而言，是理解卷积的一篇好文章。 卷积 首先，我们需要定义卷积层的几个参数。 kernel 为 3、stride 为 1，使用 padding 的 2D 卷积 卷积核大小：卷积核决定卷积的视野。2D 卷积的常见卷积核为 3，即 3x3 像素。 stride：stride 决定卷积核遍历图像时的步子大小。默认值通常为 1，我们可以将 stride 设置成 2，对图像进行类似最大池化的下采样。 Padding：padding 决定处理样本时的边界。（半）填充的卷积使输出空间维度等于输入，而未填充的卷积会裁剪部分边界，如果卷积核大于 1 的话。 输入&amp;输出通道：卷积层通常需要一定数量的输入通道 (I)，计算一定数量的输出通道 (O)。所需参数可以通过 IOK 来计算，K 就是卷积核的值。 机器之心曾介绍过用于语义分割中的各种卷积：从全连接层到大型卷积核：深度学习语义分割全指南 扩张卷积（又叫空洞卷积） kernel 为 3、扩张率为 2、没有 padding 的 2D 卷积 扩张卷积向卷积层引入另一个参数「扩张率」。它决定了卷积核中值之间的空间。3x3 卷积核、扩张率为 2 的卷积视野和 5x5 卷积核的视野相同，并且前者仅使用了 9 个参数。想象一个 5x5 的卷积核，每个都删去第二行和第二列。 这种卷积用同样的计算成本生成了更大的视野。扩张卷积在实时分隔领域中尤为流行。如果你需要一个宽阔视野，但无力使用多个卷积或更大的卷积核，那么你可以使用这种卷积。 转置卷积（解卷积或微步卷积） 解卷积（deconvolution）这种叫法不太合适，因为这并不是解卷积。解卷积确实存在，但在深度学习领域中并不常见。真正的解卷积是卷积过程的逆转。想象一下将一个图像输入到单个卷积层上。再把输出放到黑箱中，然后再次输出的是原始输入图像。这个黑箱就叫作解卷积。这是卷积层执行的数学逆运算。 转置卷积与解卷积有一些相似，因为它所输出的空间分辨率反卷积层也能够输出。但是，在这些值上真正进行的数学运算是不一样的。转置卷积层使用的是常规的卷积，但仍然能够进行空间分辨率转换。 没有 padding、stride 为 2、卷积核为 3 的 2D 卷积 到这里你可能有些疑惑，那么让我们看一下具体的例子吧。一个 5x5 的图像输入到卷积层中，stride 设置为 2，没有 padding，卷积核为 3x3。输出的是 2x2 的图像。 如果我们想逆转该过程，则我们需要进行数学逆运算，以使我们输入的每个像素都能够生成 9 个值。之后，我们用值为 2 的 stride 遍历输出图像。这就是解卷积。 没有 padding、stride 为 2、kernel 为 3 的转置 2D 卷积 转置卷积并不这么做。二者唯一的共同点是输出的都是 5x5 的图像，虽然它执行的仍旧是常规的卷积运算。为了做到这一点，我们需要在输入上执行某种 padding。 如同你能想象的，这一步不会从顶部逆转该流程，至少在数值方面不会实现逆转。 它只不过是从前面重建了空间分辨率，且完成一个卷积。这可能不是数学意义上的逆转，但对编码器-解码器架构而言，它仍旧非常有帮助。通过这种方式，我们可以将卷积和图像的 upscaling 结合起来，而不是执行两个独立的流程。 可分离卷积 在可分离卷积中，我们能把卷积核运算分离到多个步骤中。例如，我们可以把一种卷积表达为 y = conv(x, k)，其中 y 是输出图像，x 是输入图像，k 是卷积核。下面，假设 k 按 k = k1.dot(k2) 进行计算。这样可使其成为可分离卷积，因为我们不再使用 k 做 2D 卷积，而是通过用 k1 和 k2 做两个 1D 卷积得到同样的结果。 Sobel X 与 Y 滤波器 拿经常用于图像处理的 Sobel 核为例。你可以通过乘以向量 [1, 0, -1] 和 [1,2,1] 的转置向量获得相同的核。在进行相同操作时，这只需要 6 个参数，而无需 9 个。 上述实例展示了空间可分离卷积，据我所知它并不用于深度学习。我只是想让大家在看到这个术语时不会感到困惑。在神经网络中，我们通常使用深度可分离卷积（depthwise separable convolution）。 这种卷积将执行空间卷积，同时保持通道分离，接着跟从深度卷积。为了便于理解，我们来看一个实例。 假设我们在 16 个输入通道和 32 个输出通道上有一个 3x3 卷积层。每一个输入通道都由 32 个 3x3 内核遍历，产生 512（16x32）个特征图。下一步，我们通过叠加每一个输入通道中的特征图，合并形成一个特征图。由于我们这样做了 32 次，我们得到了 32 个想要的输出通道。 对于相同实例上的深度可分离卷积，我们遍历了 16 个通道（每个带有一个 3x3 内核），得到了 16 个特征图。现在，在合并之前，我们遍历了这 16 个特征图（每个带有 32 个 1x1 卷积），然后再把它们叠加在一起。相比于上述的 4608（16x32x3x3）个参数，这产生了 656（16x3x3 + 16x32x1x1）个参数。 该实例是深度可分离卷积的特定实现，深度乘数是 1，这是目前这类卷积层的最常见设置。 我们这么做是因为假设空间和深度信息可被解耦。看 Xception 模型的表现，该理论似乎是成立的。因其对参数的高效使用，深度可分离卷积也可被用于移动设备。 原文地址：https://medium.com/towards-data-science/types-of-convolutions-in-deep-learning-717013397f4d 本文为机器之心编译，转载请联系本公众号获得授权。 ✄———————————————————————— 加入机器之心（全职记者/实习生）：hr@jiqizhixin.com 投稿或寻求报道：content@jiqizhixin.com 广告&amp;商务合作：bd@jiqizhixin.com 微信扫一扫关注该公众号]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（转）从入门到精通：卷积神经网络初学者指南]]></title>
    <url>%2F2017%2F07%2F12%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A%EF%BC%9A%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%9D%E5%AD%A6%E8%80%85%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[https://www.jiqizhixin.com/articles/2016-08-01-3 By 机器之心2016年8月01日 18:55 这是一篇向初学者讲解卷积神经网络的系列文章，机器之心编译了已经发表了的 Part 1 和 Part 2。此系列文章若有更新，机器之心依然会分享给大家。 Part 1：图像识别任务介绍卷积神经网络，听起来像是计算机科学、生物学和数学的诡异组合，但它们已经成为计算机视觉领域中最具影响力的革新的一部分。神经网络在 2012 年崭露头角，Alex Krizhevsky 凭借它们赢得了那一年的 ImageNet 挑战赛（大体上相当于计算机视觉的年度奥林匹克），他把分类误差记录从 26% 降到了 15%，在当时震惊了世界。自那之后，大量公司开始将深度学习用作服务的核心。Facebook 将神经网络用于自动标注算法、谷歌将它用于图片搜索、亚马逊将它用于商品推荐、Pinterest 将它用于个性化主页推送、Instagram 将它用于搜索架构。 然而，应用这些网络最经典最流行的案例是进行图像处理。在图像处理任务中，让我们看一下如何使用卷积神经网络进行图像分类。 问题空间图像分类是对输入图像的操作，最终输出一组最好地描述了图像内容的分类（如猫、狗等）或分类的概率。对人类来说，识别是打出生便开始学习的技能之一，对成人来说更是信手拈来，毫不费力。我们只需一眼便能快速识别我们所处的环境以及环绕在我们身边的物体。当我们看到一张图片或是环看四周的时候，无需刻意观察，多数时候也能立即描述出场景特征并标记出每一个对象。快速识别不同模式、根据早前知识进行归纳、以及适应不同的图像环境一直都是人类的专属技能，机器尚未享有。 输入与输出当计算机看到一张图像（输入一张图像）时，它看的是一大堆像素值。根据图片的分辨率和尺寸，它将看到一个 32 x 32 x 3 的数组（3 指代的是 RGB 值）。为了讲清楚这一点，假设我们有一张 JPG 格式的 480 x 480 大小的彩色图片，那么它对应的数组就有 480 x 480 x 3 个元素。其中每个数字的值从 0 到 255 不等，其描述了对应那一点的像素灰度。当我们人类对图像进行分类时，这些数字毫无用处，可它们却是计算机可获得的唯一输入。其中的思想是：当你提供给计算机这一数组后，它将输出描述该图像属于某一特定分类的概率的数字（比如：80% 是猫、15% 是狗、5% 是鸟）。 我们想要计算机做什么现在我们知道了问题所在以及输入与输出，就该考虑如何处理了。我们想要计算机能够区分开所有提供给它的图片，以及搞清楚猫猫狗狗各自的特有特征。这也是我们人类的大脑中不自觉进行着的过程。当我们看到一幅狗的图片时，如果有诸如爪子或四条腿之类的明显特征，我们便能将它归类为狗。同样地，计算机也可以通过寻找诸如边缘和曲线之类的低级特点来分类图片，继而通过一系列卷积层级建构出更为抽象的概念。这是 CNN（卷积神经网络）工作方式的大体概述，下面是具体细节。 生物学连接首先介绍些背景。当你第一次听到卷积神经网络这一术语，可能会联想到神经科学或生物学，那就对了。可以这样说。CNN 的确是从视觉皮层的生物学上获得启发的。视觉皮层有小部分细胞对特定部分的视觉区域敏感。Hubel 和 Wiesel 于 1962 年进行的一项有趣的试验详细说明了这一观点，他们验证出大脑中的一些个体神经细胞只有在特定方向的边缘存在时才能做出反应（即放电）。例如，一些神经元只对垂直边缘兴奋，另一些对水平或对角边缘兴奋。Hubel 和 Wisesl 发现所有这些神经元都以柱状结构的形式进行排列，而且一起工作才能产生视觉感知。这种一个系统中的特定组件有特定任务的观点（视觉皮层的神经元细胞寻找特定特征）在机器中同样适用，这就是 CNN 的基础。 结构回到细节上来。更为详细的 CNN 工作概述指的是你挑一张图像，让它历经一系列卷积层、非线性层、池化（下采样（downsampling））层和完全连接层，最终得到输出。正如之前所说，输出可以是最好地描述了图像内容的一个单独分类或一组分类的概率。如今，难点在于理解其中每一层的工作方法。我们先来看最重要的部分。 第一层——数学部分CNN 的第一层通常是卷积层（Convolutional Layer）。首先需要了解卷积层的输入内容是什么。如上所述，输入内容为一个 32 x 32 x 3 的像素值数组。现在，解释卷积层的最佳方法是想象有一束手电筒光正从图像的左上角照过。假设手电筒光可以覆盖 5 x 5 的区域，想象一下手电筒光照过输入图像的所有区域。在机器学习术语中，这束手电筒被叫做过滤器（filter，有时候也被称为神经元（neuron）或核（kernel）），被照过的区域被称为感受野（receptive field）。过滤器同样也是一个数组（其中的数字被称作权重或参数）。重点在于过滤器的深度必须与输入内容的深度相同（这样才能确保可以进行数学运算），因此过滤器大小为 5 x 5 x 3。现在，以过滤器所处在的第一个位置为例，即图像的左上角。当筛选值在图像上滑动（卷积运算）时，过滤器中的值会与图像中的原始像素值相乘（又称为计算点积）。这些乘积被加在一起（从数学上来说，一共会有 75 个乘积）。现在你得到了一个数字。切记，该数字只是表示过滤器位于图片左上角的情况。我们在输入内容上的每一位置重复该过程。（下一步将是将过滤器右移 1 单元，接着再右移 1 单元，以此类推。）输入内容上的每一特定位置都会产生一个数字。过滤器滑过所有位置后将得到一个 28 x 28 x 1 的数组，我们称之为激活映射（activation map）或特征映射（feature map）。之所以得到一个 28 x 28 的数组的原因在于，在一张 32 x 32 的输入图像上，5 x 5 的过滤器能够覆盖到 784 个不同的位置。这 784 个位置可映射为一个 28 x 28 的数组。 （注意：包括上图在内的一些图片来自于 Micheal Nielsen 的 「神经网络与深度学习（ Neural Networks and Deep Learning）」一书。我强烈推荐这本书。这本书可免费在线浏览.） 当我们使用两个而不是一个 5 x 5 x 3 的过滤器时，输出总量将会变成 28 x 28 x 2。采用的过滤器越多，空间维度（ spatial dimensions）保留得也就越好。数学上而言，这就是卷积层上发生的事情。 第一层——高层次角度不过，从高层次角度而言卷积是如何工作的？每个过滤器可以被看成是特征标识符（ feature identifiers）。这里的特征指的是例如直边缘、原色、曲线之类的东西。想一想所有图像都共有的一些最简单的特征。假设第一组过滤器是 7 x 7 x 3 的曲线检测器。（在这一节，为了易于分析，暂且忽略该过滤器的深度为 3 个单元，只考虑过滤器和图像的顶层层面。）作为曲线过滤器，它将有一个像素结构，在曲线形状旁时会产生更高的数值（切记，我们所讨论的过滤器不过是一组数值！） 左图：过滤器的像素表示；右图：曲线检测器过滤器的可视化；对比两图可以看到数值和形状的对应 ** 回到数学角度来看这一过程。当我们将过滤器置于输入内容的左上角时，它将计算过滤器和这一区域像素值之间的点积。拿一张需要分类的照片为例，将过滤器放在它的左上角。 左图：原始图像；右图：图像上过滤器的可视化 ** 切记，我们要做的是将过滤器与图像的原始像素值相乘。 左图：感受野的可视化；右图：感受野的像素表示 \ 过滤器的像素表示* ** 简单来说，如果输入图像上某个形状看起来很像过滤器表示的曲线，那么所有点积加在一起将会得出一个很大的值！让我们看看移动过滤器时会发生什么。 这个值小了很多！这是因为图像的这一部分和曲线检测器过滤器不存在对应。记住，这个卷积层的输出是一个激活映射（activation map）。因此，在这个带有一个过滤器卷积的例子里（当筛选值为曲线检测器），激活映射将会显示出图像里最像曲线的区域。在该例子中，28 x 28 x 1 的激活映射的左上角的值为 6600。高数值意味着很有可能是输入内容中的曲线激活了过滤器。激活地图右上角的值将会是 0，因为输入内容中没有任何东西能激活过滤器（更简单地说，原始图片中的这一区域没有任何曲线）。这仅仅是一组检测右弯曲线的过滤器。还有其它检测左弯曲线或直线边缘的过滤器。过滤器越多，激活映射的深度越大，我们对输入内容的了解也就越多。 声明：我在本小节中描绘的过滤器（filter）只是为了描述卷积中的数学过程。在下图中你可以看到训练后的网络中第一个卷积层的过滤器的实际可视化。尽管如此，主要观点仍旧不变。当在输入内容中寻找特定特征时，第一层上的过滤器在输入图像上进行卷积运算和「激活」（即计算高数值）。 上图来自于斯坦福大学由 Andrej Karpathy 和 Justin Johnson 授课的 CS 231N课程，推荐给渴望更深层理解 CNN 的人们。 网络中的更深处在传统卷积神经网络架构中，卷积层之间还有其它类型的层。我强烈建议有兴趣的人阅读和它们有关的材料，并理解相应的功能和作用；但总的来说，它们提供的非线性和维度保留有助于提高网络的稳健性（robustness）并控制过拟合。一个典型的 CNN 结构看起来是这样的： 输入→卷积→ReLU→卷积→ReLU→池化→ReLU→卷积→ReLU→池化→全连接 ** 我们稍后再来讨论关键的最后一层，先回顾一下学到了哪些。我们讨论了过滤器是如何在第一个卷积层检测特征的。它们检测边缘和曲线一类的低级特征。正如想象的那样，为了预测出图片内容的分类，网络需要识别更高级的特征，例如手、爪子与耳朵的区别。第一个卷积层的输出将会是一个 28 x 28 x 3 的数组（假设我们采用三个 5 x 5 x 3 的过滤器）。当我们进入另一卷积层时，第一个卷积层的输出便是第二个卷积层的输入。解释这一点有些困难。第一层的输入是原始图像，而第二卷积层的输入正是第一层输出的激活映射。也就是说，这一层的输入大体描绘了低级特征在原始图片中的位置。在此基础上再采用一组过滤器（让它通过第 2 个卷积层），输出将是表示了更高级的特征的激活映射。这类特征可以是半圆（曲线和直线的组合）或四边形（几条直线的组合）。随着进入网络越深和经过更多卷积层后，你将得到更为复杂特征的激活映射。在网络的最后，可能会有一些过滤器会在看到手写笔迹或粉红物体等时激活。如果你想知道更多关于可视化卷积网络中过滤器的内容，可以查看 Matt Zeiler 和 Rob Fergus 的一篇讨论该问题的颇为杰出的研究论文。在 YouTube 上，Jason Yosinski 有一段视频十分视觉化地呈现了这一过程（如下）。有趣的是，越深入网络，过滤器的感受野越大，意味着它们能够处理更大范围的原始输入内容（或者说它们可以对更大区域的像素空间产生反应）。 完全连接层检测高级特征之后，网络最后的完全连接层就更是锦上添花了。简单地说，这一层处理输入内容（该输入可能是卷积层、ReLU 层或是池化层的输出）后会输出一个 N 维向量，N 是该程序必须选择的分类数量。例如，如果你想得到一个数字分类程序，如果有 10 个数字，N 就等于 10。这个 N 维向量中的每一数字都代表某一特定类别的概率。例如，如果某一数字分类程序的结果矢量是 [0 .1 .1 .75 0 0 0 0 0 .05]，则代表该图片有 10% 的概率是 1、10% 的概率是 2、75% 的概率是 3、还有 5% 的概率是 9（注：还有其他表现输出的方式，这里只展示了 softmax 的方法）。完全连接层观察上一层的输出（其表示了更高级特征的激活映射）并确定这些特征与哪一分类最为吻合。例如，如果该程序预测某一图像的内容为狗，那么激活映射中的高数值便会代表一些爪子或四条腿之类的高级特征。同样地，如果程序测定某一图片的内容为鸟，激活映射中的高数值便会代表诸如翅膀或鸟喙之类的高级特征。大体上来说，完全连接层观察高级特征和哪一分类最为吻合和拥有怎样的特定权重，因此当计算出权重与先前层之间的点积后，你将得到不同分类的正确概率。 一种全卷积神经网络（LeNet），从左至右依次为卷积→子采样→卷积→子采样→全连接→全连接→高斯连接 训练（也就是：什么能让其有效）下面是神经网络中的一个我尚未提及但却最为重要的部分。阅读过程中你可能会提出许多问题。第一卷积层中的滤波器是如何知道寻找边缘与曲线的？完全连接层怎么知道观察哪些激活图？每一层级的滤波器如何知道需要哪些值？计算机通过一个名为反向传播的训练过程来调整过滤器值（或权重）。 在探讨反向传播之前，我们首先必须回顾一下神经网络工作起来需要什么。在我们刚出生的时候，大脑一无所知。我们不晓得猫啊狗啊鸟啊都是些什么东西。与之类似的是 CNN 刚开始的时候，权重或过滤器值都是随机的。滤波器不知道要去寻找边缘和曲线。更高层的过滤器值也不知道要去寻找爪子和鸟喙。不过随着年岁的增长，父母和老师向我们介绍各式各样的图片并且一一作出标记。CNN 经历的便是一个介绍图片与分类标记的训练过程。在深入探讨之前，先设定一个训练集，在这里有上千张狗、猫、鸟的图片，每一张都依照内容被标记。下面回到反向传播的问题上来。 反向传播可分为四部分，分别是前向传导、损失函数、后向传导，以及权重更新。在前向传导中，选择一张 32×32×3 的数组训练图像并让它通过整个网络。在第一个训练样例上，由于所有的权重或者过滤器值都是随机初始化的，输出可能会是 [.1 .1 .1 .1 .1 .1 .1 .1 .1 .1]，即一个不偏向任何数字的输出。一个有着这样权重的网络无法寻找低级特征，或者说是不能做出任何合理的分类。接下来是反向传播的损失函数部分。切记我们现在使用的是既有图像又有标记的训练数据。假设输入的第一张训练图片为 3，标签将会是 [0 0 0 1 0 0 0 0 0 0]。损失函数有许多种定义方法，常见的一种是 MSE （均方误差）。 假设变量 L 等同该数值。正如所料，前两张训练图片的损失将会极高。现在，我们直观地想一下。我们想要预测标记（卷积网络的输出）与训练标记相同（意味着网络预测正确）。为了做到这一点，我们想要将损失数量最小化。将其视为微积分优化问题的话，也就是说我们想要找出是哪部分输入（例子中的权重）直接导致了网络的损失（或错误）。 这是一个 dL/dW 的数学等式，W 是特定层级的权重。我们接下来要做的是在网络中进行后向传导，测定出是哪部分权重导致了最大的损失，寻找调整方法并减少损失。一旦计算出该导数，将进行最后一步也就是权重更新。所有的过滤器的权重将会更新，以便它们顺着梯度方向改变。 学习速率是一个由程序员决定的参数。高学习速率意味着权重更新的动作更大，因此可能该模式将花费更少的时间收敛到最优权重。然而，学习速率过高会导致跳动过大，不够准确以致于达不到最优点。 总的来说，前向传导、损失函数、后向传导、以及参数更新被称为一个学习周期。对每一训练图片，程序将重复固定数目的周期过程。一旦完成了最后训练样本上的参数更新，网络有望得到足够好的训练，以便层级中的权重得到正确调整。 测试最后，为了检验 CNN 能否工作，我们准备不同的另一组图片与标记集（不能在训练和测试中使用相同的！）并让它们通过这个 CNN。我们将输出与实际情况（ground truth ）相比较，看看网络是否有效！ 企业如何使用 CNN数据、数据、数据。数据越多的企业在竞争中越发彰显优势。你提供给网络的训练数据越多，你能进行的训练迭代也越多，紧接着权重更新也多，那么当用于产品时调整出的网络自然就好。Facebook （和 Instagram）可以使用它如今拥有的十几亿用户的图片，Pinterest 可以使用它站点上 500 亿花瓣的信息，谷歌可以使用搜索数据，亚马逊可以使用每天销售的数以百万计的商品数据。而你现在也知道它们使用数据背后的神奇之处了。 Part 2：卷积神经网络中的部分问题 引言在这篇文章中，我们将更深入地介绍有关卷积神经网络（ConvNet）的详细情况。声明：我确实知道本文中一部分内容相当复杂，可以用一整篇文章进行介绍。但为了在保持全面性的同时保证简洁，我会在文章中相关位置提供一些更详细解释该相关主题的论文链接。 步幅和填充好了，现在来看一下我们的卷积神经网络。还记得过滤器、感受野和卷积吗？很好。现在，要改变每一层的行为，有两个主要参数是我们可以调整的。选择了过滤器的尺寸以后，我们还需要选择步幅（stride）和填充（padding）。 步幅控制着过滤器围绕输入内容进行卷积计算的方式。在第一部分我们举的例子中，过滤器通过每次移动一个单元的方式对输入内容进行卷积。过滤器移动的距离就是步幅。在那个例子中，步幅被默认设置为1。步幅的设置通常要确保输出内容是一个整数而非分数。让我们看一个例子。想象一个 7 x 7 的输入图像，一个 3 x 3 过滤器（简单起见不考虑第三个维度），步幅为 1。这是一种惯常的情况。 还是老一套，对吧？看你能不能试着猜出如果步幅增加到 2，输出内容会怎么样。 所以，正如你能想到的，感受野移动了两个单元，输出内容同样也会减小。注意，如果试图把我们的步幅设置成 3，那我们就会难以调节间距并确保感受野与输入图像匹配。正常情况下，程序员如果想让接受域重叠得更少并且想要更小的空间维度（spatial dimensions）时，他们会增加步幅。 现在让我们看一下填充（padding）。在此之前，想象一个场景：当你把 5 x 5 x 3 的过滤器用在 32 x 32 x 3 的输入上时，会发生什么？输出的大小会是 28 x 28 x 3。注意，这里空间维度减小了。如果我们继续用卷积层，尺寸减小的速度就会超过我们的期望。在网络的早期层中，我们想要尽可能多地保留原始输入内容的信息，这样我们就能提取出那些低层的特征。比如说我们想要应用同样的卷积层，但又想让输出量维持为 32 x 32 x 3 。为做到这点，我们可以对这个层应用大小为 2 的零填充（zero padding）。零填充在输入内容的边界周围补充零。如果我们用两个零填充，就会得到一个 36 x 36 x 3 的输入卷。 如果我们在输入内容的周围应用两次零填充，那么输入量就为 32×32×3。然后，当我们应用带有 3 个 5×5×3 的过滤器，以 1 的步幅进行处理时，我们也可以得到一个 32×32×3 的输出 如果你的步幅为 1，而且把零填充设置为 K 是过滤器尺寸，那么输入和输出内容就总能保持一致的空间维度。 计算任意给定卷积层的输出的大小的公式是 其中 O 是输出尺寸，K 是过滤器尺寸，P 是填充，S 是步幅。 选择超参数我们怎么知道要用多少层、多少卷积层、过滤器尺寸是多少、以及步幅和填充值多大呢？这些问题很重要，但又没有一个所有研究人员都在使用的固定标准。这是因为神经网络很大程度上取决于你的数据类型。图像的大小、复杂度、图像处理任务的类型以及其他更多特征的不同都会造成数据的不同。对于你的数据集，想出如何选择超参数的一个方法是找到能创造出图像在合适尺度上抽象的正确组合。 ReLU（修正线性单元）层在每个卷积层之后，通常会立即应用一个非线性层（或激活层）。其目的是给一个在卷积层中刚经过线性计算操作（只是数组元素依次（element wise）相乘与求和）的系统引入非线性特征。过去，人们用的是像双曲正切和 S 型函数这样的非线性方程，但研究者发现 ReLU 层效果好得多，因为神经网络能够在准确度不发生明显改变的情况下把训练速度提高很多（由于计算效率增加）。它同样能帮助减轻梯度消失的问题——由于梯度以指数方式在层中消失，导致网络较底层的训练速度非常慢。（这也许超出了本文的范围，但这里和这里有更好的解释.）ReLU 层对输入内容的所有值都应用了函数 f(x) = max(0, x)。用基本术语来说，这一层把所有的负激活（negative activation）都变为零。这一层会增加模型乃至整个神经网络的非线性特征，而且不会影响卷积层的感受野。 参见 Geoffrey Hinton（即深度学习之父）的论文：Rectified Linear Units Improve Restricted Boltzmann Machines 池化层在几个 ReLU 层之后，程序员也许会选择用一个池化层（pooling layer）。它同时也被叫做下采样（downsampling）层。在这个类别中，也有几种可供选择的层，最受欢迎的就是最大池化（ max-pooling）。它基本上采用了一个过滤器（通常是 2x2 的）和一个同样长度的步幅。然后把它应用到输入内容上，输出过滤器卷积计算的每个子区域中的最大数字。 带有 2×2 和过滤器的且步幅为 2 的最大池化的例子 ** 池化层还有其他选择，比如平均池化（average pooling）和 L2-norm 池化 。这一层背后的直观推理是：一旦我们知道了原始输入（这里会有一个高激活值）中一个特定的特征，它与其它特征的相对位置就比它的绝对位置更重要。可想而知，这一层大幅减小了输入卷的空间维度（长度和宽度改变了，但深度没变）。这到达了两个主要目的。第一个是权重参数的数目减少到了75%，因此降低了计算成本。第二是它可以控制过拟合（overfitting）。这个术语是指一个模型与训练样本太过匹配了，以至于用于验证和检测组时无法产生出好的结果。出现过拟合的表现是一个模型在训练集能达到 100% 或 99% 的准确度，而在测试数据上却只有50%。 Dropout 层如今，Dropout 层在神经网络有了非常明确的功能。上一节，我们讨论了经过训练后的过拟合问题：训练之后，神经网络的权重与训练样本太过匹配以至于在处理新样本的时候表现平平。Dropout 的概念在本质上非常简单。Dropout 层将「丢弃（drop out）」该层中一个随机的激活参数集，即在前向通过（forward pass）中将这些激活参数集设置为 0。简单如斯。既然如此，这些简单而且似乎不必要且有些反常的过程的好处是什么？在某种程度上，这种机制强制网络变得更加冗余。这里的意思是：该网络将能够为特定的样本提供合适的分类或输出，即使一些激活参数被丢弃。此机制将保证神经网络不会对训练样本「过于匹配」，这将帮助缓解过拟合问题。另外，Dropout 层只能在训练中使用，而不能用于测试过程，这是很重要的一点。 参考 Geoffrey Hinton 的论文：Dropout: A Simple Way to Prevent Neural Networks from Overfitting 网络层中的网络网络层中的网络指的是一个使用了 1 x 1 尺寸的过滤器的卷积层。现在，匆匆一瞥，你或许会好奇为何这种感受野大于它们所映射空间的网络层竟然会有帮助。然而，我们必须谨记 1x1 的卷积层跨越了特定深度，所以我们可以设想一个1 x 1 x N 的卷积层，此处 N 代表该层应用的过滤器数量。该层有效地使用 N 维数组元素依次相乘的乘法，此时 N 代表的是该层的输入的深度。 参阅 Min Lin 的论文：Network In Network 分类、定位、检测、分割本系列第一部分使用的案例中，我们观察了图像分类任务。这个过程是：获取输入图片，输出一套分类的类数（class number)。然而当我们执行类似目标定位的任务时，我们要做的不只是生成一个类标签,而是生成一个描述图片中物体suo所在位置的边界框。 我们也有目标检测的任务，这需要图片上所有目标的定位任务都已完成。 因此，你将获得多个边界框和多个类标签。 最终，我们将执行目标分割的任务：我们需要输出类标签的同时输出图片中每个目标的轮廓。 关于目标检测、定位、分割的论文有很多，这里就不一一列出了。可以参考的有： 目标检测/定位: RCNN, Fast RCNN, Faster RCNN, MultiBox, Bayesian Optimization, Multi-region, RCNN Minus R, Image Windows 分割: Semantic Seg, Unconstrained Video, Shape Guided, Object Regions, Shape Sharing 迁移学习如今，深度学习领域一个常见的误解在于没有谷歌那样的巨量数据，你将没有希望创建一个有效的深度学习模型。尽管数据是创建网络中至关重要的部分，迁移学习的思路将帮助我们降低数据需求。迁移学习指的是利用预训练模型（神经网络的权重和参数都已经被其他人利用更大规模的数据集训练好了）并用自己的数据集将模型「微调」的过程。这种思路中预训练模型扮演着特征提取器的角色。你将移除网络的最后一层并用你自有的分类器置换（取决于你的问题空间）。然后冻结其他所有层的权重并正常训练该网络（冻结这些层意味着在梯度下降/最优化过程中保持权值不变）。 让我们探讨一下为什么做这项工作。比如说我们正在讨论的这个预训练模型是在 ImageNet （一个包含一千多个分类，一千四百万张图像的数据集）上训练的 。当我们思考神经网络的较低层时，我们知道它们将检测类似曲线和边缘这样的特征。现在，除非你有一个极为独特的问题空间和数据集，你的神经网络也会检测曲线和边缘这些特征。相比通过随机初始化权重训练整个网络，我们可以利用预训练模型的权重（并冻结）聚焦于更重要的层（更高层）进行训练。如果你的数据集不同于 ImageNet 这样的数据集，你必须训练更多的层级而只冻结一些低层的网络。 Yoshua Bengio （另外一个深度学习先驱 ）论文：How transferable are features in deep neural networks? Ali Sharif Razavian 论文：CNN Features off-the-shelf: an Astounding Baseline for Recognition Jeff Donahue 论文：DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition 数据增强技术现在我们对卷积网络中数据的重要性可能已经感到有些麻木了，所以我们来谈下如何利用一些简单的转换方法将你现有的数据集变得更大。正如我们之前所提及的，当计算机将图片当作输入时，它将用一个包含一列像素值的数组描述（这幅图）。若是图片左移一个像素。对你和我来说，这种变化是微不可察的。然而对计算机而已，这种变化非常显著：这幅图的类别和标签保持不变，数组却变化了。这种改变训练数据的数组表征而保持标签不变的方法被称作数据增强技术。这是一种人工扩展数据集的方法。人们经常使用的增强方法包括灰度变化、水平翻转、垂直翻转、随机编组、色值跳变、翻译、旋转等其他多种方法。通过利用这些训练数据的转换方法，你将获得两倍甚至三倍于原数据的训练样本。 声明：本文由机器之心编译出品，原文Github.io，作者：Adit Deshpande，翻译参与：牛春雨、Quantum、原野、吴攀、李亚洲，转载请查看要求，机器之心对于违规侵权者保有法律追诉权。 卷积神经网络理论综述论文论文 ****** ** 提交评论 登录后参与评论去登录 关于我们寻求报道商务合作 ©2017版权所有 机器之心（北京）科技有限公司 京 ICP 备 12027496 全球人工智能信息服务 友情链接Synced Global机器之心 Medium 博客PaperWeekly网易智能动脉网硬蛋网 ******** 联系电话：+86 010-57150141 联系邮箱：contact@jiqizhixin.com]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（转）机器视角：长文揭秘图像处理和卷积神经网络架构]]></title>
    <url>%2F2017%2F07%2F12%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%92%EF%BC%9A%E9%95%BF%E6%96%87%E6%8F%AD%E7%A7%98%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%92%8C%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650728746&amp;idx=1&amp;sn=61e9cb824501ec7c505eb464e8317915&amp;chksm=871b2d54b06ca442bc049e97c97e117455fd31bd0fb0619be4592eebd0958c26e3c223bfbfe5&amp;scene=21#wechat_redirect 2017-07-06 机器之心 选自 Analyticsvidhya** 机器之心编译 作者：DISHASHREE GUPTA ** 近日，Dishashree Gupta 在 Analyticsvidhya 上发表了一篇题为《Architecture of Convolutional Neural Networks (CNNs) demystified》的文章，对用于图像识别和分类的卷积神经网络架构作了深度揭秘；作者在文中还作了通盘演示，期望对 CNN 的工作机制有一个深入的剖析。机器之心对本文进行了编译，原文链接见文末。 引言先坦白地说，有一段时间我无法真正理解深度学习。我查看相关研究论文和文章，感觉深度学习异常复杂。我尝试去理解神经网络及其变体，但依然感到困难。 接着有一天，我决定一步一步，从基础开始。我把技术操作的步骤分解开来，并手动执行这些步骤（和计算），直到我理解它们如何工作。这相当费时，且令人紧张，但是结果非凡。 现在，我不仅对深度学习有了全面的理解，还在此基础上有了好想法，因为我的基础很扎实。随意地应用神经网络是一回事，理解它是什么以及背后的发生机制是另外一回事。 今天，我将与你共享我的心得，展示我如何上手卷积神经网络并最终弄明白了它。我将做一个通盘的展示，从而使你对 CNN 的工作机制有一个深入的了解。 在本文中，我将会讨论 CNN 背后的架构，其设计初衷在于解决图像识别和分类问题。同时我也会假设你对神经网络已经有了初步了解。 目录1.机器如何看图？ 2.如何帮助神经网络识别图像？ 3.定义卷积神经网络 卷积层 池化层 输出层 4.小结 5.使用 CNN 分类图像 1. 机器如何看图？人类大脑是一非常强大的机器，每秒内能看（捕捉）多张图，并在意识不到的情况下就完成了对这些图的处理。但机器并非如此。机器处理图像的第一步是理解，理解如何表达一张图像，进而读取图片。 简单来说，每个图像都是一系列特定排序的图点（像素）。如果你改变像素的顺序或颜色，图像也随之改变。举个例子，存储并读取一张上面写着数字 4 的图像。 基本上，机器会把图像打碎成像素矩阵，存储每个表示位置像素的颜色码。在下图的表示中，数值 1 是白色，256 是最深的绿色（为了简化，我们示例限制到了一种颜色）。 一旦你以这种格式存储完图像信息，下一步就是让神经网络理解这种排序与模式。 2. 如何帮助神经网络识别图像？表征像素的数值是以特定的方式排序的。 假设我们尝试使用全连接网络识别图像，该如何做？ 全连接网络可以通过平化它，把图像当作一个数组，并把像素值当作预测图像中数值的特征。明确地说，让网络理解理解下面图中发生了什么，非常的艰难。 即使人类也很难理解上图中表达的含义是数字 4。我们完全丢失了像素的空间排列。 我们能做什么呢？可以尝试从原图像中提取特征，从而保留空间排列。 案例 1这里我们使用一个权重乘以初始像素值。 现在裸眼识别出这是「4」就变得更简单了。但把它交给全连接网络之前，还需要平整化（flatten) 它，要让我们能够保留图像的空间排列。 案例 2现在我们可以看到，把图像平整化完全破坏了它的排列。我们需要想出一种方式在没有平整化的情况下把图片馈送给网络，并且还要保留空间排列特征，也就是需要馈送像素值的 2D/3D 排列。 我们可以尝试一次采用图像的两个像素值，而非一个。这能给网络很好的洞见，观察邻近像素的特征。既然一次采用两个像素，那也就需要一次采用两个权重值了。 希望你能注意到图像从之前的 4 列数值变成了 3 列。因为我们现在一次移用两个像素（在每次移动中像素被共享），图像变的更小了。虽然图像变小了，我们仍能在很大程度上理解这是「4」。而且，要意识到的一个重点是，我们采用的是两个连贯的水平像素，因此只会考虑水平的排列。 这是我们从图像中提取特征的一种方式。我们可以看到左边和中间部分，但右边部分看起来不那么清楚。主要是因为两个问题： 图片角落左边和右边是权重相乘一次得到的。 左边仍旧保留，因为权重值高；右边因为略低的权重，有些丢失。 现在我们有两个问题，需要两个解决方案。 案例 3遇到的问题是图像左右两角只被权重通过一次。我们需要做的是让网络像考虑其他像素一样考虑角落。我们有一个简单的方法解决这一问题：把零放在权重运动的两边。 你可以看到通过添加零，来自角落的信息被再训练。图像也变得更大。这可被用于我们不想要缩小图像的情况下。 案例 4这里我们试图解决的问题是右侧角落更小的权重值正在降低像素值，因此使其难以被我们识别。我们所能做的是采取多个权重值并将其结合起来。 (1,0.3) 的权重值给了我们一个输出表格 同时表格 (0.1,5) 的权重值也将给我们一个输出表格。 两张图像的结合版本将会给我们一个清晰的图片。因此，我们所做的是简单地使用多个权重而不是一个，从而再训练图像的更多信息。最终结果将是上述两张图像的一个结合版本。 案例 5我们到现在通过使用权重，试图把水平像素（horizontal pixel）结合起来。但是大多数情况下我们需要在水平和垂直方向上保持空间布局。我们采取 2D 矩阵权重，把像素在水平和垂直方向上结合起来。同样，记住已经有了水平和垂直方向的权重运动，输出会在水平和垂直方向上低一个像素。 特别感谢 Jeremy Howard 启发我创作了这些图像。 因此我们做了什么？上面我们所做的事是试图通过使用图像的空间的安排从图像中提取特征。为了理解图像，理解像素如何安排对于一个网络极其重要。上面我们所做的也恰恰是一个卷积网络所做的。我们可以采用输入图像，定义权重矩阵，并且输入被卷积以从图像中提取特殊特征而无需损失其有关空间安排的信息。 这个方法的另一个重大好处是它可以减少图像的参数数量。正如所见，卷积图像相比于原始图像有更少的像素。 3.定义一个卷积神经网络我们需要三个基本的元素来定义一个基本的卷积网络 卷积层 池化层（可选） 输出层 卷积层在这一层中，实际所发生的就像我们在上述案例 5 中见到的一样。假设我们有一个 6*6 的图像。我们定义一个权值矩阵，用来从图像中提取一定的特征。 我们把权值初始化成一个 33 的矩阵。这个权值现在应该与图像结合，所有的像素都被覆盖至少一次，从而来产生一个卷积化的输出。上述的 429，是通过计算权值矩阵和输入图像的 33 高亮部分以元素方式进行的乘积的值而得到的。 现在 66 的图像转换成了 44 的图像。想象一下权值矩阵就像用来刷墙的刷子。首先在水平方向上用这个刷子进行刷墙，然后再向下移，对下一行进行水平粉刷。当权值矩阵沿着图像移动的时候，像素值再一次被使用。实际上，这样可以使参数在卷积神经网络中被共享。 下面我们以一个真实图像为例。 权值矩阵在图像里表现的像一个从原始图像矩阵中提取特定信息的过滤器。一个权值组合可能用来提取边缘（edge）信息，另一个可能是用来提取一个特定颜色，下一个就可能就是对不需要的噪点进行模糊化。 先对权值进行学习，然后损失函数可以被最小化，类似于多层感知机（MLP）。因此需要通过对参数进行学习来从原始图像中提取信息，从而来帮助网络进行正确的预测。当我们有多个卷积层的时候，初始层往往提取较多的一般特征，随着网络结构变得更深，权值矩阵提取的特征越来越复杂，并且越来越适用于眼前的问题。 步长（stride）和边界（padding）的概念像我们在上面看到的一样，过滤器或者说权值矩阵，在整个图像范围内一次移动一个像素。我们可以把它定义成一个超参数（hyperparameter），从而来表示我们想让权值矩阵在图像内如何移动。如果权值矩阵一次移动一个像素，我们称其步长为 1。下面我们看一下步长为 2 时的情况。 你可以看见当我们增加步长值的时候，图像的规格持续变小。在输入图像四周填充 0 边界可以解决这个问题。我们也可以在高步长值的情况下在图像四周填加不只一层的 0 边界。 我们可以看见在我们给图像填加一层 0 边界后，图像的原始形状是如何被保持的。由于输出图像和输入图像是大小相同的，所以这被称为 same padding。 这就是 same padding（意味着我们仅考虑输入图像的有效像素）。中间的 4*4 像素是相同的。这里我们已经利用边界保留了更多信息，并且也已经保留了图像的原大小。 多过滤与激活图需要记住的是权值的纵深维度（depth dimension）和输入图像的纵深维度是相同的。权值会延伸到输入图像的整个深度。因此，和一个单一权值矩阵进行卷积会产生一个单一纵深维度的卷积化输出。大多数情况下都不使用单一过滤器（权值矩阵），而是应用维度相同的多个过滤器。 每一个过滤器的输出被堆叠在一起，形成卷积图像的纵深维度。假设我们有一个 32323 的输入。我们使用 553，带有 valid padding 的 10 个过滤器。输出的维度将会是 282810。 如下图所示： 激活图是卷积层的输出。 池化层有时图像太大，我们需要减少训练参数的数量，它被要求在随后的卷积层之间周期性地引进池化层。池化的唯一目的是减少图像的空间大小。池化在每一个纵深维度上独自完成，因此图像的纵深保持不变。池化层的最常见形式是最大池化。 在这里，我们把步幅定为 2，池化尺寸也为 2。最大化执行也应用在每个卷机输出的深度尺寸中。正如你所看到的，最大池化操作后，44 卷积的输出变成了 22。 让我们看看最大池化在真实图片中的效果如何。 正如你看到的，我们卷积了图像，并最大池化了它。最大池化图像仍然保留了汽车在街上的信息。如果你仔细观察的话，你会发现图像的尺寸已经减半。这可以很大程度上减少参数。 同样，其他形式的池化也可以在系统中应用，如平均池化和 L2 规范池化。 输出维度理解每个卷积层输入和输出的尺寸可能会有点难度。以下三点或许可以让你了解输出尺寸的问题。有三个超参数可以控制输出卷的大小。 过滤器数量-输出卷的深度与过滤器的数量成正比。请记住该如何堆叠每个过滤器的输出以形成激活映射。激活图的深度等于过滤器的数量。 步幅（Stride）-如果步幅是 1，那么我们处理图片的精细度就进入单像素级别了。更高的步幅意味着同时处理更多的像素，从而产生较小的输出量。 零填充（zero padding）-这有助于我们保留输入图像的尺寸。如果添加了单零填充，则单步幅过滤器的运动会保持在原图尺寸。 我们可以应用一个简单的公式来计算输出尺寸。输出图像的空间尺寸可以计算为（[W-F + 2P] / S）+1。在这里，W 是输入尺寸，F 是过滤器的尺寸，P 是填充数量，S 是步幅数字。假如我们有一张 32323 的输入图像，我们使用 10 个尺寸为 333 的过滤器，单步幅和零填充。 那么 W=32，F=3，P=0，S=1。输出深度等于应用的滤波器的数量，即 10，输出尺寸大小为 ([32-3+0]/1)+1 = 30。因此输出尺寸是 303010。 输出层在多层卷积和填充后，我们需要以类的形式输出。卷积和池化层只会提取特征，并减少原始图像带来的参数。然而，为了生成最终的输出，我们需要应用全连接层来生成一个等于我们需要的类的数量的输出。仅仅依靠卷积层是难以达到这个要求的。卷积层可以生成 3D 激活图，而我们只需要图像是否属于一个特定的类这样的内容。输出层具有类似分类交叉熵的损失函数，用于计算预测误差。一旦前向传播完成，反向传播就会开始更新权重与偏差，以减少误差和损失。 4. 小结正如你所看到的，CNN 由不同的卷积层和池化层组成。让我们看看整个网络是什么样子： 我们将输入图像传递到第一个卷积层中，卷积后以激活图形式输出。图片在卷积层中过滤后的特征会被输出，并传递下去。 每个过滤器都会给出不同的特征，以帮助进行正确的类预测。因为我们需要保证图像大小的一致，所以我们使用同样的填充（零填充），否则填充会被使用，因为它可以帮助减少特征的数量。 随后加入池化层进一步减少参数的数量。 在预测最终提出前，数据会经过多个卷积和池化层的处理。卷积层会帮助提取特征，越深的卷积神经网络会提取越具体的特征，越浅的网络提取越浅显的特征。 如前所述，CNN 中的输出层是全连接层，其中来自其他层的输入在这里被平化和发送，以便将输出转换为网络所需的参数。 随后输出层会产生输出，这些信息会互相比较排除错误。损失函数是全连接输出层计算的均方根损失。随后我们会计算梯度错误。 错误会进行反向传播，以不断改进过滤器（权重）和偏差值。 一个训练周期由单次正向和反向传递完成。 5. 在 KERAS 中使用 CNN 对图像进行分类让我们尝试一下，输入猫和狗的图片，让计算机识别它们。这是图像识别和分类的经典问题，机器在这里需要做的是看到图像，并理解猫与狗的不同外形特征。这些特征可以是外形轮廓，也可以是猫的胡须之类，卷积层会攫取这些特征。让我们把数据集拿来试验一下吧。 以下这些图片均来自数据集。 我们首先需要调整这些图像的大小，让它们形状相同。这是处理图像之前通常需要做的，因为在拍照时，让照下的图像都大小相同几乎不可能。 为了简化理解，我们在这里只用一个卷积层和一个池化层。注意：在 CNN 的应用阶段，这种简单的情况是不会发生的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#import various packagesimport osimport numpy as npimport pandas as pdimport scipyimport sklearnimport kerasfrom keras.models import Sequentialimport cv2from skimage import io%matplotlib inline#Defining the File Pathcat=os.listdir(&quot;/mnt/hdd/datasets/dogs_cats/train/cat&quot;)dog=os.listdir(&quot;/mnt/hdd/datasets/dogs_cats/train/dog&quot;)filepath=&quot;/mnt/hdd/datasets/dogs_cats/train/cat/&quot;filepath2=&quot;/mnt/hdd/datasets/dogs_cats/train/dog/&quot;#Loading the Imagesimages=[]label = []for i in cat:image = scipy.misc.imread(filepath+i)images.append(image)label.append(0) #for cat imagesfor i in dog:image = scipy.misc.imread(filepath2+i)images.append(image)label.append(1) #for dog images#resizing all the imagesfor i in range(0,23000):images[i]=cv2.resize(images[i],(300,300))#converting images to arraysimages=np.array(images)label=np.array(label)# Defining the hyperparametersfilters=10filtersize=(5,5)epochs =5batchsize=128input_shape=(300,300,3)#Converting the target variable to the required sizefrom keras.utils.np_utils import to_categoricallabel = to_categorical(label)#Defining the modelmodel = Sequential()model.add(keras.layers.InputLayer(input_shape=input_shape))model.add(keras.layers.convolutional.Conv2D(filters, filtersize, strides=(1, 1), padding=&apos;valid&apos;, data_format=&quot;channels_last&quot;, activation=&apos;relu&apos;))model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))model.add(keras.layers.Flatten())model.add(keras.layers.Dense(units=2, input_dim=50,activation=&apos;softmax&apos;))model.compile(loss=&apos;categorical_crossentropy&apos;, optimizer=&apos;adam&apos;, metrics=[&apos;accuracy&apos;])model.fit(images, label, epochs=epochs, batch_size=batchsize,validation_split=0.3)model.summary() 在这一模型中，我只使用了单一卷积和池化层，可训练参数是 219,801。很好奇如果我在这种情况使用了 MLP 会有多少参数。通过增加更多的卷积和池化层，你可以进一步降低参数的数量。我们添加的卷积层越多，被提取的特征就会更具体和复杂。 在该模型中，我只使用了一个卷积层和池化层，可训练参数量为 219,801。如果想知道使用 MLP 在这种情况下会得到多少，你可以通过加入更多卷积和池化层来减少参数的数量。越多的卷积层意味着提取出来的特征更加具体，更加复杂。 结语希望本文能够让你认识卷积神经网络，这篇文章没有深入 CNN 的复杂数学原理。如果希望增进了解，你可以尝试构建自己的卷积神经网络，借此来了解它运行和预测的原理。** 原文链接：https://www.analyticsvidhya.com/blog/2017/06/architecture-of-convolutional-neural-networks-simplified-demystified/ 本文为机器之心编译，转载请联系本公众号获得授权。 ✄———————————————————————— 加入机器之心（全职记者/实习生）：hr@jiqizhixin.com 投稿或寻求报道：editor@jiqizhixin.com 广告&amp;商务合作：bd@jiqizhixin.com 点击阅读原文，查看机器之心官网↓↓↓ 阅读原文 微信扫一扫关注该公众号]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（转）深度 | 理解深度学习中的卷积]]></title>
    <url>%2F2017%2F07%2F12%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F%E6%B7%B1%E5%BA%A6%20%7C%20%E7%90%86%E8%A7%A3%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%8D%B7%E7%A7%AF%2F</url>
    <content type="text"><![CDATA[https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650724906&amp;idx=3&amp;sn=b6e1e70fb625222d0d0f1b30741e4e29&amp;chksm=871b1e54b06c974297180145f79525a0fbdb25b171aefd659ad1fac23356be3ad024c1e1a211&amp;scene=21#wechat_redirect 2017-03-28 码农场 机器之心 机器之心经授权转载 来源**：码农场** 译者按：本文译自 Tim Dettmers 的 Understanding Convolution in Deep Learning。有太多的公开课、教程在反复传颂卷积神经网络的好，却都没有讲什么是「卷积」，似乎默认所有读者都有相关基础。这篇外文既友好又深入，所以翻译了过来。文章高级部分通过流体力学量子力学等解释卷积的做法在我看来有点激进，这些领域恐怕比卷积更深奥，所以只需简略看看即可。以下是正文： 卷积现在可能是深度学习中最重要的概念。正是靠着卷积和卷积神经网络，深度学习才超越了几乎其他所有的机器学习手段。但卷积为什么如此强大？它的原理是什么？在这篇博客中我将讲解卷积及相关概念，帮助你彻底地理解它。 网络上已经有不少博客讲解卷积和深度学习中的卷积，但我发现它们都一上来就加入了太多不必要的数学细节，艰深晦涩，不利于理解主旨。这篇博客虽然也有很多数学细节，但我会以可视化的方式一步步展示它们，确保每个人都可以理解。文章第一部分旨在帮助读者理解卷积的概念和深度学习中的卷积网络。第二部分引入了一些高级的概念，旨在帮助深度学习方向的研究者和高级玩家进一步加深对卷积的理解。 第一部分：什么是卷积 整篇博客都会探讨这个问题，但先把握行文脉络会很有帮助。那么粗略来讲，什么是卷积呢？ 你可以把卷积想象成一种混合信息的手段。想象一下装满信息的两个桶，我们把它们倒入一个桶中并且通过某种规则搅拌搅拌。也就是说卷积是一种混合两种信息的流程。 卷积也可以形式化地描述，事实上，它就是一种数学运算，跟减加乘除没有本质的区别。虽然这种运算本身很复杂，但它非常有助于简化更复杂的表达式。在物理和工程上，卷积被广泛地用于化简等式——等会儿简单地形式化描述卷积之后——我们将把这些领域的思想和深度学习联系起来，以加深对卷积的理解。但现在我们先从实用的角度理解卷积。 我们如何对图像应用卷积 当我们在图像上应用卷积时，我们在两个维度上执行卷积——水平和竖直方向。我们混合两桶信息：第一桶是输入的图像，由三个矩阵构成——RGB 三通道，其中每个元素都是 0 到 255 之间的一个整数。第二个桶是卷积核（kernel），单个浮点数矩阵。可以将卷积核的大小和模式想象成一个搅拌图像的方法。卷积核的输出是一幅修改后的图像，在深度学习中经常被称作 feature map。对每个颜色通道都有一个 feature map。 边缘检测卷积核的效果 这是怎么做到的呢，我们现在演示一下如何通过卷积来混合这两种信息。一种方法是从输入图片中取出一个与卷积核大小相同的区块——这里假设图片为 100×100，卷积核大小为 3×3，那么我们取出的区块大小就是 3×3——然后对每对相同位置的元素执行乘法后求和（不同于矩阵乘法，却类似向量内积，这里是两个相同大小的矩阵的「点乘」）。乘积的和就生成了 feature map 中的一个像素。当一个像素计算完毕后，移动一个像素取下一个区块执行相同的运算。当无法再移动取得新区块的时候对 feature map 的计算就结束了。这个流程可以用如下的动画演示： RAM 是输入图片，Buffer 是 feature map 你可能注意到这里有个正规化因子 m，这里 m 的值为 kernel 的大小 9；这是为了保证输入图像和 feature map 的亮度相同。 为什么机器学习中图像卷积有用 图像中可能含有很多我们不关心的噪音。一个好例子是我和 Jannek Thomas 在 Burda Bootcamp 做的项目。Burda Bootcamp 是一个让学生像黑客马拉松一样在非常短的时间内创造技术风暴的实验室。与 9 名同事一起，我们在 2 个月内做了 11 个产品出来。其中之一是针对时尚图像用深度编码器做的搜索引擎：你上传一幅时尚服饰的图片，编码器自动找出款式类似的服饰。 如果你想要区分衣服的式样，那么衣服的颜色就不那么重要了；另外像商标之类的细节也不那么重要。最重要的可能是衣服的外形。一般来讲，女装衬衫的形状与衬衣、夹克和裤子的外观非常不同。如果我们过滤掉这些多余的噪音，那我们的算法就不会因颜色、商标之类的细节分心了。我们可以通过卷积轻松地实现这项处理。 我的同事 Jannek Thomas 通过索贝尔边缘检测滤波器（与上上一幅图类似）去掉了图像中除了边缘之外的所有信息——这也是为什么卷积应用经常被称作滤波而卷积核经常被称作滤波器（更准确的定义在下面）的原因。由边缘检测滤波器生成的 feature map 对区分衣服类型非常有用，因为只有外形信息被保留下来。 彩图的左上角是搜索 query，其他是搜索结果，你会发现自动编码器真的只关注衣服的外形，而不是颜色。 再进一步：有许多不同的核可以产生多种 feature map，比如锐化图像（强调细节），或者模糊图像（减少细节），并且每个 feature map 都可能帮助算法做出决策（一些细节，比如衣服上有 3 个纽扣而不是两个，可能可以区分一些服饰）。 使用这种手段——读入输入、变换输入、然后把 feature map 喂给某个算法——被称为特征工程。特征工程非常难，很少有资料帮你上手。造成的结果是，很少有人能熟练地在多个领域应用特征工程。特征工程是——纯手工——也是 Kaggle 比赛中最重要的技能。特征工程这么难的原因是，对每种数据每种问题，有用的特征都是不同的：图像类任务的特征可能对时序类任务不起作用；即使两个任务都是图像类的，也很难找出相同的有效特征，因为视待识别的物体的不同，有用的特征也不同。这非常依赖经验。 所以特征工程对新手来讲特别困难。不过对图像而言，是否可以利用卷积核自动找出某个任务中最适合的特征？ 进入卷积神经网络 卷积神经网络就是干这个的。不同于刚才使用固定数字的卷积核，我们赋予参数给这些核，参数将在数据上得到训练。随着卷积神经网络的训练，这些卷积核为了得到有用信息，在图像或 feature map 上的过滤工作会变得越来越好。这个过程是自动的，称作特征学习。特征学习自动适配新的任务：我们只需在新数据上训练一下自动找出新的过滤器就行了。这是卷积神经网络如此强大的原因——不需要繁重的特征工程了！ 通常卷积神经网络并不学习单一的核，而是同时学习多层级的多个核。比如一个 32x16x16 的核用到 256×256 的图像上去会产生 32 个 241×241（latex.png）的 feature map。所以自动地得到了 32 个有用的新特征。这些特征可以作为下个核的输入。一旦学习到了多级特征，我们简单地将它们传给一个全连接的简单的神经网络，由它完成分类。这就是在概念上理解卷积神经网络所需的全部知识了（池化也是个重要的主题，但还是在另一篇博客中讲吧）。 第二部分：高级概念 我们现在对卷积有了一个良好的初步认识，也知道了卷积神经网络在干什么、为什么它如此强大。现在让我们深入了解一下卷积运算中到底发生了什么。我们将认识到刚才对卷积的讲解是粗浅的，并且这里有更优雅的解释。通过深入理解，我们可以理解卷积的本质并将其应用到许多不同的数据上去。万事开头难，第一步是理解卷积原理。 卷积定理 要理解卷积，不得不提 convolution theorem，它将时域和空域上的复杂卷积对应到了频域中的元素间简单的乘积。这个定理非常强大，在许多科学领域中得到了广泛应用。卷积定理也是快速傅里叶变换算法被称为 20 世纪最重要的算法之一的一个原因。 第一个等式是一维连续域上两个连续函数的卷积；第二个等式是二维离散域（图像）上的卷积。这里指的是卷积，指的是傅里叶变换，表示傅里叶逆变换，是一个正规化常量。这里的「离散」指的是数据由有限个变量构成（像素）；一维指的是数据是一维的（时间），图像则是二维的，视频则是三维的。 为了更好地理解卷积定理，我们还需要理解数字图像处理中的傅里叶变换。 快速傅里叶变换 快速傅里叶变换是一种将时域和空域中的数据转换到频域上去的算法。傅里叶变换用一些正弦和余弦波的和来表示原函数。必须注意的是，傅里叶变换一般涉及到复数，也就是说一个实数被变换为一个具有实部和虚部的复数。通常虚部只在一部分领域有用，比如将频域变换回到时域和空域上；而在这篇博客里会被忽略掉。你可以在下面看到一个信号（一个以时间为参数的有周期的函数通常称为信号）是如何被傅里叶变换的： 红色是时域，蓝色为频域 你也许会说从没见过这些东西，但我敢肯定你在生活中是见过的：如果红色是一首音乐的话，那么蓝色值就是你在你的 MP3 播放器屏幕上看到的频谱： 傅里叶域上的图像 我们如何想象图片的频率呢？想象一张只有两种模式的纸片，现在把纸片竖起来顺着线条的方向看过去，就会看到一个一个的亮点。这些以一定间隔分割黑白部分的波就代表着频率。在频域中，低频率更接近中央而高频率更接近边缘。频域中高强度（亮度、白色）的位置代表着原始图像亮度改变的方向。这一点在接下来这张图与其对数傅里叶变换（对傅里叶变换的实部取对数，这样可以减小像素亮度的差别，便于观察更广的亮度区域）中特别明显： 我们马上就可以发现傅里叶变换包含了关于物体朝向的信息。如果物体被旋转了一个角度，从图像像素上可能很难判断，但从频域上可以很明显地看出来。 这是个很重要的启发，基于傅里叶定理，我们知道卷积神经网络在频域上检测图像并且捕捉到了物体的方向信息。于是卷积神经网络就比传统算法更擅长处理旋转后的图像（虽然还是比不上人类）。 频率过滤与卷积 为什么卷积经常被描述为过滤，为什么卷积核经常被称为过滤器呢？通过下一个例子可以解释： 如果我们对图像执行傅里叶变换，并且乘以一个圆形（背景填充黑色，也就是 0），我们可以过滤掉所有的高频值（它们会成为 0，因为填充是 0）。注意过滤后的图像依然有条纹模式，但图像质量下降了很多——这就是 jpeg 压缩算法的工作原理（虽然有些不同但用了类似的变换），我们变换图形，然后只保留部分频率，最后将其逆变换为二维图片；压缩率就是黑色背景与圆圈的比率。 我们现在将圆圈想象为一个卷积核，然后就有了完整的卷积过程——就像在卷积神经网络中看到的那样。要稳定快速地执行傅里叶变换还需要许多技巧，但这就是基本理念了。 现在我们已经理解了卷积定理和傅里叶变换，我们可以将这些理念应用到其他科学领域，以加强我们对深度学习中的卷积的理解。 流体力学的启发 流体力学为空气和水创建了大量的微分方程模型，傅里叶变换不但简化了卷积，也简化了微分，或者说任何利用了微分方程的领域。有时候得到解析解的唯一方法就是对微分方程左右同时执行傅里叶变换。在这个过程中，我们常常将解写成两个函数卷积的形式，以得到更简单的表达。这是在一个维度上的应用，还有在两个维度上的应用，比如天文学。 扩散 你可以混合两种液体（牛奶和咖啡），只要施加一个外力（汤勺搅拌）——这被称为对流，而且是个很快的过程。你也可以耐心等待两种液体自然混合——这被称为扩散，通常是很慢的过程。 想象一下，一个鱼缸被一块板子隔开，两边各有不同浓度的盐水。抽掉板子后，两边的盐水会逐步混合为同一个浓度。浓度差越大，这个过程越剧烈。 现在想象一下，一个鱼缸被 256×256 个板子分割为 256×256 个部分（这个数字似乎不对），每个部分都有不同浓度的盐水。如果你去掉所有的挡板，浓度类似的小块间将不会有多少扩散，但浓度差异大的区块间有巨大的扩散。这些小块就是像素点，而浓度就是像素的亮度。浓度的扩散就是像素亮度的扩散。 这说明，扩散现象与卷积有相似点——初始状态下不同浓度的液体，或不同强度的像素。为了完成下一步的解释，我们还需要理解传播子。 理解传播子 传播子就是密度函数，表示流体微粒应该往哪个方向传播。问题是神经网络中没有这样的概率函数，只有一个卷积核——我们要如何统一这两种概念呢？ 我们可以通过正规化来讲卷积核转化为概率密度函数。这有点像计算输出值的 softmax。下面就是对第一个例子中的卷积核执行的 softmax 结果： 现在我们就可以从扩散的角度来理解图像上的卷积了。我们可以把卷积理解为两个扩散流程。首先，当像素亮度改变时（黑色到白色等）会发生扩散；然后某个区域的扩散满足卷积核对应的概率分布。这意味着卷积核正在处理的区域中的像素点必须按照这些概率来扩散。 在上面那个边缘检测器中，几乎所有临近边缘的信息都会聚集到边缘上（这在流体扩散中是不可能的，但这里的解释在数学上是成立的）。比如说所有低于 0.0001 的像素都非常可能流动到中间并累加起来。与周围像素区别最大的区域会成为强度的集中地，因为扩散最剧烈。反过来说，强度最集中的地方说明与周围对比最强烈，这也就是物体的边缘所在，这解释了为什么这个核是一个边缘检测器。 所以我们就得到了物理解释：卷积是信息的扩散。我们可以直接把这种解释运用到其他核上去，有时候我们需要先执行一个 softmax 正规化才能解释，但一般来讲核中的数字已经足够说明它想要干什么。比如说，你是否能推断下面这个核的的意图？ 等等，有点迷惑 对一个概率化的卷积核，怎么会有确定的功能？我们必须根据核对应的概率分布也就是传播子来计算单个粒子的扩散不是吗？ 是的，确实如此。但是，如果你取一小部分液体，比如一滴水，你仍然有几百万水分子。虽然单个分子的随机移动满足传播子，但大量的分子宏观上的表现是基本确定的。这是统计学上的解释，也是流体力学的解释。我们可以把传播子的概率分布解释为信息或说像素亮度的平均分布；也就是说我们的解释从流体力学的角度来讲是没问题的。话说回来，这里还有一个卷积的随机解释。 量子力学的启发 传播子是量子力学中的重要概念。在量子力学中，一个微粒可能处于一种叠加态，此时它有两个或两个以上属性使其无法确定位于观测世界中的具体位置。比如，一个微粒可能同时存在于两个不同的位置。 但是如果你测量微粒的状态——比如说现在微粒在哪里——它就只能存在于一个具体位置了。换句话说，你通过观测破坏了微粒的叠加态。传播子就描述了微粒出现位置的概率分布。比如说在测量后一个微粒可能——根据传播子的概率函数——30% 在 A，70% 在 B。 通过量子纠缠，几个粒子就可以同时储存上百或上百万个状态——这就是量子计算机的威力。 如果我们将这种解释用于深度学习，我们可以把图片想象为位于叠加态，于是在每个 3*3 的区块中，每个像素同时出现在 9 个位置。一旦我们应用了卷积，我们就执行了一次观测，然后每个像素就坍缩到满足概率分布的单个位置上了，并且得到的单个像素是所有像素的平均值。为了使这种解释成立，必须保证卷积是随机过程。这意味着，同一个图片同一个卷积核会产生不同的结果。这种解释没有显式地把谁比作谁，但可能启发你如何把卷积用成随机过程，或如何发明量子计算机上的卷积网络算法。量子算法能够在线性时间内计算出卷积核描述的所有可能的状态组合。 概率论的启发 卷积与互相关紧密相连。互相关是一种衡量小段信息（几秒钟的音乐片段）与大段信息（整首音乐）之间相似度的一种手段（youtube 使用了类似的技术检测侵权视频）。 虽然互相关的公式看起来很难，但通过如下手段我们可以马上看到它与深度学习的联系。在图片搜索中，我们简单地将 query 图片上下颠倒作为核然后通过卷积进行互相关检验，结果会得到一张有一个或多个亮点的图片，亮点所在的位置就是人脸所在的位置。 这个例子也展示了通过补零来使傅里叶变换稳定的一种技巧，许多版本的傅里叶变换都使用了这种技巧。另外还有使用了其他 padding 技巧：比如平铺核，分治等等。我不会展开讲，关于傅里叶变换的文献太多了，里面的技巧特别多——特别是对图像来讲。 在更底层，卷积网络第一层不会执行互相关校验，因为第一层执行的是边缘检测。后面的层得到的都是更抽象的特征，就有可能执行互相关了。可以想象这些亮点像素会传递给检测人脸的单元（Google Brain 项目的网络结构中有一些单元专门识别人脸、猫等等；也许用的是互相关？） 统计学的启发 统计模型和机器学习模型的区别是什么？统计模型只关心很少的、可以解释的变量。它们的目的经常是回答问题：药品 A 比药品 B 好吗？ 机器学习模型是专注于预测效果的：对于年龄 X 的人群，药品 A 比 B 的治愈率高 17%，对年龄 Y 则是 23%。 机器学习模型通常比统计模型更擅长预测，但它们不是那么可信。统计模型更擅长得到准确可信的结果：就算药品 A 比 B 好 17%，我们也不知道这是不是偶然，我们需要统计模型来判断。 对时序数据，有两种重要的模型：weighted moving average 和 autoregressive 模型，后者可归入 ARIMA model (autoregressive integrated moving average model)。比起 LSTM，ARIMA 很弱。但在低维度数据（1-5 维）上，ARIMA 非常健壮。虽然它们有点难以解释，但 ARIMA 绝不是像深度学习算法那样的黑盒子。如果你需要一个可信的模型，这是个巨大的优势。 我们可以将这些统计模型写成卷积的形式，然后深度学习中的卷积就可以解释为产生局部 ARIMA 特征的函数了。这两种形式并不完全重合，使用需谨慎。 C 是一个以核为参数的函数，white noise 是正规化的均值为 0 方差为 1 的互不相关的数据。 当我们预处理数据的时候，经常将数据处理为类似 white noise 的形式：将数据移动到均值为 0，将方差调整为 1。我们很少去除数据的相关性，因为计算复杂度高。但是在概念上是很简单的，我们旋转坐标轴以重合数据的特征向量： 现在如果我们将 C 作为 bias，我们就会觉得这与卷积神经网络很像。所以卷积层的输出可被解释为白噪音数据经过 autoregressive model 的输出。 weighted moving average 的解释更简单：就是输入数据与某个固定的核的卷积。看看文末的高斯平滑核就会明白这个解释。高斯平滑核可以被看做每个像素与其邻居的平均，或者说每个像素被其邻居平均（边缘模糊）。 虽然单个核无法同时创建 autoregressive 和 weighted moving average 特征，但我们可以使用多个核来产生不同的特征。 总结 这篇博客中我们知道了卷积是什么、为什么在深度学习中这么有用。图片区块的解释很容易理解和计算，但有其理论局限性。我们通过学习傅里叶变换知道傅里叶变换后的时域上有很多关于物体朝向的信息。通过强大的卷积定理我们理解了卷积是一种在像素间的信息流动。之后我们拓展了量子力学中传播子的概念，得到了一个确定过程中的随机解释。我们展示了互相关与卷积的相似性，并且卷积网络的性能可能是基于 feature map 间的互相关程度的，互相关程度是通过卷积校验的。最后我们将卷积与两种统计模型关联了起来。 个人来讲，我觉得写这篇博客很有趣。曾经很长一段时间我都觉得本科的数学和统计课是浪费时间，因为它们太不实用了（哪怕是应用数学）。但之后——就像突然中大奖一样——这些知识都相互串起来了并且带了新的理解。我觉得这是个绝妙的例子，启示我们应该耐心地学习所有的大学课程——哪怕它们一开始看起来没有用。 上文高斯平滑核问题的答案 Reference http://timdettmers.com/2015/03/26/convolution-deep-learning/ R. B. Fisher, K. Koryllos,「Interactive Textbooks; Embedding Image Processing Operator Demonstrations in Text」, Int. J. of Pattern Recognition and Artificial Intelligence, Vol 12, No 8, pp 1095-1123, 1998. ** 原英文链接：http://timdettmers.com/2015/03/26/convolution-deep-learning/ 码农场该文章链接：http://www.hankcs.com/ml/understanding-the-convolution-in-deep-learning.html 本文为机器之心转载，转载请联系原作获得授权。 ✄———————————————————————— 加入机器之心（全职记者/实习生）：hr@jiqizhixin.com 投稿或寻求报道：editor@jiqizhixin.com 广告&amp;商务合作：bd@jiqizhixin.com 微信扫一扫关注该公众号]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elastic Search 配置和使用]]></title>
    <url>%2F2017%2F07%2F12%2FElastic%20Search%2FElastic%20Search%E9%85%8D%E7%BD%AE%E5%92%8C%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[官网：https://www.elastic.co/products/elasticsearch 最好的教程：https://es.xiaoleilu.com/ docker的ELK环境：https://hub.docker.com/r/sebp/elk/ ES 5.4中文文档 http://cwiki.apachecn.org/pages/viewpage.action?pageId=4260364 0、基本概念接近实时（NRT） Elasticsearch是一个接近实时的搜索平台。这意味着，从索引一个文档直到这个文档能够被搜索到有一个轻微的延迟（通常是1秒）。 集群（cluster） 一个集群就是由一个或多个节点组织在一起，它们共同持有你整个的数据，并一起提供索引和搜索功能。一个集群一个唯一的名字标识，这个名字默认就是 “elasticsearch”。这个名字是重要的，因为一个节点只能通过指定某个集群的名字，来加入这个集群。在产品环境中显式地设定这个名字是一个好 习惯，但是使用默认值来进行测试/开发也是不错的。 节点（node） 一个节点是你集群中的一个服务器，作为集群的一部分，它存储你的数据，参与集群的索引和搜索功能。和集群类似，一个节点也是由一个名字来标识的，默认情况 下，这个名字是一个随机的漫威漫画角色的名字，这个名字会在启动的时候赋予节点。这个名字对于管理工作来说挺重要的，因为在这个管理过程中，你会去确定网 络中的哪些服务器对应于Elasticsearch集群中的哪些节点。 一个节点可以通过配置集群名称的方式来加入一个指定的集群。默认情况下，每个节点都会被安排加入到一个叫做“elasticsearch”的集群中，这意 味着，如果你在你的网络中启动了若干个节点，并假定它们能够相互发现彼此，它们将会自动地形成并加入到一个叫做“elasticsearch”的集群中。 在一个集群里，只要你想，可以拥有任意多个节点。而且，如果当前你的网络中没有运行任何Elasticsearch节点，这时启动一个节点，会默认创建并加入一个叫做“elasticsearch”的集群。 12Relational DB -&gt; Databases -&gt; Tables -&gt; Rows -&gt; ColumnsElasticsearch -&gt; Indices -&gt; Types -&gt; Documents -&gt; Fields 索引（index） 一个索引就是一个拥有几分相似特征的文档的集合。比如说，你可以有一个客户数据的索引，另一个产品目录的索引，还有一个订单数据的索引。一个索引由一个名 字来标识（必须全部是小写字母的），并且当我们要对对应于这个索引中的文档进行索引、搜索、更新和删除的时候，都要使用到这个名字。 在一个集群中，如果你想，可以定义任意多的索引。 类型（type） 在一个索引中，你可以定义一种或多种类型。一个类型是你的索引的一个逻辑上的分类/分区，其语义完全由你来定。通常，会为具有一组共同字段的文档定义一个 类型。比如说，我们假设你运营一个博客平台并且将你所有的数据存储到一个索引中。在这个索引中，你可以为用户数据定义一个类型，为博客数据定义另一个类 型，当然，也可以为评论数据定义另一个类型。 文档（document） 一个文档是一个可被索引的基础信息单元。比如，你可以拥有某一个客户的文档，某一个产品的一个文档，当然，也可以拥有某个订单的一个文档。文档以 JSON（Javascript Object Notation）格式来表示，而JSON是一个到处存在的互联网数据交互格式。 在一个index/type里面，只要你想，你可以存储任意多的文档。注意，尽管一个文档，物理上存在于一个索引之中，文档必须被索引/赋予一个索引的type。 分片和复制（shards &amp; replicas） 一个索引可以存储超出单个结点硬件限制的大量数据。比如，一个具有10亿文档的索引占据1TB的磁盘空间，而任一节点都没有这样大的磁盘空间；或者单个节点处理搜索请求，响应太慢。 为了解决这个问题，Elasticsearch提供了将索引划分成多份的能力，这些份就叫做分片。当你创建一个索引的时候，你可以指定你想要的分片的数量。每个分片本身也是一个功能完善并且独立的“索引”，这个“索引”可以被放置到集群中的任何节点上。 分片之所以重要，主要有两方面的原因： - 允许你水平分割/扩展你的内容容量 - 允许你在分片（潜在地，位于多个节点上）之上进行分布式的、并行的操作，进而提高性能/吞吐量 至于一个分片怎样分布，它的文档怎样聚合回搜索请求，是完全由Elasticsearch管理的，对于作为用户的你来说，这些都是透明的。 在一个网络/云的环境里，失败随时都可能发生，在某个分片/节点不知怎么的就处于离线状态，或者由于任何原因消失了，这种情况下，有一个故障转移机制是非 常有用并且是强烈推荐的。为此目的，Elasticsearch允许你创建分片的一份或多份拷贝，这些拷贝叫做复制分片，或者直接叫复制。 复制之所以重要，有两个主要原因： - 在分片/节点失败的情况下，提供了高可用性。因为这个原因，注意到复制分片从不与原/主要（original/primary）分片置于同一节点上是非常重要的。 - 扩展你的搜索量/吞吐量，因为搜索可以在所有的复制上并行运行 总之，每个索引可以被分成多个分片。一个索引也可以被复制0次（意思是没有复制）或多次。一旦复制了，每个索引就有了主分片（作为复制源的原来的分片）和 复制分片（主分片的拷贝）之别。分片和复制的数量可以在索引创建的时候指定。在索引创建之后，你可以在任何时候动态地改变复制的数量，但是你事后不能改变 分片的数量。 默认情况下，Elasticsearch中的每个索引被分片5个主分片和1个复制，这意味着，如果你的集群中至少有两个节点，你的索引将会有5个主分片和另外5个复制分片（1个完全拷贝），这样的话每个索引总共就有10个分片。 1、安装-5.5.01.1 ElasticSearch下载的 elasticsearch-5.5.0.tar.gz kibana-5.5.0-linux-x86_64.tar.gz 解压到/home/david/opt，在主目录直接运行 1$ bin/elasticsearch 启动服务，启动后，访问localhost:9200，若出现 12345678910111213&#123; &quot;name&quot; : &quot;Jr1It8C&quot;, &quot;cluster_name&quot; : &quot;elasticsearch&quot;, &quot;cluster_uuid&quot; : &quot;JMo_h3-USdegKS1yZ0WCnA&quot;, &quot;version&quot; : &#123; &quot;number&quot; : &quot;5.5.0&quot;, &quot;build_hash&quot; : &quot;260387d&quot;, &quot;build_date&quot; : &quot;2017-06-30T23:16:05.735Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;6.6.0&quot; &#125;, &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125; 则说明安装成功。 1.2 Marvel5.0后集成到了x-pack中 1）安装X-pack到elasticsearch 1$ bin/elasticsearch-plugin install x-pack 2）安装到kibana 1bin/kibana-plugin install x-pack 用户名elastic 密码changeme 1.3 关闭服务关闭Elastic search 1ps -ef | grep elastic 关闭kibana 1fuser -n tcp 5601 2、第一个例子摘自教程 假设我们刚好在Megacorp工作，这时人力资源部门出于某种目的需要让我们创建一个员工目录，这个目录用于促进人文关怀和用于实时协同工作，所以它有以下不同的需求： 数据能够包含多个值的标签、数字和纯文本。 检索任何员工的所有信息。 支持结构化搜索，例如查找30岁以上的员工。 支持简单的全文搜索和更复杂的短语(phrase)搜索 高亮搜索结果中的关键字 能够利用图表管理分析这些数据 2.1 索引员工文档索引含义的区分 你可能已经注意到索引(index)这个词在Elasticsearch中有着不同的含义，所以有必要在此做一下区分: 索引（名词） 如上文所述，一个索引(index)就像是传统关系数据库中的数据库，它是相关文档存储的地方，index的复数是indices 或indexes。 索引（动词） 「索引一个文档」表示把一个文档存储到索引（名词）里，以便它可以被检索或者查询。这很像SQL中的INSERT关键字，差别是，如果文档已经存在，新的文档将覆盖旧的文档。 倒排索引 传统数据库为特定列增加一个索引，例如B-Tree索引来加速检索。Elasticsearch和Lucene使用一种叫做倒排索引(inverted index)的数据结构来达到相同目的。 创建一个员工目录 每个文档的类型为employee。 employee类型归属于索引megacorp。 megacorp索引存储在Elasticsearch集群中。 1234567891011121314151617181920212223242526PUT /megacorp/employee/1&#123; &quot;first_name&quot; : &quot;John&quot;, &quot;last_name&quot; : &quot;Smith&quot;, &quot;age&quot; : 25, &quot;about&quot; : &quot;I love to go rock climbing&quot;, &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ]&#125;PUT /megacorp/employee/2&#123; &quot;first_name&quot; : &quot;Jane&quot;, &quot;last_name&quot; : &quot;Smith&quot;, &quot;age&quot; : 32, &quot;about&quot; : &quot;I like to collect rock albums&quot;, &quot;interests&quot;: [ &quot;music&quot; ]&#125;PUT /megacorp/employee/3&#123; &quot;first_name&quot; : &quot;Douglas&quot;, &quot;last_name&quot; : &quot;Fir&quot;, &quot;age&quot; : 35, &quot;about&quot;: &quot;I like to build cabinets&quot;, &quot;interests&quot;: [ &quot;forestry&quot; ]&#125; 2.2 检索文档1GET /megacorp/employee/1 响应的内容中包含一些文档的元信息，John Smith的原始JSON文档包含在_source字段中。 1234567891011121314151617&#123; &quot;_index&quot;: &quot;megacorp&quot;, &quot;_type&quot;: &quot;employee&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;found&quot;: true, &quot;_source&quot;: &#123; &quot;first_name&quot;: &quot;John&quot;, &quot;last_name&quot;: &quot;Smith&quot;, &quot;age&quot;: 25, &quot;about&quot;: &quot;I love to go rock climbing&quot;, &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ] &#125;&#125; 简单搜索 1GET /megacorp/employee/_search 使用关键字_search来取代原来的文档ID。响应内容的hits数组中包含了我们所有的三个文档。默认情况下搜索会返回前10个结果。 12345678910111213141516171819202122232425262728293031&#123; "took": 6, "timed_out": false, "_shards": &#123; "total": 5, "successful": 5, "failed": 0 &#125;, "hits": &#123; "total": 3, "max_score": 1, "hits": [ &#123; "_index": "megacorp", "_type": "employee", "_id": "2", "_score": 1, "_source": &#123; "first_name": "Jane", "last_name": "Smith", "age": 32, "about": "I like to collect rock albums", "interests": [ "music" ] &#125; &#125;, ...... ] &#125;&#125; 接下来，让我们搜索姓氏中包含“Smith”的员工。要做到这一点，我们将在命令行中使用轻量级的搜索方法。这种方法常被称作查询字符串(query string)搜索，因为我们像传递URL参数一样去传递查询语句： 1GET /megacorp/employee/_search?q=last_name:Smith 2.3 使用DSL语句查询查询字符串搜索便于通过命令行完成特定(ad hoc)的搜索，但是它也有局限性（参阅简单搜索章节）。Elasticsearch提供丰富且灵活的查询语言叫做DSL查询(Query DSL),它允许你构建更加复杂、强大的查询。 DSL(Domain Specific Language特定领域语言)以JSON请求体的形式出现。我们可以这样表示之前关于“Smith”的查询: 12345678GET /megacorp/employee/_search&#123; &quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;last_name&quot; : &quot;Smith&quot; &#125; &#125;&#125; 2.4 更复杂的搜索我们让搜索稍微再变的复杂一些。我们依旧想要找到姓氏为“Smith”的员工，但是我们只想得到年龄大于30岁的员工。我们的语句将添加过滤器(filter),它使得我们高效率的执行一个结构化搜索： 1234567891011121314151617GET /megacorp/employee/_search&#123; &quot;query&quot; : &#123; &quot;filtered&quot; : &#123; &quot;filter&quot; : &#123; &quot;range&quot; : &#123; &quot;age&quot; : &#123; &quot;gt&quot; : 30 &#125; &lt;1&gt; &#125; &#125;, &quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;last_name&quot; : &quot;smith&quot; &lt;2&gt; &#125; &#125; &#125; &#125;&#125; 5.0以后的DSL语法变了 3、数据3.1 文档一个文档不只有数据。它还包含了元数据(metadata)——关于文档的信息。三个必须的元数据节点是： 节点 说明 _index 文档存储的地方 _type 文档代表的对象的类 _id 文档的唯一标识 _index索引(index)类似于关系型数据库里的“数据库”——它是我们存储和索引关联数据的地方。 提示： 事实上，我们的数据被存储和索引在分片(shards)中，索引只是一个把一个或多个分片分组在一起的逻辑空间。然而，这只是一些内部细节——我们的程序完全不用关心分片。对于我们的程序而言，文档存储在索引(index)中。剩下的细节由Elasticsearch关心既可。 我们将会在《索引管理》章节中探讨如何创建并管理索引，但现在，我们将让Elasticsearch为我们创建索引。我们唯一需要做的仅仅是选择一个索引名。这个名字必须是全部小写，不能以下划线开头，不能包含逗号。让我们使用website做为索引名。 _type在应用中，我们使用对象表示一些“事物”，例如一个用户、一篇博客、一个评论，或者一封邮件。每个对象都属于一个类(class)，这个类定义了属性或与对象关联的数据。user类的对象可能包含姓名、性别、年龄和Email地址。 在关系型数据库中，我们经常将相同类的对象存储在一个表里，因为它们有着相同的结构。同理，在Elasticsearch中，我们使用相同类型(type)的文档表示相同的“事物”，因为他们的数据结构也是相同的。 每个类型(type)都有自己的映射(mapping)或者结构定义，就像传统数据库表中的列一样。所有类型下的文档被存储在同一个索引下，但是类型的映射(mapping)会告诉Elasticsearch不同的文档如何被索引。 我们将会在《映射》章节探讨如何定义和管理映射，但是现在我们将依赖Elasticsearch去自动处理数据结构。 _type的名字可以是大写或小写，不能包含下划线或逗号。我们将使用blog做为类型名。 _idid仅仅是一个字符串，它与_index和_type组合时，就可以在Elasticsearch中唯一标识一个文档。当创建一个文档，你可以自定义_id，也可以让Elasticsearch帮你自动生成。 3.2 索引一个文档自定义ID123456PUT /website/blog/123&#123; &quot;title&quot;: &quot;My first blog entry&quot;, &quot;text&quot;: &quot;Just trying this out...&quot;, &quot;date&quot;: &quot;2014/01/01&quot;&#125; Elasticsearch的响应： 12345678910111213&#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;blog&quot;, &quot;_id&quot;: &quot;123&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;created&quot;: true&#125; _version ：Elasticsearch中每个文档都有版本号，每当文档变化（包括删除）都会使_version增加。 自增ID123456POST /website/blog/&#123; &quot;title&quot;: &quot;My second blog entry&quot;, &quot;text&quot;: &quot;Still trying this out...&quot;, &quot;date&quot;: &quot;2014/01/01&quot;&#125; 自动生成的ID有22个字符长，URL-safe, Base64-encoded string universally unique identifiers, 或者叫 UUIDs。 3.3 检索想要从Elasticsearch中获取文档，我们使用同样的_index、_type、_id，但是HTTP方法改为GET： 1GET /website/blog/123?pretty pretty在任意的查询字符串中增加pretty参数，类似于上面的例子。会让Elasticsearch美化输出(pretty-print)JSON响应以便更加容易阅读。_source字段不会被美化，它的样子与我们输入的一致。 {&quot;found&quot;: true}。这意味着文档已经找到。 如果我们请求一个不存在的文档，依旧会得到一个JSON，不过found值变成了false。 此外，HTTP响应状态码也会变成&#39;404 Not Found&#39;代替&#39;200 OK&#39;。我们可以在curl后加-i参数得到响应头： 1curl -i -XGET http://localhost:9200/website/blog/124?pretty 检索文档的一部分通常，GET请求将返回文档的全部，存储在_source参数中。但是可能你感兴趣的字段只是title。请求个别字段可以使用_source参数。多个字段可以使用逗号分隔： 1GET /website/blog/123?_source=title,text 或者你只想得到_source字段而不要其他的元数据，你可以这样请求： 1GET /website/blog/123/_source 3.4 更新整个文档文档在Elasticsearch中是不可变的——我们不能修改他们。如果需要更新已存在的文档，我们可以使用《索引文档》章节提到的index API 重建索引(reindex) 或者替换掉它。 123456PUT /website/blog/123&#123; &quot;title&quot;: &quot;My first blog entry&quot;, &quot;text&quot;: &quot;I am starting to get the hang of this...&quot;, &quot;date&quot;: &quot;2014/01/02&quot;&#125; 在响应中，我们可以看到Elasticsearch把_version增加了，且result是updated update API。这个API 似乎 允许你修改文档的局部，但事实上Elasticsearch遵循与之前所说完全相同的过程，这个过程如下： 从旧文档中检索JSON 修改它 删除旧文档 索引新文档 唯一的不同是update API完成这一过程只需要一个客户端请求既可，不再需要get和index请求了。 3.5 创建新文档请记住_index、_type、_id三者唯一确定一个文档。所以要想保证文档是新加入的，最简单的方式是使用POST方法让Elasticsearch自动生成唯一_id： 12POST /website/blog/&#123; ... &#125; 如果要确保是create操作 1）使用op_type查询参数： 12PUT /website/blog/123?op_type=create&#123; ... &#125; 2）在URL后加/_create做为端点： 12PUT /website/blog/123/_create&#123; ... &#125; 如果包含相同的_index、_type和_id的文档已经存在，Elasticsearch将返回409 Conflict响应状态码 3.6 删除文档使用DELETE方法： 1DELETE /website/blog/123 删除一个文档也不会立即从磁盘上移除，它只是被标记成已删除。Elasticsearch将会在你之后添加更多索引的时候才会在后台进行删除内容的清理。 3.7 MappingElasticSearch的Mapping之字段类型 （一）核心数据类型： （1）string： 默认会被分词，一个完整示例如下 12345678910111213141516171819&quot;status&quot;: &#123; &quot;type&quot;: &quot;string&quot;, //字符串类型 &quot;index&quot;: &quot;analyzed&quot;//分词，不分词是：not_analyzed ，设置成no，字段将不会被索引 &quot;analyzer&quot;:&quot;ik&quot;//指定分词器 &quot;boost&quot;:1.23//字段级别的分数加权 &quot;doc_values&quot;:false//对not_analyzed字段，默认都是开启，分词字段不能使用，对排序和聚合能提升较大性能，节约内存 &quot;fielddata&quot;:&#123;&quot;format&quot;:&quot;disabled&quot;&#125;//针对分词字段，参与排序或聚合时能提高性能，不分词字段统一建议使用doc_value &quot;fields&quot;:&#123;&quot;raw&quot;:&#123;&quot;type&quot;:&quot;string&quot;,&quot;index&quot;:&quot;not_analyzed&quot;&#125;&#125; //可以对一个字段提供多种索引模式，同一个字段的值，一个分词，一个不分词 &quot;ignore_above&quot;:100 //超过100个字符的文本，将会被忽略，不被索引 &quot;include_in_all&quot;:ture//设置是否此字段包含在_all字段中，默认是true，除非index设置成no选项 &quot;index_options&quot;:&quot;docs&quot;//4个可选参数docs（索引文档号） ,freqs（文档号+词频），positions（文档号+词频+位置，通常用来距离查询），offsets（文档号+词频+位置+偏移量，通常被使用在高亮字段）分词字段默认是position，其他的默认是docs &quot;norms&quot;:&#123;&quot;enable&quot;:true,&quot;loading&quot;:&quot;lazy&quot;&#125;//分词字段默认配置，不分词字段：默认&#123;&quot;enable&quot;:false&#125;，存储长度因子和索引时boost，建议对需要参与评分字段使用 ，会额外增加内存消耗量 &quot;null_value&quot;:&quot;NULL&quot;//设置一些缺失字段的初始化值，只有string可以使用，分词字段的null值也会被分词 &quot;position_increament_gap&quot;:0//影响距离查询或近似查询，可以设置在多值字段的数据上火分词字段上，查询时可指定slop间隔，默认值是100 &quot;store&quot;:false//是否单独设置此字段的是否存储而从_source字段中分离，默认是false，只能搜索，不能获取值 &quot;search_analyzer&quot;:&quot;ik&quot;//设置搜索时的分词器，默认跟ananlyzer是一致的，比如index时用standard+ngram，搜索时用standard用来完成自动提示功能 &quot;similarity&quot;:&quot;BM25&quot;//默认是TF/IDF算法，指定一个字段评分策略，仅仅对字符串型和分词类型有效 &quot;term_vector&quot;:&quot;no&quot;//默认不存储向量信息，支持参数yes（term存储），with_positions（term+位置）,with_offsets（term+偏移量），with_positions_offsets(term+位置+偏移量) 对快速高亮fast vector highlighter能提升性能，但开启又会加大索引体积，不适合大数据量用 &#125; 4、结构化查询DSLmatch 相当于and should 相当于or must_not 相当于not Query精确查询 Matchhttps://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl.html match查询 12345678GET /_search&#123; &quot;query&quot;: &#123; &quot;match&quot; : &#123; &quot;message&quot; : &quot;this is a test&quot; &#125; &#125;&#125; Filter判断某个字段不为空 123&quot;filter&quot;: [ &#123; &quot;script&quot;:&#123;&quot;script&quot;:&quot;doc[&apos;interests&apos;].values.length==60&quot;&#125; &#125; ] should5、聚合统计对查询的结果聚合12345678910111213141516171819GET iclick_persona/iclick/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;term&quot;: &#123; &quot;articles.domains&quot;: &quot;www.baby-kingdom.com&quot; &#125;&#125;, &#123; &quot;match&quot;: &#123; &quot;province&quot;: &quot;HK&quot; &#125;&#125;, &#123;&quot;match&quot;: &#123;&quot;interests&quot;: &quot;20&quot;&#125;&#125; ] &#125; &#125;, &quot;aggs&quot;: &#123; &quot;interests&quot;: &#123; &quot;terms&quot;: &#123;&quot;field&quot;: &quot;interests&quot;, &quot;size&quot;: 50 &#125; &#125; &#125;&#125; 统计月活跃度 123456789101112131415161718192021222324GET iclick_persona/iclick/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;term&quot;: &#123; &quot;articles.domains&quot;: &quot;www.baby-kingdom.com&quot; &#125;&#125;, &#123; &quot;term&quot;: &#123; &quot;province&quot;: &quot;HK&quot; &#125;&#125; ], &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;create_time&quot;: &#123;&quot;gte&quot; : &quot;2017-08-10&quot;&#125; &#125; &#125; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;create_time&quot;: &#123; &quot;terms&quot;: &#123;&quot;field&quot;: &quot;create_time&quot;,&quot;size&quot;: 50,&quot;order&quot;: &#123; &quot;_term&quot;: &quot;asc&quot; &#125; &#125; &#125; &#125;&#125; 看香港的人群每天有多少 123456789101112131415161718192021GET iclick_persona/iclick/_search?size=0&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;province&quot;: &quot;HK&quot; &#125;&#125; ] &#125; &#125;, &quot;aggs&quot;: &#123; &quot;dates&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;create_time&quot;, &quot;size&quot;: 100, &quot;order&quot;: &#123; &quot;_term&quot;: &quot;asc&quot; &#125; &#125; &#125; &#125;&#125; DELETE按条件删除 1234567891011121314151617181920212223POST iclick_persona/iclick/_delete_by_query&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;term&quot;: &#123; &quot;create_time&quot;: &#123; &quot;value&quot;: &quot;2017-07-15&quot; &#125; &#125; &#125;, &#123; &quot;term&quot;: &#123; &quot;province&quot;: &#123; &quot;value&quot;: &quot;HK&quot; &#125; &#125; &#125; ] &#125; &#125;&#125; keyword和text区别[ElasticSearch]数据类型keyword和text的区别 在 ES2.x 版本字符串数据是没有 keyword 和 text 类型的，只有string类型，ES更新到5版本后，取消了 string 数据类型，代替它的是 keyword 和 text 数据类型。 Text 数据类型被用来索引长文本，比如说电子邮件的主体部分或者一款产品的介绍。这些文本会被分析，在建立索引前会将这些文本进行分词，转化为词的组合，建立索引。允许 ES来检索这些词语。text 数据类型不能用来排序和聚合。 Keyword不需要进行分词。可以被用来检索过滤、排序和聚合。keyword 类型字段只能用本身来进行检索。 默认是text类型。 match最简单的一个match例子： 查询和”我的宝马多少马力”这个查询语句匹配的文档。 123456789&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;content&quot; : &#123; &quot;query&quot; : &quot;我的宝马多少马力&quot; &#125; &#125; &#125;&#125; 上面的查询匹配就会进行分词，比如”宝马多少马力”会被分词为”宝马 多少 马力”, 所有有关”宝马 多少 马力”, 那么所有包含这三个词中的一个或多个的文档就会被搜索出来。并且根据lucene的评分机制(TF/IDF)来进行评分。 match_phrase比如上面一个例子，一个文档”我的保时捷马力不错”也会被搜索出来，那么想要精确匹配所有同时包含”宝马 多少 马力”的文档怎么做？就要使用 match_phrase 了 123456789&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;content&quot; : &#123; &quot;query&quot; : &quot;我的宝马多少马力&quot; &#125; &#125; &#125;&#125; 完全匹配可能比较严，我们会希望有个可调节因子，少匹配一个也满足，那就需要使用到slop。 12345678910&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;content&quot; : &#123; &quot;query&quot; : &quot;我的宝马多少马力&quot;, &quot;slop&quot; : 1 &#125; &#125; &#125;&#125; multi_match如果我们希望两个字段进行匹配，其中一个字段有这个文档就满足的话，使用multi_match 12345678&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot; : &quot;我的宝马多少马力&quot;, &quot;fields&quot; : [&quot;title&quot;, &quot;content&quot;] &#125; &#125;&#125; 但是multi_match就涉及到匹配评分的问题了。 best_fields我们希望完全匹配的文档占的评分比较高，则需要使用best_fields 12345678910111213&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;我的宝马发动机多少&quot;, &quot;type&quot;: &quot;best_fields&quot;, &quot;fields&quot;: [ &quot;tag&quot;, &quot;content&quot; ], &quot;tie_breaker&quot;: 0.3 &#125; &#125;&#125; 意思就是完全匹配”宝马 发动机”的文档评分会比较靠前，如果只匹配宝马的文档评分乘以0.3的系数 most_fields我们希望越多字段匹配的文档评分越高，就要使用most_fields 123456789101112&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;我的宝马发动机多少&quot;, &quot;type&quot;: &quot;most_fields&quot;, &quot;fields&quot;: [ &quot;tag&quot;, &quot;content&quot; ] &#125; &#125;&#125; cross_fields我们会希望这个词条的分词词汇是分配到不同字段中的，那么就使用cross_fields 123456789101112&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;我的宝马发动机多少&quot;, &quot;type&quot;: &quot;cross_fields&quot;, &quot;fields&quot;: [ &quot;tag&quot;, &quot;content&quot; ] &#125; &#125;&#125; termterm是代表完全匹配，即不进行分词器分析，文档中必须包含整个搜索的词汇 1234567&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;content&quot;: &quot;汽车保养&quot; &#125; &#125;&#125; 查出的所有文档都包含”汽车保养”这个词组的词汇。 使用term要确定的是这个字段是否“被分析”(analyzed)，默认的字符串是被分析的。 拿官网上的例子举例： mapping是这样的： 12345678910111213141516171819202122PUT my_index&#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;full_text&quot;: &#123; &quot;type&quot;: &quot;string&quot; &#125;, &quot;exact_value&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;index&quot;: &quot;not_analyzed&quot; &#125; &#125; &#125; &#125;&#125;PUT my_index/my_type/1&#123; &quot;full_text&quot;: &quot;Quick Foxes!&quot;, &quot;exact_value&quot;: &quot;Quick Foxes!&quot; &#125; 其中的full_text是被分析过的，所以full_text的索引中存的就是[quick, foxes]，而extra_value中存的是[Quick Foxes!]。 那下面的几个请求： 12345678GET my_index/my_type/_search&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;exact_value&quot;: &quot;Quick Foxes!&quot; &#125; &#125;&#125; 请求的出数据，因为完全匹配 12345678GET my_index/my_type/_search&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;full_text&quot;: &quot;Quick Foxes!&quot; &#125; &#125;&#125; 请求不出数据的，因为full_text分词后的结果中没有[Quick Foxes!]这个分词。 对查询的结果排序1234567891011121314151617181920GET iclick_persona/iclick/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123;&quot;term&quot;: &#123; &quot;articles.domains&quot;: &#123; &quot;value&quot;: &quot;play.google.com&quot; &#125; &#125;&#125; ] &#125; &#125;, &quot;sort&quot;: [ &#123; &quot;create_time&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125; ]&#125; bool联合查询: must,should,must_not 如果我们想要请求”content中带宝马，但是tag中不带宝马”这样类似的需求，就需要用到bool联合查询。联合查询就会使用到must,should,must_not三种关键词。 这三个可以这么理解 must: 文档必须完全匹配条件 should: should下面会带一个以上的条件，至少满足一个条件，这个文档就符合should must_not: 文档必须不匹配条件 比如上面那个需求： 12345678910111213141516&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: &#123; &quot;term&quot;: &#123; &quot;content&quot;: &quot;宝马&quot; &#125; &#125;, &quot;must_not&quot;: &#123; &quot;term&quot;: &#123; &quot;tags&quot;: &quot;宝马&quot; &#125; &#125; &#125; &#125;&#125; 多条件查询比如要实现 a &amp;&amp; (b=0 || b=1)这样的需求，则通过嵌套bool来实现，例如 12345678910111213141516171819202122232425262728293031323334353637383940GET news_v1/_search&#123; &quot;size&quot;: 20, &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;美白&quot;, &quot;fields&quot;: [ &quot;meta.description&quot;, &quot;title&quot; ] &#125; &#125;, &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;term&quot;: &#123; &quot;lang&quot;: &#123; &quot;value&quot;: &quot;zh-hk&quot; &#125; &#125; &#125;, &#123; &quot;term&quot;: &#123; &quot;lang&quot;: &#123; &quot;value&quot;: &quot;en&quot; &#125; &#125; &#125; ] &#125; &#125; ] &#125; &#125;, &quot;ext&quot;: &#123;&#125;&#125; 自定义排序ES自带的排序默认只是可以对数值字段，日期字段或者是字符串字段进行排序，那么，如果我们就是要人为的让包含字段A的排在包含字段B的前面，当前的方式无法满足。 于是需要寻求另一种方式来解决，将给定的A和B转换成数值1和2 从而就能够达到要求的排序。而且是在得分相同的情况才会进行的排序方式！通过脚本实现。 12345678910111213141516171819202122232425262728GET _search &#123; &quot;_source&quot;: &#123; &quot;include&quot;: [&quot;title.Value&quot;,&quot;dataType&quot;,&quot;_score&quot;] &#125;, &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;query_string&quot;: &#123; &quot;default_field&quot;: &quot;title.Value&quot;, &quot;query&quot;: &quot;盆地^10 Unconformity&quot; &#125; &#125; ] &#125; &#125;, &quot;sort&quot; : &#123; &quot;_score&quot;:&#123; &quot;order&quot; : &quot;dese&quot; &#125;, &quot;_script&quot; : &#123; &quot;script&quot; : &quot;&apos;区带资源量数据&apos; in doc[&apos;dataType&apos;].values?2 :(&apos;其它相关资料5&apos; in doc[&apos;dataType&apos;].values? 1 :3)&quot;, &quot;type&quot; : &quot;string&quot;, &quot;order&quot; : &quot;asc&quot; &#125; &#125; &#125; 按条件删除1POST iclick_persona/iclick/_delete_by_query ES的java api连接到ES创建一个客户端连接 12345678import org.elasticsearch.client.transport.TransportClient;import org.elasticsearch.common.settings.Settings;import org.elasticsearch.common.transport.InetSocketTransportAddress;import org.elasticsearch.transport.client.PreBuiltTransportClient;TransportClient client = new PreBuiltTransportClient(Settings.EMPTY) .addTransportAddress(new InetSocketTransportAddress(InetAddress .getByName(&quot;10.1.1.111&quot;), 9300)); 创建索引并写入数据12import org.datanucleus.store.rdbms.request.BulkRequest;import org.elasticsearch.action.index.IndexResponse; 若是单个插入索引 123// 创建es索引IndexResponse response = client.prepareIndex(&quot;movie&quot;, &quot;bt&quot;).setSource(JSON.toJSONString(obj)).get(); 若是批量插入索引 12345678910111213141516171819202122232425262728293031String line = null; JSONObject obj = null; TransportClient client = new PreBuiltTransportClient(Settings.EMPTY) .addTransportAddress(new InetSocketTransportAddress(InetAddress .getByName(&quot;10.1.1.111&quot;), 9300)); //批量插入索引 BulkRequestBuilder brq = client.prepareBulk(); File file = new File(&quot;f:\\data_utf8.json&quot;); int cnt = 0; if (file.exists() &amp;&amp; file.isFile()) &#123; InputStreamReader isr = new InputStreamReader(new FileInputStream( file)); BufferedReader br = new BufferedReader(isr); while ((line = br.readLine()) != null) &#123; obj = JSON.parseObject(line); brq.add(client.prepareIndex(&quot;btmovie&quot;, &quot;bt&quot;).setSource(JSON.toJSONString(obj))); cnt ++; if (cnt%1000 == 0) System.out.println(cnt); &#125; &#125; brq.execute().actionGet(); System.out.println(&quot;done&quot;); disable_coordtrue：多个关键词命中，打分会累加 if coord factor is enabled (by default “disable_coord”: false) then it means: if we have more search keywords in text then this result would be more relevant and will get higher score. if coord factor is disabled(“disable_coord”: true) then it means: no matter how many keywords we have in search text it will be counted just once. minimum_should_match在multi_match中，minimum_should_match 相关度控制原理http://blog.csdn.net/xyh930929/article/details/72378690?utm_source=itdadao&amp;utm_medium=referral analyzerenglish_custom 1234567891011121314151617181920212223242526272829303132333435363738&quot;analysis&quot;: &#123; &quot;filter&quot;: &#123; &quot;english_stemmer&quot;: &#123; &quot;type&quot;: &quot;stemmer&quot;, &quot;language&quot;: &quot;english&quot; &#125;, &quot;english_stop&quot;: &#123; &quot;type&quot;: &quot;stop&quot;, &quot;stopwords&quot;: &quot;_english_&quot; &#125;, &quot;english_possessive_stemmer&quot;: &#123; &quot;type&quot;: &quot;stemmer&quot;, &quot;language&quot;: &quot;possessive_english&quot; &#125; &#125;, &quot;analyzer&quot;: &#123; &quot;cjk_custom&quot;: &#123; &quot;filter&quot;: [ &quot;cjk_width&quot;, &quot;lowercase&quot;, &quot;cjk_bigram&quot;, &quot;english_stop&quot;, &quot;asciifolding&quot; ], &quot;tokenizer&quot;: &quot;standard&quot; &#125;, &quot;english_custom&quot;: &#123; &quot;filter&quot;: [ &quot;english_possessive_stemmer&quot;, &quot;lowercase&quot;, &quot;english_stop&quot;, &quot;english_stemmer&quot;, &quot;asciifolding&quot; ], &quot;tokenizer&quot;: &quot;standard&quot; &#125; &#125; &#125;]]></content>
      <categories>
        <category>Elastic Search</category>
      </categories>
      <tags>
        <tag>Elastic Search</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo + Github 搭建博客入门]]></title>
    <url>%2F2017%2F07%2F12%2F%E5%B7%A5%E5%85%B7%E5%92%8C%E7%8E%AF%E5%A2%83%2FHexo%20%2B%20Github%20%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[1、安装node.js12345$ sudo add-apt-repository ppa:chris-lea/node.js$ sudo apt-get update$ sudo apt-get install nodejs#原方法没有这一步，但是后面的操作会提示npm command not found$ sudo apt-get install npm 2、安装hexo1$ sudo npm install hexo -g 如果是mac，要用 1sudo npm install hexo --no-optional 不然会报错Error: Cannot find module ‘./build/Release/DTraceProviderBindings’] code: ‘MODULE_NOT_FOUND’这个错误。 3、初始博客的根目录12$ cd ~/myblog$ hexo init mac的根目录在 1/Users/david/david/myblog 4、在github上新建仓库名称必须是 1gitusername.github.io 我的就是 1Schwimmer.github.io 并将本地的SSH KEY添加到git上（略） 5、让博客可以发布到git1）安装hexo-deployer-Git（不然会出现ERROR Deployer not found: git） 1npm install hexo-deployer-git --save 2） 配置你hexo博客根目录下的_config.yml文件(应该是最下面一行，修改成你的github) 1234deploy: type: git repo: git@github.com:Schwimmer/Schwimmer.github.io.git branch: master tips 冒号后面一定要跟空格 6、hexo常用命令12345hexo clean #清除缓存hexo new &quot;title&quot; #新建文章hexo g #生成html，或hexo generatehexo s #在本地启动服务，启动后访问localhost:4000就可以打开，或hexo serverhexo d #发布到git，发布后访问https://schwimmer.github.io/就可以打开，或hexo deploy tips 我目前用的新建文章的方法，就是直接在source/_posts/下面新建md文件 可以偷懒写成 cd ~/myblog hexo clean;hexo g;hexo s 或 hexo clean;hexo g;hexo d 7、支持数学公式更新： 现在用 如何在 hexo 中支持 Mathjax？ 搭建一个支持LaTEX的hexo博客 1npm install hexo-math --save 这时如果你会发现出了一些问题，原因是hexo先用marked.js渲染，然后再交给MathJax渲染。在marked.js渲染的时候下划线_是被escape掉并且换成了&lt;em&gt;标签，即斜体字，另外LaTeX中的\\也会被转义成一个\，这样会导致MathJax渲染时不认为它是一个换行符了。 为了使Marked.js与MathJax共存，打开node_modules/marked/lib/marked.js并做如下改动 123456Step 1: escape: /^\\([\\`*&#123;&#125;\[\]()# +\-.!_&gt;])/,11替换成 escape: /^\\([`*\[\]()# +\-.!_&gt;])/,11这一步是在原基础上取消了对\\,\&#123;,\&#125;的转义(escape)Step 2: em: /^\b_((?:[^_]|__)+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,11替换成 em:/^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, 1https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;&lt;/script&gt; 8、安装主题转自：NexT主题安装教程 9、文章阅读计数转自：Hexo添加不蒜子和LeanCloud统计无标题文章 找到站点的themes/next/layout/_partials目录下的footer.swig文件。插入代码如下。 123456789101112131415161718192021&#123;% if theme.copyright %&#125;&lt;div class=&quot;powered-by&quot;&gt; &#123;&#123; __(&apos;footer.powered&apos;, &apos;&lt;a class=&quot;theme-link&quot; href=&quot;https://hexo.io&quot;&gt;Hexo&lt;/a&gt;&apos;) &#125;&#125;&lt;/div&gt;&lt;div class=&quot;theme-info&quot;&gt; &#123;&#123; __(&apos;footer.theme&apos;) &#125;&#125; - &lt;a class=&quot;theme-link&quot; href=&quot;https://github.com/iissnan/hexo-theme-next&quot;&gt; NexT.&#123;&#123; theme.scheme &#125;&#125; &lt;/a&gt;&lt;/div&gt;# 此位置插入以下代码&lt;div&gt;&lt;script async src=&quot;https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js&quot;&gt;&lt;/script&gt;本站总访问量 &lt;span id=&quot;busuanzi_value_site_pv&quot;&gt;&lt;/span&gt; 次&amp;nbsp&amp;nbsp&amp;nbsp本站访客数&lt;span id=&quot;busuanzi_value_site_uv&quot;&gt;&lt;/span&gt;人次&lt;/div&gt;&#123;% endif %&#125; 10、增加图片1 把主页配置文件_config.yml 里的post_asset_folder:这个选项设置为true 2 在你的hexo目录下执行这样一句话npm install hexo-asset-image --save，这是下载安装一个可以上传本地图片的插件，来自dalao：dalao的git 3 等待一小段时间后，再运行hexo n &quot;xxxx&quot;来生成md博文时，/source/_posts文件夹内除了xxxx.md文件还有一个同名的文件夹 4 最后在xxxx.md中想引入图片时，先把图片复制到xxxx这个文件夹中，然后只需要在xxxx.md中按照markdown的格式引入图片： ![你想输入的替代文字](xxxx/图片名.jpg) 注意：xxxx是这个md文件的名字，也是同名文件夹的名字，你想引入的图片就只需要放入xxxx这个文件夹内就好了，很像引用相对路径。 5 最后检查一下，hexo g生成页面后，进入public\2017\02\26\index.html文件中查看相关字段，可以发现，html标签内的语句是&lt;img src=&quot;2017/02/26/xxxx/图片名.jpg&quot;&gt;，而不是&lt;img src=&quot;xxxx/图片名.jpg&gt;。这很重要，关乎你的网页是否可以真正加载你想插入的图片。 11、sitemap 插件Hexo Seo优化让你的博客在google搜索排名第一 12&lt;meta name=&quot;google-site-verification&quot; content=&quot;Mx7Ikp0IpBtTbSpHDTBV0_CMJA-E8CLn8NRIrwyq5m4&quot; /&gt;&lt;meta name=&quot;baidu-site-verification&quot; content=&quot;ZBTsWx4NdC&quot; /&gt; 12、首页显示文章摘要 进入hexo博客项目的themes/next目录 用文本编辑器打开_config.yml文件 搜索”auto_excerpt”,找到如下部分： 12345# Automatically Excerpt. Not recommand.# Please use &lt;!-- more --&gt; in the post to control excerpt accurately.auto_excerpt: enable: false length: 150 把enable改为对应的false改为true，然后hexo d -g，再进主页，问题就解决了！ 13、启用分类和标签1、在博客的开头要加上 123456title: Hexo + Github 搭建博客入门date: 2017-07-12 11:49:53categories: &quot;工具和环境&quot;tags: - hexodescription: 2、修改主题配置文档 123456menu: home: / categories: /categories #about: /about archives: /archives tags: /tags 3、hexo加上page 如分类 1hexo new page categories 然后打开source/categories/index.md，增加一行 1type: &quot;categories&quot; 增加标签也是一样 1hexo new page tags 然后打开source/tags/index.md，增加一行 1type: &quot;tags&quot; 谷歌与百度的站点地图，前者适用于其他搜索引擎，用来手动提交以增加收录 安装： 12npm install hexo-generator-sitemap@1 --savenpm install hexo-generator-baidu-sitemap@0.1.1 --save _config.yml添加代码： 12baidusitemap: path: baidusitemap.xml 谷歌的sitemap.xml不需要写到配置文件中，自动生效。 在主页后面加/baidusitemap.xml可以看到baidusitemap（谷歌同理），将该网址它提交给百度搜索：百度站长平台，贴吧账号无法在这里使用。 不过由于Github禁止了百度爬虫，百度无法抓取其中的URL： 绑定代码到Coding创建coding仓库 上传公钥 https://coding.net/user/account/setting/keys 测试SSH Key 是否配置成功 1ssh -T git@git.coding.net 用来部署Hexo博客的Coding项目地址为：git@git.coding.net:ddxy1986/DavidXu-Blog deploy的配置改为 123456deploy: type: git repo: github: git@github.com:Schwimmer/Schwimmer.github.io.git coding: git@git.coding.net:ddxy1986/DavidXu-Blog branch: master 配置Coding项目的Pages服务开启Coding项目的Pages服务 踩过的坑1）若启动时报错 1234 Error: Warning: Permanently added &apos;github.com,192.30.253.112&apos; (RSA) to the list of known hosts.sign_and_send_pubkey: signing failed: agent refused operationPermission denied (publickey).fatal: Could not read from remote repository. 处理是 12$ eval &quot;$(ssh-agent -s)&quot;$ ssh-add 参考：ubuntu下使用hexo搭建博客]]></content>
      <categories>
        <category>工具和环境</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark-streaming笔记]]></title>
    <url>%2F2017%2F07%2F12%2Fhadoop-spark%2Fspark-streaming%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[spark streaming的示例12345678910111213141516171819202122232425def main ( args : Array[ String ]): Unit = &#123; //关闭一些不必要的日志 Logger. getLogger ( "org.apache.spark" ). setLevel (Level. WARN ) Logger. getLogger ( "org.eclipse.jetty.server" ). setLevel (Level. OFF ) val conf = new SparkConf(). setAppName ( "wordStreaming" ). setMaster ( "local[2]" ). set ( "spark.sql.shuffle.partitions" , "10" ). set ( "spark.network.timeout" , "30s" ) . set ( "spark.shuffle.compress" , "true" ). set ( "spark.shuffle.spill.compress" , "true" ) . set ( "spark.shuffle.manager" , "sort" ) val sc = new SparkContext( conf ) // 创建 StreamingContext，1 秒一个批次。这里要用 sc ，而不是 conf ，因为 sc 已经创建了 val ssc = new StreamingContext( sc , Seconds ( 1 )) // 获得一个 DStream 负责连接 监听端口:地址 val lines = ssc . socketTextStream ( "192.168.37.129" , 9999 ) // 对每一行数据执行 Split 操作 val words = lines . flatMap ( _. split ( " " ) ) // 统计 word 的数量 val pairs = words . map ( word =&gt; ( word , 1 )) val wordCounts = pairs . reduceByKey (_ + _) // 输出结果 wordCounts . print () ssc . start () ssc . awaitTermination () &#125; 一开始会报错：Exception in thread “main” org.apache.spark.SparkException : Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at: org.apache.spark.SparkContext.( SparkContext.scala:82 ) 错误是在val ssc = new StreamingContext( conf , Seconds ( 1 )) 因为之前sc已经创建了，所以这里的conf要改成sc 之后，在 192.168.37.129上启动netcatnc -lk 9999输入hello world 再启动spark的程序，可以看出会输出1234Time: 1462790166000 ms(hello,1)(world,1) streaming读取本地文件1val lines = ssc.textFileStream(&quot;E:\\spark&quot;) 每当该文件夹内有新文件生成，就会自动读取 Spark Streaming将会监控dataDirectory目录，并且处理目录下生成的任何文件（嵌套目录不被支持）。需要注意一下三点：1 所有文件必须具有相同的数据格式2 所有文件必须在dataDirectory目录下创建，文件是自动的移动和重命名到数据目录下3 一旦移动，文件必须被修改。所以如果文件被持续的附加数据，新的数据不会被读取。对于简单的文本文件，有一个更简单的方法streamingContext.textFileStream(dataDirectory)可以被调用。文件流不需要运行一个receiver，所以不需要分配核。 spark streaming连接kafka12val topics = Set(&quot;test1&quot;)val kafkaParm = Map(&quot;metadata.broker.list&quot; -&gt; &quot;192.168.255.128:9092&quot;)]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
      <tags>
        <tag>spark-streaming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java出错汇总]]></title>
    <url>%2F2017%2F07%2F12%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FJAVA%2Fjava%E5%87%BA%E9%94%99%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[MD5线程不安全MessageDigest线程不安全，多线程下会报错 12345678910111213java.lang.ArrayIndexOutOfBoundsException at java.lang.System.arraycopy(Native Method) at sun.security.provider.DigestBase.engineUpdate(DigestBase.java:114) at java.security.MessageDigest$Delegate.engineUpdate(MessageDigest.java:584) at java.security.MessageDigest.update(MessageDigest.java:335) at com.buzzinate.common.util.hash.MD5Util.getMD5String(MD5Util.java:98) at com.buzzinate.common.util.hash.MD5Util.getMD5String(MD5Util.java:94) 改用apache的commons-codec 123456789&lt;dependency&gt; &lt;groupId&gt;commons-codec&lt;/groupId&gt; &lt;artifactId&gt;commons-codec&lt;/artifactId&gt; &lt;version&gt;1.10&lt;/version&gt; &lt;/dependency&gt; 使用方法 1234567public static String encodeMD5Hex(String data) &#123; return DigestUtils.md5Hex(data); &#125; 该方法是线程安全的 Eclipse闪退每次闪退后都提示查看\workspace.metadata.log，发现有如下异常信息记录： 1234!ENTRY org.eclipse.e4.ui.workbench.swt 4 2 2016-08-23 08:42:49.516 !MESSAGE Problems occurred when invoking code from plug-in: &quot;org.eclipse.e4.ui.workbench.swt&quot;. !STACK 0 java.lang.IllegalArgumentException: Argument cannot be null 出现该问题的原因是：由于项目没有正常关闭运行而导致”workbench.xmi”中的”persistedState”标签还保持在运行时的配置造成的。 解决办法： 找到&lt;workspace&gt;/.metadata/.plugins/org.eclipse.e4.workbench/workbench.xmi文件，将其删掉，再重启Eclipse，恢复正常。]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>java错误</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模型集成]]></title>
    <url>%2F2017%2F07%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%A8%A1%E5%9E%8B%E9%9B%86%E6%88%90%2F</url>
    <content type="text"><![CDATA[每个Kaggle冠军的获胜法门：揭秘Python中的模型集成 .png) Example Schematics of an ensemble. An input array X is fed through two proprocessing pipelines and then to a set of base learners f(i). The ensemble combines all base learner predictions into a final prediction array P. By the end of the post, you will: understand the fundamentals of ensembles know how to code them understand the main pitfalls and drawbacks of ensembles Predicting Republican and Democratic donationswe’ll use a data set on U.S. political contributions. The original data set was prepared by Ben Wieder at FiveThirtyEight, who dug around the U.S. government’s political contribution registry and found that when scientists donate to politician, it’s usually to Democrats. 12345678910111213141516171819202122232425262728293031323334import numpy as npimport pandas as pdimport matplotlib.pyplot as plt%matplotlib inline### Import data# Always good to set a seed for reproducibilitySEED = 222np.random.seed(SEED)df = pd.read_csv('input.csv')### Training and test setfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import roc_auc_scoredef get_train_test(test_size=0.95): """Split Data into train and test sets.""" y = 1 * (df.cand_pty_affiliation == "REP") X = df.drop(["cand_pty_affiliation"], axis=1) X = pd.get_dummies(X, sparse=True) X.drop(X.columns[X.std() == 0], axis=1, inplace=True) return train_test_split(X, y, test_size=test_size, random_state=SEED)xtrain, xtest, ytrain, ytest = get_train_test()# A look at the dataprint("\nExample data:")df.head()df.cand_pty_affiliation.value_counts(normalize=True).plot( kind="bar", title="Share of No. donations")plt.show() This claim is based on the observation on the share of donations being made to Republicans and Democrats. However, there’s plenty more that can be said: for instance, which scientific discipline is most likely to make a Republican donation, and which state is most likely to make Democratic donations? We will go one step further and predict whether a donation is most likely to be a to a Republican or Democrat. What is an ensemble?Combining predictions from several models averages out idiosyncratic errors and yield better overall predictions. How to combine predictions? Machine learning is remarkably similar in classification problems: taking the most common class label prediction is equivalent to a majority voting rule. But there are many other ways to combine predictions, and more generally we can use a model to learn how to best combine predictions. Understanding ensembles by combining decision treesThe deeper the tree, the more complex the patterns it can capture, but the more prone to overfitting it will be. Because of this, we will need an alternative way of building complex models of decision trees, and an ensemble of different decision trees is one such way. We’ll use the below helper function to visualize our decision rules: 1234567891011121314151617181920import pydotplus # you can install pydotplus with: pip install pydotplus from IPython.display import Imagefrom sklearn.metrics import roc_auc_scorefrom sklearn.tree import DecisionTreeClassifier, export_graphvizdef print_graph(clf, feature_names): """Print decision tree.""" graph = export_graphviz( clf, label="root", proportion=True, impurity=False, out_file=None, feature_names=feature_names, class_names=&#123;0: "D", 1: "R"&#125;, filled=True, rounded=True ) graph = pydotplus.graph_from_dot_data(graph) return Image(graph.create_png())]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器之心</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利率计算]]></title>
    <url>%2F2017%2F07%2F12%2F%E6%95%B0%E5%AD%A6%2F%E5%88%A9%E7%8E%87%E8%AE%A1%E7%AE%97%2F</url>
    <content type="text"><![CDATA[等额本息 1234每月月供额=〔贷款本金×月利率×(1＋月利率)＾还款月数〕÷〔(1＋月利率)＾还款月数-1〕每月应还利息=贷款本金×月利率×〔(1+月利率)^还款月数-(1+月利率)^(还款月序号-1)〕÷〔(1+月利率)^还款月数-1〕每月应还本金=贷款本金×月利率×(1+月利率)^(还款月序号-1)÷〔(1+月利率)^还款月数-1〕总利息=还款月数×每月月供额-贷款本金 等额本金 12345每月月供额=(贷款本金÷还款月数)+(贷款本金-已归还本金累计额)×月利率每月应还本金=贷款本金÷还款月数每月应还利息=剩余本金×月利率=(贷款本金-已归还本金累计额)×月利率每月月供递减额=每月应还本金×月利率=贷款本金÷还款月数×月利率总利息=还款月数×(总贷款额×月利率-月利率×(总贷款额÷还款月数)*(还款月数-1)÷2+总贷款额÷还款月数) 信用卡账单分期真实年化利率 年利率利息率=利息量÷本金÷时间×100% IRR内部收益率 (IRR) 的定义是：净现值 (NPV) 为零时的折现率。 综合考虑了每期的流入流出现金的量和时间，加权出来的结果。IRR实质上是一个折现率，用IRR折现时会达到该项目的净现值NPV为0的状态。也可以理解为一个项目的预期收益率。举例来说，如IRR为8%，可以简单解释为以8%的利率借钱投资于此项目，刚好可以不赚不赔。]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>利率</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu java反编译]]></title>
    <url>%2F2017%2F07%2F12%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FJAVA%2Fubuntu-java%E5%8F%8D%E7%BC%96%E8%AF%91%2F</url>
    <content type="text"><![CDATA[从https://sourceforge.net/projects/jadx/下载 假设安装路径为~/opt/jadx 12cd ~/opt/jadx/jadx/build/jadx/bin./jadx-gui 打开gui，拖入jar包或者class文件]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>java反编译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java笔记]]></title>
    <url>%2F2017%2F07%2F12%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FJAVA%2Fjava%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[list转array123List&lt;Double&gt; scores = new ArrayList&lt;Double&gt;();Double[] arrays = scores.toArray(new Double[0]); 判断字符类型判断中文字符1Character.isLetter() 判断是否为英文或数字123456789public static boolean isAlphaOrDigit(char ch) &#123; if (ch &gt;= 'a' &amp;&amp; ch &lt;= 'z') return true; if (ch &gt;= 'A' &amp;&amp; ch &lt;= 'Z') return true; if (ch &gt;= '0' &amp;&amp; ch &lt;= '9') return true; return false;&#125; 文件读写123456789101112String encoding = &quot;GBK&quot;; String lineText = null; File file = new File(&quot;D:\\00projects\\20buzz-cookie-info\\chinaz.json&quot;); if (file.isFile() &amp;&amp; file.exists()) &#123; BufferedReader read = new BufferedReader(new InputStreamReader( new FileInputStream(file), encoding)); while((lineText=read.readLine())!=null)&#123; &#125; read.close(); &#125; 写文件 日期和字符串互转字符串转日期 123//2017-05-26-16-40-13SimpleDateFormat simpleDateFormat = new SimpleDateFormat(&quot;yyyy-MM-dd-HH-mm-ss&quot;);Date date = simpleDateFormat.parse(xmoEvent.getOpxdate()); 日期输出为字符串 12simpleDateFormat = new SimpleDateFormat(&quot;yyyyMMdd&quot;);System.out.println(simpleDateFormat.format(date)); 当前日期 12345678System.currentTimeMillis()或者Date date = new Date(System.currentTimeMillis());字符串表示SimpleDateFormat simpleDateFormat = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);Date date = new Date(System.currentTimeMillis());System.out.println(simpleDateFormat.format(date)); 两个日期之间的天数和周数http://www.jb51.net/article/100441.htm 123456789Calendar end = Calendar.getInstance();//取日期是一年中第几天d1 = end.get(Calendar.DAY_OF_YEAR)//两个日期相隔几周(d2-d1)/7//星期中相隔几天Date转为Calendar 读写json常规解析对于某个json 1&#123;"classic_payload":&#123;"opxvrsn":"ut","opxuid":"0","opxclientid":"1127","opxcounter":"1","rnum":"4208937089424580.5","re":"http%3A%2F%2Fwww.liba.com%2Findex.shtml","opxpid":"201705221928320003451270013251741","opxsid":"2ebfa12ef6fe88985d2c1b04ea8d22df","opxdate":"2017-05-26-16-40-13","opxip":"120.92.161.27","opxleadsite":"http://www.liba.com/index.shtml","opxreferringsite":"http%3A%2F%2Fshuaxinfuwu.nipponpaint.com.cn%2Findex.aspx%3Futm_medium%3DVM%26utm_source%3DLB","useragent":"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.101 Safari/537.36","language":"zh-CN,zh;q=0.8,zh-TW;q=0.6,en-US;q=0.4,en;q=0.2","accept":"*/*","connection":"close","encoding":"gzip, deflate","hversion":"HTTP/1.0","frompaidsearch":0,"hascookie":0,"opxeventid":"13057"&#125;,"machine":"10.11.10.12","uuid":"318553958122302938064698429658924956664"&#125; 123456789101112131415//取其中某个字段JSONObject jsonObj = JSONObject.parseObject(content);String uuid = jsonObj.getString(&quot;uuid&quot;);//如果是数组JSONArray array = json.getJSONArray(&quot;result&quot;); List&lt;Keyword&gt; keywords = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; array.size(); i++) &#123; JSONObject jo = array.getJSONObject(i); assertEquals(topic.getId(), jo.getInteger(&quot;topicId&quot;)); keywords.add(new Keyword(topic, jo.getInteger(&quot;id&quot;), jo.getString(&quot;keyword&quot;), null, jo.getJSONArray(&quot;spiderTopics&quot;), jo.getString(&quot;updatedAt&quot;), jo.getString(&quot;createdAt&quot;))); &#125; //如果是某个类 解析JSONArray123456789101112List&lt;IP&gt; iplist = new ArrayList&lt;IP&gt;();iplist.add(new IP("1.4.255.255"));iplist.add(new IP("1.0.255.255"));iplist.add(new IP("1.2.255.255"));String ipliststr = JSON.toJSONString(iplist);List&lt;IP&gt; ipl = new ArrayList&lt;IP&gt;(JSONArray.parseArray(ipliststr,IP.class));for (IP ip : ipl) &#123; System.out.println(ip.getIp_address());&#125; 解析的类中有嵌套的ArrayList&lt;类&gt;，可以直接解析，但对ArrayList的命名有要求，例如 1private ArrayList&lt;KeywordItem&gt; keywordItems; 解析为复杂集合1234567TypeReference&lt;HashMap&lt;String, ArrayList&lt;WebPageInfo&gt;&gt;&gt; ty = new TypeReference&lt;HashMap&lt;String, ArrayList&lt;WebPageInfo&gt;&gt;&gt;()&#123;&#125;; try &#123; d2wps = JSON.parseObject(info,ty); &#125; catch (Exception e) &#123; continue; &#125; 数据类型比较两个double是否相等123long thisBits = Double.doubleToLongBits(d1); long anotherBits = Double.doubleToLongBits(d2); return (thisBits == anotherBits ? 0 : // Values are equal 正则1234567//匹配的是括号中的内容，匹配时，先matcher.find()，然后找group，group（0）是本身。group（1）是第一个括号内容//这里匹配的是可能包括&amp;，然后有q=，括号是提取的内容，用懒惰模式。最后要么是结尾，要么是&amp;，Pattern pattern = Pattern.compile("&amp;?(q|p|wd|word|query)=(.*?)(&amp;|$)");Matcher matcher = pattern.matcher(url.getQuery());while (matcher.find()) &#123; System.out.println(matcher.group(1));&#125; hdfs读数据 123FSDataInputStream dis = null;FileSystem fs = FileSystem.get(HDFSHandler.conf);dis = fs.open(new Path(modelFile)); 集合ArrayList一句话初始化12345678910111213ArrayList&lt;String&gt; list = new ArrayList&lt;String&gt;() &#123;&#123; add("A"); add("B"); add("C");&#125;&#125;List&lt;String&gt; places = Arrays.asList("Buenos Aires", "Córdoba", "La Plata");// 初始化一个 List，而不是 ArrayListList&lt;String&gt; list = ["A", "B", "C"];// 初始化的更好看些List&lt;Long&gt; cptIds = Lists.newArrayList(); 优先级队列java中PriorityQueue优先级队列使用方法 优先级队列是不同于先进先出队列的另一种队列。每次从队列中取出的是具有最高优先权的元素。如果不提供Comparator的话，优先队列中元素默认按自然顺序排列，也就是数字默认是小的在队列头，字符串则按字典序排列。 如果想实现按照自己的意愿进行优先级排列的队列的话，需要实现Comparator接口。下面的方法，实现了根据某个变量，来进行优先级队列的建立。 1234567891011Comparator&lt;test&gt; OrderIsdn = new Comparator&lt;test&gt;()&#123; public int compare(test o1, test o2) &#123; // TODO Auto-generated method stub int numbera = o1.getPopulation(); int numberb = o2.getPopulation(); if(numberb &gt; numbera) return 1; else if(numberb&lt;numbera) return -1; else return 0; &#125; &#125;; Queue&lt;test&gt; priorityQueue = new PriorityQueue&lt;test&gt;(11,OrderIsdn); TreeSetTreeSet中的对象要实现comparable 1234567891011121314151617181920212223public class KeywordItem implements Comparable&lt;KeywordItem&gt; &#123; public String name; public Double score; public KeywordItem(String name, Double score) &#123; this.name = name; this.score = score; &#125; @Override public String toString() &#123; return this.name + &quot;\t&quot; + score; &#125; @Override public int compareTo(KeywordItem o) &#123; if (this.score &lt; o.score) &#123; return 1; &#125; else &#123; return -1; &#125; &#125;&#125; 插入的时候自动比较 1234567TreeSet&lt;KeywordItem&gt; looklikeCodeTree = new TreeSet&lt;KeywordItem&gt;();if (entry.getValue() &gt; this.minLookLike) &#123; looklikeCodeTree.add(new KeywordItem(entry.getKey().toString(), entry.getValue())); if (looklikeCodeTree.size() &gt; maxLookLikeTagCnt) &#123; looklikeCodeTree.pollLast(); &#125; &#125; 随机数12345678import java.util.Random;private Random random = new Random();//随机打印一些日志if (random.nextDouble() &lt; 0.00001 ) &#123; logger.info(JSON.toJSONString(obj));&#125; 二维数组1int[][] arr = new int[3][5]; URLDecoder Illegal hex characters in escape (%) pattern - For input string: “u0” Incomplete trailing escape (%) pattern 1searchString=URLDecoder.decode(searchString.replaceAll(&quot;%&quot;, &quot;%25&quot;),&quot;utf-8&quot;); List对象的去重123456789101112def dedupBy[T](list: List[T])(f: T =&gt; String): List[T] = &#123; val buf = new ListBuffer[T] val set = new HashSet[String] list foreach &#123; e =&gt; val key = f(e) if (!set.contains(key)) &#123; buf += e set += key &#125; &#125; buf.result &#125; 最简单的多线程如果要测试多线程 1234567891011121314151617181920212223class MyThread extends Thread &#123; public MyThread(String name) &#123; super(name); &#125; public void run() &#123; for (int i = 0; i &lt; 20; i++) &#123; System.out.println(Thread.currentThread().getName()); try &#123; Thread.sleep(1000l); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125;public static void main(String[] args) &#123; MyThread myThread1 = new MyThread(&quot;myThread1&quot;); MyThread myThread2 = new MyThread(&quot;myThread2&quot;); myThread1.start(); myThread2.start(); &#125; 能看到两个线程交替打印 并发CountDownLatchBlockingQueue两者的使用参考InterestTimerTask.java http://www.importnew.com/15731.html Redis锁setnx是「SET if Not eXists」的缩写，可以用来实现锁。如果setnx存在，则执行更新，更新后删除这个key。在锁的时候，其他对redis的访问要么等待，要么用过期的缓存 注意要加上锁的过期时间，防止请求意外退出而锁得不到释放。 检测语言是否为繁体 1234String simpleText = TraditionalChineseDictionary.convertToSimplifiedChinese(text);if (!simpleText.equals(text)) &#123; lang = &quot;zh-hk&quot;; &#125; 是否为英文 123456789public static boolean isEnglish(String str) &#123; String regEx = &quot;[a-zA-Z]+&quot;; Pattern p = Pattern.compile(regEx); Matcher m = p.matcher(str); if (m.find()) &#123; return true; &#125; return false; &#125; 是否为中文 123456789public static boolean isChinese(String str) &#123; String regEx = &quot;[\\u4e00-\\u9fa5]+&quot;; Pattern p = Pattern.compile(regEx); Matcher m = p.matcher(str); if (m.find()) &#123; return true; &#125; return false; &#125; 枚举定义一个枚举类 12345678910111213public enum TypeGroup &#123; CN(&quot;cn&quot;), EN(&quot;en&quot;), CN_EN(&quot;cn_en&quot;),BR(&quot;br&quot;),ALL(&quot;all&quot;); private final String type; private TypeGroup(String type) &#123; this.type = type; &#125; public String getType() &#123; return type; &#125;&#125; jsoup获取url的html123String url = &quot;http://www.jb51.net/softjc/414254.html&quot;;String html = Jsoup.connect(url).userAgent(&quot;Mozilla&quot;).get().toString();System.out.println(html); list排序12static List&lt;Integer&gt; intList = Arrays.asList(2, 3, 1);Collections.sort(intList); 结果默认是正序排序 11,2,3 如何实现逆序呢，这就要使用第二种方式了，即通过实现Comparator接口的compare方法来完成自定义排序，代码如下： 12345678Collections.sort(intList,new Comparator&lt;Integer&gt;() &#123; @Override public int compare(Integer o1, Integer o2) &#123; // 返回值为int类型，大于0表示正序，小于0表示逆序 return o2-o1; &#125; &#125;); 随机打乱1Collections.shuffle 多线程Atomic保证在高并发的情况下只有一个线程能够访问这个属性值。 AtomicBoolean使用 AtomicBoolean 高效并发处理 “只初始化一次” 的功能要求： 12345678private static AtomicBoolean initialized = new AtomicBoolean(false);public void init()&#123; if( initialized.compareAndSet(false, true) ) &#123; // 这里放置初始化代码.... &#125;&#125;]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模型评估与选择]]></title>
    <url>%2F2017%2F07%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9%2F</url>
    <content type="text"><![CDATA[1、模型评估方法 可重复采样：在训练集小，难以划分训练/测试集是有用。此外，能产生多个不同训练集，对集成学习等方法有很大的好处。但是会改变初始数据集分布。 在初始数据量足够时，用留出法或者交叉验证法。 1.1 留出法Hold-out将数据集D分成两个互斥的集合。 训练/测试集尽量保证数据一致性，用分层采样，正负样本同比例。 由于单次估计结果往往不可靠，使用留出法时，一般要采用若干次随机划分，重复进行实验后取平均值作为评估值。 ‘ 1.2 交叉验证法将D分成k个大小相似的互斥子集，每个子集用分层采样得到。 每次用k-1个子集的并集作为训练集，余下的子集作为测试集。这样获得k组训练/测试集。最终返回是k个测试结果的均值。 常用10折交叉验证。 1.3 可重复采样bootstrapping sampling：给定包含m个样本的数据集D，我们进行采样产生数据集$D’$，每次随机从D中挑选一个样本，将其拷贝放入$D’$，再将样本放回D。重复m次，得到包含m个样本的$D’$。 样本在m次采样中始终不被采到的概率是$(1-\frac 1 m)^m$，取极限得到 {\lim_{m \mapsto \infty }}(1-\frac 1 m)^m \mapsto \frac 1 e\approx0.368即通过bootstrapping，D中有36.8%的样本未出现在$D’$中，于是可以将$D’$作为训练集，$D-D’$作为测试集，这样可以有1/3个未出现在训练集的样本用于测试。测试结果称为“包外估计”（out-of-bag estimate）。 1.4 调参 我们在模型评估时往往用来确定算法和参数。当这些确定后，要用所有的D再训练一次，才是最终的模型。 2、性能度量回归最常用的是“均方误差”（mean squared error） E(f;D) = \frac 1 m \sum_{i=1}^m(f(x_i)-y_i )^2更一般的，对于数据分布D和概率密度函数$p(\cdot )$，均方误差可描述为 E(f;D) = \int_{x \in D}(f(x)-y)^2p(x)dx分类的性能度量更复杂 2.1 错误率和精度错误率：分类错误的样本占总样本的比例 精度：分类正确的样本占总样本的比例]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>西瓜书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈在线最优化求解算法-以CTR预测模型为例]]></title>
    <url>%2F2017%2F07%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%B5%85%E8%B0%88%E5%9C%A8%E7%BA%BF%E6%9C%80%E4%BC%98%E5%8C%96%E6%B1%82%E8%A7%A3%E7%AE%97%E6%B3%95-%E4%BB%A5CTR%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B%E4%B8%BA%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[[TOC] 1、最优化求解问题通常，我们需要求解的最优化问题有如下三类： （1）无约束优化问题： X=\arg \underset{X}{min}f(X)含义是求解X，令目标函数$f(X)$最小。 对于这类问题，在$f(X)$ 是凸函数的前提下，通常做法就是对$f(X)$ 求导，并令$\frac {\partial} {\partial X} f(X) =0$ ，求解可以得到最优值。 凸函数 如果$f(x)$是定义在N维向量空间上的实变量函数，对于在$f(x)$的定义域C上的任意两个点$x_1$和$x_2$，以及任意[0,1]之间的值t都有： f(tX_1 + (1-t)X_2) \leq tf(X_1)+(1-t)f(X_2)\\ \forall X_1,X_2 \in C,\ \ 0 \leq t \leq 1 则称$f(x)$是凸函数。一个函数是凸函数是其存在最优解的充要条件。 此外，如果$f(x)$满足 f(tX_1 + (1-t)X_2)< tf(X_1)+(1-t)f(X_2)\\ \forall X_1,X_2 \in C,\ \ 0 \leq t \leq 1 则$f(x)$为严格凸函数。如下图所示，左边是严格凸函数，右边是凸函数 （2）有等式约束的最优化问题： X=\arg \underset{X}{min}f(X)\\ s.t. h_k(X)=0;k=1,2,...,n含义是在n个等式约束$h_k(X)$ 的条件下求解X，另目标函数$f(X)$最小。 针对有等式的最优化问题，采用拉格朗日乘数法进行求解，通过拉格朗日系数$A=[a_1,a_2,…,a_n]^T$ 把等式约束和目标函数组合成一个式子 X=\arg \underset{X}{min}[f(X)+ A^TH(X)]相当于转化成无约束最优化求解问题，解决方法是分别对X，A求偏导并令其等于0。 （3）不等式约束的优化问题求解 ： X=\arg \underset{X}{min}f(X)\\ s.t. h_k(X)=0;k=1,2,...,n\\ g_l(X)\leq 0;l=1,2,...,m对于不等式约束，通过KKT条件求解。将所有的约束和目标函数写为一个式子 L(X,A,B)=f(X)+A^TH(X)+B^TG(X)KKT条件是说最优值必须满足以下条件： \frac \partial {\partial X} L(X,A,B)=0\\ H(X)=0\\ B^TG(X)=0KKT条件是求解最优值的必要条件，要使其成为充要条件，还需要f(x)为凸函数。 2、批量最优化求解算法一些定义： $i=1,2,…,N$表示向量维度 $j=1,2,…,M$表示样本个数 $t=1,2,…$表示迭代次数 2.1 批量和随机求解我们面对的最优化问题都是无约束的最优化问题（有约束的也可以转成无约束的），因此通常可以将其描述为 W=\arg \underset{W}{min}\ l(W,Z)\\ Z=\{ (X_j,y_j) | j=1,2,...,M \}\\ y_j=h(W,X_j) \tag {2-1-1}就是在已知训练集的情况下，求使得目标函数最小的权重矩阵。其中，$Z$是训练集，$\mathbf{X}$是特征向量，$X_j$是其中一个样本，$Y$是预测值，$y_j$是其中一个样本对应的预测值。一共有M个样本。$h(W,X_j)$ 是特征向量到预测值的映射函数，$ l(W,Z)$ 最优化求解的目标函数，也称为损失函数，$W$ 为特征权重，也就是在损失函数中需要求解的参数。 损失函数一般包括损失项和正则项 常用的损失函数有： （1）平方损失函数（线性回归） 最小二乘法（Ordinary Least Squares）是常用的一种平方损失函数，最小二乘的基本原理是：最优拟合直线应该是使各点到回归直线的距离和最小的直线，即平方和最小。 线性回归的映射函数为： h(W,X_j)=W^TX_j损失函数可以表示为 l(W,Z)=\sum_{j=1}^M (y_j-W^TX_j)^2（2）Logistics损失函数（逻辑回归） 逻辑回归的映射函数为： h(W,X_j)=\frac 1 {1+e^{-W^TX_j}} logistic函数的优点是： 1、他的输入范围是$-\infty \rightarrow + \infty $ ，输出范围是(0,1)，正好满足概率分布为（0，1）的要求。我们用概率去描述分类器，自然比单纯的某个阈值要方便很多； 2、是一个单调上升的函数，具有良好的连续性，不存在不连续点。 由于该函数服从伯努利分布（0-1分布），通过最大似然估计，对于每一维的权重W，损失函数可以表示为 l(W,Z)=(Y-h_W(\mathbf X))X 推导过程 令 h_W(X) = \frac 1 {1+e^{-W^T\mathbf X}}该函数服从伯努利分布（一次点击要么成功，要么失败，通过训练集可以知道不同特征组合下成功和失败的概率） P(Y=1 | \mathbf X;W) = h_W(\mathbf X)\\ P(Y=0 | \mathbf X;W) = 1-h_W(\mathbf X)则概率分布函数为 P(Y|\mathbf X;W) = (h_W(\mathbf X))^Y*(1-h_W(\mathbf X))^{1-Y}（也就是说，我们有样本，通过样本能知道概率分布，那么我们需要知道得到这个概率分布的最有可能的参数W。即我们通过样本知道一些特征组合下的点击率，现在需要求概率函数中的系数。） 我们假设样本数据相互独立，所以它们的联合分布可以表示为各边际分布的乘积，用似然函数表示为： \begin{aligned} L(W)=P(Y|\mathbf X;W) &= (h_W(\mathbf X))^Y(1-h_W(\mathbf X))^{1-Y}\\ &=\prod_{j=1}^M(h_W(X_j))^{y_j}(1-h_W(X_j))^{1-y_j} \end{aligned} \tag {2-1-2}从而，损失函数的求解，可以转化为求最有可能导致这样概率分布的W，也就是求L(W)的最大值。最简单的方法就是对W求偏导，并令导数为零。 在多数情况下，直接对变量进行求导反而会使得计算式子更加的复杂，此时可以借用对数函数。由于对数函数是单调增函数，因此与（2-1-2）具有相同的最大值，上式变为 \begin{aligned} l(W) &= Log\ L(W)\\ &=\sum_{j=1}^M(y_jln\ h(X_j)+(1-y_j)ln\ (1-h(X_j))) \end{aligned}对其求关于W的偏导 首先求logistic函数的导数，得（最后一个X是对$W^TX$的求导） h_W^{'}(\mathbf X) = h_W(\mathbf X)(1-h_W(\mathbf X)) 推导过程如下 为了求解方便，将l(W)转为（其实1/M没用，完全可以去掉，不懂为何要加上） J(W) = -\frac {1}{M} l(W)则就变成求J(W)的最小值。求偏导的过程如下： 最后得到目标函数（损失函数）为： \frac {\partial }{\partial W}J(W) =-\frac{1}{M} (Y-h_W(\mathbf X))X 对于损失函数的求解，一个典型的方法就是梯度下降法，由于损失函数是凸函数，因此沿着梯度下降的方向找到最小点。 假设样本总数为m，批量梯度下降是： Repeat\ until\ convergence \{ \\ W^{(t+1)} := W^t - \eta^t\triangledown _{W}l(W^{t},Z) \\ \}\\ \tag{1-2}而随机梯度下降（SGD）是： Repeat\ until\ convergence \{ \\ for\ j=1\ to\ M, \{ \\ W^{(t+1)} := W^t - \eta^t\triangledown _{W}l(W^{t},Z_j) \\ \}两者的区别是： 前者每次更新$W$都需要遍历一次整个样本集合；而后者在遍历样本集合的时候，每个样本都能改变$W$ ，有更快的收敛速度 。由于SGD针对观测到的随机一条数据进行权重的更新，很适合进行增量计算，实现梯度下降的online模式。 2.2 正则化正则化的主要目的是防止过拟合。对于损失函数构成的模型，可能会出现有些权重很大，有些权重很小的情况，导致过拟合，使得模型的复杂度提高，泛化能力较差（对未知数据的预测能力）。 而正则化就是对损失函数中权重的限制，限制其模不要太大： W=\arg \underset{W}{min}\ l(W,Z)\\ s.t. \Psi(W)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>最优化问题</tag>
        <tag>CTR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas笔记]]></title>
    <url>%2F2017%2F07%2F12%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FPYTHON%2Fpandas%2F</url>
    <content type="text"><![CDATA[DataFrame创建随机引入pandas包 12345In [1]: import pandas as pdIn [2]: import numpy as npIn [3]: import matplotlib.pyplot as plt 创建一个Series，pandas可以生成一个默认的索引 1s = pd.Series([1,3,5,np.nan,6,8]) 通过numpy创建DataFrame，包含一个日期索引，以及标记的列 123456789101112dates = pd.date_range(&apos;20170101&apos;, periods=6)df = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list(&apos;ABCD&apos;))dfOut[4]: A B C D2016-10-10 0.630275 1.081899 -1.594402 -2.5716832016-10-11 -0.211379 -0.166089 -0.480015 -0.3467062016-10-12 -0.416171 -0.640860 0.944614 -0.7566512016-10-13 0.652248 0.186364 0.943509 0.0532822016-10-14 -0.430867 -0.494919 -0.280717 -1.3274912016-10-15 0.306519 -2.103769 -0.019832 0.035211 其中，np.random.randn可以返回一个随机数组 123np.random.randn(1,2)Out[11]: array([[-2.67809797, 1.49728361]]) np.random.rand 随机样本位于[0,1)中 np.random.randn 从标准正态分布$N=(\mu , \sigma ^2)$中返回样本，默认的范围是$N(0,1)$，等价于np.random.standard_normal 如果要返回2*4的$N(3,6.25)$的随机分布，可知均值是3，标准差是2.5，则 12&gt; 3+2.5*np.random.randn(2,4)&gt; 如果在matlib模块中使用，则返回的是matrix而不是array 1234import numpy.matlibnp.matlib.randn(1,2)Out[13]: matrix([[ 0.13107513, -0.87977247]]) 通过dict创建12345678910111213df2 = pd.DataFrame(&#123; &apos;A&apos; : 1., &apos;B&apos; : pd.Timestamp(&apos;20130102&apos;), &apos;C&apos; : pd.Series(1,index=list(range(4)),dtype=&apos;float32&apos;), &apos;D&apos; : np.array([3] * 4,dtype=&apos;int32&apos;), &apos;E&apos; : pd.Categorical([&quot;test&quot;,&quot;train&quot;,&quot;test&quot;,&quot;train&quot;]), &apos;F&apos; : &apos;foo&apos; &#125;) Out[20]: A B C D E F0 1.0 2013-01-02 1.0 3 test foo1 1.0 2013-01-02 1.0 3 train foo2 1.0 2013-01-02 1.0 3 test foo3 1.0 2013-01-02 1.0 3 train foo 有几个方法可以构造一个Timestamp对象 pd.Timestamp 1234567891011121314151617181920import pandas as pdfrom datetime import datetime as dtp1=pd.Timestamp(2017,6,19)p2=pd.Timestamp(dt(2017,6,19,hour=9,minute=13,second=45))p3=pd.Timestamp(&quot;2017-6-19 9:13:45&quot;)print(&quot;type of p1:&quot;,type(p1))print(p1)print(&quot;type of p2:&quot;,type(p2))print(p2)print(&quot;type of p3:&quot;,type(p3))print(p3)(&apos;type of p1:&apos;, &lt;class &apos;pandas.tslib.Timestamp&apos;&gt;)2017-06-19 00:00:00(&apos;type of p2:&apos;, &lt;class &apos;pandas.tslib.Timestamp&apos;&gt;)2017-06-19 09:13:45(&apos;type of p3:&apos;, &lt;class &apos;pandas.tslib.Timestamp&apos;&gt;)2017-06-19 09:13:45 to_datetime() 123456789101112131415import pandas as pdfrom datetime import datetime as dtp4=pd.to_datetime(&quot;2017-6-19 9:13:45&quot;)p5=pd.to_datetime(dt(2017,6,19,hour=9,minute=13,second=45))print(&quot;type of p4:&quot;,type(p4))print(p4)print(&quot;type of p5:&quot;,type(p5))print(p5)(&apos;type of p4:&apos;, &lt;class &apos;pandas.tslib.Timestamp&apos;&gt;)2017-06-19 09:13:45(&apos;type of p5:&apos;, &lt;class &apos;pandas.tslib.Timestamp&apos;&gt;)2017-06-19 09:13:45 在df2对象中，每一列都有对应的dtypes 12345678910df2.dtypesOut[30]: A float64B datetime64[ns]C float32D int32E categoryF objectdtype: object 查看数据参考Basics section 查看head和tail 12df.head(1)df.tail(3) 查看index、column和数据 123df.indexdf.columnsdf.values 显示数据的快速统计 1234567891011df.describe()Out[19]: A B C Dcount 6.000000 6.000000 6.000000 6.000000mean 0.073711 -0.431125 -0.687758 -0.233103std 0.843157 0.922818 0.779887 0.973118min -0.861849 -2.104569 -1.509059 -1.13563225% -0.611510 -0.600794 -1.368714 -1.07661050% 0.022070 -0.228039 -0.767252 -0.38618875% 0.658444 0.041933 -0.034326 0.461706max 1.212112 0.567020 0.276232 1.071804 转置 1df.T 通过列名来排序 12#对于矩阵，axis=0表示行，1表示列df.sort_index(axis=1, ascending=False) 通过某一列的数值排序 1df.sort_values(by=&apos;B&apos;) 选择选择某一列 1df[&apos;A&apos;] 选择某几行 123df[0:3]#也可以通过行的索引来选择，但是不能单独写某一行df[&apos;20130102&apos;:&apos;20130104&apos;] 做groupby1234567891011121314from sklearn.datasets.samples_generator import make_blobsfrom matplotlib import pyplotfrom pandas import DataFrame# generate 2d classification datasetX, y = make_blobs(n_samples=100, centers=3, n_features=2)# dict中定义三个key，分别是坐标和label，再通过dict创建DataFramedf = DataFrame(dict(x=X[:,0], y=X[:,1], label=y))colors = &#123;0:'red', 1:'blue', 2:'green'&#125;fig, ax = pyplot.subplots()#groupby可以通过传入需要分组的参数实现对数据的分组grouped = df.groupby('label')for key, group in grouped: group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])pyplot.show() 过滤某些行和值pandas如何去掉、过滤数据集中的某些值或者某些行？ 读取csv，做group利用pandas进行数据分组及可视化 pandas聚合和分组运算——GroupBy技术(1) 123456789101112131415161718192021import pandas as pdimport matplotlib.pyplot as plt# 把数据划分到自定义的区间中def cla(n,lim): return'[%.f,%.f)'%(lim*(n//lim),lim*(n//lim)+lim) # map function# 默认第一行是标题，从第二行开始是数据。sep是分隔符df = pd.read_csv('/home/david/iaudience-plan-statistics.csv', sep=',')# 设置某列的数据类型df['precent'] = df['precent'].astype('float64')# 对planid做group，group后对precent做sumgrouped = df['precent'].groupby(df['planid']).sum()c = pd.DataFrame(grouped)# 用c.precent或c['precent']都可以addone = pd.Series([cla(s,1) for s in c.precent])c['addone'] = addonegroups3 = c.groupby(['addone']).count()groups3['precent'].plot('bar')plt.show() 参考10 Minutes to pandas pandas中Timestamp类用法讲解]]></content>
      <categories>
        <category>PYTHON</category>
      </categories>
      <tags>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop笔记]]></title>
    <url>%2F2017%2F07%2F12%2Fhadoop-spark%2Fhadoop%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[查看namenode在集群的每个节点上都有配置文件， vim /etc/hadoop/conf/hdfs-site.xml 1234567&lt;property&gt; &lt;name&gt;dfs.ha.namenodes.iclick&lt;/name&gt; &lt;value&gt;srv-buzz-cloudpmnn1.buzz.com,srv-buzz-cloudpmnn2.buzz.com&lt;/value&gt;&lt;/property&gt; 常用IO操作123456789public static void testIOUtils() throws IOException &#123;Configuration conf = new Configuration();FileSystem fs = FileSystem.get(conf);Path p = new Path(&quot;/test/in/point&quot;);FSDataInputStream fdis = fs.open(p);IOUtils.copyBytes(fdis, System.out, conf,false);IOUtils.closeStream(fdis);fs.close();&#125;]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[本地操作服务器hadoop]]></title>
    <url>%2F2017%2F07%2F11%2Fhadoop-spark%2F%E6%9C%AC%E5%9C%B0%E6%93%8D%E4%BD%9C%E6%9C%8D%E5%8A%A1%E5%99%A8hadoop%2F</url>
    <content type="text"><![CDATA[配置1、添加hadoop的必备jar包 123456789101112&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.5.0-cdh5.3.2&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.5.0-cdh5.3.2&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; 2、如果只要读取数据，直接配namenode地址12val conf = new Configurationconf.set("fs.defaultFS","hdfs://10.11.40.207:9000/") 2 读取数据12345678910111213141516171819import org.apache.hadoop.fs.Pathimport org.apache.hadoop.conf.Configurationimport org.apache.hadoop.fs.FileSystemimport java.io.InputStreamReaderimport java.io.BufferedReaderval fs = FileSystem.get(conf)val path = new Path("/user/david/testdir/a.txt")val in = fs.open(path)val buff = new BufferedReader(new InputStreamReader(in))var str = buff.readLinewhile (str != null) &#123; println(str) str = buff.readLine&#125;buff.closein.closefs.close]]></content>
      <categories>
        <category>hadoop-spark</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JPA连接池问题]]></title>
    <url>%2F2016%2F05%2F21%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2FJAVA%2FJPA%E8%BF%9E%E6%8E%A5%E6%B1%A0%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[用hibernate的JPA框架连接MySql并提供API接口，往往过一夜就会报错 1org.hibernate.exception.JDBCConnectionException: Unable to acquire JDBC Connection 根据stackflow上的解释，因为有太多connection，导致无法建立新的connection。因此需要设置连接池 连接池的作用 1.JDBC数据库连接池的必要性 在使用开发基于数据库的web程序时，传统的模式基本是按以下步骤： 在主程序（如servlet、beans）中建立数据库连接。 进行sql操作 断开数据库连接。 这种模式开发，存在的问题: 普通的JDBC数据库连接使用 DriverManager 来获取，每次向数据库建立连接的时候都要将 Connection 加载到内存中，再验证用户名和密码(得花费0.05s～1s的时间)。需要数据库连接的时候，就向数据库要求一个，执行完成后再断开连接。这样的方式将会消耗大量的资源和时间。数据库的连接资源并没有得到很好的重复利用.若同时有几百人甚至几千人在线，频繁的进行数据库连接操作将占用很多的系统资源，严重的甚至会造成服务器的崩溃。 对于每一次数据库连接，使用完后都得断开。否则，如果程序出现异常而未能关闭，将会导致数据库系统中的内存泄漏，最终将导致重启数据库。 这种开发不能控制被创建的连接对象数，系统资源会被毫无顾及的分配出去，如连接过多，也可能导致内存泄漏，服务器崩溃. 2.数据库连接池（connection pool） 为解决传统开发中的数据库连接问题，可以采用数据库连接池技术。 数据库连接池的基本思想就是为数据库连接建立一个“缓冲池”。预先在缓冲池中放入一定数量的连接，当需要建立数据库连接时，只需从“缓冲池”中取出一个，使用完毕之后再放回去。 数据库连接池负责分配、管理和释放数据库连接，它允许应用程序重复使用一个现有的数据库连接，而不是重新建立一个。 数据库连接池在初始化时将创建一定数量的数据库连接放到连接池中，这些数据库连接的数量是由最小数据库连接数来设定的。无论这些数据库连接是否被使用，连接池都将一直保证至少拥有这么多的连接数量。连接池的最大数据库连接数量限定了这个连接池能占有的最大连接数，当应用程序向连接池请求的连接数超过最大连接数量时，这些请求将被加入到等待队列中。 3.数据库连接池技术的优点 (1)资源重用：由于数据库连接得以重用，避免了频繁创建，释放连接引起的大量性能开销。在减少系统消耗的基础上，另一方面也增加了系统运行环境的平稳性。 (2)更快的系统反应速度:数据库连接池在初始化过程中，往往已经创建了若干数据库连接置于连接池中备用。此时连接的初始化工作均已完成。对于业务请求处理而言，直接利用现有可用连接避免了数据库连接初始化和释放过程的时间开销，从而减少了系统的响应时间 (3)新的资源分配手段对于多应用共享同一数据库的系统而言，可在应用层通过数据库连接池的配置实现某一应用最大可用数据库连接数的限制避免某一应用独占所有的数据库资源. (4)统一的连接管理，避免数据库连接泄露在较为完善的数据库连接池实现中，可根据预先的占用超时设定，强制回收被占用连接，从而避免了常规数据库连接操作中可能出现的资源泄露。 4.c3p0数据库连接池 设置的方法参见 How to configure the C3P0 connection pool in Hibernate 配置后的persistence.xml 12345678910111213141516171819202122232425&lt;properties&gt; &lt;property name="hibernate.dialect" value="org.hibernate.dialect.MySQL5Dialect" /&gt; &lt;property name="hibernate.connection.driver_class" value="com.mysql.jdbc.Driver" /&gt; &lt;property name="hibernate.connection.username" value="usr_dba" /&gt; &lt;property name="hibernate.connection.password" value="4rfv%TGB^YHN" /&gt; &lt;property name="hibernate.connection.url" value="jdbc:mysql://10.11.10.44:3306/symphony" /&gt; &lt;property name="hibernate.max_fetch_depth" value="3" /&gt; &lt;property name="hibernate.hbm2ddl.auto" value="update" /&gt; &lt;!-- 下面开始c3p0的配置 --&gt; &lt;property name="hibernate.connection.provider_class" value="org.hibernate.service.jdbc.connections.internal.C3P0ConnectionProvider"/&gt; &lt;!-- 最小连接数 --&gt; &lt;property name="hibernate.c3p0.min_size" value="1"/&gt; &lt;!-- 最大连接数 --&gt; &lt;property name="hibernate.c3p0.max_size" value="100"/&gt; &lt;!-- 获得连接的超时时间,如果超过这个时间,会抛出异常，单位（毫秒） --&gt; &lt;property name="hibernate.c3p0.timeout" value="10"/&gt; &lt;!-- 指定连接池里最大缓存多少个Statement对象 --&gt; &lt;property name="hibernate.c3p0.max_statements" value="100"/&gt; &lt;!-- 每隔3000秒检查连接池里的空闲连接 ，单位是（秒）--&gt; &lt;property name="hibernate.c3p0.idle_test_period" value="3000"/&gt; &lt;!-- 当连接池里面的连接用完的时候，C3P0自动一次性获取多少个新的连接 --&gt; &lt;property name="hibernate.c3p0.acquire_increment" value="5"/&gt; &lt;!-- 每次都验证连接是否可用 --&gt; &lt;property name="hibernate.c3p0.validate" value="true"/&gt;&lt;/properties&gt;]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>JPA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[待解决的问题]]></title>
    <url>%2F2000%2F01%2F01%2F%E6%80%BB%E7%BB%93%E4%B8%8E%E6%80%9D%E8%80%83%2F%E5%BE%85%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[2018年02月26日 如何对英文按词组分词？ url提取关键词并插入到ES的时候，对于英文，不能提取短语，所以在ES查询的时候不方便，比如说hong kong，提取关键词的时候是hong和kong。 然后elastic search 查询的时候，如何能按照英文的短语来查 GBDT回归和分类树的区别]]></content>
      <categories>
        <category>总结与思考</category>
      </categories>
  </entry>
</search>
