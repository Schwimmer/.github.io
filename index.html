<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Schwimmer&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Schwimmer&#39;s Blog">
<meta property="og:url" content="http://Schwimmer.github.io/index.html">
<meta property="og:site_name" content="Schwimmer&#39;s Blog">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Schwimmer&#39;s Blog">
  
    <link rel="alternate" href="/atom.xml" title="Schwimmer&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Schwimmer&#39;s Blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://Schwimmer.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-决策树" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/07/12/决策树/" class="article-date">
  <time datetime="2017-07-12T09:30:50.979Z" itemprop="datePublished">2017-07-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><h1 id="1-决策树"><a href="#1-决策树" class="headerlink" title="1 决策树"></a>1 决策树</h1><p>设数据集是D。</p>
<h2 id="1-1-关键步骤"><a href="#1-1-关键步骤" class="headerlink" title="1.1 关键步骤"></a>1.1 关键步骤</h2><p>创建决策树分支的createBranch()伪代码函数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">检查数据集中每个子项是否属于同一个分类：</div><div class="line">	IF YES return 类标签；</div><div class="line">	ELSE </div><div class="line">		寻找划分数据集的最好特征；</div><div class="line">		划分数据集；</div><div class="line">		创建分支节点；</div><div class="line">			for 每个划分的子集</div><div class="line">				递归调用createBranch()并增加返回结果到分支节点中</div><div class="line">        return 分支节点</div></pre></td></tr></table></figure>
<p>对label的分类计算熵</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcEnt</span><span class="params">(dataSet)</span>:</span></div><div class="line">	labelNum = len(dataSet)</div><div class="line">    ent = <span class="number">0.0</span></div><div class="line">	<span class="comment">#定义字典存放每个类别的count统计</span></div><div class="line">	labelCounts = &#123;&#125;</div><div class="line">    <span class="comment">#统计每个label的个数</span></div><div class="line">	<span class="keyword">for</span> featureVec <span class="keyword">in</span> dataSet:</div><div class="line">        <span class="comment">#最后一列是label</span></div><div class="line">		label = featureVec[<span class="number">-1</span>]</div><div class="line">        <span class="keyword">if</span> label <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys(): labelCounts[label] = <span class="number">0</span></div><div class="line">        labelCounts[label] += <span class="number">1</span></div><div class="line">    <span class="comment">#计算概率以及熵</span></div><div class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> labelCounts:</div><div class="line">        prob = float(labelCounts[key]) / labelNum</div><div class="line">        ent -= prob * log(<span class="number">2</span>, prob)</div><div class="line">    <span class="keyword">return</span> ent</div></pre></td></tr></table></figure>
<p>对数据集进行划分</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">splitDataSet</span><span class="params">(dataSet, axis, value)</span>:</span></div><div class="line">    subDataSet = []</div><div class="line">    <span class="keyword">for</span> featureVec <span class="keyword">in</span> dataSet:</div><div class="line">        <span class="keyword">if</span> featureVec[axis] == value:</div><div class="line">            reducedFeatVec = featureVec[:axis]</div><div class="line">            reducedFeatVec.extend(featureVec[axis+<span class="number">1</span>:])</div><div class="line">    		resDataSet.append(reducedFeatVec)</div><div class="line">    <span class="keyword">return</span> subDataSet</div></pre></td></tr></table></figure>
<p>选出最好的数据集划分方式</p>
<p><strong>信息增益</strong></p>
<p><strong>熵</strong>的定义是<br>$$<br>H(X) = -\sum_{i=1}^{n}p(x_i)log_2p(x_i)<br>$$<br>n是类别总数。</p>
<p><strong>条件熵</strong>$H(Y|X)$表示在已知X的条件下Y的不确定性，定义为给定X时Y的条件概率分布的熵对X的期望<br>$$<br>H(Y|X)=\sum_{i=1}^np_iH(Y|X=x_i)<br>$$<br>对于训练集D以及其中的特征A，熵就是<br>$$<br>H(D) = -\sum_{k=1}^K \frac {|C_k|}{|D|} log_2\frac{|C_k|}{|D|}<br>$$<br>其中，K是标签分类的数量，$C_k$是每个分类的样本数</p>
<p>条件熵就是<br>$$<br>\begin{aligned}<br>H(D|A) &amp;=\sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i) \\<br>&amp;=\sum_{i=1}^n\frac{|D_i|}{|D|}(-\sum_{k=1}^K \frac {|D_{ik}|}{|D_i|} log_2\frac{|D_{ik}|}{|D_i|})<br>\end{aligned}<br>$$<br>其中，n是特征A的类别总数，$D_i$是特征A的每种类别的数量。</p>
<p>信息增益就是两者之差<br>$$<br>g(D,A)=H(D)-H(D|A)<br>$$<br>信息增益也称为<strong>互信息</strong>。</p>
<p>找出信息增益最大的来划分数据集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseBestFeature</span><span class="params">(dataSet)</span>:</span></div><div class="line">	<span class="comment">#feature数量，最后一列是label</span></div><div class="line">	numFeature = len(dataSet[<span class="number">0</span>]<span class="number">-1</span>)</div><div class="line">    bestInfoGain = <span class="number">0.0</span></div><div class="line">    bestFeature = <span class="number">-1</span></div><div class="line">	<span class="comment">#先计算熵</span></div><div class="line">	baseEntropy = calcEnt(dataSet)</div><div class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(numFeature):</div><div class="line">		<span class="comment">#首先需要知道该特征有几个值</span></div><div class="line">		uniqueValue = set([sample[i] <span class="keyword">for</span> sample <span class="keyword">in</span> dataSet]) <span class="comment">#用set去重是最快方法</span></div><div class="line">		newEntropy = <span class="number">0.0</span></div><div class="line">        <span class="comment">#对于每个特征，计算条件熵</span></div><div class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> uniqueValue:</div><div class="line">            <span class="comment">#用这个特征划分数据集</span></div><div class="line">            subDataSet = splitDataSet(dataSet, i, value)</div><div class="line">            newEntropy += calcEnt(subDataSet)</div><div class="line">        <span class="comment">#计算信息增益</span></div><div class="line">        infoGain = baseEntropy-newEntropy</div><div class="line">        <span class="keyword">if</span> infoGain &gt; bestInfoGain:</div><div class="line">            bestInfoGain = infoGain</div><div class="line">            bestFeature = i</div><div class="line"><span class="keyword">return</span> bestFeature</div></pre></td></tr></table></figure>
<p>如果所有特征都处理过了，但是类标签依然不是唯一的，用投票决定</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">majorityCnt</span><span class="params">(classList)</span>:</span></div><div class="line">	classCount=&#123;&#125;</div><div class="line">    <span class="keyword">for</span> vote <span class="keyword">in</span> classList:</div><div class="line">        <span class="keyword">if</span> vote <span class="keyword">not</span> <span class="keyword">in</span> classCount.keys() classCount[vote] = <span class="number">0</span></div><div class="line">        classCount[vote] += <span class="number">1</span></div><div class="line">    sortedClassCount = sorted(classCount, key = operator.itemgetter(<span class="number">1</span>), reverse = <span class="keyword">True</span>)</div><div class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</div></pre></td></tr></table></figure>
<h2 id="1-2-ID3算法"><a href="#1-2-ID3算法" class="headerlink" title="1.2 ID3算法"></a>1.2 ID3算法</h2><p>与上面的步骤类似。但是ID3只有树的生成，容易过拟合。</p>
<h2 id="1-3-C4-5算法"><a href="#1-3-C4-5算法" class="headerlink" title="1.3 C4.5算法"></a>1.3 C4.5算法</h2><p>与ID3相比，C4.5用信息增益比来选择特征。</p>
<p><strong>信息增益比</strong></p>
<p>在面对类别比较少的离散数据时，两者差不多。但如果面对连续的数据（如体重、身高、年龄、距离等），或者每列数据没有明显的类别之分（最极端的例子的该列所有数据都独一无二）。</p>
<p>那么根据信息增益公式，$H(D)$不变，当数据独一无二时，<br>$$<br>H(D|A)=\sum_{i=1}^n \frac {1}{n}H(D_i)<br>$$<br>这样$H(D|A)$最小，程序会倾向于这种划分，导致划分效果差。</p>
<p>信息增益比的公式为<br>$$<br>g_R(D,A)=\frac {g(D,A)}{H(D)}<br>$$<br>可以理解成对分支数目的惩罚项。</p>
<h2 id="1-4-决策树的剪枝"><a href="#1-4-决策树的剪枝" class="headerlink" title="1.4 决策树的剪枝"></a>1.4 决策树的剪枝</h2><p>剪枝是为了解决过拟合。通过极小化决策树整体的损失函数来实现。设树T的叶结点个数为$|T|$，t是T的叶结点，该叶结点有$N_t$个样本点，其中k类的样本点有$N_{tk}$个，则损失函数定义为<br>$$<br>C_{\alpha}(T) = \sum_{t=1}^T N_tH_t(T) + \alpha|T|<br>$$<br>由于<br>$$<br>H_t(T) =  - \sum_{k=1}^K \frac {N_{tk}}{N_t} log_2\frac {N_{tk}}{N_t}<br>$$<br>则令<br>$$<br>C(T) = - \sum_{t=1}^T\sum_{k=1}^KN_{tk}log_2\frac {N_{tk}}{N_t}<br>$$<br>于是<br>$$<br>C_\alpha(T) = C(T) +\alpha|T|<br>$$<br>这里，$C(T)$表示训练数据的预测误差，$|T|$表示模型复杂度，$\alpha$控制两者影响，较大时选择较简单的树，反之亦然，等于0时就不考虑模型复杂度。</p>
<p>参考文献：</p>
<p>[1] 统计学习方法</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://Schwimmer.github.io/2017/07/12/决策树/" data-id="cj50tbv6v0002sxsk6qs4hubm" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>





  
    <article id="post-aa" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/07/12/aa/" class="article-date">
  <time datetime="2017-07-12T03:49:53.000Z" itemprop="datePublished">2017-07-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/07/12/aa/">aa</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://Schwimmer.github.io/2017/07/12/aa/" data-id="cj50tbv680000sxsksijlcfu4" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>





  
    <article id="post-线性回归" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/07/12/线性回归/" class="article-date">
  <time datetime="2017-07-12T03:49:53.000Z" itemprop="datePublished">2017-07-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/07/12/线性回归/">线性回归</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>首先定义线性回归的函数方程</p>
<p>$$<br>h=\theta_0+\theta_1x_1+\theta_2x_2=\sum_{i=1}^n\theta_ix_i=\theta^Tx<br>$$</p>
<p>这里的$\theta$是<strong>权重</strong>，等式的最右边中$\theta$和$x$都是向量，n是输入变量的个数（不包括$x_0$）。</p>
<p>为了使$h(x)$接近$y$，定义损失函数为<br>$$<br>J(\theta)=\frac 1 2 \sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2<br>$$<br>其中m是样本个数。找到使公式最小的$\theta$的取值。</p>
<h4 id="【插入知识点】-损失函数"><a href="#【插入知识点】-损失函数" class="headerlink" title="【插入知识点】- 损失函数"></a>【插入知识点】- <a href="http://www.360doc.com/content/16/0327/06/1317564_545544873.shtml" target="_blank" rel="external">损失函数</a></h4><p>损失函数（loss function)是估计预测值和实际值的不一致程度。通常使用$L(Y, f(x))$表示。损失函数是 <strong>经验风险函数</strong> 的核心部分，也是 <strong>结构风险函数</strong>重要组成部分。结构风险函数包括<strong>经验风险项</strong>和<strong>正则项</strong>：<br>$$<br>\theta^<em>=arg\min \limits_{\theta} \frac 1 N \sum_{i=1}^NL(y_i,f(x_i,\theta))+\lambda\Phi(\theta)<br>$$<br>其中，前面的均值函数表示经验风险函数，L代表损失函数，后面的$\Phi$是正则化项。整个式子的含义是：<em>*找到使目标函数最小时的$\theta$值</em></em>。常用的损失函数有：</p>
<h5 id="1、平方损失函数（线性回归）"><a href="#1、平方损失函数（线性回归）" class="headerlink" title="1、平方损失函数（线性回归）"></a>1、平方损失函数（线性回归）</h5><p>最小二乘法（Ordinary Least Squares）是常用的一种平方损失函数，最小二乘的基本原理是：最优拟合直线应该是使各点到回归直线的距离和最小的直线，即平方和最小。<br>平方损失的标准形式：<br>$$<br>L(Y, f(X)) = (Y-f(X))^2<br>$$<br>当样本个数为n时，损失函数变为：<br>$$<br>L(y,f(x)) = \sum_{i=1}^n(y-f(x_i))^2<br>$$<br>其中$y-f(x)$表示残差，整个式子的目的就是最小化<strong>残差的平方和</strong>（residual sum of squares，RSS）。</p>
<p>在实际中，通常用均方误差（MSE）作为衡量指标：<br>$$<br>MSE = \frac{1}{n} \sum_{i=1} ^{n} (\tilde{Y_i} - Y_i )^2<br>$$</p>
<h5 id="2、对数损失函数（逻辑回归）"><a href="#2、对数损失函数（逻辑回归）" class="headerlink" title="2、对数损失函数（逻辑回归）"></a>2、对数损失函数（逻辑回归）</h5><p>逻辑回归假设样本服从<strong>伯努利分布（0-1分布）</strong>，因此不用平方损失函数。</p>
<p>线性回归是以<strong>高斯分布</strong>（正态分布）为误差分析模型，逻辑回归采用的是<strong>伯努利分布</strong>分析误差。而高斯分布、伯努利分布、贝塔分布、迪特里特分布，都属于指数分布。</p>
<p><strong>【插入知识点】</strong> - 离散型随机变量分布</p>
<p>1、0-1分布 随机变量X只能取0和1两个值，分布律是<br>$$<br>P\{X=k\} = p^k(1-p)^k, k=0,1, (0&lt;P&lt;1)<br>$$</p>
<p>2、伯努利分布、二项分布</p>
<p>实验E只有两个结果，$A$和$\bar A$ 。就称为伯努利实验。n重伯努利实验必须要相互独立（放回抽样，大数据下可以不放回抽样）。</p>
<p>以随机变量X表示n重伯努利实验中事件A发生的次数，求X的分布律。</p>
<p>X所有可能的取值为0,1,2,…n。由于每次实验都是独立的，因此事件A在指定的k次实验中发生，在其他n-k次实验中不发生的概率为：<br>$$<br>p^k(1-p)^{n-k}<br>$$<br>这种指定的方式一共有$C_n^k$种（概率的计算后，因为取样有很多种不同的取法，所以还要再乘以$C_n^k$ ）<br>$$<br>20 *0.2^10.8^{19} = 0.058<br>$$</p>
<p>$$<br>19 <em> 0.2^2 </em> 0.8^{18} =<br>$$</p>
<h1 id="1、Gradient-Descent"><a href="#1、Gradient-Descent" class="headerlink" title="1、Gradient Descent"></a>1、Gradient Descent</h1><p>我们要选择$\theta$来最小化$J(\theta)$，因此给定一个$\theta$的初值，通过梯度下降来优化（1-1）。<br>$$<br>\theta_j:=\theta_j-\alpha\frac {\partial} {\partial\theta_j} J(\theta) \tag{1-1}<br>$$</p>
<p>每次会更新所有的$j=0,…,n$。其中，$\alpha$是学习率。</p>
<p>为了实现算法，我们需要解决偏导的问题。假设我们只有一个训练样本$(x,y)$ ，我们可以先忽略$J(\theta)$的累加。推导过程为：</p>
<p>$$<br>\frac {\partial} {\partial\theta_j} J(\theta) = \frac {\partial} {\partial\theta_j} \frac 1 2 (h_\theta(x)-y)^2 \\<br>= (h_\theta(x)-y) \cdot \frac {\partial} {\partial\theta_j}  (h_\theta(x)-y) \\<br>= (h_\theta(x)-y) \cdot \frac {\partial} {\partial\theta_j}  (\sum_{i=0}^n \theta_ix_i-y) \\<br>= (h_\theta(x)-y) \cdot x_j<br>$$</p>
<p>这里的i是指这一个训练样本的每个分量。</p>
<p>假设样本总数为m，<strong>批量梯度下降</strong>是：<br>$$<br>Repeat until convergence \{ \\<br>\theta_j := \theta_j - \alpha \sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})\cdot x_j^{(i)} \tag{1-2}   (for eveny j) \\<br>\}<br>$$<br>而<strong>随机梯度下降</strong>是：<br>$$<br>Repeat until convergence \{ \\<br>      for i=1 to m, \{ \\<br>          \theta_j := \theta_j - \alpha (h_\theta(x^{(i)})-y^{(i)})\cdot x_j^{(i)} \tag{1-3}   (for eveny j) \\<br>\}<br>$$<br>两者的区别是：</p>
<p>前者，每次更新$\theta$都需要遍历一次整个样本集合；而后者，在遍历样本集合的时候，每个样本都能改变$\theta$ ，有更快的收敛速度 。</p>
<blockquote>
<p>使用梯度下降要注意feature scaling（数据规范化），能减少寻找最优解的时间。</p>
</blockquote>
<p> <img src="pic\feature scaling.jpg" alt="feature scaling"></p>
<blockquote>
<p>$\alpha$的选择</p>
<p>1、对于足够小的$\alpha$， $J(\theta)$会单调减少</p>
<p><img src="http://images.cnitblog.com/blog/575572/201401/261254436426608.png" alt="img"></p>
<p>2、如果$\alpha$过小，梯度下降会很慢；</p>
<p>3、如果$\alpha$很大，$J(\theta)$可能会不收敛。</p>
<p><img src="http://images.cnitblog.com/blog/575572/201401/261257015324349.png" alt="img"></p>
<p>如何选择α，如下:</p>
<p>…, 0.001, 0.01, 0.1, 1, …, 或者 …, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, ….</p>
</blockquote>
<h1 id="2、Normal-Equation"><a href="#2、Normal-Equation" class="headerlink" title="2、Normal Equation"></a>2、Normal Equation</h1><h2 id="2-1-矩阵的求导"><a href="#2-1-矩阵的求导" class="headerlink" title="2.1 矩阵的求导"></a>2.1 矩阵的求导</h2><p>梯度下降是最小化$J$的一种途径。</p>
<p>矩阵求导。对于一个函数$f: \mathbb{R}^{m \times n} \mapsto \mathbb{R}$，我们定义函数$f$关于$A$的梯度$\triangledown_Af(A)$为：<br>$$<br>\triangledown_Af(A)= \begin{bmatrix}<br>\frac {\partial f} {\partial A_{11}}       &amp; \cdots &amp; \frac {\partial f} {\partial A_{1n}}      \\<br> \vdots &amp; \ddots &amp; \vdots \\<br>  \frac {\partial f} {\partial A_{m1}}      &amp; \cdots &amp; \frac {\partial f} {\partial A_{mn}}      \\<br>\end{bmatrix}<br>$$<br>假设$A=\begin{bmatrix}A_{11} &amp; A_{12}\ A_{21} &amp; A_{22}\end{bmatrix}$ ，函数$f: \mathbb{R}^{2 \times 2} \mapsto \mathbb{R}$ 为<br>$$<br>f(A)=\frac{3}{2}A_{11}+5A_{12}^2 + A_{21}A_{22}<br>$$<br>则<br>$$<br>\triangledown_Af(A)= \begin{bmatrix}<br>\frac {3} {2}       &amp; 10A_{12}      \\</p>
<p>  A_{22}      &amp; A_{21}      \\<br>\end{bmatrix}<br>$$<br>对于一个n<em>n的矩阵，我们引入<em>*trace</em></em>操作符，$trA$定义为对角线之和<br>$$<br>trA = \sum_{i=1}^nA_{ii}<br>$$</p>
<h2 id="2-2-最小均方的推导"><a href="#2-2-最小均方的推导" class="headerlink" title="2.2 最小均方的推导"></a>2.2 最小均方的推导</h2><p>得出<br>$$<br>\theta = (X^TX)^{-1}X^Ty<br>$$</p>
<blockquote>
<p>优势：不管特征的scale。因为是纯粹的矩阵算法，没有迭代。</p>
<p>劣势：需要大量矩阵计算。NG建议维数&lt;10000时用normal。</p>
</blockquote>
<p>其中，X可逆的条件是X的列向量不相关。所以在特征选取时不要选线性相关的特征。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://Schwimmer.github.io/2017/07/12/线性回归/" data-id="cj50tbv720003sxsktgb011r6" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>





  
    <article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/07/12/hello-world/" class="article-date">
  <time datetime="2017-07-12T03:04:38.801Z" itemprop="datePublished">2017-07-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/07/12/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://Schwimmer.github.io/2017/07/12/hello-world/" data-id="cj50tbv6m0001sxskhsclmi8s" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>





  

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">July 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/07/12/决策树/">(no title)</a>
          </li>
        
          <li>
            <a href="/2017/07/12/aa/">aa</a>
          </li>
        
          <li>
            <a href="/2017/07/12/线性回归/">线性回归</a>
          </li>
        
          <li>
            <a href="/2017/07/12/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 Schwimmer<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>