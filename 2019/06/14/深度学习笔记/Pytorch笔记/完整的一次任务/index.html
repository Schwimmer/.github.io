<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">






















<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">

<link rel="stylesheet" href="/css/main.css?v=7.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.2.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.2.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '7.2.0',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="《PyTorch模型训练实用教程》 代码在：/Users/david/david/code/deep_learning/PyTorch_Tutorial_yuting 预先安装 requirement.txt 运行时报错 12OMP: Error #15: Initializing libiomp5.dylib, but found libomp.dylib already initialized">
<meta property="og:type" content="article">
<meta property="og:title" content="Schwimmer&#39;s Blog">
<meta property="og:url" content="https://schwimmer.github.io/2019/06/14/深度学习笔记/Pytorch笔记/完整的一次任务/index.html">
<meta property="og:site_name" content="Schwimmer&#39;s Blog">
<meta property="og:description" content="《PyTorch模型训练实用教程》 代码在：/Users/david/david/code/deep_learning/PyTorch_Tutorial_yuting 预先安装 requirement.txt 运行时报错 12OMP: Error #15: Initializing libiomp5.dylib, but found libomp.dylib already initialized">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://schwimmer.github.io/2019/06/14/深度学习笔记/Pytorch笔记/完整的一次任务/pic/image-20190616085944731.png">
<meta property="og:updated_time" content="2019-06-16T01:16:07.416Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Schwimmer&#39;s Blog">
<meta name="twitter:description" content="《PyTorch模型训练实用教程》 代码在：/Users/david/david/code/deep_learning/PyTorch_Tutorial_yuting 预先安装 requirement.txt 运行时报错 12OMP: Error #15: Initializing libiomp5.dylib, but found libomp.dylib already initialized">
<meta name="twitter:image" content="https://schwimmer.github.io/2019/06/14/深度学习笔记/Pytorch笔记/完整的一次任务/pic/image-20190616085944731.png">





  
  
  <link rel="canonical" href="https://schwimmer.github.io/2019/06/14/深度学习笔记/Pytorch笔记/完整的一次任务/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title> | Schwimmer's Blog</title>
  




  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-143240576-1"></script>
  <script>
    var host = window.location.hostname;
    if (host !== "localhost" || !true) {
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-143240576-1');
    }
  </script>









  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Schwimmer's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
        </li>
      
    </ul>
  

  
    

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/14/深度学习笔记/Pytorch笔记/完整的一次任务/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">

              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-14 10:59:11" itemprop="dateCreated datePublished" datetime="2019-06-14T10:59:11+08:00">2019-06-14</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-16 09:16:07" itemprop="dateModified" datetime="2019-06-16T09:16:07+08:00">2019-06-16</time>
              </span>
            
          

          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/14/深度学习笔记/Pytorch笔记/完整的一次任务/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/14/深度学习笔记/Pytorch笔记/完整的一次任务/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
                 阅读次数： 
                <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
              </span>
            </span>
          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>《PyTorch模型训练实用教程》</p>
<p>代码在：<code>/Users/david/david/code/deep_learning/PyTorch_Tutorial_yuting</code></p>
<p>预先安装</p>
<p>requirement.txt</p>
<p>运行时报错</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">OMP: Error #15: Initializing libiomp5.dylib, but found libomp.dylib already initialized.</span><br><span class="line">OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://www.intel.com/software/products/support/.</span><br></pre></td></tr></table></figure>
<p>解决方案</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">os.environ[&quot;KMP_DUPLICATE_LIB_OK&quot;]=&quot;TRUE&quot;</span><br></pre></td></tr></table></figure>
<h1 id="第一章-数据"><a href="#第一章-数据" class="headerlink" title="第一章 数据"></a>第一章 数据</h1><h2 id="1-1-Cifar10-转-png"><a href="#1-1-Cifar10-转-png" class="headerlink" title="1.1 Cifar10 转 png"></a>1.1 Cifar10 转 png</h2><p>cifar-10 的测试集，10000张图片。</p>
<p>官网:<a href="http://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="noopener">http://www.cs.toronto.edu/~kriz/cifar.html</a></p>
<h3 id="读取并保存为图片"><a href="#读取并保存为图片" class="headerlink" title="读取并保存为图片"></a>读取并保存为图片</h3><p><strong>图片是序列化的，不能直接读取。</strong></p>
<p>运行代码:Code/1_data_prepare/1_1_cifar10_to_png.py</p>
<p>可在文件夹 Data/cifar-10-png/raw_test/下看到 0-9 个文件夹，对应 9 个类别。</p>
<p>将 测试集中的 10000 张图片解压出来，作为原始图片，将从这 10000 张图片中划分出训练集 (train)，验证集(valid)，测试集(test)。 </p>
<h2 id="1-2-训练集、验证集、测试集的划分"><a href="#1-2-训练集、验证集、测试集的划分" class="headerlink" title="1.2 训练集、验证集、测试集的划分"></a>1.2 训练集、验证集、测试集的划分</h2><p>把原始数据按 8:1:1 的比例划分为训练集(train set)、验证集(valid/dev set)和测试集(test set) </p>
<p>运行代码：Code/1_data_prepare/1_2_split_dataset.py</p>
<p>数据划分完毕，下一步是制作存放有图片路径及其标签的 txt。pytorch会根据txt的信息寻找图片，并读取图片数据和标签数据。</p>
<h2 id="1-3-Pytorch读图片数据集"><a href="#1-3-Pytorch读图片数据集" class="headerlink" title="1.3 Pytorch读图片数据集"></a>1.3 Pytorch读图片数据集</h2><h3 id="Dataset类"><a href="#Dataset类" class="headerlink" title="Dataset类"></a>Dataset类</h3><p>PyTorch 读取<strong>图片</strong>，主要是通过 Dataset 类，所以先简单了解一下 Dataset 类。抽象类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dataset</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""An abstract class representing a Dataset.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    All other datasets should subclass it. All subclasses should override</span></span><br><span class="line"><span class="string">    ``__len__``, that provides the size of the dataset, and ``__getitem__``,</span></span><br><span class="line"><span class="string">    supporting integer indexing in range from 0 to len(self) exclusive.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">		</span><br><span class="line">    <span class="comment"># 接收一个 index，然后返回图片数据和标签</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__add__</span><span class="params">(self, other)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> ConcatDataset([self, other])</span><br></pre></td></tr></table></figure>
<p>需要将list放到txt里面，读取txt，获取list。那么读取自己数据的基本流程就是:</p>
<ol>
<li>制作存储了图片的路径和标签信息的 txt</li>
<li>将这些信息转化为 list，该 list 每一个元素对应一个样本</li>
<li>通过 getitem 函数，读取数据和标签，并返回数据和标签</li>
</ol>
<p>在训练代码里是感觉不到这些操作的，只会看到通过 DataLoader 就可以获取一个batch 的数据，其实触发去读取图片这些操作的是 DataLoader 里的<strong>iter</strong>(self)，后面会详细讲解读取过程。在本小节，主要讲 Dataset 子类。</p>
<p>要让 PyTorch 能读取自己的数据集，只需要两步:</p>
<ol>
<li>制作图片数据的索引</li>
<li><strong>构建 Dataset 子类</strong></li>
</ol>
<h3 id="1、制作图片索引"><a href="#1、制作图片索引" class="headerlink" title="1、制作图片索引"></a>1、制作图片索引</h3><p>就是获取图片路径和标签，保存到txt。</p>
<p>运行代码 Code/1_data_prepare/1_3_generate_txt.py</p>
<h3 id="2、构建Dataset子类"><a href="#2、构建Dataset子类" class="headerlink" title="2、构建Dataset子类"></a>2、构建Dataset子类</h3><p>构建了MyDataset类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">  	<span class="comment"># 初始化中，从txt读到imgs对象</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, txt_path, transform = None, target_transform = None)</span>:</span></span><br><span class="line">        fh = open(txt_path, <span class="string">'r'</span>)</span><br><span class="line">        imgs = []</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> fh:</span><br><span class="line">            line = line.rstrip()</span><br><span class="line">            words = line.split()</span><br><span class="line">            imgs.append((words[<span class="number">0</span>], int(words[<span class="number">1</span>])))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 最主要就是要生成这个list， 然后DataLoader中给index，通过getitem读取图片数据</span></span><br><span class="line">        self.imgs = imgs        </span><br><span class="line">        <span class="comment"># Compose 类型，里边有一个 list，list定义了对图像的各种操作，如减均值，除标准差，随机裁剪，仿射变换等。</span></span><br><span class="line">        self.transform = transform</span><br><span class="line">        self.target_transform = target_transform</span><br><span class="line"></span><br><span class="line">    <span class="comment"># python内建的魔法方法，访问索引就会触发</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        fn, label = self.imgs[index]</span><br><span class="line">        img = Image.open(fn).convert(<span class="string">'RGB'</span>)     <span class="comment"># 像素值 0~255，在transfrom.totensor会除以255，使像素值变成 0~1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            img = self.transform(img)   <span class="comment"># 在这里做transform，转为tensor等等</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> img, label</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.imgs)</span><br></pre></td></tr></table></figure>
<p>Pytorch对图片的处理不能生成新图，而是覆盖原图（不是真正的覆盖，就是对象赋值）。当采用randomcrop之类的随机操作时，每个 epoch 输入进来的图片几乎不会是一模一样的，这达到了样本多样性的功能。</p>
<p>当 Mydataset 构建好，剩下的操作就交给 DataLoder，在 DataLoder 中，会触发Mydataset 中的 getiterm 函数读取一张图片的数据和标签，并拼接成一个 batch 返回，作为模型真正的输入。</p>
<h2 id="1-4-DataLoder加载图片"><a href="#1-4-DataLoder加载图片" class="headerlink" title="1.4 DataLoder加载图片"></a>1.4 DataLoder加载图片</h2><p>getitem是在DataLoader中触发的，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建MyDataset实例</span></span><br><span class="line">train_data = MyDataset(txt_path=train_txt_path, transform=trainTransform)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建DataLoder</span></span><br><span class="line">train_loader = DataLoader(dataset=train_data, batch_size=train_bs, shuffle=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里的data就是__getitem__返回的img, label</span></span><br><span class="line"><span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">-&gt;     def __iter__(self):</span></span><br><span class="line"><span class="string">        return _DataLoaderIter(self)</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    class _DataLoaderIter(object):</span></span><br><span class="line"><span class="string">    	def __next__(self):</span></span><br><span class="line"><span class="string">    		# collate_fn (callable, optional): merges a list of samples to form a mini-batch.</span></span><br><span class="line"><span class="string">        # 这里调用__getitem__</span></span><br><span class="line"><span class="string">    		batch = self.collate_fn([self.dataset[i] for i in indices])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">  </span><br><span class="line">	<span class="comment"># 获取图片和标签</span></span><br><span class="line">  inputs, labels = data</span><br><span class="line">  inputs, labels = Variable(inputs), Variable(labels)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>图片是通过 Image.open()函数读取进来的，当涉及如下问题：</p>
<p>图片的通道顺序(RGB ? BGR ?)</p>
<p>图片是<code>w*h*c ? c*w*h ?</code></p>
<p>像素值范围[0-1] or [0-255] ?</p>
<p>就要查看 MyDataset()类中 <strong>getitem</strong>()下读取图片用的是什么方法</p>
</blockquote>
<h2 id="1-5-数据增强和数据标准化"><a href="#1-5-数据增强和数据标准化" class="headerlink" title="1.5 数据增强和数据标准化"></a>1.5 数据增强和数据标准化</h2><p>在 PyTorch 中，数据增强方法放在了 transforms.py 文件中。</p>
<p>这一节主要介绍transforms的操作。</p>
<p>1.6 transforms 的二十二个方法</p>
<h1 id="第二章-模型"><a href="#第二章-模型" class="headerlink" title="第二章 模型"></a>第二章 模型</h1><h2 id="2-1-模型的搭建"><a href="#2-1-模型的搭建" class="headerlink" title="2.1 模型的搭建"></a>2.1 模型的搭建</h2><h3 id="2-1-1-模型定义的三要素"><a href="#2-1-1-模型定义的三要素" class="headerlink" title="2.1.1 模型定义的三要素"></a>2.1.1 模型定义的三要素</h3><p>1）首先，必须继承 nn.Module 这个类，要让 PyTorch 知道这个类是一个 Module。</p>
<p>2）其次，在<strong>init</strong>(self)中设置好需要的“组件”(如 conv、pooling、Linear、BatchNorm等）。</p>
<p>3）最后，在 forward(self, x)中用定义好的“组件”进行组装，就像搭积木，把网络结构搭建</p>
<p>在/Code/main<em>training/main.py 中可以看到定义了一个类<code>class Net(nn.Module)</code>，集成了nn.Module，先看<em>_init</em></em>(self)函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">		<span class="comment"># 初始化</span></span><br><span class="line">    super(Net, self).__init__()</span><br><span class="line">    <span class="comment"># 定义了一系列组件</span></span><br><span class="line">    self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">    self.pool1 = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">    self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">3</span>)</span><br><span class="line">    self.pool2 = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">    self.conv3 = nn.Conv2d(<span class="number">16</span>, <span class="number">64</span>, <span class="number">3</span>)</span><br><span class="line">    self.pool3=nn.MaxPool2d(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">    self.fc1 = nn.Linear(<span class="number">64</span> * <span class="number">3</span> * <span class="number">3</span>, <span class="number">120</span>)</span><br><span class="line">    self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">    self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p>当这些组件定义好之后，就可以定义 forward()函数，用来搭建网络结构</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">		<span class="comment"># x作为模型的输入，x 经过 conv1，然后经过激活函数 relu，再经过 pool1 操作</span></span><br><span class="line">    x = self.pool1(F.relu(self.conv1(x)))</span><br><span class="line">    <span class="comment"># 再做一轮</span></span><br><span class="line">    x = self.pool2(F.relu(self.conv2(x)))</span><br><span class="line">    <span class="comment"># 再做一轮</span></span><br><span class="line">    x=self.pool3(F.relu(self.conv3(x)))</span><br><span class="line">    <span class="comment"># 将 x 进行 reshape，为了后面做为全连接层的输入</span></span><br><span class="line">    x = x.view(<span class="number">-1</span>, <span class="number">64</span> * <span class="number">3</span>* <span class="number">3</span>)</span><br><span class="line">    <span class="comment"># 先经过全连接层 fc，然后经过 relu</span></span><br><span class="line">    x = F.relu(self.fc1(x))</span><br><span class="line">    x = F.relu(self.fc2(x))</span><br><span class="line">    x = self.fc3(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="2-1-2-一个更复杂的模型"><a href="#2-1-2-一个更复杂的模型" class="headerlink" title="2.1.2 一个更复杂的模型"></a>2.1.2 一个更复杂的模型</h3><p>来看一个更复杂的模型，看Resnet网络的定义方法<a href="https://github.com/yuanlairuci110/pytorch-best-practice-master/blob/master/models/ResNet34.py" target="_blank" rel="noopener">https://github.com/yuanlairuci110/pytorch-best-practice-master/blob/master/models/ResNet34.py</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> .BasicModule <span class="keyword">import</span> BasicModule</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResidualBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    实现子module: Residual Block</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, inchannel, outchannel, stride=<span class="number">1</span>, shortcut=None)</span>:</span></span><br><span class="line">        super(ResidualBlock, self).__init__()</span><br><span class="line">        self.left = nn.Sequential(</span><br><span class="line">                nn.Conv2d(inchannel, outchannel, <span class="number">3</span>, stride, <span class="number">1</span>, bias=<span class="keyword">False</span>),</span><br><span class="line">                nn.BatchNorm2d(outchannel),</span><br><span class="line">                nn.ReLU(inplace=<span class="keyword">True</span>),</span><br><span class="line">                nn.Conv2d(outchannel, outchannel, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, bias=<span class="keyword">False</span>),</span><br><span class="line">                nn.BatchNorm2d(outchannel) )</span><br><span class="line">        self.right = shortcut</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        out = self.left(x)</span><br><span class="line">        residual = x <span class="keyword">if</span> self.right <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">else</span> self.right(x)</span><br><span class="line">        out += residual</span><br><span class="line">        <span class="keyword">return</span> F.relu(out)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet34</span><span class="params">(BasicModule)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    实现主module：ResNet34</span></span><br><span class="line"><span class="string">    ResNet34包含多个layer，每个layer又包含多个Residual block</span></span><br><span class="line"><span class="string">    用子module来实现Residual block，用_make_layer函数来实现layer</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_classes=<span class="number">2</span>)</span>:</span></span><br><span class="line">        super(ResNet34, self).__init__()</span><br><span class="line">        self.model_name = <span class="string">'resnet34'</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 前几层: 图像转换</span></span><br><span class="line">        self.pre = nn.Sequential(</span><br><span class="line">                nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">3</span>, bias=<span class="keyword">False</span>),</span><br><span class="line">                nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">                nn.ReLU(inplace=<span class="keyword">True</span>),</span><br><span class="line">                nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 重复的layer，分别有3，4，6，3个residual block</span></span><br><span class="line">        self.layer1 = self._make_layer( <span class="number">64</span>, <span class="number">128</span>, <span class="number">3</span>)</span><br><span class="line">        self.layer2 = self._make_layer( <span class="number">128</span>, <span class="number">256</span>, <span class="number">4</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.layer3 = self._make_layer( <span class="number">256</span>, <span class="number">512</span>, <span class="number">6</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.layer4 = self._make_layer( <span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#分类用的全连接</span></span><br><span class="line">        self.fc = nn.Linear(<span class="number">512</span>, num_classes)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_make_layer</span><span class="params">(self,  inchannel, outchannel, block_num, stride=<span class="number">1</span>)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        构建layer,包含多个residual block</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        shortcut = nn.Sequential(</span><br><span class="line">                nn.Conv2d(inchannel,outchannel,<span class="number">1</span>,stride, bias=<span class="keyword">False</span>),</span><br><span class="line">                nn.BatchNorm2d(outchannel))</span><br><span class="line">        </span><br><span class="line">        layers = []</span><br><span class="line">        layers.append(ResidualBlock(inchannel, outchannel, stride, shortcut))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, block_num):</span><br><span class="line">            layers.append(ResidualBlock(outchannel, outchannel))</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.pre(x)</span><br><span class="line">        </span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = self.layer2(x)</span><br><span class="line">        x = self.layer3(x)</span><br><span class="line">        x = self.layer4(x)</span><br><span class="line"></span><br><span class="line">        x = F.avg_pool2d(x, <span class="number">7</span>)</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">return</span> self.fc(x)</span><br></pre></td></tr></table></figure>
<p>这里用到了<code>torch.nn.Sequential</code></p>
<h3 id="2-1-3-nn-Sequential"><a href="#2-1-3-nn-Sequential" class="headerlink" title="2.1.3 nn.Sequential"></a>2.1.3 nn.Sequential</h3><p>这个是Sequential容器，将一系列操作包起来。例如Resnet有很多重复的block，就可以包起来。</p>
<p>官方文档中给了两个例子</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example of using Sequential model = nn.Sequential(</span></span><br><span class="line">nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>), nn.ReLU(), nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>), nn.ReLU()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example of using Sequential with OrderedDict</span></span><br><span class="line">model = nn.Sequential(OrderedDict([</span><br><span class="line">(<span class="string">'conv1'</span>, nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>)), (<span class="string">'relu1'</span>, nn.ReLU()),</span><br><span class="line">(<span class="string">'conv2'</span>, nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>)), (<span class="string">'relu2'</span>, nn.ReLU())</span><br><span class="line">]))</span><br></pre></td></tr></table></figure>
<p>总结：模型的定义就是先继承，再构建组件，最后组装(forward)。</p>
<h2 id="2-2-权值初始化的十种方法"><a href="#2-2-权值初始化的十种方法" class="headerlink" title="2.2 权值初始化的十种方法"></a>2.2 权值初始化的十种方法</h2><p>初始化方法会直接影响模型的收敛与否</p>
<h3 id="2-2-1-权重初始化流程"><a href="#2-2-1-权重初始化流程" class="headerlink" title="2.2.1 权重初始化流程"></a>2.2.1 权重初始化流程</h3><p>总共两步，</p>
<p>1）先设定什么层用什么初始化方法，初始化方法在torch.nn.init中给出；</p>
<p>2）实例化一个模型之后，执行该函数，即可完成初始化。</p>
<p>重点是第一步，看Main的方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义权值初始化</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize_weights</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> isinstance(m, nn.Conv2d):</span><br><span class="line">                torch.nn.init.xavier_normal_(m.weight.data)</span><br><span class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                    <span class="comment"># 若有bias，初始化全为0</span></span><br><span class="line">                    m.bias.data.zero_()</span><br><span class="line">            <span class="keyword">elif</span> isinstance(m, nn.BatchNorm2d):</span><br><span class="line">                m.weight.data.fill_(<span class="number">1</span>)</span><br><span class="line">                m.bias.data.zero_()</span><br><span class="line">            <span class="keyword">elif</span> isinstance(m, nn.Linear):</span><br><span class="line">                torch.nn.init.normal_(m.weight.data, <span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">                m.bias.data.zero_()</span><br></pre></td></tr></table></figure>
<h3 id="2-2-2-常用初始化方法"><a href="#2-2-2-常用初始化方法" class="headerlink" title="2.2.2 常用初始化方法"></a>2.2.2 常用初始化方法</h3><p>1）Xavier，kaiming系列</p>
<p>2）其他方法分布</p>
<p>Xavier 初始化方法，论文在《Understanding the difficulty of training deep feedforward neural  networks》 </p>
<p>公式推导是从“方差一致性”出发，初始化的分布有均匀分布和正态分布两种。</p>
<h4 id="1、Xavier均匀分布"><a href="#1、Xavier均匀分布" class="headerlink" title="1、Xavier均匀分布"></a>1、Xavier均匀分布</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.init.xavier_uniform_(tensor, gain=1)</span><br></pre></td></tr></table></figure>
<p>服从均匀分布U(-a, a)，分布的参数$a=gain * sqrt(6/fan_in+fan_out)$</p>
<p>这里有一个gain，增益的大小是依据激活函数类型来设定。如</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nn.init.xavier_uniform_(w, gain=nn.init.calculate_gain(&apos;relu&apos;))</span><br></pre></td></tr></table></figure>
<p>上述方法也成为Glorot initialization</p>
<h4 id="2、Xavier正态分布"><a href="#2、Xavier正态分布" class="headerlink" title="2、Xavier正态分布"></a>2、Xavier正态分布</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.init.xavier_normal_(tensor, gain=1)</span><br></pre></td></tr></table></figure>
<h4 id="3、kaiming均匀分布"><a href="#3、kaiming均匀分布" class="headerlink" title="3、kaiming均匀分布"></a>3、kaiming均匀分布</h4><p>论文在《 Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification》。公式推导同样从“方差一致性”出法，kaiming是针对 xavier 初始化方法在 relu 这一类激活函数表现不佳而提出的改进</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.init.kaiming_uniform_(tensor, a=0, mode=&apos;fan_in&apos;, nonlinearity=&apos;leaky_relu&apos;)</span><br></pre></td></tr></table></figure>
<p>其中，a是激活函数的负半轴的斜率，relu是0</p>
<p>mode可选为fan_in或fan_out，前者使正向传播时方差一致，后者使反向传播时方差一致。</p>
<p>nonlinearity可选relu和leaky_relu，默认值为leaky_relu。</p>
<p>4、kaiming正态分布</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.init.kaiming_normal_(tensor, a=0, mode=&apos;fan_in&apos;, nonlinearity=&apos;leaky_relu&apos;)</span><br></pre></td></tr></table></figure>
<p>5、其他方法</p>
<p>参见文档</p>
<p><strong>权值初始化杂谈</strong></p>
<p>1、从代码中发现，即使不进行初始化，模型的权重也不为空，而是有值的，这些值是什么时候赋给的呢？</p>
<blockquote>
<p>其实，在创建网络实例的过程中，一旦调用nn.Conv2d的时候就会对权值进行初始化。</p>
<p>初始化过程是在Conv2d的基类_ConvNd中进行的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; class Conv2d(_ConvNd):</span><br><span class="line">&gt; --&gt; 在_ConvNd 中:</span><br><span class="line">&gt; --&gt; self.reset_parameters()</span><br><span class="line">&gt; ---&gt; def reset_parameters(self)</span><br><span class="line">&gt; ---&gt; self.weight.data.uniform_(-stdv, stdv)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p>&gt;</p>
<blockquote>
<p>可以看出这里是均匀分布，其中-stdv与kernel的size有关。</p>
<p>补充：在Pytorch1.0版本中，这里改用了kaiming<em>uniform</em>()进行初始化。</p>
</blockquote>
<p>2、按需定义初始化方法，例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">if isinstance(m, nn.Conv2d):</span><br><span class="line">	n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels 		</span><br><span class="line">	m.weight.data.normal_(0, math.sqrt(2. / n))</span><br></pre></td></tr></table></figure>
<h2 id="2-3-模型Finetune"><a href="#2-3-模型Finetune" class="headerlink" title="2.3 模型Finetune"></a>2.3 模型Finetune</h2><p>实际应用中，通常采用一个已经训练模型的权值参数作为我们模型的初始化参数，也称之为finetune—迁移学习。迁移学习中的 Finetune 技术，本质上就是让我们新构建的模型，拥有一个较好的权值初始值。</p>
<p>finetune 权值初始化三步曲，finetune 就相当于给模型进行初始化，其流程共用三步:<br>第一步:保存模型，拥有一个预训练模型;<br>第二步:加载模型，把预训练模型中的权值取出来;<br>第三步:初始化，将权值对应的“放”到新模型中</p>
<h3 id="2-3-1-权值初始化"><a href="#2-3-1-权值初始化" class="headerlink" title="2.3.1 权值初始化"></a>2.3.1 权值初始化</h3><p>在进行 finetune 之前我们需要拥有一个模型或者是模型参数，因此需要了解如何保存 模型。官方文档中介绍了两种保存模型的方法，一种是保存整个模型，另外一种是仅保存 模型参数(官方推荐用这种方法)，这里采用官方推荐的方法。 </p>
<h4 id="1、保存模型参数"><a href="#1、保存模型参数" class="headerlink" title="1、保存模型参数"></a>1、保存模型参数</h4><p>若拥有模型参数，可跳过这一步。<br>假设创建了一个 net = Net()，并且经过训练，通过以下方式保存:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(net.state_dict(), &apos;net_params.pkl&apos;)</span><br></pre></td></tr></table></figure>
<h4 id="2、加载模型"><a href="#2、加载模型" class="headerlink" title="2、加载模型"></a>2、加载模型</h4><p>进行三步曲中的第二步，加载模型，这里只是加载模型的参数: </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pretrained_dict = torch.load(&apos;net_params.pkl&apos;)</span><br></pre></td></tr></table></figure>
<h4 id="3、初始化"><a href="#3、初始化" class="headerlink" title="3、初始化"></a>3、初始化</h4><p>进行三步曲中的第三步，将取到的权值，对应的放到新模型中: 首先我们创建新模型，并且获取新模型的参数字典 net_state_dict: </p>
<p>net=Net()# 创建net</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net_state_dict = net.state_dict() # 获取已创建 net 的 state_dict</span><br></pre></td></tr></table></figure>
<p>接着将 pretrained_dict 里不属于 net_state_dict 的键<strong>剔除掉</strong>: </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pretrained_dict_1 = &#123;k: v for k, v in pretrained_dict.items() if k in net_state_dict&#125;</span><br></pre></td></tr></table></figure>
<p>然后，用预训练模型的参数字典 对 新模型的参数字典 net_state_dict 进行更新: </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net_state_dict.update(pretrained_dict_1)</span><br></pre></td></tr></table></figure>
<p>最后，将更新了参数的字典 “放”回到网络中: </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net.load_state_dict(net_state_dict)</span><br></pre></td></tr></table></figure>
<p>采用 finetune 的训练过程中，<strong>有时候希望前面层的学习率低一些，改变不要太大，而 后面的全连接层的学习率相对大一些</strong>。这时就需要对不同的层设置不同的学习率，下面就 介绍如何为不同层配置不同的学习率。 </p>
<h3 id="2-3-2-不同层设置不同学习率"><a href="#2-3-2-不同层设置不同学习率" class="headerlink" title="2.3.2 不同层设置不同学习率"></a>2.3.2 不同层设置不同学习率</h3><p>在利用 pre-trained model 的参数做初始化之后，我们可能想让 fc 层更新相对快一些，而希望前面的权值更新小一些，这就可以通过为不同的层设置不同的学习率来达到此目的。</p>
<p>为不同层设置不同的学习率，主要通过优化器对多个参数组进行设置不同的参数。所以，只需要将原始的参数组，划分成两个，甚至更多的参数组，然后分别进行设置学习率。</p>
<p>这里将原始参数“切分”成 fc3 层参数和其余参数，为 fc3 层设置更大的学习率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 返回的是 parameters 的 内存地址 </span></span><br><span class="line"><span class="comment"># 将fc3层的参数从原始参数net.parameters中剥离出来。</span></span><br><span class="line"><span class="comment"># base_params是剥离了fc3层参数的其他参数。</span></span><br><span class="line">ignored_params = list(map(id, net.fc3.parameters())) </span><br><span class="line">base_params = filter(<span class="keyword">lambda</span> p: id(p) <span class="keyword">not</span> <span class="keyword">in</span> 	ignored_params, net.parameters())</span><br><span class="line"><span class="comment"># 然后优化器中为fc3的单独设置学习率</span></span><br><span class="line">optimizer = optim.SGD([</span><br><span class="line">  &#123;<span class="string">'params'</span>: base_params&#125;,</span><br><span class="line">	&#123;<span class="string">'params'</span>: net.fc3.parameters(), <span class="string">'lr'</span>: <span class="number">0.001</span>*<span class="number">10</span>&#125;], <span class="number">0.001</span>, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">1e-4</span>)</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<p>完整代码在<code>/Code/2_model/2_finetune.py</code></p>
<h1 id="第三章-损失函数和优化器"><a href="#第三章-损失函数和优化器" class="headerlink" title="第三章 损失函数和优化器"></a>第三章 损失函数和优化器</h1><p>Pytorch中十七个损失函数，十个优化器和六个学习率调整方法。</p>
<h2 id="3-1-十七个损失函数"><a href="#3-1-十七个损失函数" class="headerlink" title="3.1 十七个损失函数"></a>3.1 十七个损失函数</h2><p>我们所说的优化，即优化网络权值使得损失函数值变小。但是，损失函数值变小是否能代表模型的分类/回归精度变高呢?那么多种损失函数，应该如何选择呢?请来了解PyTorch 中给出的十七种损失函数吧。</p>
<h3 id="3-1-1-L1loss"><a href="#3-1-1-L1loss" class="headerlink" title="3.1.1 L1loss"></a>3.1.1 L1loss</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">class torch.nn.L1Loss(size_average=None, reduce=None)</span><br></pre></td></tr></table></figure>
<p>官方文档中仍有 reduction=’elementwise_mean’参数，但代码实现中已经删除该参数</p>
<p><strong>功能：</strong></p>
<p>计算output和target之差的绝对值，可选返回同维度的tensor或一个标量。</p>
<p><strong>公式：</strong></p>
<p><img src="/2019/06/14/深度学习笔记/Pytorch笔记/完整的一次任务/pic/image-20190616085944731.png" alt="image-20190616085944731"></p>
<p><strong>参数:</strong> </p>
<p>reduce(bool)- 返回值是否为标量，默认为 True<br>size_average(bool)- 当 reduce=True 时有效。为 True 时，返回的 loss 为平均值;为 False 时，返回的各样本的 loss 之和。<br><strong>实例:</strong> </p>
<p><code>/Code/3_optimizer/3_1_lossFunction/1_L1Loss.py</code></p>
<h1 id="第四章-监控模型-可视化"><a href="#第四章-监控模型-可视化" class="headerlink" title="第四章 监控模型-可视化"></a>第四章 监控模型-可视化</h1><h2 id="4-1-TensorBoardX"><a href="#4-1-TensorBoardX" class="headerlink" title="4.1 TensorBoardX"></a>4.1 TensorBoardX</h2><p>流行的有两种方法，本文重点介绍第二种。</p>
<p>1、构建Logger类</p>
<p>Logger 类中“包”了 tf.summary.FileWriter ，截至目前(2018.10.17)，只有三种操作，分别是 scalar_summary(), image_summary(), histo_summary()。</p>
<p>优点:轻便，可满足大部分需求</p>
<p>参考github：<a href="https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/04-utils/tensorboard" target="_blank" rel="noopener">https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/04-utils/tensorboard</a></p>
<p>2、借助TensorBoardX包</p>
<p>TensorBoardX 包的功能就比较全，截至目前(2018.10.17)，支持除<br>beholder 之外的所有 tensorboard 的记录类型。</p>
<p>github：<a href="https://github.com/lanpa/tensorboardX" target="_blank" rel="noopener">https://github.com/lanpa/tensorboardX</a></p>
<p>API文档：<a href="https://tensorboard-pytorch.readthedocs.io/en/latest/tutorial_zh.html" target="_blank" rel="noopener">https://tensorboard-pytorch.readthedocs.io/en/latest/tutorial_zh.html</a></p>
<p><strong>代码实现:</strong></p>
<p>tensorboardX 提供 13 个函数，可以记录标量、图像、语音、文字等等，功能十分丰富。<br>本节将对这些函数进行介绍，所用代码为 tensorboardX 的官方 demo.py，放在：</p>
<p><code>/Code/4_viewer/1_tensorboardX_demo.py</code></p>
<p>运行该文件，再打开一个terminal，进入/Result/，执行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir=runs</span><br></pre></td></tr></table></figure>
<p>然后浏览器打开</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">localhost:6006</span><br></pre></td></tr></table></figure>
<p>可以看到显示界面如下：</p>
<h2 id="4-2-TensorBoardX的函数"><a href="#4-2-TensorBoardX的函数" class="headerlink" title="4.2 TensorBoardX的函数"></a>4.2 TensorBoardX的函数</h2><h3 id="4-2-1-add-scalar"><a href="#4-2-1-add-scalar" class="headerlink" title="4.2.1 add_scalar()"></a>4.2.1 add_scalar()</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">add_scalar(tag, scalar_value, global_step=None, walltime=None)</span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong></p>
<p>在一个图表中记录一个标量的变化，常用于Loss和Accuracy曲线的记录。</p>
<p><strong>参数:</strong> </p>
<p>tag(string)- 该图的标签，类似于 polt.title。 </p>
<p>scalar_value(float or string/blobname)- 用于存储的值，曲线图的 y 坐标 </p>
<p>global_step(int)- 曲线图的 x 坐标<br> walltime(float)- 为 event 文件的文件名设置时间，默认为 time.time() 运行 demo 中的: </p>
<p>用 github 首页 demo 运行这一行:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">writer.add_scalar(&apos;data/scalar1&apos;, dummy_s1[0], n_iter)</span><br></pre></td></tr></table></figure>
<p> 可以得到下图: </p>

      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      
      
        <div class="post-widgets">
        

        

        
          
          <div class="social_share">
            
            
            
              <div>
                
  <div class="bdsharebuttonbox">
    <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
    <a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a>
    <a href="#" class="bds_sqq" data-cmd="sqq" title="分享到QQ好友"></a>
    <a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a>
    <a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a>
    <a href="#" class="bds_tieba" data-cmd="tieba" title="分享到百度贴吧"></a>
    <a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a>
    <a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a>
    <a href="#" class="bds_more" data-cmd="more"></a>
    <a class="bds_count" data-cmd="count"></a>
  </div>
  <script>
    window._bd_share_config = {
      "common": {
        "bdText": "",
        "bdMini": "2",
        "bdMiniList": false,
        "bdPic": ""
      },
      "share": {
        "bdSize": "16",
        "bdStyle": "0"
      },
      "image": {
        "viewList": ["tsina", "douban", "sqq", "qzone", "weixin", "twi", "fbook"],
        "viewText": "分享到：",
        "viewSize": "16"
      }
    }
  </script>

<script>
  with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='//bdimg.share.baidu.com/static/api/js/share.js?cdnversion='+~(-new Date()/36e5)];
</script>

              </div>
            
          </div>
        
        </div>
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/06/12/深度学习笔记/Pytorch训练营/D4 数据读取、数据扩增/" rel="next" title="">
                <i class="fa fa-chevron-left"></i> 
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/06/14/机器学习和深度学习算法理论/CNN/一些概念/" rel="prev" title="">
                 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Schwimmer</p>
              <div class="site-description motion-element" itemprop="description">Record and Think!</div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">313</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">20</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">56</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#第一章-数据"><span class="nav-number">1.</span> <span class="nav-text">第一章 数据</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-Cifar10-转-png"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 Cifar10 转 png</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#读取并保存为图片"><span class="nav-number">1.1.1.</span> <span class="nav-text">读取并保存为图片</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-训练集、验证集、测试集的划分"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 训练集、验证集、测试集的划分</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-Pytorch读图片数据集"><span class="nav-number">1.3.</span> <span class="nav-text">1.3 Pytorch读图片数据集</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Dataset类"><span class="nav-number">1.3.1.</span> <span class="nav-text">Dataset类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1、制作图片索引"><span class="nav-number">1.3.2.</span> <span class="nav-text">1、制作图片索引</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2、构建Dataset子类"><span class="nav-number">1.3.3.</span> <span class="nav-text">2、构建Dataset子类</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-4-DataLoder加载图片"><span class="nav-number">1.4.</span> <span class="nav-text">1.4 DataLoder加载图片</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-5-数据增强和数据标准化"><span class="nav-number">1.5.</span> <span class="nav-text">1.5 数据增强和数据标准化</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第二章-模型"><span class="nav-number">2.</span> <span class="nav-text">第二章 模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-模型的搭建"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 模型的搭建</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-1-模型定义的三要素"><span class="nav-number">2.1.1.</span> <span class="nav-text">2.1.1 模型定义的三要素</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-2-一个更复杂的模型"><span class="nav-number">2.1.2.</span> <span class="nav-text">2.1.2 一个更复杂的模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-3-nn-Sequential"><span class="nav-number">2.1.3.</span> <span class="nav-text">2.1.3 nn.Sequential</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-权值初始化的十种方法"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 权值初始化的十种方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-1-权重初始化流程"><span class="nav-number">2.2.1.</span> <span class="nav-text">2.2.1 权重初始化流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-2-常用初始化方法"><span class="nav-number">2.2.2.</span> <span class="nav-text">2.2.2 常用初始化方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1、Xavier均匀分布"><span class="nav-number">2.2.2.1.</span> <span class="nav-text">1、Xavier均匀分布</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2、Xavier正态分布"><span class="nav-number">2.2.2.2.</span> <span class="nav-text">2、Xavier正态分布</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3、kaiming均匀分布"><span class="nav-number">2.2.2.3.</span> <span class="nav-text">3、kaiming均匀分布</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-模型Finetune"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 模型Finetune</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-1-权值初始化"><span class="nav-number">2.3.1.</span> <span class="nav-text">2.3.1 权值初始化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1、保存模型参数"><span class="nav-number">2.3.1.1.</span> <span class="nav-text">1、保存模型参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2、加载模型"><span class="nav-number">2.3.1.2.</span> <span class="nav-text">2、加载模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3、初始化"><span class="nav-number">2.3.1.3.</span> <span class="nav-text">3、初始化</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-2-不同层设置不同学习率"><span class="nav-number">2.3.2.</span> <span class="nav-text">2.3.2 不同层设置不同学习率</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第三章-损失函数和优化器"><span class="nav-number">3.</span> <span class="nav-text">第三章 损失函数和优化器</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-十七个损失函数"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 十七个损失函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-1-L1loss"><span class="nav-number">3.1.1.</span> <span class="nav-text">3.1.1 L1loss</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第四章-监控模型-可视化"><span class="nav-number">4.</span> <span class="nav-text">第四章 监控模型-可视化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-TensorBoardX"><span class="nav-number">4.1.</span> <span class="nav-text">4.1 TensorBoardX</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-TensorBoardX的函数"><span class="nav-number">4.2.</span> <span class="nav-text">4.2 TensorBoardX的函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-1-add-scalar"><span class="nav-number">4.2.1.</span> <span class="nav-text">4.2.1 add_scalar()</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Schwimmer</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.9.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> v7.2.0</div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span>
    <span class="site-uv" title="总访客量">
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="post-meta-divider">|</span>
  

  
    <span class="post-meta-item-icon">
      <i class="fa fa-eye"></i>
    </span>
    <span class="site-pv" title="总访问量">
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/utils.js?v=7.2.0"></script>

  <script src="/js/motion.js?v=7.2.0"></script>



  
  


  <script src="/js/schemes/muse.js?v=7.2.0"></script>




  
  <script src="/js/scrollspy.js?v=7.2.0"></script>
<script src="/js/post-details.js?v=7.2.0"></script>



  


  <script src="/js/next-boot.js?v=7.2.0"></script>


  

  

  

  
  
<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://https-schwimmer-github-io.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>


<script>
  var disqus_config = function() {
    this.page.url = "https://schwimmer.github.io/2019/06/14/深度学习笔记/Pytorch笔记/完整的一次任务/";
    this.page.identifier = "2019/06/14/深度学习笔记/Pytorch笔记/完整的一次任务/";
    this.page.title = '';
    };
  function loadComments() {
    var d = document, s = d.createElement('script');
    s.src = 'https://https-schwimmer-github-io.disqus.com/embed.js';
    s.setAttribute('data-timestamp', '' + +new Date());
    (d.head || d.body).appendChild(s);
  }
  
    window.addEventListener('load', loadComments, false);
  
</script>





  


  
  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      document.getElementById(all[i].inputID + '-Frame').parentNode.className += ' has-jax';
    }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

    
  


  

  

  

  

  

  

  

  

  

  

  

</body>
</html>
