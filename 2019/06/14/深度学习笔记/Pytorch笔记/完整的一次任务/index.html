<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222"/>






















<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0"/>

<link rel="stylesheet" href="/css/main.css?v=7.2.0"/>


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.2.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.2.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '7.2.0',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="《PyTorch模型训练实用教程》 代码在：/Users/david/david/code/deep_learning/PyTorch_Tutorial_yuting 预先安装 requirement.txt 运行时报错 12OMP: Error #15: Initializing libiomp5.dylib, but found libomp.dylib already initialized">
<meta property="og:type" content="article">
<meta property="og:title" content="Schwimmer&#39;s Blog">
<meta property="og:url" content="https://schwimmer.github.io/2019/06/14/深度学习笔记/Pytorch笔记/完整的一次任务/index.html">
<meta property="og:site_name" content="Schwimmer&#39;s Blog">
<meta property="og:description" content="《PyTorch模型训练实用教程》 代码在：/Users/david/david/code/deep_learning/PyTorch_Tutorial_yuting 预先安装 requirement.txt 运行时报错 12OMP: Error #15: Initializing libiomp5.dylib, but found libomp.dylib already initialized">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://schwimmer.github.io/2019/06/14/深度学习笔记/Pytorch笔记/完整的一次任务/pic/image-20190616085944731.png">
<meta property="og:updated_time" content="2019-06-16T01:16:07.416Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Schwimmer&#39;s Blog">
<meta name="twitter:description" content="《PyTorch模型训练实用教程》 代码在：/Users/david/david/code/deep_learning/PyTorch_Tutorial_yuting 预先安装 requirement.txt 运行时报错 12OMP: Error #15: Initializing libiomp5.dylib, but found libomp.dylib already initialized">
<meta name="twitter:image" content="https://schwimmer.github.io/2019/06/14/深度学习笔记/Pytorch笔记/完整的一次任务/pic/image-20190616085944731.png">





  
  
  <link rel="canonical" href="https://schwimmer.github.io/2019/06/14/深度学习笔记/Pytorch笔记/完整的一次任务/"/>



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title> | Schwimmer's Blog</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Schwimmer's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Navigationsleiste an/ausschalten">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br/>Startseite</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br/>Schlagwörter</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br/>Kategorien</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br/>Archiv</a>

  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/14/深度学习笔记/Pytorch笔记/完整的一次任务/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">

              
            
          </h2>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2019-06-14 10:59:11" itemprop="dateCreated datePublished" datetime="2019-06-14T10:59:11+08:00">2019-06-14</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Bearbeitet am</span>
                
                <time title="Geändert am: 2019-06-16 09:16:07" itemprop="dateModified" datetime="2019-06-16T09:16:07+08:00">2019-06-16</time>
              </span>
            
          

          

          
            
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>《PyTorch模型训练实用教程》</p>
<p>代码在：<code>/Users/david/david/code/deep_learning/PyTorch_Tutorial_yuting</code></p>
<p>预先安装</p>
<p>requirement.txt</p>
<p>运行时报错</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">OMP: Error #15: Initializing libiomp5.dylib, but found libomp.dylib already initialized.</div><div class="line">OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://www.intel.com/software/products/support/.</div></pre></td></tr></table></figure>
<p>解决方案</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">os.environ[&quot;KMP_DUPLICATE_LIB_OK&quot;]=&quot;TRUE&quot;</div></pre></td></tr></table></figure>
<h1 id="第一章-数据"><a href="#第一章-数据" class="headerlink" title="第一章 数据"></a>第一章 数据</h1><h2 id="1-1-Cifar10-转-png"><a href="#1-1-Cifar10-转-png" class="headerlink" title="1.1 Cifar10 转 png"></a>1.1 Cifar10 转 png</h2><p>cifar-10 的测试集，10000张图片。</p>
<p>官网:<a href="http://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="noopener">http://www.cs.toronto.edu/~kriz/cifar.html</a></p>
<h3 id="读取并保存为图片"><a href="#读取并保存为图片" class="headerlink" title="读取并保存为图片"></a>读取并保存为图片</h3><p><strong>图片是序列化的，不能直接读取。</strong></p>
<p>运行代码:Code/1_data_prepare/1_1_cifar10_to_png.py</p>
<p>可在文件夹 Data/cifar-10-png/raw_test/下看到 0-9 个文件夹，对应 9 个类别。</p>
<p>将 测试集中的 10000 张图片解压出来，作为原始图片，将从这 10000 张图片中划分出训练集 (train)，验证集(valid)，测试集(test)。 </p>
<h2 id="1-2-训练集、验证集、测试集的划分"><a href="#1-2-训练集、验证集、测试集的划分" class="headerlink" title="1.2 训练集、验证集、测试集的划分"></a>1.2 训练集、验证集、测试集的划分</h2><p>把原始数据按 8:1:1 的比例划分为训练集(train set)、验证集(valid/dev set)和测试集(test set) </p>
<p>运行代码：Code/1_data_prepare/1_2_split_dataset.py</p>
<p>数据划分完毕，下一步是制作存放有图片路径及其标签的 txt。pytorch会根据txt的信息寻找图片，并读取图片数据和标签数据。</p>
<h2 id="1-3-Pytorch读图片数据集"><a href="#1-3-Pytorch读图片数据集" class="headerlink" title="1.3 Pytorch读图片数据集"></a>1.3 Pytorch读图片数据集</h2><h3 id="Dataset类"><a href="#Dataset类" class="headerlink" title="Dataset类"></a>Dataset类</h3><p>PyTorch 读取<strong>图片</strong>，主要是通过 Dataset 类，所以先简单了解一下 Dataset 类。抽象类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dataset</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="string">"""An abstract class representing a Dataset.</span></div><div class="line"></div><div class="line">    All other datasets should subclass it. All subclasses should override</div><div class="line">    ``__len__``, that provides the size of the dataset, and ``__getitem__``,</div><div class="line">    supporting integer indexing in range from 0 to len(self) exclusive.</div><div class="line">    """</div><div class="line">		</div><div class="line">    <span class="comment"># 接收一个 index，然后返回图片数据和标签</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></div><div class="line">        <span class="keyword">raise</span> NotImplementedError</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">raise</span> NotImplementedError</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__add__</span><span class="params">(self, other)</span>:</span></div><div class="line">        <span class="keyword">return</span> ConcatDataset([self, other])</div></pre></td></tr></table></figure>
<p>需要将list放到txt里面，读取txt，获取list。那么读取自己数据的基本流程就是:</p>
<ol>
<li>制作存储了图片的路径和标签信息的 txt</li>
<li>将这些信息转化为 list，该 list 每一个元素对应一个样本</li>
<li>通过 getitem 函数，读取数据和标签，并返回数据和标签</li>
</ol>
<p>在训练代码里是感觉不到这些操作的，只会看到通过 DataLoader 就可以获取一个batch 的数据，其实触发去读取图片这些操作的是 DataLoader 里的<strong>iter</strong>(self)，后面会详细讲解读取过程。在本小节，主要讲 Dataset 子类。</p>
<p>要让 PyTorch 能读取自己的数据集，只需要两步:</p>
<ol>
<li>制作图片数据的索引</li>
<li><strong>构建 Dataset 子类</strong></li>
</ol>
<h3 id="1、制作图片索引"><a href="#1、制作图片索引" class="headerlink" title="1、制作图片索引"></a>1、制作图片索引</h3><p>就是获取图片路径和标签，保存到txt。</p>
<p>运行代码 Code/1_data_prepare/1_3_generate_txt.py</p>
<h3 id="2、构建Dataset子类"><a href="#2、构建Dataset子类" class="headerlink" title="2、构建Dataset子类"></a>2、构建Dataset子类</h3><p>构建了MyDataset类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDataset</span><span class="params">(Dataset)</span>:</span></div><div class="line">  	<span class="comment"># 初始化中，从txt读到imgs对象</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, txt_path, transform = None, target_transform = None)</span>:</span></div><div class="line">        fh = open(txt_path, <span class="string">'r'</span>)</div><div class="line">        imgs = []</div><div class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> fh:</div><div class="line">            line = line.rstrip()</div><div class="line">            words = line.split()</div><div class="line">            imgs.append((words[<span class="number">0</span>], int(words[<span class="number">1</span>])))</div><div class="line"></div><div class="line">        <span class="comment"># 最主要就是要生成这个list， 然后DataLoader中给index，通过getitem读取图片数据</span></div><div class="line">        self.imgs = imgs        </div><div class="line">        <span class="comment"># Compose 类型，里边有一个 list，list定义了对图像的各种操作，如减均值，除标准差，随机裁剪，仿射变换等。</span></div><div class="line">        self.transform = transform</div><div class="line">        self.target_transform = target_transform</div><div class="line"></div><div class="line">    <span class="comment"># python内建的魔法方法，访问索引就会触发</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></div><div class="line">        fn, label = self.imgs[index]</div><div class="line">        img = Image.open(fn).convert(<span class="string">'RGB'</span>)     <span class="comment"># 像素值 0~255，在transfrom.totensor会除以255，使像素值变成 0~1</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> self.transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            img = self.transform(img)   <span class="comment"># 在这里做transform，转为tensor等等</span></div><div class="line"></div><div class="line">        <span class="keyword">return</span> img, label</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> len(self.imgs)</div></pre></td></tr></table></figure>
<p>Pytorch对图片的处理不能生成新图，而是覆盖原图（不是真正的覆盖，就是对象赋值）。当采用randomcrop之类的随机操作时，每个 epoch 输入进来的图片几乎不会是一模一样的，这达到了样本多样性的功能。</p>
<p>当 Mydataset 构建好，剩下的操作就交给 DataLoder，在 DataLoder 中，会触发Mydataset 中的 getiterm 函数读取一张图片的数据和标签，并拼接成一个 batch 返回，作为模型真正的输入。</p>
<h2 id="1-4-DataLoder加载图片"><a href="#1-4-DataLoder加载图片" class="headerlink" title="1.4 DataLoder加载图片"></a>1.4 DataLoder加载图片</h2><p>getitem是在DataLoader中触发的，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 构建MyDataset实例</span></div><div class="line">train_data = MyDataset(txt_path=train_txt_path, transform=trainTransform)</div><div class="line"></div><div class="line"><span class="comment"># 构建DataLoder</span></div><div class="line">train_loader = DataLoader(dataset=train_data, batch_size=train_bs, shuffle=<span class="keyword">True</span>)</div><div class="line"></div><div class="line"><span class="comment"># 这里的data就是__getitem__返回的img, label</span></div><div class="line"><span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(train_loader):</div><div class="line"></div><div class="line"><span class="string">"""</span></div><div class="line">-&gt;     def __iter__(self):</div><div class="line">        return _DataLoaderIter(self)</div><div class="line">        </div><div class="line">    class _DataLoaderIter(object):</div><div class="line">    	def __next__(self):</div><div class="line">    		# collate_fn (callable, optional): merges a list of samples to form a mini-batch.</div><div class="line">        # 这里调用__getitem__</div><div class="line">    		batch = self.collate_fn([self.dataset[i] for i in indices])</div><div class="line">"""</div><div class="line">  </div><div class="line">	<span class="comment"># 获取图片和标签</span></div><div class="line">  inputs, labels = data</div><div class="line">  inputs, labels = Variable(inputs), Variable(labels)</div></pre></td></tr></table></figure>
<blockquote>
<p>图片是通过 Image.open()函数读取进来的，当涉及如下问题：</p>
<p>图片的通道顺序(RGB ? BGR ?)</p>
<p>图片是<code>w*h*c ? c*w*h ?</code></p>
<p>像素值范围[0-1] or [0-255] ?</p>
<p>就要查看 MyDataset()类中 <strong>getitem</strong>()下读取图片用的是什么方法</p>
</blockquote>
<h2 id="1-5-数据增强和数据标准化"><a href="#1-5-数据增强和数据标准化" class="headerlink" title="1.5 数据增强和数据标准化"></a>1.5 数据增强和数据标准化</h2><p>在 PyTorch 中，数据增强方法放在了 transforms.py 文件中。</p>
<p>这一节主要介绍transforms的操作。</p>
<p>1.6 transforms 的二十二个方法</p>
<h1 id="第二章-模型"><a href="#第二章-模型" class="headerlink" title="第二章 模型"></a>第二章 模型</h1><h2 id="2-1-模型的搭建"><a href="#2-1-模型的搭建" class="headerlink" title="2.1 模型的搭建"></a>2.1 模型的搭建</h2><h3 id="2-1-1-模型定义的三要素"><a href="#2-1-1-模型定义的三要素" class="headerlink" title="2.1.1 模型定义的三要素"></a>2.1.1 模型定义的三要素</h3><p>1）首先，必须继承 nn.Module 这个类，要让 PyTorch 知道这个类是一个 Module。</p>
<p>2）其次，在<strong>init</strong>(self)中设置好需要的“组件”(如 conv、pooling、Linear、BatchNorm等）。</p>
<p>3）最后，在 forward(self, x)中用定义好的“组件”进行组装，就像搭积木，把网络结构搭建</p>
<p>在/Code/main<em>training/main.py 中可以看到定义了一个类<code>class Net(nn.Module)</code>，集成了nn.Module，先看<em>_init</em></em>(self)函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">		<span class="comment"># 初始化</span></div><div class="line">    super(Net, self).__init__()</div><div class="line">    <span class="comment"># 定义了一系列组件</span></div><div class="line">    self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</div><div class="line">    self.pool1 = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</div><div class="line">    self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">3</span>)</div><div class="line">    self.pool2 = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</div><div class="line">    self.conv3 = nn.Conv2d(<span class="number">16</span>, <span class="number">64</span>, <span class="number">3</span>)</div><div class="line">    self.pool3=nn.MaxPool2d(<span class="number">2</span>,<span class="number">1</span>)</div><div class="line">    self.fc1 = nn.Linear(<span class="number">64</span> * <span class="number">3</span> * <span class="number">3</span>, <span class="number">120</span>)</div><div class="line">    self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</div><div class="line">    self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</div></pre></td></tr></table></figure>
<p>当这些组件定义好之后，就可以定义 forward()函数，用来搭建网络结构</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">		<span class="comment"># x作为模型的输入，x 经过 conv1，然后经过激活函数 relu，再经过 pool1 操作</span></div><div class="line">    x = self.pool1(F.relu(self.conv1(x)))</div><div class="line">    <span class="comment"># 再做一轮</span></div><div class="line">    x = self.pool2(F.relu(self.conv2(x)))</div><div class="line">    <span class="comment"># 再做一轮</span></div><div class="line">    x=self.pool3(F.relu(self.conv3(x)))</div><div class="line">    <span class="comment"># 将 x 进行 reshape，为了后面做为全连接层的输入</span></div><div class="line">    x = x.view(<span class="number">-1</span>, <span class="number">64</span> * <span class="number">3</span>* <span class="number">3</span>)</div><div class="line">    <span class="comment"># 先经过全连接层 fc，然后经过 relu</span></div><div class="line">    x = F.relu(self.fc1(x))</div><div class="line">    x = F.relu(self.fc2(x))</div><div class="line">    x = self.fc3(x)</div><div class="line">    <span class="keyword">return</span> x</div></pre></td></tr></table></figure>
<h3 id="2-1-2-一个更复杂的模型"><a href="#2-1-2-一个更复杂的模型" class="headerlink" title="2.1.2 一个更复杂的模型"></a>2.1.2 一个更复杂的模型</h3><p>来看一个更复杂的模型，看Resnet网络的定义方法<a href="https://github.com/yuanlairuci110/pytorch-best-practice-master/blob/master/models/ResNet34.py" target="_blank" rel="noopener">https://github.com/yuanlairuci110/pytorch-best-practice-master/blob/master/models/ResNet34.py</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#coding:utf8</span></div><div class="line"><span class="keyword">from</span> .BasicModule <span class="keyword">import</span> BasicModule</div><div class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</div><div class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResidualBlock</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="string">'''</span></div><div class="line">    实现子module: Residual Block</div><div class="line">    '''</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, inchannel, outchannel, stride=<span class="number">1</span>, shortcut=None)</span>:</span></div><div class="line">        super(ResidualBlock, self).__init__()</div><div class="line">        self.left = nn.Sequential(</div><div class="line">                nn.Conv2d(inchannel, outchannel, <span class="number">3</span>, stride, <span class="number">1</span>, bias=<span class="keyword">False</span>),</div><div class="line">                nn.BatchNorm2d(outchannel),</div><div class="line">                nn.ReLU(inplace=<span class="keyword">True</span>),</div><div class="line">                nn.Conv2d(outchannel, outchannel, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, bias=<span class="keyword">False</span>),</div><div class="line">                nn.BatchNorm2d(outchannel) )</div><div class="line">        self.right = shortcut</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        out = self.left(x)</div><div class="line">        residual = x <span class="keyword">if</span> self.right <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">else</span> self.right(x)</div><div class="line">        out += residual</div><div class="line">        <span class="keyword">return</span> F.relu(out)</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet34</span><span class="params">(BasicModule)</span>:</span></div><div class="line">    <span class="string">'''</span></div><div class="line">    实现主module：ResNet34</div><div class="line">    ResNet34包含多个layer，每个layer又包含多个Residual block</div><div class="line">    用子module来实现Residual block，用_make_layer函数来实现layer</div><div class="line">    '''</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_classes=<span class="number">2</span>)</span>:</span></div><div class="line">        super(ResNet34, self).__init__()</div><div class="line">        self.model_name = <span class="string">'resnet34'</span></div><div class="line"></div><div class="line">        <span class="comment"># 前几层: 图像转换</span></div><div class="line">        self.pre = nn.Sequential(</div><div class="line">                nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">3</span>, bias=<span class="keyword">False</span>),</div><div class="line">                nn.BatchNorm2d(<span class="number">64</span>),</div><div class="line">                nn.ReLU(inplace=<span class="keyword">True</span>),</div><div class="line">                nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>))</div><div class="line">        </div><div class="line">        <span class="comment"># 重复的layer，分别有3，4，6，3个residual block</span></div><div class="line">        self.layer1 = self._make_layer( <span class="number">64</span>, <span class="number">128</span>, <span class="number">3</span>)</div><div class="line">        self.layer2 = self._make_layer( <span class="number">128</span>, <span class="number">256</span>, <span class="number">4</span>, stride=<span class="number">2</span>)</div><div class="line">        self.layer3 = self._make_layer( <span class="number">256</span>, <span class="number">512</span>, <span class="number">6</span>, stride=<span class="number">2</span>)</div><div class="line">        self.layer4 = self._make_layer( <span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, stride=<span class="number">2</span>)</div><div class="line"></div><div class="line">        <span class="comment">#分类用的全连接</span></div><div class="line">        self.fc = nn.Linear(<span class="number">512</span>, num_classes)</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_make_layer</span><span class="params">(self,  inchannel, outchannel, block_num, stride=<span class="number">1</span>)</span>:</span></div><div class="line">        <span class="string">'''</span></div><div class="line">        构建layer,包含多个residual block</div><div class="line">        '''</div><div class="line">        shortcut = nn.Sequential(</div><div class="line">                nn.Conv2d(inchannel,outchannel,<span class="number">1</span>,stride, bias=<span class="keyword">False</span>),</div><div class="line">                nn.BatchNorm2d(outchannel))</div><div class="line">        </div><div class="line">        layers = []</div><div class="line">        layers.append(ResidualBlock(inchannel, outchannel, stride, shortcut))</div><div class="line">        </div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, block_num):</div><div class="line">            layers.append(ResidualBlock(outchannel, outchannel))</div><div class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</div><div class="line">        </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        x = self.pre(x)</div><div class="line">        </div><div class="line">        x = self.layer1(x)</div><div class="line">        x = self.layer2(x)</div><div class="line">        x = self.layer3(x)</div><div class="line">        x = self.layer4(x)</div><div class="line"></div><div class="line">        x = F.avg_pool2d(x, <span class="number">7</span>)</div><div class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)</div><div class="line">        <span class="keyword">return</span> self.fc(x)</div></pre></td></tr></table></figure>
<p>这里用到了<code>torch.nn.Sequential</code></p>
<h3 id="2-1-3-nn-Sequential"><a href="#2-1-3-nn-Sequential" class="headerlink" title="2.1.3 nn.Sequential"></a>2.1.3 nn.Sequential</h3><p>这个是Sequential容器，将一系列操作包起来。例如Resnet有很多重复的block，就可以包起来。</p>
<p>官方文档中给了两个例子</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Example of using Sequential model = nn.Sequential(</span></div><div class="line">nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>), nn.ReLU(), nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>), nn.ReLU()</div><div class="line">)</div><div class="line"></div><div class="line"><span class="comment"># Example of using Sequential with OrderedDict</span></div><div class="line">model = nn.Sequential(OrderedDict([</div><div class="line">(<span class="string">'conv1'</span>, nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>)), (<span class="string">'relu1'</span>, nn.ReLU()),</div><div class="line">(<span class="string">'conv2'</span>, nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>)), (<span class="string">'relu2'</span>, nn.ReLU())</div><div class="line">]))</div></pre></td></tr></table></figure>
<p>总结：模型的定义就是先继承，再构建组件，最后组装(forward)。</p>
<h2 id="2-2-权值初始化的十种方法"><a href="#2-2-权值初始化的十种方法" class="headerlink" title="2.2 权值初始化的十种方法"></a>2.2 权值初始化的十种方法</h2><p>初始化方法会直接影响模型的收敛与否</p>
<h3 id="2-2-1-权重初始化流程"><a href="#2-2-1-权重初始化流程" class="headerlink" title="2.2.1 权重初始化流程"></a>2.2.1 权重初始化流程</h3><p>总共两步，</p>
<p>1）先设定什么层用什么初始化方法，初始化方法在torch.nn.init中给出；</p>
<p>2）实例化一个模型之后，执行该函数，即可完成初始化。</p>
<p>重点是第一步，看Main的方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义权值初始化</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize_weights</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</div><div class="line">            <span class="keyword">if</span> isinstance(m, nn.Conv2d):</div><div class="line">                torch.nn.init.xavier_normal_(m.weight.data)</div><div class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">                    <span class="comment"># 若有bias，初始化全为0</span></div><div class="line">                    m.bias.data.zero_()</div><div class="line">            <span class="keyword">elif</span> isinstance(m, nn.BatchNorm2d):</div><div class="line">                m.weight.data.fill_(<span class="number">1</span>)</div><div class="line">                m.bias.data.zero_()</div><div class="line">            <span class="keyword">elif</span> isinstance(m, nn.Linear):</div><div class="line">                torch.nn.init.normal_(m.weight.data, <span class="number">0</span>, <span class="number">0.01</span>)</div><div class="line">                m.bias.data.zero_()</div></pre></td></tr></table></figure>
<h3 id="2-2-2-常用初始化方法"><a href="#2-2-2-常用初始化方法" class="headerlink" title="2.2.2 常用初始化方法"></a>2.2.2 常用初始化方法</h3><p>1）Xavier，kaiming系列</p>
<p>2）其他方法分布</p>
<p>Xavier 初始化方法，论文在《Understanding the difficulty of training deep feedforward neural  networks》 </p>
<p>公式推导是从“方差一致性”出发，初始化的分布有均匀分布和正态分布两种。</p>
<h4 id="1、Xavier均匀分布"><a href="#1、Xavier均匀分布" class="headerlink" title="1、Xavier均匀分布"></a>1、Xavier均匀分布</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torch.nn.init.xavier_uniform_(tensor, gain=1)</div></pre></td></tr></table></figure>
<p>服从均匀分布U(-a, a)，分布的参数$a=gain * sqrt(6/fan_in+fan_out)$</p>
<p>这里有一个gain，增益的大小是依据激活函数类型来设定。如</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">nn.init.xavier_uniform_(w, gain=nn.init.calculate_gain(&apos;relu&apos;))</div></pre></td></tr></table></figure>
<p>上述方法也成为Glorot initialization</p>
<h4 id="2、Xavier正态分布"><a href="#2、Xavier正态分布" class="headerlink" title="2、Xavier正态分布"></a>2、Xavier正态分布</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torch.nn.init.xavier_normal_(tensor, gain=1)</div></pre></td></tr></table></figure>
<h4 id="3、kaiming均匀分布"><a href="#3、kaiming均匀分布" class="headerlink" title="3、kaiming均匀分布"></a>3、kaiming均匀分布</h4><p>论文在《 Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification》。公式推导同样从“方差一致性”出法，kaiming是针对 xavier 初始化方法在 relu 这一类激活函数表现不佳而提出的改进</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torch.nn.init.kaiming_uniform_(tensor, a=0, mode=&apos;fan_in&apos;, nonlinearity=&apos;leaky_relu&apos;)</div></pre></td></tr></table></figure>
<p>其中，a是激活函数的负半轴的斜率，relu是0</p>
<p>mode可选为fan_in或fan_out，前者使正向传播时方差一致，后者使反向传播时方差一致。</p>
<p>nonlinearity可选relu和leaky_relu，默认值为leaky_relu。</p>
<p>4、kaiming正态分布</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torch.nn.init.kaiming_normal_(tensor, a=0, mode=&apos;fan_in&apos;, nonlinearity=&apos;leaky_relu&apos;)</div></pre></td></tr></table></figure>
<p>5、其他方法</p>
<p>参见文档</p>
<p><strong>权值初始化杂谈</strong></p>
<p>1、从代码中发现，即使不进行初始化，模型的权重也不为空，而是有值的，这些值是什么时候赋给的呢？</p>
<blockquote>
<p>其实，在创建网络实例的过程中，一旦调用nn.Conv2d的时候就会对权值进行初始化。</p>
<p>初始化过程是在Conv2d的基类_ConvNd中进行的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&gt; class Conv2d(_ConvNd):</div><div class="line">&gt; --&gt; 在_ConvNd 中:</div><div class="line">&gt; --&gt; self.reset_parameters()</div><div class="line">&gt; ---&gt; def reset_parameters(self)</div><div class="line">&gt; ---&gt; self.weight.data.uniform_(-stdv, stdv)</div><div class="line">&gt;</div></pre></td></tr></table></figure>
</blockquote>
<p>&gt;</p>
<blockquote>
<p>可以看出这里是均匀分布，其中-stdv与kernel的size有关。</p>
<p>补充：在Pytorch1.0版本中，这里改用了kaiming<em>uniform</em>()进行初始化。</p>
</blockquote>
<p>2、按需定义初始化方法，例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">if isinstance(m, nn.Conv2d):</div><div class="line">	n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels 		</div><div class="line">	m.weight.data.normal_(0, math.sqrt(2. / n))</div></pre></td></tr></table></figure>
<h2 id="2-3-模型Finetune"><a href="#2-3-模型Finetune" class="headerlink" title="2.3 模型Finetune"></a>2.3 模型Finetune</h2><p>实际应用中，通常采用一个已经训练模型的权值参数作为我们模型的初始化参数，也称之为finetune—迁移学习。迁移学习中的 Finetune 技术，本质上就是让我们新构建的模型，拥有一个较好的权值初始值。</p>
<p>finetune 权值初始化三步曲，finetune 就相当于给模型进行初始化，其流程共用三步:<br>第一步:保存模型，拥有一个预训练模型;<br>第二步:加载模型，把预训练模型中的权值取出来;<br>第三步:初始化，将权值对应的“放”到新模型中</p>
<h3 id="2-3-1-权值初始化"><a href="#2-3-1-权值初始化" class="headerlink" title="2.3.1 权值初始化"></a>2.3.1 权值初始化</h3><p>在进行 finetune 之前我们需要拥有一个模型或者是模型参数，因此需要了解如何保存 模型。官方文档中介绍了两种保存模型的方法，一种是保存整个模型，另外一种是仅保存 模型参数(官方推荐用这种方法)，这里采用官方推荐的方法。 </p>
<h4 id="1、保存模型参数"><a href="#1、保存模型参数" class="headerlink" title="1、保存模型参数"></a>1、保存模型参数</h4><p>若拥有模型参数，可跳过这一步。<br>假设创建了一个 net = Net()，并且经过训练，通过以下方式保存:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torch.save(net.state_dict(), &apos;net_params.pkl&apos;)</div></pre></td></tr></table></figure>
<h4 id="2、加载模型"><a href="#2、加载模型" class="headerlink" title="2、加载模型"></a>2、加载模型</h4><p>进行三步曲中的第二步，加载模型，这里只是加载模型的参数: </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pretrained_dict = torch.load(&apos;net_params.pkl&apos;)</div></pre></td></tr></table></figure>
<h4 id="3、初始化"><a href="#3、初始化" class="headerlink" title="3、初始化"></a>3、初始化</h4><p>进行三步曲中的第三步，将取到的权值，对应的放到新模型中: 首先我们创建新模型，并且获取新模型的参数字典 net_state_dict: </p>
<p>net=Net()# 创建net</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">net_state_dict = net.state_dict() # 获取已创建 net 的 state_dict</div></pre></td></tr></table></figure>
<p>接着将 pretrained_dict 里不属于 net_state_dict 的键<strong>剔除掉</strong>: </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pretrained_dict_1 = &#123;k: v for k, v in pretrained_dict.items() if k in net_state_dict&#125;</div></pre></td></tr></table></figure>
<p>然后，用预训练模型的参数字典 对 新模型的参数字典 net_state_dict 进行更新: </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">net_state_dict.update(pretrained_dict_1)</div></pre></td></tr></table></figure>
<p>最后，将更新了参数的字典 “放”回到网络中: </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">net.load_state_dict(net_state_dict)</div></pre></td></tr></table></figure>
<p>采用 finetune 的训练过程中，<strong>有时候希望前面层的学习率低一些，改变不要太大，而 后面的全连接层的学习率相对大一些</strong>。这时就需要对不同的层设置不同的学习率，下面就 介绍如何为不同层配置不同的学习率。 </p>
<h3 id="2-3-2-不同层设置不同学习率"><a href="#2-3-2-不同层设置不同学习率" class="headerlink" title="2.3.2 不同层设置不同学习率"></a>2.3.2 不同层设置不同学习率</h3><p>在利用 pre-trained model 的参数做初始化之后，我们可能想让 fc 层更新相对快一些，而希望前面的权值更新小一些，这就可以通过为不同的层设置不同的学习率来达到此目的。</p>
<p>为不同层设置不同的学习率，主要通过优化器对多个参数组进行设置不同的参数。所以，只需要将原始的参数组，划分成两个，甚至更多的参数组，然后分别进行设置学习率。</p>
<p>这里将原始参数“切分”成 fc3 层参数和其余参数，为 fc3 层设置更大的学习率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 返回的是 parameters 的 内存地址 </span></div><div class="line"><span class="comment"># 将fc3层的参数从原始参数net.parameters中剥离出来。</span></div><div class="line"><span class="comment"># base_params是剥离了fc3层参数的其他参数。</span></div><div class="line">ignored_params = list(map(id, net.fc3.parameters())) </div><div class="line">base_params = filter(<span class="keyword">lambda</span> p: id(p) <span class="keyword">not</span> <span class="keyword">in</span> 	ignored_params, net.parameters())</div><div class="line"><span class="comment"># 然后优化器中为fc3的单独设置学习率</span></div><div class="line">optimizer = optim.SGD([</div><div class="line">  &#123;<span class="string">'params'</span>: base_params&#125;,</div><div class="line">	&#123;<span class="string">'params'</span>: net.fc3.parameters(), <span class="string">'lr'</span>: <span class="number">0.001</span>*<span class="number">10</span>&#125;], <span class="number">0.001</span>, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">1e-4</span>)</div><div class="line">])</div></pre></td></tr></table></figure>
<p>完整代码在<code>/Code/2_model/2_finetune.py</code></p>
<h1 id="第三章-损失函数和优化器"><a href="#第三章-损失函数和优化器" class="headerlink" title="第三章 损失函数和优化器"></a>第三章 损失函数和优化器</h1><p>Pytorch中十七个损失函数，十个优化器和六个学习率调整方法。</p>
<h2 id="3-1-十七个损失函数"><a href="#3-1-十七个损失函数" class="headerlink" title="3.1 十七个损失函数"></a>3.1 十七个损失函数</h2><p>我们所说的优化，即优化网络权值使得损失函数值变小。但是，损失函数值变小是否能代表模型的分类/回归精度变高呢?那么多种损失函数，应该如何选择呢?请来了解PyTorch 中给出的十七种损失函数吧。</p>
<h3 id="3-1-1-L1loss"><a href="#3-1-1-L1loss" class="headerlink" title="3.1.1 L1loss"></a>3.1.1 L1loss</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">class torch.nn.L1Loss(size_average=None, reduce=None)</div></pre></td></tr></table></figure>
<p>官方文档中仍有 reduction=’elementwise_mean’参数，但代码实现中已经删除该参数</p>
<p><strong>功能：</strong></p>
<p>计算output和target之差的绝对值，可选返回同维度的tensor或一个标量。</p>
<p><strong>公式：</strong></p>
<p><img src="/2019/06/14/深度学习笔记/Pytorch笔记/完整的一次任务/pic/image-20190616085944731.png" alt="image-20190616085944731"></p>
<p><strong>参数:</strong> </p>
<p>reduce(bool)- 返回值是否为标量，默认为 True<br>size_average(bool)- 当 reduce=True 时有效。为 True 时，返回的 loss 为平均值;为 False 时，返回的各样本的 loss 之和。<br><strong>实例:</strong> </p>
<p><code>/Code/3_optimizer/3_1_lossFunction/1_L1Loss.py</code></p>
<h1 id="第四章-监控模型-可视化"><a href="#第四章-监控模型-可视化" class="headerlink" title="第四章 监控模型-可视化"></a>第四章 监控模型-可视化</h1><h2 id="4-1-TensorBoardX"><a href="#4-1-TensorBoardX" class="headerlink" title="4.1 TensorBoardX"></a>4.1 TensorBoardX</h2><p>流行的有两种方法，本文重点介绍第二种。</p>
<p>1、构建Logger类</p>
<p>Logger 类中“包”了 tf.summary.FileWriter ，截至目前(2018.10.17)，只有三种操作，分别是 scalar_summary(), image_summary(), histo_summary()。</p>
<p>优点:轻便，可满足大部分需求</p>
<p>参考github：<a href="https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/04-utils/tensorboard" target="_blank" rel="noopener">https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/04-utils/tensorboard</a></p>
<p>2、借助TensorBoardX包</p>
<p>TensorBoardX 包的功能就比较全，截至目前(2018.10.17)，支持除<br>beholder 之外的所有 tensorboard 的记录类型。</p>
<p>github：<a href="https://github.com/lanpa/tensorboardX" target="_blank" rel="noopener">https://github.com/lanpa/tensorboardX</a></p>
<p>API文档：<a href="https://tensorboard-pytorch.readthedocs.io/en/latest/tutorial_zh.html" target="_blank" rel="noopener">https://tensorboard-pytorch.readthedocs.io/en/latest/tutorial_zh.html</a></p>
<p><strong>代码实现:</strong></p>
<p>tensorboardX 提供 13 个函数，可以记录标量、图像、语音、文字等等，功能十分丰富。<br>本节将对这些函数进行介绍，所用代码为 tensorboardX 的官方 demo.py，放在：</p>
<p><code>/Code/4_viewer/1_tensorboardX_demo.py</code></p>
<p>运行该文件，再打开一个terminal，进入/Result/，执行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tensorboard --logdir=runs</div></pre></td></tr></table></figure>
<p>然后浏览器打开</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">localhost:6006</div></pre></td></tr></table></figure>
<p>可以看到显示界面如下：</p>
<h2 id="4-2-TensorBoardX的函数"><a href="#4-2-TensorBoardX的函数" class="headerlink" title="4.2 TensorBoardX的函数"></a>4.2 TensorBoardX的函数</h2><h3 id="4-2-1-add-scalar"><a href="#4-2-1-add-scalar" class="headerlink" title="4.2.1 add_scalar()"></a>4.2.1 add_scalar()</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">add_scalar(tag, scalar_value, global_step=None, walltime=None)</div></pre></td></tr></table></figure>
<p><strong>功能：</strong></p>
<p>在一个图表中记录一个标量的变化，常用于Loss和Accuracy曲线的记录。</p>
<p><strong>参数:</strong> </p>
<p>tag(string)- 该图的标签，类似于 polt.title。 </p>
<p>scalar_value(float or string/blobname)- 用于存储的值，曲线图的 y 坐标 </p>
<p>global_step(int)- 曲线图的 x 坐标<br> walltime(float)- 为 event 文件的文件名设置时间，默认为 time.time() 运行 demo 中的: </p>
<p>用 github 首页 demo 运行这一行:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">writer.add_scalar(&apos;data/scalar1&apos;, dummy_s1[0], n_iter)</div></pre></td></tr></table></figure>
<p> 可以得到下图: </p>

      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/06/12/深度学习笔记/Pytorch训练营/D4 数据读取、数据扩增/" rel="next" title="">
                <i class="fa fa-chevron-left"></i> 
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/06/14/机器学习和深度学习算法理论/CNN/一些概念/" rel="prev" title="">
                 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Inhaltsverzeichnis
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Übersicht
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Schwimmer</p>
              <div class="site-description motion-element" itemprop="description">Record and Think!</div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">315</span>
                    <span class="site-state-item-name">Artikel</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">17</span>
                    <span class="site-state-item-name">Kategorien</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">55</span>
                    <span class="site-state-item-name">schlagwörter</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#第一章-数据"><span class="nav-number">1.</span> <span class="nav-text">第一章 数据</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-Cifar10-转-png"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 Cifar10 转 png</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#读取并保存为图片"><span class="nav-number">1.1.1.</span> <span class="nav-text">读取并保存为图片</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-训练集、验证集、测试集的划分"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 训练集、验证集、测试集的划分</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-Pytorch读图片数据集"><span class="nav-number">1.3.</span> <span class="nav-text">1.3 Pytorch读图片数据集</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Dataset类"><span class="nav-number">1.3.1.</span> <span class="nav-text">Dataset类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1、制作图片索引"><span class="nav-number">1.3.2.</span> <span class="nav-text">1、制作图片索引</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2、构建Dataset子类"><span class="nav-number">1.3.3.</span> <span class="nav-text">2、构建Dataset子类</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-4-DataLoder加载图片"><span class="nav-number">1.4.</span> <span class="nav-text">1.4 DataLoder加载图片</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-5-数据增强和数据标准化"><span class="nav-number">1.5.</span> <span class="nav-text">1.5 数据增强和数据标准化</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第二章-模型"><span class="nav-number">2.</span> <span class="nav-text">第二章 模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-模型的搭建"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 模型的搭建</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-1-模型定义的三要素"><span class="nav-number">2.1.1.</span> <span class="nav-text">2.1.1 模型定义的三要素</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-2-一个更复杂的模型"><span class="nav-number">2.1.2.</span> <span class="nav-text">2.1.2 一个更复杂的模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-3-nn-Sequential"><span class="nav-number">2.1.3.</span> <span class="nav-text">2.1.3 nn.Sequential</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-权值初始化的十种方法"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 权值初始化的十种方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-1-权重初始化流程"><span class="nav-number">2.2.1.</span> <span class="nav-text">2.2.1 权重初始化流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-2-常用初始化方法"><span class="nav-number">2.2.2.</span> <span class="nav-text">2.2.2 常用初始化方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1、Xavier均匀分布"><span class="nav-number">2.2.2.1.</span> <span class="nav-text">1、Xavier均匀分布</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2、Xavier正态分布"><span class="nav-number">2.2.2.2.</span> <span class="nav-text">2、Xavier正态分布</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3、kaiming均匀分布"><span class="nav-number">2.2.2.3.</span> <span class="nav-text">3、kaiming均匀分布</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-模型Finetune"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 模型Finetune</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-1-权值初始化"><span class="nav-number">2.3.1.</span> <span class="nav-text">2.3.1 权值初始化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1、保存模型参数"><span class="nav-number">2.3.1.1.</span> <span class="nav-text">1、保存模型参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2、加载模型"><span class="nav-number">2.3.1.2.</span> <span class="nav-text">2、加载模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3、初始化"><span class="nav-number">2.3.1.3.</span> <span class="nav-text">3、初始化</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-2-不同层设置不同学习率"><span class="nav-number">2.3.2.</span> <span class="nav-text">2.3.2 不同层设置不同学习率</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第三章-损失函数和优化器"><span class="nav-number">3.</span> <span class="nav-text">第三章 损失函数和优化器</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-十七个损失函数"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 十七个损失函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-1-L1loss"><span class="nav-number">3.1.1.</span> <span class="nav-text">3.1.1 L1loss</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第四章-监控模型-可视化"><span class="nav-number">4.</span> <span class="nav-text">第四章 监控模型-可视化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-TensorBoardX"><span class="nav-number">4.1.</span> <span class="nav-text">4.1 TensorBoardX</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-TensorBoardX的函数"><span class="nav-number">4.2.</span> <span class="nav-text">4.2 TensorBoardX的函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-1-add-scalar"><span class="nav-number">4.2.1.</span> <span class="nav-text">4.2.1 add_scalar()</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Schwimmer</span>

  

  
</div>


  <div class="powered-by">Erstellt mit  <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.5.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Design – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> v7.2.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/utils.js?v=7.2.0"></script>

  <script src="/js/motion.js?v=7.2.0"></script>



  
  


  <script src="/js/schemes/muse.js?v=7.2.0"></script>




  
  <script src="/js/scrollspy.js?v=7.2.0"></script>
<script src="/js/post-details.js?v=7.2.0"></script>



  


  <script src="/js/next-boot.js?v=7.2.0"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


  

  

  

  


  


  




  

  

  

  

  

  

  

  

  

  

  

  

  

  

</body>
</html>
