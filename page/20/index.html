<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222"/>






















<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0"/>

<link rel="stylesheet" href="/css/main.css?v=7.2.0"/>


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.2.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.2.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '7.2.0',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="Record and Think!">
<meta property="og:type" content="website">
<meta property="og:title" content="Schwimmer&#39;s Blog">
<meta property="og:url" content="https://schwimmer.github.io/page/20/index.html">
<meta property="og:site_name" content="Schwimmer&#39;s Blog">
<meta property="og:description" content="Record and Think!">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Schwimmer&#39;s Blog">
<meta name="twitter:description" content="Record and Think!">





  
  
  <link rel="canonical" href="https://schwimmer.github.io/page/20/"/>



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Schwimmer's Blog</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Schwimmer's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Navigationsleiste an/ausschalten">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br/>Startseite</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br/>Schlagwörter</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br/>Kategorien</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br/>Archiv</a>

  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2018/04/09/工具和环境/idea技巧/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/04/09/工具和环境/idea技巧/" class="post-title-link" itemprop="url">Unbenannt</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2018-04-09 23:04:41" itemprop="dateCreated datePublished" datetime="2018-04-09T23:04:41+08:00">2018-04-09</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Bearbeitet am</span>
                
                <time title="Geändert am: 2019-02-13 14:00:10" itemprop="dateModified" datetime="2019-02-13T14:00:10+08:00">2019-02-13</time>
              </span>
            
          

          

          
            
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="cannot-resolve-symbol"><a href="#cannot-resolve-symbol" class="headerlink" title="cannot resolve symbol"></a>cannot resolve symbol</h1><p>用idea跑spark的单机项目，总是提示</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cannot resolve symbol apache</div></pre></td></tr></table></figure>
<p>但maven的.m2目录下是有这个文件夹和jar包的，试过网上各种办法，包括</p>
<p>点击File | Invalidate Caches，清理了缓存重启IDEA</p>
<p>最后结果是靠</p>
<p>项目右键-&gt;maven -&gt; Reimport</p>
<h1 id="go-to-class"><a href="#go-to-class" class="headerlink" title="go to class"></a>go to class</h1><p>command+o</p>
<h1 id="代码片段快捷键"><a href="#代码片段快捷键" class="headerlink" title="代码片段快捷键"></a>代码片段快捷键</h1><p><a href="https://blog.csdn.net/tiantiandjava/article/details/42269173" target="_blank" rel="noopener">https://blog.csdn.net/tiantiandjava/article/details/42269173</a></p>
<p>更多的提示可以CTRL + j 可以查看，mac系统下是command＋j。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2018/04/08/hadoop-spark/spark/使用spark DStream的foreachRDD时要注意哪些坑/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/04/08/hadoop-spark/spark/使用spark DStream的foreachRDD时要注意哪些坑/" class="post-title-link" itemprop="url">使用spark DStream的foreachRDD时要注意哪些坑</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2018-04-08 11:49:59 / Geändert am: 11:52:28" itemprop="dateCreated datePublished" datetime="2018-04-08T11:49:59+08:00">2018-04-08</time>
            </span>
          

          
            

            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">in</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/spark/" itemprop="url" rel="index"><span itemprop="name">spark</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>转自<a href="https://www.cnblogs.com/realzjx/p/5853094.html" target="_blank" rel="noopener">使用spark DStream的foreachRDD时要注意哪些坑？</a></p>
<p>答案： 两个坑， 性能坑和线程坑</p>
<p>DStream是抽象类，它把连续的数据流拆成很多的小RDD数据块， 这叫做“微批次”， spark的流式处理， 都是“微批次处理”。 DStream内部实现上有批次处理时间间隔，滑动窗口等机制来保证每个微批次的时间间隔里， 数据流以RDD的形式发送给spark做进一步处理。因此， 在一个为批次的处理时间间隔里， DStream只产生一个RDD。 </p>
<p>可以利用dstream.foreachRDD把数据发送给外部系统。 但是想要正确地， 有效率的使用它， 必须理解一下背后的机制。通常向外部系统写数据需要一个Connection对象（通过它与外部服务器交互）。程序员可能会想当然地在spark上创建一个connection对象， 然后在spark线程里用这个对象来存RDD。比如下面的程序：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">dstream.foreachRDD &#123; rdd =&gt;</div><div class="line">  val connection = createNewConnection()  // executed at the driver</div><div class="line">  rdd.foreach &#123; record =&gt;</div><div class="line">    connection.send(record) // executed at the worker</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个代码会产生执行错误， 因为rdd是分布式存储的，它是一个数据结构，它是一组指向集群数据的指针， <strong>rdd.foreach会在集群里的不同机器上创建spark工作线程， 而connection对象则不会在集群里的各个机器之间传递， 所以有些spark工作线程就会产生connection对象没有被初始化的执行错误</strong>。 解决的办法可以是在spark worker里为每一个worker创建一个connection对象， 但是如果你这么做， 程序要为每一条record创建一次connection，显然效率和性能都非常差。</p>
<p>另一种改进方法是<strong>为每个spark分区创建一个connection对象，同时维护一个全局的静态的连接池对象</strong>， 这样就可以最好的复用connection。 另外需要注意： 虽然有多个connection对象， 但在同一时间只有一个connection.send(record)执行， 因为在同一个时间里， 只有 一个微批次的RDD产生出来。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">dstream.foreachRDD &#123; rdd =&gt;</div><div class="line">  rdd.foreachPartition &#123; partitionOfRecords =&gt;</div><div class="line">    // ConnectionPool is a static, lazily initialized pool of connections</div><div class="line">    val connection = ConnectionPool.getConnection()</div><div class="line">    partitionOfRecords.foreach(record =&gt; connection.send(record))</div><div class="line">    ConnectionPool.returnConnection(connection)  // return to the pool for future reuse</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>有人问了个问题，为什么foreachRDD里有两层嵌套的foreach? 为什么dstream.foreachRDD里还要再套一层rdd.foreach</p>
<p>可以这么理解, DStream.foreachRDD 是一个输出操作符，它返回的不是RDD里的一行数据， 而是输出DStream后面的RDD,在一个时间间隔里， 只返回一个RDD的“微批次”， 为了访问这个“微批次”RDD里的数据， 我们还需要在RDD数据对象上做进一步操作.。 参考下面的代码实例， 更容易理解。</p>
<p>给顶一个 RDD [Security, Prices]数据结构</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">dstream.foreachRDD &#123; pricesRDD =&gt;  // Loop over RDD</div><div class="line"></div><div class="line">  val x= pricesRDD.count</div><div class="line"></div><div class="line">  if (x &gt; 0)  // RDD has data</div><div class="line"></div><div class="line">  &#123;</div><div class="line"></div><div class="line">    for(line &lt;- pricesRDD.collect.toArray) // Look for each record in the RDD</div><div class="line"></div><div class="line">    &#123;</div><div class="line"></div><div class="line">      var index = line._2.split(&apos;,&apos;).view(0).toInt          // That is the index</div><div class="line"></div><div class="line">      var timestamp = line._2.split(&apos;,&apos;).view(1).toString   // This is the timestamp from source</div><div class="line"></div><div class="line">      var security =  line._2.split(&apos;,&apos;).view(12.toString   // This is the name of the security</div><div class="line"></div><div class="line">      var price = line._2.split(&apos;,&apos;).view(3).toFloat        // This is the price of the security</div><div class="line"></div><div class="line">      if (price.toFloat &gt; 90.0)</div><div class="line"></div><div class="line">      &#123;</div><div class="line"></div><div class="line">       // Do something here</div><div class="line"></div><div class="line">       // Sent notification, write to HDFS etc</div><div class="line"></div><div class="line">      &#125;</div><div class="line"></div><div class="line">    &#125;</div><div class="line"></div><div class="line">  &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2018/04/07/hadoop-spark/spark/spark问题集/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/04/07/hadoop-spark/spark/spark问题集/" class="post-title-link" itemprop="url">Spark问题集</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2018-04-07 22:21:45" itemprop="dateCreated datePublished" datetime="2018-04-07T22:21:45+08:00">2018-04-07</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Bearbeitet am</span>
                
                <time title="Geändert am: 2019-07-03 11:25:05" itemprop="dateModified" datetime="2019-07-03T11:25:05+08:00">2019-07-03</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">in</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/spark/" itemprop="url" rel="index"><span itemprop="name">spark</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="读本地文件报错"><a href="#读本地文件报错" class="headerlink" title="读本地文件报错"></a>读本地文件报错</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">Exception in thread &quot;main&quot; java.lang.VerifyError: class com.fasterxml.jackson.module.scala.ser.ScalaIteratorSerializer overrides final method withResolved.(Lcom/fasterxml/jackson/databind/BeanProperty;Lcom/fasterxml/jackson/databind/jsontype/TypeSerializer;Lcom/fasterxml/jackson/databind/JsonSerializer;)Lcom/fasterxml/jackson/databind/ser/std/AsArraySerializerBase;</div><div class="line">	at org.apache.spark.rdd.RDDOperationScope$.&lt;init&gt;(RDDOperationScope.scala:81)</div><div class="line">	at org.apache.spark.rdd.RDDOperationScope$.&lt;clinit&gt;(RDDOperationScope.scala)</div><div class="line">	at org.apache.spark.SparkContext.withScope(SparkContext.scala:714)</div><div class="line">	at org.apache.spark.SparkContext.textFile(SparkContext.scala:830)</div><div class="line">	at com.iclick.word_segmentation.WordTest$.main(WordTest.scala:40)</div><div class="line">	at com.iclick.word_segmentation.WordTest.main(WordTest.scala)</div></pre></td></tr></table></figure>
<p>在spark版本是</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&lt;dependency&gt;</div><div class="line">			&lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</div><div class="line">			&lt;artifactId&gt;spark-sql_2.10&lt;/artifactId&gt;</div><div class="line">			&lt;version&gt;1.6.1&lt;/version&gt;</div><div class="line">		&lt;/dependency&gt;</div></pre></td></tr></table></figure>
<p>必须要要需要引入jackson的scala版本，这里的jackson-module-scala_2.10原来我用的2.4.4还依旧报错，改为2.7.3就OK了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">&lt;dependency&gt;</div><div class="line">			&lt;groupId&gt;com.fasterxml.jackson.module&lt;/groupId&gt;</div><div class="line">			&lt;artifactId&gt;jackson-module-scala_2.10&lt;/artifactId&gt;</div><div class="line">			&lt;version&gt;2.7.3&lt;/version&gt;</div><div class="line">		&lt;/dependency&gt;</div><div class="line">		&lt;dependency&gt;</div><div class="line">            &lt;groupId&gt;com.fasterxml.jackson.module&lt;/groupId&gt;</div><div class="line">            &lt;artifactId&gt;jackson-module-jaxb-annotations&lt;/artifactId&gt;</div><div class="line">            &lt;version&gt;2.7.4&lt;/version&gt;</div><div class="line">        &lt;/dependency&gt;</div><div class="line">        &lt;dependency&gt;</div><div class="line">            &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;</div><div class="line">            &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;</div><div class="line">            &lt;version&gt;2.7.4&lt;/version&gt;</div><div class="line">        &lt;/dependency&gt;</div><div class="line">        &lt;dependency&gt;</div><div class="line">            &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;</div><div class="line">            &lt;artifactId&gt;jackson-annotations&lt;/artifactId&gt;</div><div class="line">            &lt;version&gt;2.7.4&lt;/version&gt;</div><div class="line">        &lt;/dependency&gt;</div></pre></td></tr></table></figure>
<p>报错</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Caused by: java.lang.UnsatisfiedLinkError: no snappyjava in java.library.path</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">SnappyError: [FAILED_TO_LOAD_NATIVE_LIBRARY] null</div></pre></td></tr></table></figure>
<p>maven引用的版本问题，一开始用1.6.0就报错，改成1.6.1就可以了</p>
<p>报错</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">java.net.BindException: Address already in use</div></pre></td></tr></table></figure>
<p>默认端口是4040，被占用了就会递增，几次都不行就报错，我是手动设置的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">conf.set(&quot;spark.ui.port&quot;,&quot;4049&quot;)</div></pre></td></tr></table></figure>
<p>也可以这样<a href="https://blog.csdn.net/a921122/article/details/45095845不过我没有成功" target="_blank" rel="noopener">https://blog.csdn.net/a921122/article/details/45095845不过我没有成功</a></p>
<p>报错</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Javax.servlet.FilterRegistration&quot;&apos;s signer information does not match signer information of other classes in the same package</div></pre></td></tr></table></figure>
<p>出现的原因是在spark项目里面引用了一个hive udf的jar包。</p>
<h1 id="一直在Accepted"><a href="#一直在Accepted" class="headerlink" title="一直在Accepted"></a>一直在Accepted</h1><p>一致处于</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">18/05/24 16:33:03 INFO yarn.Client: Application report for application_1524027932434_173978 (state: ACCEPTED)</div></pre></td></tr></table></figure>
<p>多次重复后任务失败，报错</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">18/05/24 16:40:08 INFO yarn.Client: </div><div class="line">	 client token: N/A</div><div class="line">	 diagnostics: Application application_1524027932434_173980 failed 2 times due to AM Container for appattempt_1524027932434_173980_000002 exited with  exitCode: 15</div><div class="line">For more detailed output, check application tracking page:http://hlt-data-171.mljr.com:8088/proxy/application_1524027932434_173980/Then, click on links to logs of each attempt.</div><div class="line">Diagnostics: Exception from container-launch.</div><div class="line">Container id: container_e79_1524027932434_173980_02_000001</div><div class="line">Exit code: 15</div><div class="line">Stack trace: ExitCodeException exitCode=15: </div><div class="line">	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601)</div><div class="line">	at org.apache.hadoop.util.Shell.run(Shell.java:504)</div><div class="line">	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786)</div><div class="line">	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213)</div><div class="line">	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)</div><div class="line">	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)</div><div class="line">	at java.util.concurrent.FutureTask.run(FutureTask.java:262)</div><div class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)</div><div class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)</div><div class="line">	at java.lang.Thread.run(Thread.java:745)</div></pre></td></tr></table></figure>
<p><a href="http://www.itkeyword.com/doc/4214879305995466x649/application-report-for-application-state-accepted-never-ends-for-spark-submi" target="_blank" rel="noopener">http://www.itkeyword.com/doc/4214879305995466x649/application-report-for-application-state-accepted-never-ends-for-spark-submi</a></p>
<p>可以尝试，用yarn-cluster代替yarn或yarn-client</p>
<p>或者减小申请的内存，可能此时资源不够</p>
<h1 id="执行包括hive的任务是OOM"><a href="#执行包括hive的任务是OOM" class="headerlink" title="执行包括hive的任务是OOM"></a>执行包括hive的任务是OOM</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">java.lang.OutOfMemoryError: PermGen space</div></pre></td></tr></table></figure>
<p><a href="https://blog.csdn.net/xiao_jun_0820/article/details/45038205" target="_blank" rel="noopener">https://blog.csdn.net/xiao_jun_0820/article/details/45038205</a></p>
<pre><code>在Spark中使用hql方法执行hive语句时，由于其在查询过程中调用的是Hive的获取元数据信息、SQL解析，并且使用Cglib等进行序列化反序列化，中间可能产生较多的class文件，导致JVM中的持久代使用较多，如果配置不当，可能引起OOM问题。
</code></pre><p>原因是实际使用时，如果用的是JDK1.6版本，Server模式的持久代默认大小是64M，Client模式的持久代默认大小是32M，而Driver端进行SQL处理时，其持久代的使用可能会达到90M，导致OOM溢出，任务失败。</p>
<p>解决方法就是在Spark的conf目录中的spark-defaults.conf里，增加对Driver的JVM配置，因为Driver才负责SQL的解析和元数据获取。</p>
<p>因此增加了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">--driver-java-options &quot;-Xms2048m -Xmx2048m -XX:PermSize=1024M -XX:MaxPermSize=1024M -Xss10m&quot;</div></pre></td></tr></table></figure>
<h1 id="spark提交后task数量过多"><a href="#spark提交后task数量过多" class="headerlink" title="spark提交后task数量过多"></a>spark提交后task数量过多</h1><h2 id="num-executors"><a href="#num-executors" class="headerlink" title="num-executors"></a><strong>num-executors</strong></h2><p>　　<strong>参数说明</strong>：该参数用于设置Spark作业总共要用多少个Executor进程来执行。Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。</p>
<p>　　<strong>参数调优建议</strong>：每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。</p>
<h2 id="executor-memory"><a href="#executor-memory" class="headerlink" title="executor-memory"></a><strong>executor-memory</strong></h2><p>　　<strong>参数说明</strong>：该参数用于设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。</p>
<p>　　<strong>参数调优建议</strong>：每个Executor进程的内存设置4G~8G较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少，num-executors乘以executor-memory，就代表了你的Spark作业申请到的总内存量（也就是所有Executor进程的内存总和），这个量是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，那么申请的总内存量最好不要超过资源队列最大总内存的1/3~1/2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。</p>
<h2 id="executor-cores"><a href="#executor-cores" class="headerlink" title="executor-cores"></a><strong>executor-cores</strong></h2><p>　　<strong>参数说明</strong>：该参数用于设置每个Executor进程的CPU core数量。这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。</p>
<p>　　<strong>参数调优建议</strong>：Executor的CPU core数量设置为2~4个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么num-executors * executor-cores不要超过队列总CPU core的1/3~1/2左右比较合适，也是避免影响其他同学的作业运行。</p>
<h2 id="spark-default-parallelism"><a href="#spark-default-parallelism" class="headerlink" title="spark.default.parallelism"></a><strong>spark.default.parallelism</strong></h2><p>　　<strong>参数说明</strong>：该参数用于设置每个stage的默认task数量。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。</p>
<p>　　<strong>参数调优建议</strong>：Spark作业的默认task数量为500~1000个较为合适。很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此Spark官网建议的设置原则是，设置该参数为num-executors * executor-cores的2~3倍较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。</p>
<h1 id="spark未序列化"><a href="#spark未序列化" class="headerlink" title="spark未序列化"></a>spark未序列化</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">org.apache.spark.SparkException: Task not serializable</div></pre></td></tr></table></figure>
<p><a href="https://blog.csdn.net/javastart/article/details/51206715" target="_blank" rel="noopener">https://blog.csdn.net/javastart/article/details/51206715</a></p>
<h1 id="spark读不了资源文件"><a href="#spark读不了资源文件" class="headerlink" title="spark读不了资源文件"></a>spark读不了资源文件</h1><p>错误的写法是</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">InputStream is = ScriptReader.class.getClassLoader().getResourceAsStream(</div><div class="line">                &quot;/gps.sql&quot;);</div></pre></td></tr></table></figure>
<p>正确的写法是</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">ScriptReader.class.getResourceAsStream(</div><div class="line">                &quot;/gps.sql&quot;);</div></pre></td></tr></table></figure>
<p>报错</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scala.collection.mutable.ArrayBuffer cannot be cast to scala.collection.mutable.ListBuffer</div></pre></td></tr></table></figure>
<h1 id="spark提交任务还没RUNNING就报错"><a href="#spark提交任务还没RUNNING就报错" class="headerlink" title="spark提交任务还没RUNNING就报错"></a>spark提交任务还没RUNNING就报错</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line">Exit code: 10</div><div class="line">Stack trace: ExitCodeException exitCode=10: </div><div class="line">	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601)</div><div class="line">	at org.apache.hadoop.util.Shell.run(Shell.java:504)</div><div class="line">	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786)</div><div class="line">	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213)</div><div class="line">	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)</div><div class="line">	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)</div><div class="line">	at java.util.concurrent.FutureTask.run(FutureTask.java:262)</div><div class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)</div><div class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)</div><div class="line">	at java.lang.Thread.run(Thread.java:745)</div><div class="line"></div><div class="line"></div><div class="line">Container exited with a non-zero exit code 10</div><div class="line">Failing this attempt. Failing the application.</div><div class="line">	 ApplicationMaster host: N/A</div><div class="line">	 ApplicationMaster RPC port: -1</div><div class="line">	 queue: root.infra.dmp</div><div class="line">	 start time: 1528194816590</div><div class="line">	 final status: FAILED</div><div class="line">	 tracking URL: http://hlt-data-171.mljr.com:8088/cluster/app/application_1524027932434_235300</div><div class="line">	 user: dmp</div><div class="line">Exception in thread &quot;main&quot; org.apache.spark.SparkException: Application application_1524027932434_235300 finished with failed status</div><div class="line">	at org.apache.spark.deploy.yarn.Client.run(Client.scala:1035)</div><div class="line">	at org.apache.spark.deploy.yarn.Client$.main(Client.scala:1082)</div><div class="line">	at org.apache.spark.deploy.yarn.Client.main(Client.scala)</div><div class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</div><div class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</div><div class="line">	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</div><div class="line">	at java.lang.reflect.Method.invoke(Method.java:498)</div><div class="line">	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)</div><div class="line">	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)</div><div class="line">	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)</div><div class="line">	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)</div><div class="line">	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">Application application_1524027932434_235300 failed 2 times due to AM Container for appattempt_1524027932434_235300_000002 exited with exitCode: 10</div><div class="line">For more detailed output, check application tracking page:http://hlt-data-171.mljr.com:8088/proxy/application_1524027932434_235300/Then, click on links to logs of each attempt.</div><div class="line">Diagnostics: Exception from container-launch.</div><div class="line">Container id: container_e79_1524027932434_235300_02_000003</div><div class="line">Exit code: 10</div><div class="line">Stack trace: ExitCodeException exitCode=10:</div><div class="line">at org.apache.hadoop.util.Shell.runCommand(Shell.java:601)</div><div class="line">at org.apache.hadoop.util.Shell.run(Shell.java:504)</div><div class="line">at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786)</div><div class="line">at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213)</div><div class="line">at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)</div><div class="line">at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)</div><div class="line">at java.util.concurrent.FutureTask.run(FutureTask.java:262)</div><div class="line">at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)</div><div class="line">at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)</div><div class="line">at java.lang.Thread.run(Thread.java:745)</div><div class="line">Container exited with a non-zero exit code 10</div><div class="line">Failing this attempt. Failing the application.</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">18/06/05 18:33:48 ERROR yarn.ApplicationMaster: Uncaught exception: </div><div class="line">java.lang.ClassNotFoundException: com.mljr.spark.gps.sample.GPSMetricInfo</div><div class="line">	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)</div><div class="line">	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)</div><div class="line">	at java.security.AccessController.doPrivileged(Native Method)</div><div class="line">	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)</div><div class="line">	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)</div><div class="line">	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)</div><div class="line">	at org.apache.spark.deploy.yarn.ApplicationMaster.startUserApplication(ApplicationMaster.scala:536)</div><div class="line">	at org.apache.spark.deploy.yarn.ApplicationMaster.runDriver(ApplicationMaster.scala:319)</div><div class="line">	at org.apache.spark.deploy.yarn.ApplicationMaster.run(ApplicationMaster.scala:185)</div><div class="line">	at org.apache.spark.deploy.yarn.ApplicationMaster$$anonfun$main$1.apply$mcV$sp(ApplicationMaster.scala:653)</div><div class="line">	at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:69)</div><div class="line">	at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:68)</div><div class="line">	at java.security.AccessController.doPrivileged(Native Method)</div><div class="line">	at javax.security.auth.Subject.doAs(Subject.java:415)</div><div class="line">	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)</div><div class="line">	at org.apache.spark.deploy.SparkHadoopUtil.runAsSparkUser(SparkHadoopUtil.scala:68)</div><div class="line">	at org.apache.spark.deploy.yarn.ApplicationMaster$.main(ApplicationMaster.scala:651)</div><div class="line">	at org.apache.spark.deploy.yarn.ApplicationMaster.main(ApplicationMaster.scala)</div><div class="line">18/06/05 18:33:48 INFO yarn.ApplicationMaster: Final app status: FAILED, exitCode: 10, (reason: Uncaught exception: java.lang.ClassNotFoundException: com.mljr.spark.gps.sample.GPSMetricInfo)</div><div class="line">18/06/05 18:33:49 INFO util.ShutdownHookManager: Shutdown hook called</div></pre></td></tr></table></figure>
<p>仅仅是main函数所在class的package变更，而提交任务时忘了修改</p>
<h1 id="Container-killed-on-request"><a href="#Container-killed-on-request" class="headerlink" title="Container killed on request"></a>Container killed on request</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e79_1524027932434_251117_02_000006 on host: hlt-data-195.mljr.com. Exit status: 137. Diagnostics: Container killed on request. Exit code is 137</div></pre></td></tr></table></figure>
<p>可能是内存设置太大。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Number of dynamic partitions created is 1381, which is more than 1000. To solve this try to set hive.exec.max.dynamic.partitions to at least 1381.</div><div class="line">	at org.apache.hadoop.hive.ql.metadata.Hive.loadDynamicPartitions(Hive.java:1604)</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hiveContext.sql(&quot;SET hive.exec.max.dynamic.partitions=1500&quot;)</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">User class threw exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 42 in stage 4.0 failed 4 times, most recent failure: Lost task 42.3 in stage 4.0 (TID 130, hlt-data-166.mljr.com, executor 12): java.lang.ArrayIndexOutOfBoundsException</div><div class="line">Driver stacktrace:</div></pre></td></tr></table></figure>
<p>最后一个日期字段没有加上</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">User class threw exception: java.lang.reflect.InvocationTargetException</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">Container marked as failed: container_e79_1524027932434_277197_01_000355 on host: hlt-data-166.mljr.com. Exit status: 50. Diagnostics: Exception from container-launch.</div><div class="line"></div><div class="line">ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e79_1524027932434_277197_01_000355 on host: hlt-data-166.mljr.com. Exit status: 50. Diagnostics: Exception from container-launch.</div><div class="line">Container id: container_e79_1524027932434_277197_01_000355</div><div class="line">Exit code: 50</div><div class="line">Stack trace: ExitCodeException exitCode=50: </div><div class="line">	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601)</div><div class="line">	at org.apache.hadoop.util.Shell.run(Shell.java:504)</div><div class="line">	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786)</div><div class="line">	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213)</div><div class="line">	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)</div><div class="line">	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)</div><div class="line">	at java.util.concurrent.FutureTask.run(FutureTask.java:262)</div><div class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)</div><div class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)</div><div class="line">	at java.lang.Thread.run(Thread.java:745)</div></pre></td></tr></table></figure>
<p>虽然Executor上有这个报错，但是不影响任务运行</p>
<h1 id="空指针错误"><a href="#空指针错误" class="headerlink" title="空指针错误"></a>空指针错误</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">User class threw exception: java.lang.NullPointerException</div></pre></td></tr></table></figure>
<p>没有定位到具体的行，往往是</p>
<h1 id="no-snappyjava"><a href="#no-snappyjava" class="headerlink" title="no snappyjava"></a>no snappyjava</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">no snappyjava in java.library.path</div></pre></td></tr></table></figure>
<p>参考<a href="https://blog.csdn.net/fansy1990/article/details/53954347" target="_blank" rel="noopener">https://blog.csdn.net/fansy1990/article/details/53954347</a></p>
<p>1、输出maven依赖包，找到snappy的jar包，如<code>snappy-java-1.0.4.1.jar</code></p>
<p>2、<code>unzip snappy.java.jar</code>解压后，找到<code>libsnappyjava.jnilib</code>，重命名为<code>libsnappyjava.dylib</code>，复制文件所在目录</p>
<p>3、配置idea的项目环境，在VM options加上</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">-Djava.library.path=/Users/david/david/tmp</div></pre></td></tr></table></figure>
<p>后面路径就是<code>libsnappyjava</code>的路径</p>
<h2 id="bad-symbolic-reference"><a href="#bad-symbolic-reference" class="headerlink" title="bad symbolic reference"></a>bad symbolic reference</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">Error:scalac: Error: bad symbolic reference. A signature in Vectors.class refers to term linalg</div><div class="line">in value breeze which is not available.</div><div class="line">It may be completely missing from the current classpath, or the version on</div><div class="line">the classpath might be incompatible with the version used when compiling Vectors.class.</div><div class="line">scala.reflect.internal.Types$TypeError: bad symbolic reference. A signature in Vectors.class refers to term linalg</div><div class="line">in value breeze which is not available.</div><div class="line">It may be completely missing from the current classpath, or the version on</div><div class="line">the classpath might be incompatible with the version used when compiling Vectors.class.</div><div class="line">	at scala.reflect.internal.pickling.UnPickler$Scan.toTypeError(UnPickler.scala:847)</div><div class="line">	at scala.reflect.internal.pickling.UnPickler$Scan$LazyTypeRef.complete(UnPickler.scala:854)</div><div class="line">	at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1231)</div><div class="line">	at scala.reflect.internal.Symbols$Symbol.tpe(Symbols.scala:1202)</div><div class="line">	at scala.tools.nsc.transform.SpecializeTypes$$a Compiler.scala:111)</div><div class="line">	at sbt.internal.inc.AnalyzingCompiler.compile(AnalyzingCompiler.scala:90)</div><div class="line">	at org.jetbrains.jps.incremental.scala.local.IdeaIncrementalCompiler.compile(IdeaIncrementalCompiler.scala:40)</div><div class="line">	at org.jetbrains.jps.incremental.scala.local.LocalServer.compile(LocalServer.scala:30)</div><div class="line">	at org.jetbrains.jps.incremental.scala.remote.Main$.make(Main.scala:68)</div><div class="line">	at org.jetbrains.jps.incremental.scala.remote.Main$.nailMain(Main.scala:25)</div><div class="line">	at org.jetbrains.jps.incremental.scala.remote.Main.nailMain(Main.scala)</div><div class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</div><div class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</div><div class="line">	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</div><div class="line">	at java.lang.reflect.Method.invoke(Method.java:498)</div><div class="line">	at com.martiansoftware.nailgun.NGSession.run(NGSession.java:319)</div></pre></td></tr></table></figure>
<p>尚未找到具体原因，把POM文件覆盖过去后就不会报错了。</p>
<h2 id="mapValue后不能serializable"><a href="#mapValue后不能serializable" class="headerlink" title="mapValue后不能serializable"></a>mapValue后不能serializable</h2><p><a href="https://stackoverflow.com/questions/32900862/map-can-not-be-serializable-in-scala" target="_blank" rel="noopener">https://stackoverflow.com/questions/32900862/map-can-not-be-serializable-in-scala</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">rdd.groupBy(_.segment).mapValues(v =&gt; ...).map(identity)</div></pre></td></tr></table></figure>
<h1 id="UDF：Schema-for-type-Any-is-not-supported"><a href="#UDF：Schema-for-type-Any-is-not-supported" class="headerlink" title="UDF：Schema for type Any is not supported"></a>UDF：Schema for type Any is not supported</h1><p>UDF的返回值不能是any</p>
<h1 id="插入表报错ArrayIndexOutOfBoundsException"><a href="#插入表报错ArrayIndexOutOfBoundsException" class="headerlink" title="插入表报错ArrayIndexOutOfBoundsException"></a>插入表报错ArrayIndexOutOfBoundsException</h1><p>在create表的时候报错</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">val hiveSql = &quot;create table carthage_dev.gps_feature_info_wx2 as select * from tmp_feature_info_days&quot;</div><div class="line"></div><div class="line">    hiveContext.sql(&quot;use carthage&quot;)</div><div class="line">    hiveContext.sql(&quot;SET mapreduce.job.reduces=40&quot;)</div><div class="line">    hiveContext.createDataFrame(gpsFeatureInfoRDD, schema).registerTempTable(tempTableName)</div><div class="line">    hiveContext.sql(&quot;drop table if exists carthage_dev.gps_feature_info_wx2&quot;)</div><div class="line">    hiveContext.sql(hiveSql)</div></pre></td></tr></table></figure>
<p>执行到<code>hiveContext.sql(hiveSql)</code>时，报错</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">19/07/03 10:55:25 ERROR yarn.ApplicationMaster: User class threw exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 5.0 failed 4 times, most recent failure: Lost task 16.3 in stage 5.0 (TID 4136, hlt-data-186.mljr.com, executor 20): java.lang.ArrayIndexOutOfBoundsException</div></pre></td></tr></table></figure>
<blockquote>
<p> 解决</p>
</blockquote>
<p>不一定是插入表的问题，要细到stage中看，比如在这里看到</p>
<p><img src="/2018/04/07/hadoop-spark/spark/spark问题集/pic/image-20190703112452654.png" alt="image-20190703112452654"></p>
<p>实际上是之前逻辑的问题</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2018/04/07/hadoop-spark/spark/map-flatMap-mapPartitions/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/04/07/hadoop-spark/spark/map-flatMap-mapPartitions/" class="post-title-link" itemprop="url">map-flatMap-mapPartitions</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2018-04-07 22:17:17" itemprop="dateCreated datePublished" datetime="2018-04-07T22:17:17+08:00">2018-04-07</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Bearbeitet am</span>
                
                <time title="Geändert am: 2018-04-08 11:40:46" itemprop="dateModified" datetime="2018-04-08T11:40:46+08:00">2018-04-08</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">in</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/spark/" itemprop="url" rel="index"><span itemprop="name">spark</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>用map实现</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.&#123; <span class="type">SparkConf</span>, <span class="type">SparkContext</span> &#125;</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">DoubleNum</span> </span>&#123;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">mapDoubleFunc</span></span>(a: <span class="type">Int</span>): (<span class="type">Int</span>, <span class="type">Int</span>) = &#123;</div><div class="line">    (a, a * <span class="number">2</span>)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"Samples"</span>).setMaster(<span class="string">"local[1]"</span>).</div><div class="line">      set(<span class="string">"spark.sql.shuffle.partitions"</span>, <span class="string">"1"</span>).set(<span class="string">"spark.network.timeout"</span>, <span class="string">"30s"</span>)</div><div class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</div><div class="line">    sc.setLogLevel(<span class="string">"WARN"</span>)</div><div class="line"></div><div class="line">    <span class="keyword">val</span> a = sc.parallelize(<span class="number">1</span> to <span class="number">9</span>, <span class="number">3</span>)</div><div class="line"></div><div class="line">    <span class="keyword">val</span> mapResult = a.map(mapDoubleFunc)</div><div class="line"></div><div class="line">    println(mapResult.collect().mkString)</div><div class="line"></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>如果这里改成flatMap？</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"></div></pre></td></tr></table></figure>
<p>用mapPartitions实现</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> a = sc.parallelize(<span class="number">1</span> to <span class="number">9</span>, <span class="number">3</span>)</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">doubleFunc</span></span>(iter: <span class="type">Iterator</span>[<span class="type">Int</span>]) : <span class="type">Iterator</span>[(<span class="type">Int</span>,<span class="type">Int</span>)] = &#123;</div><div class="line">    <span class="keyword">var</span> res = <span class="type">List</span>[(<span class="type">Int</span>,<span class="type">Int</span>)]()</div><div class="line">    <span class="keyword">while</span> (iter.hasNext)</div><div class="line">    &#123;</div><div class="line">      <span class="keyword">val</span> cur = iter.next;</div><div class="line">      res .::= (cur,cur*<span class="number">2</span>)</div><div class="line">    &#125;</div><div class="line">    res.iterator</div><div class="line">  &#125;</div><div class="line"><span class="keyword">val</span> result = a.mapPartitions(doubleFunc)</div><div class="line">println(result.collect().mkString)</div></pre></td></tr></table></figure>
<p>写成匿名函数</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> result = a.mapPartitions(iter =&gt; &#123;</div><div class="line">      <span class="keyword">var</span> res = <span class="type">List</span>[(<span class="type">Int</span>, <span class="type">Int</span>)]()</div><div class="line">      <span class="keyword">while</span> (iter.hasNext) &#123;</div><div class="line">        <span class="keyword">val</span> cur = iter.next</div><div class="line">        res.::=(cur, cur * <span class="number">2</span>)</div><div class="line">      &#125;</div><div class="line">      res.iterator</div><div class="line">    &#125;)</div></pre></td></tr></table></figure>
<p>也可以声明iter的数据类型<code>(iter: Iterator[Int])</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> result = a.mapPartitions( (iter: <span class="type">Iterator</span>[<span class="type">Int</span>]) =&gt; &#123;</div><div class="line">      <span class="keyword">var</span> res = <span class="type">List</span>[(<span class="type">Int</span>, <span class="type">Int</span>)]()</div><div class="line">      <span class="keyword">while</span> (iter.hasNext) &#123;</div><div class="line">        <span class="keyword">val</span> cur = iter.next</div><div class="line">        res.::=(cur, cur * <span class="number">2</span>)</div><div class="line">      &#125;</div><div class="line">      res.iterator</div><div class="line">    &#125;)</div></pre></td></tr></table></figure>
<p>foreachPartitions</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2018/04/06/hadoop-spark/spark/spark任务提交/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/04/06/hadoop-spark/spark/spark任务提交/" class="post-title-link" itemprop="url">spark任务提交</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2018-04-06 11:30:47 / Geändert am: 23:27:08" itemprop="dateCreated datePublished" datetime="2018-04-06T11:30:47+08:00">2018-04-06</time>
            </span>
          

          
            

            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">in</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/spark/" itemprop="url" rel="index"><span itemprop="name">spark</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">export CLASSPATH=.:$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar:$CLASSPATH</div><div class="line">export JAVA_HOME=/opt/jdk1.7.0_67</div><div class="line">export PATH=$JAVA_HOME/bin:$PATH</div><div class="line">export HADOOP_MAPRED_HOME=/usr/lib/hadoop-mapreduce</div><div class="line"></div><div class="line">nohup /usr/lib/spark/bin/spark-submit </div><div class="line">--master spark://srv-buzz-bosk1:7077 </div><div class="line">--jars $CLASSPATH </div><div class="line">--class com.iclick.data.main.PredictAgeGender </div><div class="line">--executor-memory 100g </div><div class="line">--driver-memory 100g  </div><div class="line">--total-executor-cores 100 </div><div class="line">--conf spark.akka.frameSize=2000  </div><div class="line">$&#123;PIG_HOME&#125;/spark-data-1.0.jar /shortdata/vid_url/  10 $&#123;Today&#125;  /rawdata/genome/$&#123;Yesterday&#125;  /nlp/ageIndex.model  /shortdata/vid_profile/$&#123;Today&#125;  &gt;&gt;$&#123;PIG_EXT_LOG&#125; 2&gt;&amp;1 &amp;</div></pre></td></tr></table></figure>
<p><a href="https://blog.csdn.net/chenjieit619/article/details/53421080" target="_blank" rel="noopener">spark submit参数调优</a></p>
<p><a href="http://spark.apache.org/docs/latest/configuration.html" target="_blank" rel="noopener">官方文档Spark Configuration</a></p>
<p>用submit提交一个任务后，会启动一个对应的Driver进程，根据你使用的部署模式（deploy-mode）不同，Driver进程可能在本地启动，也可能在集群中某个工作节点上启动。Driver进程本身会根据我们设置的参数，占有一定数量的内存和CPU core。而Driver进程要做的第一件事情，就是向集群管理器（可以是Spark Standalone集群，也可以是其他的资源管理集群，美团•大众点评使用的是YARN作为资源管理集群）申请运行Spark作业需要使用的资源，这里的资源指的就是Executor进程。YARN集群管理器会根据我们为Spark作业设置的资源参数，在各个工作节点上，启动一定数量的Executor进程，每个Executor进程都占有一定数量的内存和CPU core。</p>
<h3 id="1-num-executors"><a href="#1-num-executors" class="headerlink" title="1.num-executors"></a>1.num-executors</h3><ul>
<li>参数说明：该参数用于设置Spark作业总共要用多少个Executor进程来执行。Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。</li>
<li>参数调优建议：每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。</li>
</ul>
<h3 id="2-executor-memory"><a href="#2-executor-memory" class="headerlink" title="2.executor-memory"></a>2.executor-memory</h3><ul>
<li>参数说明：该参数用于设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。</li>
<li>参数调优建议：每个Executor进程的内存设置4G~8G较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少，num-executors乘以executor-memory，是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，那么申请的内存量最好不要超过资源队列最大总内存的1/3~1/2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。</li>
</ul>
<h3 id="3-executor-cores"><a href="#3-executor-cores" class="headerlink" title="3.executor-cores"></a>3.executor-cores</h3><ul>
<li>参数说明：该参数用于设置每个Executor进程的CPU core数量。这个参数决定了每个Executor进程并行执行task线程的能力。因为<strong>每个CPU core同一时间只能执行一个task线程</strong>，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。</li>
<li>参数调优建议：Executor的CPU core数量设置为2~4个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么<strong>num-executors * executor-cores不要超过队列总CPU core的1/3~1/2左右</strong>比较合适，也是避免影响其他同学的作业运行。</li>
</ul>
<h3 id="4-driver-memory"><a href="#4-driver-memory" class="headerlink" title="4.driver-memory"></a>4.driver-memory</h3><ul>
<li>参数说明：该参数用于设置Driver进程的内存。</li>
<li>参数调优建议：Driver的内存通常来说不设置，或者设置1G左右应该就够了。唯一需要注意的一点是，<strong>如果需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题</strong>。</li>
</ul>
<h3 id="5-spark-default-parallelism"><a href="#5-spark-default-parallelism" class="headerlink" title="5.spark.default.parallelism"></a>5.spark.default.parallelism</h3><ul>
<li>参数说明：该参数用于设置每个<strong>stage的默认task数量</strong>。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。</li>
<li>参数调优建议：Spark作业的默认task数量为500~1000个较为合适。很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此<strong>Spark官网建议的设置原则是，设置该参数为num-executors * executor-cores的2~3倍较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的</strong>，此时可以充分地利用Spark集群的资源。</li>
</ul>
<h3 id="6-spark-storage-memoryFraction"><a href="#6-spark-storage-memoryFraction" class="headerlink" title="6.spark.storage.memoryFraction"></a>6.spark.storage.memoryFraction</h3><ul>
<li>参数说明：该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。</li>
<li>参数调优建议：如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</li>
</ul>
<h3 id="7-spark-shuffle-memoryFraction"><a href="#7-spark-shuffle-memoryFraction" class="headerlink" title="7.spark.shuffle.memoryFraction"></a>7.spark.shuffle.memoryFraction</h3><ul>
<li>参数说明：该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。</li>
<li>参数调优建议：如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</li>
</ul>
<h3 id="8-total-executor-cores"><a href="#8-total-executor-cores" class="headerlink" title="8.total-executor-cores"></a>8.total-executor-cores</h3><ul>
<li>参数说明：Total cores for all executors.</li>
</ul>
<h3 id="9-资源参数参考示例"><a href="#9-资源参数参考示例" class="headerlink" title="9.资源参数参考示例"></a>9.资源参数参考示例</h3><p>以下是一份spark-submit命令的示例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"> --master spark://192.168.1.1:7077 \</div><div class="line"></div><div class="line"> --master yarn-cluster \  </div><div class="line"></div><div class="line"> --master</div><div class="line"></div><div class="line"> yarn-client \  </div><div class="line"></div><div class="line">./bin/spark-submit \</div><div class="line">  --master spark://192.168.1.1:7077 \</div><div class="line">  --num-executors 100 \</div><div class="line">  --executor-memory 6G \</div><div class="line">  --executor-cores 4 \</div><div class="line">　--total-executor-cores 400 \ ##standalone default all cores </div><div class="line">  --driver-memory 1G \</div><div class="line">  --conf spark.default.parallelism=1000 \</div><div class="line">  --conf spark.storage.memoryFraction=0.5 \</div><div class="line">  --conf spark.shuffle.memoryFraction=0.3 \</div></pre></td></tr></table></figure>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2018/04/04/编程语言学习/PYTHON/指数、幂函数拟合/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/04/04/编程语言学习/PYTHON/指数、幂函数拟合/" class="post-title-link" itemprop="url">Unbenannt</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2018-04-04 14:12:35" itemprop="dateCreated datePublished" datetime="2018-04-04T14:12:35+08:00">2018-04-04</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Bearbeitet am</span>
                
                <time title="Geändert am: 2019-02-13 17:02:24" itemprop="dateModified" datetime="2019-02-13T17:02:24+08:00">2019-02-13</time>
              </span>
            
          

          

          
            
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>转自：<a href="https://blog.csdn.net/yefengzhichen/article/details/52767733" target="_blank" rel="noopener">python指数、幂数拟合curve_fit</a></p>
<p>1、一次二次多项式拟合</p>
<p>一次二次比较简单，直接使用numpy中的函数即可，polyfit(x, y, degree)。</p>
<p>2、指数幂数拟合curve_fit</p>
<p>使用scipy.optimize 中的curve_fit，幂数拟合例子如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">from scipy.optimize import curve_fit  </div><div class="line">import matplotlib.pyplot as plt  </div><div class="line">import numpy as np  </div><div class="line">  </div><div class="line">def func(x, a, b, c):  </div><div class="line">    return a * np.exp(-b * x) + c  </div><div class="line">  </div><div class="line">xdata = np.linspace(0, 4, 50)  </div><div class="line">y = func(xdata, 2.5, 1.3, 0.5)  </div><div class="line">ydata = y + 0.2 * np.random.normal(size=len(xdata))  </div><div class="line">plt.plot(xdata,ydata,&apos;b-&apos;)  </div><div class="line">popt, pcov = curve_fit(func, xdata, ydata)  </div><div class="line">#popt数组中，三个值分别是待求参数a,b,c  </div><div class="line">y2 = [func(i, popt[0],popt[1],popt[2]) for i in xdata]  </div><div class="line">plt.plot(xdata,y2,&apos;r--&apos;)  </div><div class="line">print popt</div></pre></td></tr></table></figure>
<p>下面是指数拟合例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">def fund(x, a, b):  </div><div class="line">    return x**a + b  </div><div class="line">      </div><div class="line">xdata = np.linspace(0, 4, 50)  </div><div class="line">y = fund(xdata, 2.5, 1.3)  </div><div class="line">ydata = y + 4 * np.random.normal(size=len(xdata))  </div><div class="line">plt.plot(xdata,ydata,&apos;b-&apos;)  </div><div class="line">popt, pcov = curve_fit(fund, xdata, ydata)  </div><div class="line">#popt数组中，三个值分别是待求参数a,b,c  </div><div class="line">y2 = [fund(i, popt[0],popt[1]) for i in xdata]  </div><div class="line">plt.plot(xdata,y2,&apos;r--&apos;)  </div><div class="line">print popt</div></pre></td></tr></table></figure>
<p>其他相关文章</p>
<p><a href="http://python.jobbole.com/87015/" target="_blank" rel="noopener">Python科学计算——任意波形拟合</a></p>
<p><a href="https://blog.csdn.net/lsldd/article/details/41251583" target="_blank" rel="noopener">用Python开始机器学习（3：数据拟合与广义线性回归）</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2018/04/03/地理信息挖掘/computing with spatial trajectories/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/04/03/地理信息挖掘/computing with spatial trajectories/" class="post-title-link" itemprop="url">computing with spatial trajectories</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2018-04-03 21:55:41" itemprop="dateCreated datePublished" datetime="2018-04-03T21:55:41+08:00">2018-04-03</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Bearbeitet am</span>
                
                <time title="Geändert am: 2018-10-05 11:27:06" itemprop="dateModified" datetime="2018-10-05T11:27:06+08:00">2018-10-05</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">in</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/地理信息挖掘/" itemprop="url" rel="index"><span itemprop="name">地理信息挖掘</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>专著<code>computing with spatial trajectories.pdf</code></p>
<p><strong>第一章</strong>：数据预处理。对于位置点的高采样可以生成精确轨迹，但会数据量过大影响性能。所以要设计轨迹数据的压缩方法。同时，轨迹数据会生成离群点和噪音，要过滤这些。</p>
<p>对于<strong>压缩</strong>，有离线数据的批处理模式，和在线数据的在线模式。</p>
<p>对于<strong>消除噪音</strong>，包括mean and median filtering，the Kalman filter, the particle filter.</p>
<p><strong>第二章</strong>：数据量大，处理时间长，数据集没有很好的组织。比如，要查找跨十字路口的轨迹原理简单，但是在在线系统中采用直接扫描的方式遍历大量数据是不可行的。同时，我们需要搜索满足特定条件的轨迹数据。比如我们可以检索旅行者在特定时间穿过指定区域的数据，以帮助旅行规划。</p>
<p>第二章介绍了在轨迹数据库中经常用到的<strong>查询类型和查询方法</strong>。</p>
<p><strong>第三章</strong>：一般的GPS设备有10米以上的误差，这些移动的轨迹数据就比较难精确定位到POI上，尤其在高密度的城市区域。同时，对象在连续的移动，而位置只能按时间离线的更新，则两个更新点之间是不确定的。导致这种long-interval updates的原因是save energy consumption和communication bandwidth。当interval逐渐增大时，对轨迹搜索会带来新的挑战。</p>
<p>为解决上述的<strong>不确定性问题</strong>，第三章介绍了对不确定性表达和建模的Moving Objects Databases(MOD)模型。同时讨论了处理时空查询的高效算法。要注意的是，<strong>第二章介绍的查询方法没有考虑不确定性</strong>。</p>
<p><strong>第四章</strong>：LBS的<strong>隐私问题</strong>。通常有两种类型的LBS。</p>
<p>snapshot LBS，移动用户只需要在得到想要的信息时汇报当前位置。事实上，当使用此类服务时，用户并不需要告诉LBS他的当前位置。比如，找到附近的宾馆只需要一个粗的地理范围。</p>
<p>continuous LBS，用户必须持续汇报位置，比如导航。保护用户隐私更困难。</p>
<p><strong>第五章</strong>：分析<strong>轨迹模式</strong>。比如一个个体的轨迹包含什么模式，一组轨迹是否有相似模式。也可以轨迹聚类。比如，找到有相似空间形状的轨迹的clusters，能够帮助检测热门驾驶路线。另外，判断一群一起移动的人可用于探索社交关系。</p>
<p><strong>第六章</strong>：用户<strong>行为认知</strong>。空间轨迹隐含了用户的行为和活动。通过多人的活动信息，能够估计相关性。</p>
<p><strong>第七章</strong>：<strong>车辆轨迹挖掘</strong>。可以挖掘路网信息，交通状况，司机行为。用于优化路线等。</p>
<p><strong>第八九章</strong>：用户喜欢分享位置信息，可以挖掘用户关系。</p>
<h1 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h1><p>车的轨迹一天1400条，若分析一个月的数据才需要压缩？</p>
<h2 id="批量压缩"><a href="#批量压缩" class="headerlink" title="批量压缩"></a>批量压缩</h2><p>batched compression techniques</p>
<h2 id="在线压缩"><a href="#在线压缩" class="headerlink" title="在线压缩"></a>在线压缩</h2><p>on-line data reduction</p>
<p>1） 使用线段拟合尽可能多的轨迹点。主要关注空间属性</p>
<p>2）预测轨迹，并上报偏离预测的点。为了预测，需要关注速度、朝向等。</p>
<p>压缩的时候计算误差</p>
<p>1）perpendicular Euclidean distance</p>
<p><img src="/2018/04/03/地理信息挖掘/computing with spatial trajectories/Users/david/Library/Application Support/typora-user-images/image-20181005095933362.png" alt="image-20181005095933362"></p>
<p>2）synchronized Euclidean distance</p>
<p>安装时间计算投影的误差</p>
<p><img src="/2018/04/03/地理信息挖掘/computing with spatial trajectories/Users/david/Library/Application Support/typora-user-images/image-20181005110407128.png" alt="image-20181005110407128"></p>
<p><img src="/2018/04/03/地理信息挖掘/computing with spatial trajectories/Users/david/Library/Application Support/typora-user-images/image-20181005110439685.png" alt="image-20181005110439685"></p>
<h1 id="去噪音"><a href="#去噪音" class="headerlink" title="去噪音"></a>去噪音</h1><p>P46</p>
<p>Kalman Filter</p>
<p>Particle Filter</p>
<h1 id="CH2-轨迹索引"><a href="#CH2-轨迹索引" class="headerlink" title="CH2 轨迹索引"></a>CH2 轨迹索引</h1><p>应用场景：</p>
<p>1）轨迹和点（查询某段时间在某个地点附近的轨迹点）</p>
<p>2）轨迹和区域（查询某段时间某个轨迹经过的区域）</p>
<p>3）轨迹和轨迹（某段时间内相似路径的轨迹）</p>
<h2 id="轨迹查询的类别"><a href="#轨迹查询的类别" class="headerlink" title="轨迹查询的类别"></a>轨迹查询的类别</h2><h1 id="CH5-轨迹模式挖掘"><a href="#CH5-轨迹模式挖掘" class="headerlink" title="CH5 轨迹模式挖掘"></a>CH5 轨迹模式挖掘</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>轨迹模式的应用案例：</p>
<p>1、交通优化应用[25]需要找到一组相似的轨迹，来指示一起行驶物体的相关性。例如，拼车应用需要找到相同轨迹组的司机，从而降低交通费用。或者检测相同轨迹的送货卡车以更好指定送货规划。</p>
<p>2、预测方法[50]能够探索轨迹的知识，用于理解物体行为。这些知识可用于提供有效通知effective notifications，用于精准推送广告，提供定制化LBS服务。</p>
<p>3、相关专业需要研究动物的迁徙轨迹。</p>
<p>4、团队体育运动能够提供运动员的轨迹数据。</p>
<p>5、交通应用，使用轨迹研究聚集和离群crowds and outliers。挖掘离群点可用于检测和移除错误数据，或者用于判断危险驾驶行为。</p>
<h2 id="Overview-of-Trajectory-Patterns"><a href="#Overview-of-Trajectory-Patterns" class="headerlink" title="Overview of Trajectory Patterns"></a>Overview of Trajectory Patterns</h2>
          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2018/04/03/hadoop-spark/spark/spark工作机制/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/04/03/hadoop-spark/spark/spark工作机制/" class="post-title-link" itemprop="url">spark工作机制</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2018-04-03 21:55:41" itemprop="dateCreated datePublished" datetime="2018-04-03T21:55:41+08:00">2018-04-03</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Bearbeitet am</span>
                
                <time title="Geändert am: 2018-04-08 12:00:54" itemprop="dateModified" datetime="2018-04-08T12:00:54+08:00">2018-04-08</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">in</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/spark/" itemprop="url" rel="index"><span itemprop="name">spark</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="spark应用执行机制"><a href="#spark应用执行机制" class="headerlink" title="spark应用执行机制"></a>spark应用执行机制</h1><h2 id="Spark执行机制总览"><a href="#Spark执行机制总览" class="headerlink" title="Spark执行机制总览"></a>Spark执行机制总览</h2><p>1、RDD的Action算子触发Job的提交；</p>
<p>2、提交到Spark的Job生成RDD DAG；</p>
<p>3、由DAGSchedule转化为Stage DAG；</p>
<p>4、每个Stage中产生相应的Task集合；</p>
<p>5、TaskScheduler将Task分发到Executor执行。</p>
<p>6、每个Task对应一个数据块，使用用户定义的函数处理数据块。</p>
<p><img src="/2018/04/03/hadoop-spark/spark/spark工作机制/spark应用转换流程.png" alt=""></p>
<h2 id="Spark执行的底层实现原理"><a href="#Spark执行的底层实现原理" class="headerlink" title="Spark执行的底层实现原理"></a>Spark执行的底层实现原理</h2><p>对RDD的块管理通过BlockManager完成。BlockManager将数据抽象为数据块，在内存或磁盘存储。</p>
<p>在Executor中会创建线程池，tasks通过线程池并发执行。</p>
<h2 id="Spark应用的概念"><a href="#Spark应用的概念" class="headerlink" title="Spark应用的概念"></a>Spark应用的概念</h2><p>执行模式有Local，Standalone，YARN，Mesos。</p>
<p><a href="https://blog.csdn.net/cds86333774/article/details/51216452" target="_blank" rel="noopener">Spark的三种分布式部署模式：Standalone, Mesos,Yarn</a></p>
<p>Standalone可以单独部署到一个集群中，无需依赖任何其他资源管理系统。</p>
<p>借鉴Spark开发模式，我们可以得到一种开发新型计算框架的一般思路：先设计出它的standalone模式，为了快速开发，起初不需要考虑服务（比如master/slave）的容错性，之后再开发相应的wrapper，将stanlone模式下的服务原封不动的部署到资源管理系统yarn或者mesos上，由资源管理系统负责服务本身的容错。</p>
<p>目前Spark在standalone模式下是没有任何单点故障问题的，这是借助zookeeper实现的，思想类似于Hbase master单点故障解决方案。将Spark standalone与MapReduce比较，会发现它们两个在架构上是完全一致的。</p>
<p>集群启动后，用jps在主节点会看到Master进程，从节点看到Worker进程。其中Master负责接收客户端提交的作业，管理Worker。</p>
<p>Mesos官方推荐模式。有粗粒度和细粒度模式。</p>
<p>YARN。目前仅支持粗粒度模式。</p>
<p>如果使用Standalone模式，只需要提供Hadoop的HDFS支持；如果使用Yarn模式，则同时需要提供Hadoop的Yarn。</p>
<p>根据Driver Program是否在集群中运行，又可以分为Cluster和Client模式。</p>
<p>一个应用的基本组件包括：</p>
<ul>
<li>Application：用户写的Spark程序。</li>
<li>Driver Program：运行Application的main函数并创建SparkContext</li>
<li>RDD Graph：当RDD遇到Action算子时，将之前所有算子形成一个DAG，也就是RDD Graph。再在Spark中转化为Job，提交到集群执行。一个App可以包含多个Job。</li>
<li>Job：一个RDD Graph触发的作业。</li>
<li>Stage：每个Job根据RDD的宽依赖关系被切分成多个Stage，每个Stage包含一组Task。</li>
<li>Task：一个分区对应一个Task，Task封装好后装入Executor的线程池执行。</li>
</ul>
<h1 id="spark作业提交"><a href="#spark作业提交" class="headerlink" title="spark作业提交"></a>spark作业提交</h1><p>以WordCount为例说明RDD从转换到作业提交的过程</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sc.textFile(<span class="string">"/User/david/key.txt"</span>).flatMap(line=&gt;line.split(<span class="string">" "</span>)).map(word=&gt;(word,<span class="number">1</span>)).reduceByKey(_+_)</div></pre></td></tr></table></figure>
<p>步骤1：<code>val rawFile = sc.textFile(&quot;/User/david/key.txt&quot;)</code></p>
<p>textFile先生成HadoopRDD，然后再通过map操作生成MappedRDD。在spark-shell中可以看到</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">scala&gt; val rawFile = sc.textFile(&quot;/User/david/key.txt&quot;)</div><div class="line">rawFile: org.apache.spark.rdd.RDD[String] = /User/david/key.txt MapPartitionsRDD[3] at textFile at &lt;console&gt;:27</div><div class="line">1.6.3版本变成了MapPartitionsRDD</div></pre></td></tr></table></figure>
<p>步骤2：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">val splittedText = rawFile.flatMap(line=&gt;line.split(&quot; &quot;))</div></pre></td></tr></table></figure>
<p>flatMap将原来的MappedRDD转换为FlatMappedRDD。</p>
<p>步骤3</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">val wordCount = splittedText.map(word=&gt;(word,1))</div></pre></td></tr></table></figure>
<p>步骤4：<code>reduceByKey</code></p>
<h2 id="作业执行"><a href="#作业执行" class="headerlink" title="作业执行"></a>作业执行</h2><p>spark执行中相关概念</p>
<p><a href="https://blog.csdn.net/u013013024/article/details/72876427" target="_blank" rel="noopener">Spark中Task，Partition，RDD、节点数、Executor数、core数目的关系和Application，Driver，Job，Task，Stage理解</a></p>
<p><img src="http://www.zezhi.net/wp-content/uploads/2016/04/spark-learning.png" alt=""></p>
<p>若干个block合并成一个输入分片InputSplit，一个InputSplit对应一个Task，一个Task生成一个Partition。</p>
<p>随后这些具体的Task每个都会被分配到集群上的某个节点的某个<strong>Executor</strong>去执行。</p>
<ul>
<li>每个节点可以启一个或多个Executor。</li>
<li>每个Executor由若干<strong>core</strong>组成，每个Executor的每个core<strong>一次只能执行一个</strong>Task。</li>
<li>每个<strong>Task</strong>执行的结果就是生成了目标<strong>RDD</strong>的一个<strong>partiton</strong>。每个partition再下一步又由一个task来执行。</li>
</ul>
<p><strong>注意: </strong>这里的core是虚拟的core而不是机器的物理CPU核，可以理解为就是Executor的一个工作线程。</p>
<p>而 Task被执行的并发度 = Executor数目 * 每个Executor核数。</p>
<p>所以，如果一共要执行8个task，但只有一个Executor，2个core，则并发度是2。那么需要分成4个批次，每次并发执行两个Task。</p>
<p>至于<strong>partition的数目</strong>：</p>
<ul>
<li>对于数据读入阶段，例如sc.textFile，输入文件被划分为多少InputSplit就会需要多少初始Task。</li>
<li>在Map阶段partition数目保持不变。</li>
<li>在Reduce阶段，RDD的聚合会触发shuffle操作，聚合后的RDD的partition数目跟具体操作有关，例如repartition操作会聚合成指定分区数，还有一些算子是可配置的。</li>
</ul>
<p>在任务提交中主要涉及Driver和Executor两个节点。</p>
<p><strong>Driver</strong>可以理解为我们自己编写的程序。主要解决</p>
<ul>
<li>RDD依赖性分析，以生成DAG</li>
<li>根据RDD DAG将Job分割为多个stage</li>
<li>Stage确认后，生成相应的task，分发到Executor执行。</li>
</ul>
<p><strong>Executor</strong>：在每个WorkerNode上为某应用启动的一个进程，是一个执行task的容器。一个Executor执行多个Task。</p>
<p>另外</p>
<p><strong>Job</strong>：包含很多task的并行计算，可以认为<strong>是Spark RDD 里面的action</strong>,每个action的计算会生成一个job。</p>
<p>　　用户提交的Job会提交给DAGScheduler，Job会被分解成Stage和Task。</p>
<p> Spark中的Job和MR中Job不一样。MR中Job主要是Map或者Reduce Job。而<strong>Spark的Job其实很好区别，一个action算子就算一个Job</strong>，比方说count，first等。</p>
<p><strong>Stage</strong>：</p>
<p><strong>一个Job会被拆分为多组Task，每组任务被称为一个Stage就像Map Stage， Reduce Stage</strong>。</p>
<p>　　Stage的划分在RDD的论文中有详细的介绍，简单的说是以shuffle和result这两种类型来划分。在Spark中有两类task，一类是shuffleMapTask，一类是resultTask，第一类task的输出是shuffle所需数据，第二类task的输出是result，stage的划分也以此为依据，shuffle之前的所有变换是一个stage，shuffle之后的操作是另一个stage。比如 rdd.parallize(1 to 10).foreach(println) 这个操作没有shuffle，直接就输出了，那么只有它的task是resultTask，stage也只有一个；如果是rdd.map(x =&gt; (x, 1)).reduceByKey(<em> + </em>).foreach(println), 这个job因为有reduce，所以有一个shuffle过程，那么reduceByKey之前的是一个stage，执行shuffleMapTask，输出shuffle所需的数据，reduceByKey到最后是一个stage，直接就输出结果了。如果job中有多次shuffle，那么每个shuffle之前都是一个stage。</p>
<p><strong>Task</strong></p>
<p>即 stage 下的一个任务执行单元，一般来说，<strong>一个 rdd 有多少个 partition，就会有多少个 task</strong>，因为每一个 task 只是处理一个 partition 上的数据.</p>
<h3 id="依赖性分析和stage划分"><a href="#依赖性分析和stage划分" class="headerlink" title="依赖性分析和stage划分"></a>依赖性分析和stage划分</h3><p>RDD之间的依赖分为窄依赖和宽依赖。</p>
<p>窄依赖是指父RDD所有输出都会被执行的子RDD消费，也就是输出路径固定。例如如下的Transformation：</p>
<p>map、flatMap、filter、sample</p>
<p>宽依赖是指父RDD输出会由不同子RDD消费，输出路径不固定。例如：</p>
<p>sortByKey、reduceByKey、groupByKey、cogroupByKey、join、cartensian</p>
<p>调度器（Scheduler）会计算RDD之间的依赖关系，将窄依赖的RDD归并到同一个stage，而宽依赖则作为划分不同Stage的判断标准。<strong>宽依赖和窄依赖的边界就是stage的划分点</strong></p>
<h2 id="任务的创建和分发"><a href="#任务的创建和分发" class="headerlink" title="任务的创建和分发"></a>任务的创建和分发</h2><p>由Executor执行的Task分为ShuffleMapTask和ResultTask两种，相当于Map和Reduce。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2018/04/03/hadoop-spark/spark/spark算子分类及功能/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/04/03/hadoop-spark/spark/spark算子分类及功能/" class="post-title-link" itemprop="url">spark算子分类及功能</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2018-04-03 21:55:41" itemprop="dateCreated datePublished" datetime="2018-04-03T21:55:41+08:00">2018-04-03</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Bearbeitet am</span>
                
                <time title="Geändert am: 2018-06-22 15:12:16" itemprop="dateModified" datetime="2018-06-22T15:12:16+08:00">2018-06-22</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">in</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/spark/" itemprop="url" rel="index"><span itemprop="name">spark</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>大致分为3类：</p>
<p>1、Value数据类型的Transformation算子。不触发提交作业，针对处理的数据项是Value型的数据。</p>
<p>2、Key-Value型的Transformation算子。针对Key-Value数据对。</p>
<p>3、Action算子。触发SparkContext提交Job。</p>
<h1 id="Value型Transformation算子"><a href="#Value型Transformation算子" class="headerlink" title="Value型Transformation算子"></a>Value型Transformation算子</h1><p>可以根据RDD变换算子的输入分区与输出分区关系分为以下几个类型：</p>
<ul>
<li>输入分区和输出分区一对一型；</li>
<li>多对一型；</li>
<li>多对多型；</li>
<li>输出为输入子集型。</li>
</ul>
<h2 id="一对一型"><a href="#一对一型" class="headerlink" title="一对一型"></a>一对一型</h2><h3 id="map"><a href="#map" class="headerlink" title="map"></a>map</h3><p>等action触发提交后，在同一个stage中运算。</p>
<h3 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a>flatMap</h3><h3 id="mapPartitions"><a href="#mapPartitions" class="headerlink" title="mapPartitions"></a>mapPartitions</h3><p>map是对rdd中的每一个元素进行操作，而mapPartitions(foreachPartition)则是对rdd中的<strong>每个分区的迭代器</strong>进行操作。</p>
<p>如果在map过程中需要频繁创建额外的对象(例如将rdd中的数据通过jdbc写入<a href="http://lib.csdn.net/base/mysql" target="_blank" rel="noopener">数据库</a>,map需要为每个元素创建一个链接而mapPartition为每个partition创建一个链接),则mapPartitions效率比map高的多。</p>
<p>SparkSql或DataFrame默认会对程序进行mapPartition的优化。</p>
<h3 id="与map的区别"><a href="#与map的区别" class="headerlink" title="与map的区别"></a>与map的区别</h3><p><a href="https://blog.csdn.net/lsshlsw/article/details/48627737" target="_blank" rel="noopener">https://blog.csdn.net/lsshlsw/article/details/48627737</a></p>
<p>使用map</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">val a = sc.parallelize(1 to 9, 3)</div><div class="line">def mapDoubleFunc(a : Int) : (Int,Int) = &#123;</div><div class="line">    (a,a*2)</div><div class="line">&#125;</div><div class="line">val mapResult = a.map(mapDoubleFunc)</div><div class="line"></div><div class="line">println(mapResult.collect().mkString)</div></pre></td></tr></table></figure>
<p>使用mapPartitions</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">val a = sc.parallelize(1 to 9, 3)</div><div class="line">  def doubleFunc(iter: Iterator[Int]) : Iterator[(Int,Int)] = &#123;</div><div class="line">    var res = List[(Int,Int)]()</div><div class="line">    while (iter.hasNext)</div><div class="line">    &#123;</div><div class="line">      val cur = iter.next;</div><div class="line">      res .::= (cur,cur*2)</div><div class="line">    &#125;</div><div class="line">    res.iterator</div><div class="line">  &#125;</div><div class="line">val result = a.mapPartitions(doubleFunc)</div><div class="line">println(result.collect().mkString)</div></pre></td></tr></table></figure>
<h3 id="glom"><a href="#glom" class="headerlink" title="glom"></a>glom</h3><p>将每个RDD分区形成一个数组。</p>
<p><img src="/2018/04/03/hadoop-spark/spark/spark算子分类及功能/glom.png" alt=""></p>
<h2 id="多对一型"><a href="#多对一型" class="headerlink" title="多对一型"></a>多对一型</h2><h3 id="union"><a href="#union" class="headerlink" title="union"></a>union</h3><p>++符号相当于union</p>
<h2 id="cartesian"><a href="#cartesian" class="headerlink" title="cartesian"></a>cartesian</h2><p>对两个RDD中所有元素进行笛卡尔积操作。</p>
<p><img src="/2018/04/03/hadoop-spark/spark/spark算子分类及功能/cartesian.png" alt=""></p>
<h2 id="多对多型"><a href="#多对多型" class="headerlink" title="多对多型"></a>多对多型</h2><h3 id="groupBy"><a href="#groupBy" class="headerlink" title="groupBy"></a>groupBy</h3><p>例如</p>
<p>1）将用户数据预处理</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">val cleanF = sc.clean(f)</div></pre></td></tr></table></figure>
<p>2）对数据map进行函数操作，最后再对groupByKey进行分组操作</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">this.map(t =&gt; (cleanF(t), t)).groupByKey(p)</div></pre></td></tr></table></figure>
<p>其中，p确定了分区个数和分区函数，也就决定了并行化的程度。</p>
<h2 id="子集型"><a href="#子集型" class="headerlink" title="子集型"></a>子集型</h2><h3 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h3><h3 id="distinct"><a href="#distinct" class="headerlink" title="distinct"></a>distinct</h3><h3 id="substract"><a href="#substract" class="headerlink" title="substract"></a>substract</h3><p>集合的差操作。RDD1去除RDD2交集中的所有元素。</p>
<h3 id="sample"><a href="#sample" class="headerlink" title="sample"></a>sample</h3><p>采样。可以设定有放回的采样、百分比、随机种子。</p>
<h3 id="takeSample"><a href="#takeSample" class="headerlink" title="takeSample"></a>takeSample</h3><p>不使用相对比例采样，而是按设定的采样个数进行采样。同时返回结果也不是RDD，而是相当于对采样后的数据进行Collect()，返回单机的数组。</p>
<p><img src="/2018/04/03/hadoop-spark/spark/spark算子分类及功能/takeSample.png" alt=""></p>
<h2 id="Cache型"><a href="#Cache型" class="headerlink" title="Cache型"></a>Cache型</h2><h3 id="cache"><a href="#cache" class="headerlink" title="cache"></a>cache</h3><p>将RDD从磁盘缓存到内存，相当于<code>persist(MEMORY_ONLY)</code> 的功能。</p>
<p><img src="/2018/04/03/hadoop-spark/spark/spark算子分类及功能/cache.png" alt=""></p>
<h2 id="persist"><a href="#persist" class="headerlink" title="persist"></a>persist</h2><p>缓存到哪里，由StorageLevel枚举类型决定。有以下几种类型的组合：DISK磁盘，MEMORY内存，SER数据是否序列化存储。可以缓存的模式有</p>
<p><img src="/2018/04/03/hadoop-spark/spark/spark算子分类及功能/persist缓存模式.png" alt=""></p>
<h1 id="Key-Value型Transformation算子"><a href="#Key-Value型Transformation算子" class="headerlink" title="Key-Value型Transformation算子"></a>Key-Value型Transformation算子</h1><h2 id="输入和输出分区一对一"><a href="#输入和输出分区一对一" class="headerlink" title="输入和输出分区一对一"></a>输入和输出分区一对一</h2><h3 id="mapValues"><a href="#mapValues" class="headerlink" title="mapValues"></a>mapValues</h3><p>针对(Key, Value)型数据中的Value进行Map操作，而不对key进行处理。</p>
<p>比如<code>a=&gt;a+2</code>只对value进行+2操作</p>
<h2 id="对单个或两个RDD聚集"><a href="#对单个或两个RDD聚集" class="headerlink" title="对单个或两个RDD聚集"></a>对单个或两个RDD聚集</h2><p>单个RDD聚集</p>
<h3 id="combineByKey"><a href="#combineByKey" class="headerlink" title="combineByKey"></a>combineByKey</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">combineByKey[C](createCombiner:(V)=&gt; C,</div><div class="line">mergeValue:(C, V)=&gt; C,</div><div class="line">mergeCombiners:(C, C)=&gt; C,</div><div class="line">partitioner: Partitioner</div><div class="line">mapSideCombine: Boolean = true,</div><div class="line">serializer: Serializer =null): RDD[(K, C)]</div></pre></td></tr></table></figure>
<p>其中，</p>
<p><code>createCombiner:(V)=&gt;C</code> 会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过，要么就<br>和之前的某个元素的键相同。如果这是一个新的元素， combineByKey() 会使用一个叫作 createCombiner() 的函数来创建 。</p>
<p><code>mergeValue</code> 当C已经存在的情况下,需要merge,如把item V加到seq C中,或者叠加。</p>
<p><code>mergeCombiners</code> 合并两个C</p>
<p><code>partitioner</code> 分区器，Shuffle时需要通过Partitioner的分区策略进行分区。</p>
<p><code>mapSideCombine</code> 为了减小传输量,很多combine可以在map端先做。例如,叠加可以先在一个partition中把所有相同的Key的Value叠加，再shuffle。</p>
<p><code>serializer</code> 传输序列化</p>
<p>整个过程相当于将元素为<code>(int,int)</code>的转变为了<code>(int, Seq[Int])</code>的RDD。</p>
<p><img src="/2018/04/03/hadoop-spark/spark/spark算子分类及功能/combineByKey.png" alt=""></p>
<h3 id="reduceByKey"><a href="#reduceByKey" class="headerlink" title="reduceByKey"></a>reduceByKey</h3><p>是combineByKey的一种简单情况。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">def reduceByKey(partitioner: Partitioner, func: (V, V) =&gt; V): RDD[(K, V)] = &#123;</div><div class="line">	combineByKey[V]((v: V) =&gt; v, func, func, partitioner)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><img src="/2018/04/03/hadoop-spark/spark/spark算子分类及功能/reduceByKey.png" alt=""></p>
<h3 id="combineByKey和reduceByKey区别"><a href="#combineByKey和reduceByKey区别" class="headerlink" title="combineByKey和reduceByKey区别"></a>combineByKey和reduceByKey区别</h3><p>转自<a href="https://www.zhihu.com/question/45420080/answer/99044117" target="_blank" rel="noopener">请教Spark 中 combinebyKey 和 reduceByKey的传入函数参数的区别？</a>例如：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> testData = sc.parallelize(<span class="type">Seq</span>((<span class="string">"t1"</span>, <span class="number">1</span>), (<span class="string">"t1"</span>, <span class="number">2</span>), (<span class="string">"t1"</span>, <span class="number">3</span>), (<span class="string">"t2"</span>, <span class="number">2</span>), (<span class="string">"t2"</span>, <span class="number">5</span>)))</div><div class="line"><span class="keyword">val</span> testDataCombine = testData.combineByKey(x=&gt;x,(x:<span class="type">Int</span>,y:<span class="type">Int</span>)=&gt;x+y,(x:<span class="type">Int</span>,y:<span class="type">Int</span>)=&gt;x+y)</div><div class="line"><span class="keyword">val</span> testDataReduce = testData.reduceByKey((x,y)=&gt;x+y)</div></pre></td></tr></table></figure>
<p>combineByKey要指定x和y的Int类型，不然会报错无法识别”+”这个方法符，而reduceByKey不用指派类型。</p>
<p>从两者的方法签名</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">PairRDDFunctions</span>[<span class="type">K</span>, <span class="type">V</span>](<span class="params">...</span>) </span>&#123;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">combineByKey</span></span>[<span class="type">C</span>](</div><div class="line">      createCombiner: <span class="type">V</span> =&gt; <span class="type">C</span>,</div><div class="line">      mergeValue: (<span class="type">C</span>, <span class="type">V</span>) =&gt; <span class="type">C</span>,</div><div class="line">      mergeCombiners: (<span class="type">C</span>, <span class="type">C</span>) =&gt; <span class="type">C</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">C</span>)]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>可以看到 reduceByKey 的 func 参数的类型只依赖于 PairRDDFunction 的类型参数 V，在这个例子里也就是 Int。于是 func 的类型已经确定为 (Int, Int) =&gt; Int，所以就不需要额外标识类型了。</p>
<p>而 combineByKey 比 reduceByKey 更加通用，它允许各个 partition 在 shuffle 前先做 local reduce 得到一个类型为 C 的中间值，待 shuffle 后再做合并得到各个 key 对应的 C。也就是说<strong>C可以是任意定义的数据类型</strong>，所以必须要指定数据类型，在上面的例子中比较简单，看不出来。</p>
<p>以求均值为例，我们可以让每个 partiton 先求出单个 partition 内各个 key 对应的所有整数的和 sum 以及个数 count，然后返回一个 pair (sum, count)。在 shuffle 后累加各个 key 对应的所有 sum 和 count，再相除得到均值：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> sumCountPairs: <span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Long</span>))] = testData.combineByKey(</div><div class="line">  (_: <span class="type">Int</span>) =&gt; (<span class="number">0</span>, <span class="number">0</span>L),</div><div class="line"></div><div class="line">  (pair: (<span class="type">Int</span>, <span class="type">Long</span>), value: <span class="type">Int</span>) =&gt;</div><div class="line">    (pair._1 + value, pair._2 + <span class="number">1</span>L),</div><div class="line"></div><div class="line">  (pair1: (<span class="type">Int</span>, <span class="type">Long</span>), pair2: (<span class="type">Int</span>, <span class="type">Long</span>)) =&gt;</div><div class="line">    (pair1._1 + part2._1, pair2._2 + pair2._2)</div><div class="line">)</div><div class="line"></div><div class="line"><span class="keyword">val</span> averages: <span class="type">RDD</span>[<span class="type">String</span>, <span class="type">Double</span>] = sumCountPairs.mapValues &#123;</div><div class="line">  <span class="keyword">case</span> (sum, <span class="number">0</span>L) =&gt; <span class="number">0</span>D</div><div class="line">  <span class="keyword">case</span> (sum, count) =&gt; sum.toDouble / count</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这里的C就通过V定义成了<code>(Int, Long)</code></p>
<p>另一个，学生平均成绩的例子</p>
<p><a href="https://blog.csdn.net/t1dmzks/article/details/70249743" target="_blank" rel="noopener">https://blog.csdn.net/t1dmzks/article/details/70249743</a></p>
<p><a href="https://www.edureka.co/blog/apache-spark-combinebykey-explained" target="_blank" rel="noopener">https://www.edureka.co/blog/apache-spark-combinebykey-explained</a></p>
<h3 id="partitionBy"><a href="#partitionBy" class="headerlink" title="partitionBy"></a>partitionBy</h3><p>对RDD进行分区操作。如果原有RDD的partitioner与现有的一致，则不变；否则相当于根据partitioner生成一个新的ShuffledRDD。</p>
<p>对两个RDD进行聚集</p>
<h3 id="？cogroup"><a href="#？cogroup" class="headerlink" title="？cogroup"></a>？cogroup</h3><p>将连个RDD协同划分。每个RDD中相同Key的元素分别聚合为一个集合，并返回两个RDD中对应Key的元素集合的迭代器。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">(K, (Iterable[V], Iterable[W]))</div></pre></td></tr></table></figure>
<h2 id="连接"><a href="#连接" class="headerlink" title="连接"></a>连接</h2><h3 id="join"><a href="#join" class="headerlink" title="join"></a>join</h3><p>对两个需要连接的RDD进行cogroup操作。之后形成新的RDD，对每个key下的元素进行笛卡尔积操作，返回的结果再flat。函数实现为</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">this.cogroup(other, partitioner).flatMapValues &#123; case (vs, ws) =&gt;</div><div class="line">for (v &lt;- vs; w &lt;- ws) yield (v, w) &#125;</div></pre></td></tr></table></figure>
<p>本质就是通过cogroup先进行协同划分，再通过flatMapValues将合并的数据打散。</p>
<p>例如</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">DataTypes</span>, <span class="type">StructField</span>, <span class="type">StructType</span>&#125;</div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Row</span>, <span class="type">SQLContext</span>&#125;</div><div class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">Run</span> </span>&#123;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</div><div class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local"</span>)</div><div class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</div><div class="line">    sc.setLogLevel(<span class="string">"ERROR"</span>)</div><div class="line">    <span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</div><div class="line"></div><div class="line">    <span class="comment">/**</span></div><div class="line">      * id      name</div><div class="line">      * 1       zhangsan</div><div class="line">      * 2       lisi</div><div class="line">      * 3       wangwu</div><div class="line">      */</div><div class="line">    <span class="keyword">val</span> idName = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>, <span class="string">"zhangsan"</span>), (<span class="number">2</span>, <span class="string">"lisi"</span>), (<span class="number">3</span>, <span class="string">"wangwu"</span>)))</div><div class="line"></div><div class="line">    <span class="comment">/**</span></div><div class="line">      * id      age</div><div class="line">      * 1       30</div><div class="line">      * 2       29</div><div class="line">      * 4       21</div><div class="line">      */</div><div class="line">    <span class="keyword">val</span> idAge = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>, <span class="number">30</span>), (<span class="number">2</span>, <span class="number">29</span>), (<span class="number">4</span>, <span class="number">21</span>)))</div><div class="line"></div><div class="line">    <span class="comment">/** *******************************RDD **********************************/</span></div><div class="line"></div><div class="line">    println(<span class="string">"*********************************RDD**********************************"</span>)</div><div class="line"></div><div class="line">    println(<span class="string">"\n内关联（inner join）\n"</span>)</div><div class="line">    <span class="comment">// 内关联（inner join）</span></div><div class="line">    <span class="comment">//  只保留两边id相等的部分</span></div><div class="line">    <span class="comment">/**</span></div><div class="line">      * (1,(zhangsan,30))</div><div class="line">      * (2,(lisi,29))</div><div class="line">      */</div><div class="line">    idName.join(idAge).collect().foreach(println)</div><div class="line"></div><div class="line">    println(<span class="string">"\n左外关联（left out join）\n"</span>)</div><div class="line">    <span class="comment">// 左外关联（left out join）</span></div><div class="line">    <span class="comment">// 以左边的数据为标准, 左边的数据一律保留</span></div><div class="line">    <span class="comment">// 右边分三情况:</span></div><div class="line">    <span class="comment">//      一: 左边的id, 右边有, 则合并数据; (1,(zhangsan,Some(30)))</span></div><div class="line">    <span class="comment">//      二: 左边的id, 右边没有, 则右边为空; (3,(wangwu,None))</span></div><div class="line">    <span class="comment">//      三: 右边的id, 左边没有, 则不保留; 右边有id为4的行, 但结果中并未保留</span></div><div class="line">    <span class="comment">/**</span></div><div class="line">      * (1,(zhangsan,Some(30)))</div><div class="line">      * (2,(lisi,Some(29)))</div><div class="line">      * (3,(wangwu,None))</div><div class="line">      */</div><div class="line">    idName.leftOuterJoin(idAge).collect().foreach(println)</div><div class="line"></div><div class="line">    println(<span class="string">"\n右外关联（right outer join）\n"</span>)</div><div class="line">    <span class="comment">// 右外关联（right outer join）</span></div><div class="line">    <span class="comment">// 以右边的数据为标准, 右边的数据一律保留</span></div><div class="line">    <span class="comment">// 左边分三种情况:</span></div><div class="line">    <span class="comment">//      一: 右边的id, 左边有, 则合并数据; (1,(Some(zhangsan),30))</span></div><div class="line">    <span class="comment">//      二: 右边的id, 左边没有, 则左边为空; (4,(None,21))</span></div><div class="line">    <span class="comment">//      三: 左边的id, 右边没有, 则不保留; 左边有id为3的行, 但结果中并为保留</span></div><div class="line">    <span class="comment">/**</span></div><div class="line">      * (1,(Some(zhangsan),30))</div><div class="line">      * (2,(Some(lisi),29))</div><div class="line">      * (4,(None,21))</div><div class="line">      */</div><div class="line">    idName.rightOuterJoin(idAge).collect().foreach(println)</div><div class="line"></div><div class="line">    println(<span class="string">"\n全外关联（full outer join）\n"</span>)</div><div class="line">    <span class="comment">// 全外关联（full outer join）</span></div><div class="line">    <span class="comment">/**</span></div><div class="line">      *</div><div class="line">      * (1,(Some(zhangsan),Some(30)))</div><div class="line">      * (2,(Some(lisi),Some(29)))</div><div class="line">      * (3,(Some(wangwu),None))</div><div class="line">      * (4,(None,Some(21)))</div><div class="line">      */</div><div class="line">    idName.fullOuterJoin(idAge).collect().foreach(println)</div><div class="line"></div><div class="line">    <span class="comment">/** *******************************DataFrame **********************************/</span></div><div class="line">    <span class="keyword">val</span> schema1 = <span class="type">StructType</span>(<span class="type">Array</span>(<span class="type">StructField</span>(<span class="string">"id"</span>, <span class="type">DataTypes</span>.<span class="type">IntegerType</span>, nullable = <span class="literal">true</span>), <span class="type">StructField</span>(<span class="string">"name"</span>, <span class="type">DataTypes</span>.<span class="type">StringType</span>, nullable = <span class="literal">true</span>)))</div><div class="line">    <span class="keyword">val</span> idNameDF = sqlContext.createDataFrame(idName.map(t =&gt; <span class="type">Row</span>(t._1, t._2)), schema1)</div><div class="line"></div><div class="line">    <span class="keyword">val</span> schema2 = <span class="type">StructType</span>(<span class="type">Array</span>(<span class="type">StructField</span>(<span class="string">"id"</span>, <span class="type">DataTypes</span>.<span class="type">IntegerType</span>, nullable = <span class="literal">true</span>), <span class="type">StructField</span>(<span class="string">"age"</span>, <span class="type">DataTypes</span>.<span class="type">IntegerType</span>, nullable = <span class="literal">true</span>)))</div><div class="line">    <span class="keyword">val</span> idAgeDF = sqlContext.createDataFrame(idAge.map(t =&gt; <span class="type">Row</span>(t._1, t._2)), schema2)</div><div class="line">    println(<span class="string">"*********************************DataFrame**********************************"</span>)</div><div class="line"></div><div class="line">    println(<span class="string">"\n内关联（inner join）\n"</span>)</div><div class="line">    <span class="comment">// 相当于调用, idNameDF.join(idAgeDF, Seq("id"), "inner").collect().foreach(println)</span></div><div class="line">    <span class="comment">// 这里只是调用了封装的API</span></div><div class="line">    idNameDF.join(idAgeDF, <span class="string">"id"</span>).collect().foreach(println)</div><div class="line"></div><div class="line">    println(<span class="string">"\n左外关联（left out join）\n"</span>)</div><div class="line">    idNameDF.join(idAgeDF, <span class="type">Seq</span>(<span class="string">"id"</span>), <span class="string">"left_outer"</span>).collect().foreach(println)</div><div class="line"></div><div class="line">    println(<span class="string">"\n右外关联（right outer join）\n"</span>)</div><div class="line">    idNameDF.join(idAgeDF, <span class="type">Seq</span>(<span class="string">"id"</span>), <span class="string">"right_outer"</span>).collect().foreach(println)</div><div class="line"></div><div class="line">    println(<span class="string">"\n全外关联（full outer join）\n"</span>)</div><div class="line">    idNameDF.join(idAgeDF, <span class="type">Seq</span>(<span class="string">"id"</span>), <span class="string">"outer"</span>).collect().foreach(println)</div><div class="line"></div><div class="line">    println(<span class="string">"\nleft semi join\n"</span>)</div><div class="line">    <span class="comment">// left semi join</span></div><div class="line">    <span class="comment">// 左边的id, 在右边有, 就保留左边的数据; 右边的数据不保留, 只有id的有意义的</span></div><div class="line">    <span class="comment">/**</span></div><div class="line">      * [1,zhangsan]</div><div class="line">      * [2,lisi]</div><div class="line">      */</div><div class="line">    idNameDF.join(idAgeDF, <span class="type">Seq</span>(<span class="string">"id"</span>), <span class="string">"leftsemi"</span>).collect().foreach(println)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>当出现相同Key时, join会出现笛卡尔积, 而cogroup的处理方式不同</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SQLContext</span></div><div class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">Run</span> </span>&#123;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</div><div class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"test"</span>).setMaster(<span class="string">"local"</span>)</div><div class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</div><div class="line">    sc.setLogLevel(<span class="string">"ERROR"</span>)</div><div class="line">    <span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</div><div class="line"></div><div class="line">    <span class="comment">/**</span></div><div class="line">      * id      name</div><div class="line">      * 1       zhangsan</div><div class="line">      * 2       lisi</div><div class="line">      * 3       wangwu</div><div class="line">      */</div><div class="line">    <span class="keyword">val</span> idName = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>, <span class="string">"zhangsan"</span>), (<span class="number">2</span>, <span class="string">"lisi"</span>), (<span class="number">3</span>, <span class="string">"wangwu"</span>)))</div><div class="line"></div><div class="line">    <span class="comment">/**</span></div><div class="line">      * id      age</div><div class="line">      * 1       30</div><div class="line">      * 2       29</div><div class="line">      * 4       21</div><div class="line">      */</div><div class="line">    <span class="keyword">val</span> idAge = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>, <span class="number">30</span>), (<span class="number">2</span>, <span class="number">29</span>), (<span class="number">4</span>, <span class="number">21</span>)))</div><div class="line"></div><div class="line">    println(<span class="string">"\ncogroup\n"</span>)</div><div class="line"></div><div class="line">    <span class="comment">/**</span></div><div class="line">      * (1,(CompactBuffer(zhangsan),CompactBuffer(30)))</div><div class="line">      * (2,(CompactBuffer(lisi),CompactBuffer(29)))</div><div class="line">      * (3,(CompactBuffer(wangwu),CompactBuffer()))</div><div class="line">      * (4,(CompactBuffer(),CompactBuffer(21)))</div><div class="line">      */</div><div class="line">    idName.cogroup(idAge).collect().foreach(println)</div><div class="line"></div><div class="line">    println(<span class="string">"\njoin\n"</span>)</div><div class="line">    <span class="comment">// fullOuterJoin于cogroup的结果类似, 只是数据结构不一样</span></div><div class="line">    <span class="comment">/**</span></div><div class="line">      * (1,(Some(zhangsan),Some(30)))</div><div class="line">      * (2,(Some(lisi),Some(29)))</div><div class="line">      * (3,(Some(wangwu),None))</div><div class="line">      * (4,(None,Some(21)))</div><div class="line">      */</div><div class="line">    idName.fullOuterJoin(idAge).collect().foreach(println)</div><div class="line"></div><div class="line">    <span class="comment">/**</span></div><div class="line">      * id      score</div><div class="line">      * 1       100</div><div class="line">      * 2       90</div><div class="line">      * 2       95</div><div class="line">      */</div><div class="line">    <span class="keyword">val</span> idScore = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>, <span class="number">100</span>), (<span class="number">2</span>, <span class="number">90</span>), (<span class="number">2</span>, <span class="number">95</span>)))</div><div class="line"></div><div class="line">    println(<span class="string">"\ncogroup, 出现相同id时\n"</span>)</div><div class="line"></div><div class="line">    <span class="comment">/**</span></div><div class="line">      * (1,(CompactBuffer(zhangsan),CompactBuffer(100)))</div><div class="line">      * (2,(CompactBuffer(lisi),CompactBuffer(90, 95)))</div><div class="line">      * (3,(CompactBuffer(wangwu),CompactBuffer()))</div><div class="line">      */</div><div class="line">    idName.cogroup(idScore).collect().foreach(println)</div><div class="line"></div><div class="line">    println(<span class="string">"\njoin, 出现相同id时\n"</span>)</div><div class="line"></div><div class="line">    <span class="comment">/**</span></div><div class="line">      * (1,(Some(zhangsan),Some(100)))</div><div class="line">      * (2,(Some(lisi),Some(90)))</div><div class="line">      * (2,(Some(lisi),Some(95)))</div><div class="line">      * (3,(Some(wangwu),None))</div><div class="line">      */</div><div class="line">    idName.fullOuterJoin(idScore).collect().foreach(println)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h1 id="Actions算子"><a href="#Actions算子" class="headerlink" title="Actions算子"></a>Actions算子</h1><h2 id="无输出"><a href="#无输出" class="headerlink" title="无输出"></a>无输出</h2><h3 id="foreach"><a href="#foreach" class="headerlink" title="foreach"></a>foreach</h3><h3 id="foreachPartitions"><a href="#foreachPartitions" class="headerlink" title="foreachPartitions"></a>foreachPartitions</h3><p>Foreach与ForeachPartition都是在每个partition中对iterator进行操作,</p>
<p>不同的是,foreach是直接在每个partition中直接对iterator执行foreach操作,而传入的function只是在foreach内部使用,</p>
<p>而foreachPartition是在每个partition中把iterator给传入的function,让function自己对iterator进行处理（可以避免内存溢出）.</p>
<h2 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h2><h3 id="saveAsTextFile"><a href="#saveAsTextFile" class="headerlink" title="saveAsTextFile"></a>saveAsTextFile</h3><p>将数据输出，存储到HDFS的指定目录。</p>
<h3 id="saveAsObjectFile"><a href="#saveAsObjectFile" class="headerlink" title="saveAsObjectFile"></a>saveAsObjectFile</h3><p>将分区中每10个元素组成一个Array，然后将这个Array序列化，映射为<code>(Null, BytesWritable(Y))</code>的元素，写入HDFS为SequenceFile的格式。</p>
<h2 id="Scala集合和数据类型"><a href="#Scala集合和数据类型" class="headerlink" title="Scala集合和数据类型"></a>Scala集合和数据类型</h2><h3 id="collect"><a href="#collect" class="headerlink" title="collect"></a>collect</h3><p>相当于toArray，但toArray已经过时不推荐使用。collect将RDD返回为一个单机的scala Array数组。</p>
<h3 id="collectAsMap"><a href="#collectAsMap" class="headerlink" title="collectAsMap"></a>collectAsMap</h3><p>返回一个单机的HashMap。对于重复Key的RDD，后面覆盖前面的。</p>
<h3 id="reduceByKeyLocally"><a href="#reduceByKeyLocally" class="headerlink" title="reduceByKeyLocally"></a>reduceByKeyLocally</h3><p>先对RDD整体进行reduce再collectAsMap。</p>
<h3 id="lookup"><a href="#lookup" class="headerlink" title="lookup"></a>lookup</h3><p>Lookup函数对(Key,Value)型的RDD操作，返回指定Key对应的元素形成的Seq。这</p>
<p>个函数处理优化的部分在于，如果这个RDD包含分区器，则只会对应处理K所在的分区，然</p>
<p>后返回由(K,V)形成的Seq。如果RDD不包含分区器，则需要对全RDD元素进行暴力扫描</p>
<p>处理，搜索指定K对应的元素。</p>
<h3 id="count"><a href="#count" class="headerlink" title="count"></a>count</h3><h3 id="top"><a href="#top" class="headerlink" title="top"></a>top</h3><p>返回最大的k个元素</p>
<h3 id="take"><a href="#take" class="headerlink" title="take"></a>take</h3><p>返回最小的k个</p>
<h3 id="takeOrdered"><a href="#takeOrdered" class="headerlink" title="takeOrdered"></a>takeOrdered</h3><p>返回最小的k个元素，并且在返回的数组中保持元素的顺序。</p>
<h3 id="first"><a href="#first" class="headerlink" title="first"></a>first</h3><p>相当于top(1)，可以定义排序的方式Ordering[T]</p>
<h3 id="reduce"><a href="#reduce" class="headerlink" title="reduce"></a>reduce</h3><p>相当于对RDD中的元素进行reduceLeft函数的操作。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Some(iter.reduceLeft(cleanF))</div></pre></td></tr></table></figure>
<p>先对每个分区的集合进行reduceLeft，结果形成一个元素，再对这个结果做reduceLeft。</p>
<p>例如，用户自定义函数为</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">f:(A,B)=&gt;(A._1+&quot;@&quot;+B._1,A._2+B._2)</div></pre></td></tr></table></figure>
<p><img src="/2018/04/03/hadoop-spark/spark/spark算子分类及功能/reduce.png" alt=""></p>
<h3 id="fold"><a href="#fold" class="headerlink" title="fold"></a>fold</h3><p>和reduce原理相同，接收与reduce接收的函数签名相同的函数，不同点是，另外再加上一个初始值作为第一次调用的结果。</p>
<h3 id="aggregate"><a href="#aggregate" class="headerlink" title="aggregate"></a>aggregate</h3><p>可以对两个不同类型的元素进行聚合，即支持异构。它先聚合每一个分区里的元素，然后将所有结果返回回来，再用一个给定的conbine方法以及给定的初始值zero value进行聚合。</p>
<blockquote>
<p>没懂</p>
<p>aggreagate与fold和reduce的不同之处在于,aggregate相当于采用归并的方式进行数<br>据聚集,这种聚集是并行化的。而在fold和reduce函数的运算过程中,每个分区中需要进行<br>串行处理,每个分区串行计算完结果,结果再按之前的方式进行聚集,并返回最终聚集结<br>果。</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">def aggregate [U: ClassTag] (zeroValue: U) (seqOp: (U,T)=&gt;U，combOp: (U,U)=&gt;U):U</div></pre></td></tr></table></figure>
<p>由以上可以看到，(zeroValue: U)是给定一个初值，后半部分有两个函数，seqOp与combOp。<br>seqOp相当于是在各个分区里进行的聚合操作，它支持(U,T)=&gt;U，也就是支持不同类型的聚合。<br>combOp是将seqOp后的结果再进行聚合，此时的结果全部是U类，只能进行同构聚合。</p>
<h1 id="两个特殊变量"><a href="#两个特殊变量" class="headerlink" title="两个特殊变量"></a>两个特殊变量</h1><h2 id="broadcast"><a href="#broadcast" class="headerlink" title="broadcast"></a>broadcast</h2><p>用于广播Map Side Join中的小表，以及广播大变量等场景。这些数据集合在单节点内存能够容纳,不需要像RDD那样在节点之间打散存储。Spark运行时把广播变量数据发到各个节点,并保存下来,后续计算可以复用。</p>
<p>相比Hadoop的distributed cache，广播的内容可以跨作业共享。</p>
<h2 id="accumulator"><a href="#accumulator" class="headerlink" title="accumulator"></a>accumulator</h2><p>允许做全局累加操作，如accumulator变量广泛使用在应用中记录当前的运行指标的情景。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2018/04/01/算法与数据结构/布隆过滤器/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/04/01/算法与数据结构/布隆过滤器/" class="post-title-link" itemprop="url">Unbenannt</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2018-04-01 21:08:16" itemprop="dateCreated datePublished" datetime="2018-04-01T21:08:16+08:00">2018-04-01</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Bearbeitet am</span>
                
                <time title="Geändert am: 2019-02-18 19:00:46" itemprop="dateModified" datetime="2019-02-18T19:00:46+08:00">2019-02-18</time>
              </span>
            
          

          

          
            
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>转自《程序员代码面试指南 IT名企算法与数据结构题目最优解 ,左程云著》</p>
<p>如果碰到网页黑名单系统、爬虫的网址判重等，如果系统容忍一定程度的失误率，但对空间要求比较严格，往往是要求了解布隆过滤器。</p>
<p>一个布隆过滤器精确代表一个集合，并可以精确判断一个元素是否在集合中。但是有多精确，取决于具体的设计，但不可能完全正确。</p>
<h1 id="哈希函数"><a href="#哈希函数" class="headerlink" title="哈希函数"></a>哈希函数</h1><p>输入是任意，输出是固定范围，假设为S，并具有如下性质：</p>
<p>1、典型的哈希函数都有无限的输入值域。</p>
<p>2、传入相同的输入值，返回值一样。</p>
<p>3、传入不同输入值，返回值可能一样，也可能不一样。</p>
<p>4、最重要的性质：很多不同输入值得到的返回值会均匀分布在S上。</p>
<p>第4点是评价一个哈希函数优劣的关键，这种均匀分布与输入值出现的规律无关。比如MD5和SHA1算法。</p>
<h1 id="布隆过滤器"><a href="#布隆过滤器" class="headerlink" title="布隆过滤器"></a>布隆过滤器</h1><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>假设有一个长度为m的bit类型的数组，即数组中每个位置只占一个bit，每个bit只有0和1两种状态</p>
<p><img src="/2018/04/01/算法与数据结构/布隆过滤器/bit array.png" alt=""></p>
<p>再假设一共有k个哈希函数，这些函数的输出域S都大于或等于m，且这些哈希函数彼此独立。那么对同一个输入对象，经过k个哈希函数算出来的结果也是独立的。对算出来的每个结果都对m取余，然后再bit array上把相应位置设置1。</p>
<p><img src="/2018/04/01/算法与数据结构/布隆过滤器/bit array mod.png" alt=""></p>
<p>把bit类型的数组记为bitMap。处理完所有输入对象后，可能bitMap中已经有很多位置被涂黑。至此，一个布隆过滤器生成完毕，这个代表之前所有输入对象组成的集合。</p>
<p>在检查阶段，如何检查某一个对象是否是之前的某一个输入对象？</p>
<blockquote>
<p>一个输入对象，通过k个哈希算出k个值，把k个值对m取余，在bitMap上看这些位置是不是都是黑，如果有一个不为黑，则一定不在集合。如果都是黑，说明在集合，但是这里存在误判。</p>
</blockquote>
<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>如果bitMap的大小m相比于输入对象的个数n过小，失误率会变大。</p>
<p>假设黑名单中样本个数为100亿个，记为n；失误率不希望超过0.01%，记为p；每个样本（url）大小64B，当然这个大小不会影响布隆过滤器的大小。</p>
<p>所以n=100亿，p=0.01%，布隆过滤器的大小m由以下公式确定：</p>
<script type="math/tex; mode=display">
m = - \frac{n*\ln p}{(\ln 2)^2}</script><p>根据公式计算出m=19.19n，大约20n，即需要2000亿个bit，也即是25G。</p>
<p>哈希函数的个数由以下公式决定：</p>
<script type="math/tex; mode=display">
k = \ln 2 * \frac {m} {n}</script><p>计算出约为14个。</p>
<p>然后用25GB的bitMap再单独实现14个哈希函数，根据如上描述生成布隆过滤器。</p>
<p>失误率的计算</p>
<p>失误率就是某个不认识的url，经过k个哈希函数后，在布隆过滤器取余后发现这些位置上都是黑，这样的情况称为一个失误。那么<strong>失误率就是看k个位置都是黑的概率</strong>。</p>
<p>假设k个哈希函数独立，对于某一个bit位，一个输入对象在被k个哈希函数散列后，这个位置依然没有被涂黑的概率为：</p>
<script type="math/tex; mode=display">
(1-\frac 1 m)^k</script><p>经过n个输入对象后，这个位置依然没有被涂黑的概率为：</p>
<script type="math/tex; mode=display">
(1-\frac 1 m)^{kn}</script><p>被涂黑的概率就是</p>
<script type="math/tex; mode=display">
1-(1-\frac 1 m)^{kn}</script><p>则在检查阶段，检查k个位置都为黑的概率为：</p>
<script type="math/tex; mode=display">
(1-(1-\frac 1 m)^{kn})^k = (1-(1-\frac 1 m)^{-m*- \frac {-kn} {m}})^k</script><p>在$x \to 0$时，$(1+x)^{\frac 1 x} \to e$ 。这里m是很大的数，所以$-\frac 1m \to 0$，则简化为</p>
<script type="math/tex; mode=display">
(1-e^{\frac {-nk} {m}})^k</script><h2 id="误报的解决"><a href="#误报的解决" class="headerlink" title="误报的解决"></a>误报的解决</h2><p>通过建立白名单防止误报。比如已经发现某个样本不在布隆过滤器，但每次计算后结果都显示其在布隆过滤器中，则可以把该样本加入白名单中，以后就知道的确不在过滤器中。</p>
<h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p>爬虫项目中用到了布隆过滤器</p>
<p><code>[SimpleBloomFilter.java](../../../../gitlab/data-platform/page-feature/rtbreq-frontier/src/main/java/com/buzzinate/util/SimpleBloomFilter.java)</code></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> com.buzzinate.util;</div><div class="line"></div><div class="line"><span class="keyword">import</span> java.io.File;</div><div class="line"><span class="keyword">import</span> java.io.FileInputStream;</div><div class="line"><span class="keyword">import</span> java.io.FileOutputStream;</div><div class="line"><span class="keyword">import</span> java.io.ObjectInputStream;</div><div class="line"><span class="keyword">import</span> java.io.ObjectOutputStream;</div><div class="line"><span class="keyword">import</span> java.util.BitSet;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.commons.lang.StringUtils;</div><div class="line"><span class="keyword">import</span> org.apache.log4j.Logger;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SimpleBloomFilter</span></span>&#123;</div><div class="line">	<span class="keyword">private</span> <span class="keyword">static</span> Logger logger = Logger.getLogger(SimpleBloomFilter.class);</div><div class="line">	<span class="keyword">private</span> <span class="keyword">static</span> SimpleBloomFilter instance = <span class="keyword">null</span>;</div><div class="line">	<span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> DEFAULT_SIZE = Integer.MAX_VALUE;</div><div class="line">	<span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span>[] seeds = <span class="keyword">new</span> <span class="keyword">int</span>[] &#123; <span class="number">5</span>, <span class="number">7</span>, <span class="number">11</span>, <span class="number">13</span>, <span class="number">31</span>, <span class="number">37</span>, <span class="number">61</span> &#125;;</div><div class="line">	<span class="keyword">private</span> BitSet bits = <span class="keyword">new</span> BitSet(DEFAULT_SIZE);</div><div class="line">	<span class="keyword">private</span> SimpleHash[] func = <span class="keyword">new</span> SimpleHash[seeds.length];</div><div class="line"></div><div class="line">	<span class="keyword">public</span> <span class="keyword">static</span> String filename = <span class="string">"/opt/tomcat/bloomfilter"</span>;</div><div class="line">	<span class="function"><span class="keyword">private</span> <span class="title">SimpleBloomFilter</span><span class="params">()</span> </span>&#123;</div><div class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; seeds.length; i++) &#123;</div><div class="line">			func[i] = <span class="keyword">new</span> SimpleHash(DEFAULT_SIZE, seeds[i]);</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	</div><div class="line">	<span class="function"><span class="keyword">public</span> BitSet <span class="title">getBits</span><span class="params">()</span> </span>&#123;</div><div class="line">		<span class="keyword">return</span> bits;</div><div class="line">	&#125;</div><div class="line"></div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setBits</span><span class="params">(BitSet bits)</span> </span>&#123;</div><div class="line">		<span class="keyword">this</span>.bits = bits;</div><div class="line">	&#125;</div><div class="line"></div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">add</span><span class="params">(String value)</span> </span>&#123;</div><div class="line">		<span class="keyword">for</span> (SimpleHash f : func) &#123;</div><div class="line">			bits.set(f.hash(value), <span class="keyword">true</span>);</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">contains</span><span class="params">(String value)</span> </span>&#123;</div><div class="line">		<span class="keyword">if</span> (value == <span class="keyword">null</span>) &#123;</div><div class="line">			<span class="keyword">return</span> <span class="keyword">false</span>;</div><div class="line">		&#125;</div><div class="line">		<span class="keyword">boolean</span> ret = <span class="keyword">true</span>;</div><div class="line">		<span class="keyword">for</span> (SimpleHash f : func) &#123;</div><div class="line">			ret = ret &amp;&amp; bits.get(f.hash(value));</div><div class="line">		&#125;</div><div class="line">		<span class="keyword">return</span> ret;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">saveBit</span><span class="params">()</span> </span>&#123;</div><div class="line">		<span class="keyword">try</span> &#123;</div><div class="line">			</div><div class="line">			<span class="keyword">int</span> count = <span class="number">0</span>;</div><div class="line">			<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; DEFAULT_SIZE;i ++)&#123;</div><div class="line">				<span class="keyword">if</span>(bits.get(i))&#123;</div><div class="line">					count ++ ;</div><div class="line">				&#125;</div><div class="line">			&#125;</div><div class="line">			logger.info(<span class="string">"bloomfilter true bits:"</span>+count);</div><div class="line">			</div><div class="line">			File file = <span class="keyword">new</span> File(filename+<span class="string">".tmp"</span>);</div><div class="line">			logger.info(<span class="string">"save bloomfilter, path "</span>+ file.getAbsolutePath());</div><div class="line">			ObjectOutputStream oos = <span class="keyword">new</span> ObjectOutputStream(</div><div class="line">					<span class="keyword">new</span> FileOutputStream(file, <span class="keyword">false</span>));</div><div class="line">			oos.writeObject(bits);</div><div class="line">			</div><div class="line">			oos.flush();</div><div class="line">			oos.close();</div><div class="line">			file.renameTo(<span class="keyword">new</span> File(filename));</div><div class="line">			logger.info(<span class="string">"saved bloomfilter to "</span> + file.getAbsolutePath());</div><div class="line">		&#125; <span class="keyword">catch</span> (Exception e) &#123;</div><div class="line">			logger.error(e, e);</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">public</span> BitSet <span class="title">readBit</span><span class="params">(String path)</span> </span>&#123;</div><div class="line">		BitSet bits = <span class="keyword">new</span> BitSet(DEFAULT_SIZE);</div><div class="line">		File file = <span class="keyword">new</span> File(filename);</div><div class="line">		<span class="keyword">if</span>(StringUtils.isNotBlank(path))&#123;</div><div class="line">			file = <span class="keyword">new</span> File(path);</div><div class="line">		&#125;</div><div class="line">		logger.info(<span class="string">"read bloomfilter, path "</span>+ file.getAbsolutePath());</div><div class="line">		<span class="keyword">if</span> (!file.exists()) &#123;</div><div class="line">			logger.info(file.getAbsolutePath()+<span class="string">" not exists!"</span>);</div><div class="line">			<span class="keyword">return</span> bits;</div><div class="line">		&#125;</div><div class="line">		<span class="keyword">try</span> &#123;</div><div class="line">			ObjectInputStream ois = <span class="keyword">new</span> ObjectInputStream(<span class="keyword">new</span> FileInputStream(</div><div class="line">					file));</div><div class="line">			bits = (BitSet) ois.readObject();</div><div class="line">			ois.close();</div><div class="line">			<span class="keyword">int</span> count = <span class="number">0</span>;</div><div class="line">			<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; DEFAULT_SIZE;i ++)&#123;</div><div class="line">				<span class="keyword">if</span>(bits.get(i))&#123;</div><div class="line">					count ++ ;</div><div class="line">				&#125;</div><div class="line">			&#125;</div><div class="line">			logger.info(<span class="string">"bloomfilter true bits:"</span>+count);</div><div class="line">		&#125; <span class="keyword">catch</span> (Exception e) &#123;</div><div class="line">			logger.error(<span class="string">"read bloomfilter fail!"</span>, e);</div><div class="line">		&#125;</div><div class="line">		<span class="keyword">return</span> bits;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="comment">// 内部类，simpleHash</span></div><div class="line">	<span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SimpleHash</span> </span>&#123;</div><div class="line">		<span class="keyword">private</span> <span class="keyword">int</span> cap;</div><div class="line">		<span class="keyword">private</span> <span class="keyword">int</span> seed;</div><div class="line"></div><div class="line">		<span class="function"><span class="keyword">public</span> <span class="title">SimpleHash</span><span class="params">(<span class="keyword">int</span> cap, <span class="keyword">int</span> seed)</span> </span>&#123;</div><div class="line">			<span class="keyword">this</span>.cap = cap;</div><div class="line">			<span class="keyword">this</span>.seed = seed;</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">hash</span><span class="params">(String value)</span> </span>&#123;</div><div class="line">			<span class="keyword">int</span> result = <span class="number">0</span>;</div><div class="line">			<span class="keyword">int</span> len = value.length();</div><div class="line">			<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; len; i++) &#123;</div><div class="line">				result = seed * result + value.charAt(i);</div><div class="line">			&#125;</div><div class="line">			<span class="keyword">return</span> (cap - <span class="number">1</span>) &amp; result;</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">	</div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> SimpleBloomFilter <span class="title">getInstance</span><span class="params">()</span></span>&#123;</div><div class="line">		<span class="keyword">if</span>(instance == <span class="keyword">null</span>)&#123;</div><div class="line">			instance = <span class="keyword">new</span> SimpleBloomFilter();</div><div class="line">			<span class="keyword">if</span>(<span class="keyword">new</span> File(filename).exists())&#123;</div><div class="line">				instance.setBits(instance.readBit(<span class="string">""</span>));</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">		<span class="keyword">return</span> instance;</div><div class="line">	&#125;</div><div class="line">	</div><div class="line">	<span class="comment">/**</span></div><div class="line">	 * 改变实例</div><div class="line">	 * <span class="doctag">@param</span> path</div><div class="line">	 */</div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">changeInstance</span><span class="params">(String path)</span></span>&#123;</div><div class="line">		SimpleBloomFilter tmpInstance = <span class="keyword">new</span> SimpleBloomFilter();</div><div class="line">		File file = <span class="keyword">new</span> File(path);</div><div class="line">		<span class="keyword">if</span>(System.currentTimeMillis() - file.lastModified() &lt; <span class="number">3600</span> * <span class="number">1000</span>)&#123;</div><div class="line">			tmpInstance.setBits(tmpInstance.readBit(path));</div><div class="line">			instance = tmpInstance;</div><div class="line">			<span class="keyword">return</span> <span class="keyword">true</span>;</div><div class="line">		&#125;</div><div class="line">		<span class="keyword">return</span> <span class="keyword">false</span>;</div><div class="line">	&#125;</div><div class="line">	</div><div class="line">	<span class="comment">/**</span></div><div class="line">	 * 返回bit为1的数量</div><div class="line">	 * <span class="doctag">@return</span></div><div class="line">	 */</div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">trueBits</span><span class="params">()</span></span>&#123;</div><div class="line">		<span class="keyword">int</span> count = <span class="number">0</span>;</div><div class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; DEFAULT_SIZE;i ++)&#123;</div><div class="line">			<span class="keyword">if</span>(instance.bits.get(i))&#123;</div><div class="line">				count ++ ;</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">		<span class="keyword">return</span> count;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</div><div class="line">		SimpleBloomFilter filter = <span class="keyword">new</span> SimpleBloomFilter();</div><div class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt;<span class="number">100000</span>; i++) &#123;</div><div class="line">			filter.add(i + <span class="string">""</span>);</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		<span class="keyword">long</span> t1 = System.currentTimeMillis();</div><div class="line">		filter.saveBit();</div><div class="line">		<span class="keyword">long</span> t2 = System.currentTimeMillis();</div><div class="line">		System.out.println(<span class="string">"it takes "</span> + (t2 - t1)+<span class="string">"ms"</span>);</div><div class="line">		</div><div class="line">		</div><div class="line"><span class="comment">//		System.out.println("add end");</span></div><div class="line"><span class="comment">//		int cnt = 0;</span></div><div class="line"><span class="comment">//		for (int i = 200000; i &gt; 100000; i--) &#123;</span></div><div class="line"><span class="comment">//			if (filter.contains(i + "")) &#123;</span></div><div class="line"><span class="comment">//				cnt++;</span></div><div class="line"><span class="comment">//			&#125;</span></div><div class="line"><span class="comment">//		&#125;</span></div><div class="line"><span class="comment">//</span></div><div class="line"><span class="comment">//		System.out.println("total:" + cnt / 300000000.0);</span></div><div class="line">	&#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/19/"><i class="fa fa-angle-left" aria-label="Vorherige Seite"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/19/">19</a><span class="page-number current">20</span><a class="page-number" href="/page/21/">21</a><span class="space">&hellip;</span><a class="page-number" href="/page/32/">32</a><a class="extend next" rel="next" href="/page/21/"><i class="fa fa-angle-right" aria-label="Nächste Seite"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Schwimmer</p>
              <div class="site-description motion-element" itemprop="description">Record and Think!</div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">315</span>
                    <span class="site-state-item-name">Artikel</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">17</span>
                    <span class="site-state-item-name">Kategorien</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">55</span>
                    <span class="site-state-item-name">schlagwörter</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          

          

          
          

          
            
          
          

        </div>
      </div>

      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Schwimmer</span>

  

  
</div>


  <div class="powered-by">Erstellt mit  <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.5.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Design – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v7.2.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/utils.js?v=7.2.0"></script>

  <script src="/js/motion.js?v=7.2.0"></script>



  
  


  <script src="/js/schemes/muse.js?v=7.2.0"></script>



  

  


  <script src="/js/next-boot.js?v=7.2.0"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


  

  

  

  



  




  

  

  

  

  

  

  

  

  

  

  

  

  

  

</body>
</html>
