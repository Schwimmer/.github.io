<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">






















<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">

<link rel="stylesheet" href="/css/main.css?v=7.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.2.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.2.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '7.2.0',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="Record and Think!">
<meta property="og:type" content="website">
<meta property="og:title" content="Schwimmer&#39;s Blog">
<meta property="og:url" content="https://schwimmer.github.io/page/6/index.html">
<meta property="og:site_name" content="Schwimmer&#39;s Blog">
<meta property="og:description" content="Record and Think!">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Schwimmer&#39;s Blog">
<meta name="twitter:description" content="Record and Think!">





  
  
  <link rel="canonical" href="https://schwimmer.github.io/page/6/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Schwimmer's Blog</title>
  




  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-143240576-1"></script>
  <script>
    var host = window.location.hostname;
    if (host !== "localhost" || !true) {
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-143240576-1');
    }
  </script>









  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Schwimmer's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
        </li>
      
    </ul>
  

  
    

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/05/机器学习和深度学习算法专题/EM/EM算法入门/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/05/机器学习和深度学习算法专题/EM/EM算法入门/" class="post-title-link" itemprop="url">未命名</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-05 13:52:55 / 修改时间：13:54:49" itemprop="dateCreated datePublished" datetime="2019-06-05T13:52:55+08:00">2019-06-05</time>
            </span>
          

          
            

            
          

          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/05/机器学习和深度学习算法专题/EM/EM算法入门/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/05/机器学习和深度学习算法专题/EM/EM算法入门/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://zhuanlan.zhihu.com/p/61768577?utm_source=ZHShareTargetIDMore&amp;utm_medium=social&amp;utm_oi=28206795063296" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/61768577?utm_source=ZHShareTargetIDMore&amp;utm_medium=social&amp;utm_oi=28206795063296</a></p>
<p>这个推导啥子的也太难了把。不过经过我不停不停不停不停的看这个算法，到今天我突然觉得自己好像明白了，然后我决定把我的理解写成一篇文章，毕竟只有给别人讲明白了才能算自己真正的明白。那么就进入我们这篇文章的主题:<strong>EM算法。</strong></p>
<p>我们先讲一下极大似然估计法，然后再引申出EM算法</p>
<p><strong>1.极大似然估计法</strong></p>
<p>假设我们有如下的一维高斯分布</p>
<p><img src="https://www.zhihu.com/equation?tex=X%5Csim+N%3D%28%5Cmu%2C+%5Csigma%5E%7B2%7D%29+" alt="X\sim N=(EM/pic/equation-20190605135301319) "></p>
<p>X的概率密度函数为:</p>
<p><img src="https://www.zhihu.com/equation?tex=f%28x%3B%5Cmu%2C%5Csigma%5E%7B2%7D%29%3D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%5Csigma%7D%7De%5E%7B-%5Cfrac%7B%28x-%5Cmu%29%5E%7B2%7D%7D%7B2%5Csigma%5E%7B2%7D%7D%7D" alt="f(EM/pic/equation-20190605135301359)=\frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}}"></p>
<p>其似然函数为</p>
<p><img src="https://www.zhihu.com/equation?tex=L%28%5Cmu%2C%5Csigma%5E%7B%5E%7B2%7D%7D%29%3D%5Cprod_%7Bi%3D1%7D%5E%7Bn%7D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%5Csigma%7D%7De%5E%7B-%5Cfrac%7B%28x-%5Cmu%29%5E%7B2%7D%7D%7B2%5Csigma%5E%7B2%7D%7D%7D" alt="L(EM/pic/equation-20190605135332617)=\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}}"></p>
<p>求对数为</p>
<p><img src="https://www.zhihu.com/equation?tex=lnL%3D-%5Cfrac%7Bn%7D%7B2%7Dln%5Cpi-%5Cfrac%7Bn%7D%7B2%7Dln%5Csigma%5E%7B2%7D-%5Cfrac%7B1%7D%7B2%5Csigma%5E%7B2%7D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7B%28x_%7Bi%7D-%5Cmu%29%5E%7B2%7D%7D%7D" alt="lnL=-\frac{n}{2}ln\pi-\frac{n}{2}ln\sigma^{2}-\frac{1}{2\sigma^{2}\sum_{i=1}^{n}{(EM/pic/equation-20190605135332570)^{2}}}"></p>
<p>对其求导，可以得到如下似然方程组</p>
<p><img src="//schwimmer.github.io/2019/06/05/机器学习和深度学习算法专题/EM/EM算法入门/pic/image-20190605135436560.png" alt="image-20190605135436560"></p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cdelta%7D%7B%5Cdelta%5Csigma%5E%7B2%7D%7D%3D-%5Cfrac%7Bn%7D%7B2%5Csigma%5E%7B2%7D%7D%2B%5Cfrac%7B1%7D%7B2%28%5Csigma%5E%7B2%7D%29%5E%7B2%7D%7D%5B%5Csum_%7Bi%3D1%7D%5E%7Bn%7Dx_%7Bi%7D-n%5Cmu%5D" alt="\frac{\delta}{\delta\sigma^{2}}=-\frac{n}{2\sigma^{2}}+\frac{1}{2(EM/pic/equation-20190605135332601)^{2}}[\sum_{i=1}^{n}x_{i}-n\mu]"></p>
<p>我们可以使用</p>
<ul>
<li>梯度下降法</li>
<li>极大似然估计法</li>
</ul>
<p>这两种方法来根据样本估计高斯分布的参数，具体代码如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_guassian_theta</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># u=1 siga=4</span></span><br><span class="line">    y = <span class="number">1</span> + <span class="number">2</span>*np.random.randn(<span class="number">1000</span>,<span class="number">1</span>)</span><br><span class="line">    n,m = y.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 梯度下降</span></span><br><span class="line">    u1 = np.random.randn(<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">    siga1 = np.random.randn(<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">    lr = <span class="number">0.001</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">        lu = (np.sum(y,axis=<span class="number">0</span>) - n*u1)/(siga1)</span><br><span class="line">        lsiga = (np.sum((y-u1)**<span class="number">2</span>, axis=<span class="number">0</span>)/siga1 - n)/(<span class="number">2</span>*siga1)</span><br><span class="line">        u1 = u1+lr*lu</span><br><span class="line">        siga1 = siga1+lr*lsiga</span><br><span class="line">    print(<span class="string">"u1:   %lf, siga1:   %lf"</span>%(u1, siga1))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 解析解</span></span><br><span class="line">    u2 = np.sum(y, axis = <span class="number">0</span>)/n</span><br><span class="line">    siga2 = np.sum((y-u2)**<span class="number">2</span>, axis = <span class="number">0</span>)/n</span><br><span class="line">    print(<span class="string">"u2:   %lf, siga2:   %lf"</span>%(u2, siga2))</span><br></pre></td></tr></table></figure>
<p>我们使用均值为1，标准差为2的高斯分布随机生成了1000个样本，然后分别使用梯度下降和极大似然估计法两种方式来估计参数，得到的参数如下:</p>
<p><img src="//schwimmer.github.io/2019/06/05/机器学习和深度学习算法专题/EM/EM算法入门/pic/v2-4d49b54725f875723438ba87a94020e2_hd.png" alt="img"></p>
<p>两种方法得到的结果还是挺不错的。</p>
<p><strong>2. EM算法</strong></p>
<p><strong>极大似然算法确实可以很方便的根据样本估算模型的参数，如果样本来自一个以上的模型，我们又不知道某个样本点到底是来自某个模型的</strong>，那么此时极大似然算法就无能为力了。</p>
<p>我们依旧用高斯分布来举例子，混合高斯分布的模型如下:</p>
<p><img src="https://www.zhihu.com/equation?tex=P%28y%7C%5Ctheta%29%3D%5Csum_%7Bk%3D1%7D%5E%7BK%7D%5Calpha_%7Bk%7D%5Cphi%28y%7C%5Ctheta_%7Bk%7D%29" alt="P(EM/pic/equation-20190605135345806)=\sum_{k=1}^{K}\alpha_{k}\phi(y|\theta_{k})"></p>
<p>其中 <img src="https://www.zhihu.com/equation?tex=%5Calpha_%7Bk%7D" alt="\alpha_{k}"> 是系数， <img src="https://www.zhihu.com/equation?tex=%5Calpha_%7Bk%7D%5Cgeq0" alt="\alpha_{k}\geq0"> ，同时 <img src="//schwimmer.github.io/2019/06/05/机器学习和深度学习算法专题/EM/EM算法入门/pic/equation-20190605135345792" alt="\sum_{k=1}^{K}a_{k}=1"> ； <img src="https://www.zhihu.com/equation?tex=%5Cphi%28y%7C%5Ctheta_%7Bk%7D%29" alt="\phi(y|\theta_{k})"> 是高斯分布密度函数， <img src="https://www.zhihu.com/equation?tex=%5Ctheta_%7Bk%7D%3D%28u_%7Bk%7D%2C+%5Csigma_%7Bk%7D%5E%7B2%7D%29" alt="\theta_{k}=(u_{k}, \sigma_{k}^{2})"> 。</p>
<p>这个时候我们需要估计的参数有</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Ctheta%3D%28%5Calpha_%7B1%7D%2C+...%2C+%5Calpha_%7Bk%7D%3B%5Ctheta_%7B1%7D%2C+.....%2C%5Ctheta_%7Bk%7D%29" alt="\theta=(EM/pic/equation?tex=%5Ctheta%3D%28%5Calpha_%7B1%7D%2C+...%2C+%5Calpha_%7Bk%7D%3B%5Ctheta_%7B1%7D%2C+.....%2C%5Ctheta_%7Bk%7D%29)"></p>
<p><strong>此时阻挡我们使用极大似然法的原因就是:我们不知道到底哪些样本点由哪个模型生成。</strong></p>
<p>现在假设我们有1000个样本点，由两个独立的高斯分布生成。我们知道其中第一类有300个，第二类有700个，那么我们就可以对两个高斯分布分别使用极大似然法估计他们的参数了。</p>
<p>但事实上我们知道的只有一堆样本点以及其可能的类别数 <img src="//schwimmer.github.io/2019/06/05/机器学习和深度学习算法专题/EM/EM算法入门/pic/equation-20190605135345838" alt="K"> ，至于某个样本到底属于那个模型我们是不知道。此时就要到EM算法登场的时候了，<strong>EM算法的主要思想如下：</strong></p>
<ul>
<li><strong>E步:先随便设置一下各种参数<img src="https://www.zhihu.com/equation?tex=%5Ctheta%3D%28%5Calpha_%7B1%7D%2C+...%2C+%5Calpha_%7Bk%7D%3B%5Ctheta_%7B1%7D%2C+.....%2C%5Ctheta_%7Bk%7D%29" alt="\theta=(\alpha_{1}, ..., \alpha_{k};\theta_{1}, .....,\theta_{k})">，然后再算一下在当前情况下每个样本点属于哪一个模型的概率值；</strong></li>
<li><strong>M步:此时我们知道了一个样本点属于某个模型的概率，然后再次计算各个模型的参数（具体计算方法在下面）；然后返回上一步，直至算法收敛。</strong></li>
</ul>
<p>现在我们知道了EM算法的思想，那么EM算法是怎么在第二步估算出各个模型的参数呢。</p>
<p>我们先介绍一些概念：用 <img src="//schwimmer.github.io/2019/06/05/机器学习和深度学习算法专题/EM/EM算法入门/pic/equation-20190605135345838-9714025." alt="Y"> 来表示观测随机变量的数据， <img src="//schwimmer.github.io/2019/06/05/机器学习和深度学习算法专题/EM/EM算法入门/pic/equation-20190605135345835" alt="Z"> 表示隐随机变量的数据（比如上述混合高斯分布样本点属于某个模型的概率）， <img src="//schwimmer.github.io/2019/06/05/机器学习和深度学习算法专题/EM/EM算法入门/pic/equation-20190605135345838-9714025." alt="Y"> 和 <img src="//schwimmer.github.io/2019/06/05/机器学习和深度学习算法专题/EM/EM算法入门/pic/equation-20190605135345837" alt="Z "> 连在一起称为<strong>完全数据(这个我们是没法知道的)，</strong>观测数据 <img src="//schwimmer.github.io/2019/06/05/机器学习和深度学习算法专题/EM/EM算法入门/pic/equation-20190605135345838-9714025." alt="Y"> 也被称为<strong>不完全数据(这个我们知道)，</strong> <img src="//schwimmer.github.io/2019/06/05/机器学习和深度学习算法专题/EM/EM算法入门/pic/equation-20190605135345835" alt="Z"> 被称为<strong>隐变量(我们不知道)</strong>，假定给定观测数据 <img src="//schwimmer.github.io/2019/06/05/机器学习和深度学习算法专题/EM/EM算法入门/pic/equation-20190605135345838-9714025." alt="Y"> ，其概率分布是 <img src="https://www.zhihu.com/equation?tex=P%28Y%7C%5Ctheta%29" alt="P(Y|\theta)"> ，其中 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="\theta"> 是需要估计的模型参数，那么不完全数据 <img src="//schwimmer.github.io/2019/06/05/机器学习和深度学习算法专题/EM/EM算法入门/pic/equation-20190605135345838-9714025." alt="Y"> 的似然函数是 <img src="https://www.zhihu.com/equation?tex=P%28Y%7C%5Ctheta%29" alt="P(Y|\theta)"> ，对数似然函数为 <img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%29%3DlogP%28Y%7C%5Ctheta%29" alt="L(\theta)=logP(Y|\theta)"> ；<img src="//schwimmer.github.io/2019/06/05/机器学习和深度学习算法专题/EM/EM算法入门/pic/equation-20190605135345838-9714025." alt="Y"> 和 <img src="//schwimmer.github.io/2019/06/05/机器学习和深度学习算法专题/EM/EM算法入门/pic/equation-20190605135345837" alt="Z "> 的联合概率分布是 <img src="https://www.zhihu.com/equation?tex=P%28Y%2CZ%7C%5Ctheta%29" alt="P(Y,Z|\theta)"> ，其对数似然函数是 <img src="https://www.zhihu.com/equation?tex=logP%28Y%2CZ%7C%5Ctheta%29" alt="logP(Y,Z|\theta)"> 。</p>
<p>EM算法是通过迭代来求 <img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%29%3DlogP%28Y%7C%5Ctheta%29" alt="L(EM/pic/equation-20190605135345863)=logP(Y|\theta)">的极大似然估计，也就是在估计出来的参数条件下，模型产生给定样本点的概率最大~。因此我们要最大化下式。</p>
<p><img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%29%3DlogP%28Y%7C%5Ctheta%29%3Dlog%5Csum_%7BZ%7DP%28Y%2CZ%7C%5Ctheta%29%3Dlog%28%5Csum_%7BZ%7DP%28Y%7CZ%2C%5Ctheta%29P%28Z%7C%5Ctheta%29%29" alt="L(EM/pic/equation-20190605135345914)=logP(Y|\theta)=log\sum_{Z}P(Y,Z|\theta)=log(\sum_{Z}P(Y|Z,\theta)P(Z|\theta))"></p>
<p><strong>上式中最右边的 <img src="https://www.zhihu.com/equation?tex=P%28Z%7C%5Ctheta%29" alt="P(EM/pic/equation-20190605135345874)"> 指的的一个模型被选择的概率， <img src="https://www.zhihu.com/equation?tex=P%28Y%7CZ%2C%5Ctheta%29" alt="P(Y|Z,\theta)"> 是指我们选定了一个模型，此模型产生这个样本点的概率</strong>。</p>
<p><img src="//schwimmer.github.io/2019/06/05/机器学习和深度学习算法专题/EM/EM算法入门/pic/v2-dcb858e6f913ffd51cf8219a57f96122_hd.jpg" alt="img"></p>
<p>上图是一维高斯混合分布，黄色的那个高斯分布均值为0，方差为1，被选择的概率为0.3；红色的那个高斯分布均值为3，方差为4，被选择的概率为0.7。</p>
<p>(下面部分参考《李航统计学习》P.159，推导更详细了一点)</p>
<p>EM是通过迭代的方法来逐步逼近近似极大化 <img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%29" alt="L(EM/pic/equation-20190605135345840)"> ，假设某一次我们得到了模型的参数估计值为 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%28i%29%7D" alt="\theta^{(i)}"> (是一个我们知道的值)，我们要求估计新的 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="\theta">可以时 <img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%29" alt="L(EM/pic/equation-20190605135345840)"> 增大，即 <img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta+%29%3EL%28%5Ctheta%5E%7B%28i%29%7D%29" alt="L(\theta )&gt;L(\theta^{(i)})"> 。我们计算两者的差</p>
<p><img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%29-L%28%5Ctheta%5E%7B%28i%29%7D%29%3Dlog%28%5Csum_%7BZ%7DP%28Y%7CZ%2C%5Ctheta%29P%28Z%7C%5Ctheta%29%29-logP%28Y%7C%5Ctheta%5E%7B%28i%29%7D%29" alt="L(\theta)-L(\theta^{(i)})=log(\sum_{Z}P(Y|Z,\theta)P(Z|\theta))-logP(Y|\theta^{(i)})"></p>
<p>利用Jenson不等式</p>
<p><img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%29-L%28%5Ctheta%5E%7Bi%7D%29%3Dlog%28%5Csum_%7BZ%7DP%28Y%7CZ%2C+%5Ctheta%5E%7B%28i%29%7D%29%5Cfrac%7BP%28Y%7CZ%2C%5Ctheta%29P%28Z%7C%5Ctheta%29%7D%7BP%28Y%7CZ%2C+%5Ctheta%5E%7B%28i%29%7D%29%7D%29-logP%28Y%7C%5Ctheta%5E%7B%28i%29%7D%29%5C%5C+%5Cgeq+%5Csum_%7BZ%7DP%28Z%7CY%2C+%5Ctheta%5E%7B%28i%29%7D%29log%5Cfrac%7BP%28Y%7CZ%2C%5Ctheta%29P%28Z%7C%5Ctheta%29%7D%7BP%28Z%7CY%2C+%5Ctheta%5E%7B%28i%29%7D%29%7D-logP%28Y%7C%5Ctheta%5E%7B%28i%29%7D%29%5C%5C+%3D%5Csum_%7BZ%7DP%28Z%7CY%2C+%5Ctheta%5E%7B%28i%29%7D%29log%5Cfrac%7BP%28Y%7CZ%2C%5Ctheta%29P%28Z%7C%5Ctheta%29%7D%7BP%28Z%7CY%2C+%5Ctheta%5E%7B%28i%29%7D%29%7D-logP%28Y%7C%5Ctheta%5E%7B%28i%29%7D%29%5Csum_%7BZ%7DP%28Z%7CY%2C+%5Ctheta%5E%7B%28i%29%7D%29%5C%5C+%3D%5Csum_%7BZ%7DP%28Z%7CY%2C+%5Ctheta%5E%7B%28i%29%7D%29log%5Cfrac%7BP%28Y%7CZ%2C%5Ctheta%29P%28Z%7C%5Ctheta%29%7D%7BP%28Z%7CY%2C+%5Ctheta%5E%7B%28i%29%7D%29P%28Y%7C%5Ctheta%5E%7B%28i%29%7D%29%7D" alt="L(EM/pic/equation-20190605135346171)-L(\theta^{i})=log(\sum_{Z}P(Y|Z, \theta^{(i)})\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Y|Z, \theta^{(i)})})-logP(Y|\theta^{(i)})\\ \geq \sum_{Z}P(Z|Y, \theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y, \theta^{(i)})}-logP(Y|\theta^{(i)})\\ =\sum_{Z}P(Z|Y, \theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y, \theta^{(i)})}-logP(Y|\theta^{(i)})\sum_{Z}P(Z|Y, \theta^{(i)})\\ =\sum_{Z}P(Z|Y, \theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y, \theta^{(i)})P(Y|\theta^{(i)})}"></p>
<p>第一步到第二步除了使用了Jenson不等式，还使用了 <img src="https://www.zhihu.com/equation?tex=P%28X%7CY%29%3D%5Cfrac%7BP%28Y%7CX%29P%28X%29%7D%7BP%28Y%29%7D" alt="P(EM/pic/equation-20190605135345893)=\frac{P(Y|X)P(X)}{P(Y)}"> ，其中的 <img src="https://www.zhihu.com/equation?tex=P%28X%29%2CP%28Y%29" alt="P(X),P(Y)"> 都被约去。</p>
<p>令</p>
<p><img src="https://www.zhihu.com/equation?tex=B%28%5Ctheta%2C%5Ctheta%5E%7B%28i%29%7D%29%3DL%28%5Ctheta%5E%7B%28i%29%7D%29%2B%5Csum_%7BZ%7DP%28Z%7CY%2C+%5Ctheta%5E%7B%28i%29%7D%29log%5Cfrac%7BP%28Y%7CZ%2C%5Ctheta%29P%28Z%7C%5Ctheta%29%7D%7BP%28Z%7CY%2C+%5Ctheta%5E%7B%28i%29%7D%29P%28Y%7C%5Ctheta%5E%7B%28i%29%7D%29%7D" alt="B(EM/pic/equation-20190605135346027)=L(\theta^{(i)})+\sum_{Z}P(Z|Y, \theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y, \theta^{(i)})P(Y|\theta^{(i)})}"></p>
<p>则有 <img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%29+%5Cgeq+B%28%5Ctheta%2C+%5Ctheta%5E%7B%28i%29%7D%29" alt="L(EM/pic/equation-20190605135345897) \geq B(\theta, \theta^{(i)})"> ，为了使 <img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%29" alt="L(EM/pic/equation-20190605135345840)"> 尽可能的大，我们应该选择 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%28i%2B1%29%7D" alt="\theta^{(i+1)}"> 使 <img src="https://www.zhihu.com/equation?tex=+B%28%5Ctheta%2C+%5Ctheta%5E%7B%28i%29%7D%29" alt=" B(\theta, \theta^{(i)})"> 达到极大值。即 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%28i%2B1%29%7D%3Darg%5Cmax_%7B%5Ctheta%7DB%28%5Ctheta%2C%5Ctheta%5E%7B%28i%29%7D%29" alt="\theta^{(i+1)}=arg\max_{\theta}B(\theta,\theta^{(i)})"></p>
<p>通过省去对 <img src="//schwimmer.github.io/2019/06/05/机器学习和深度学习算法专题/EM/EM算法入门/pic/equation-20190605135345850" alt="\theta"> 极大化是常数的项。</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%28i%2B1%29%7D%3Darg%5Cmax_%7B%5Ctheta%7D%28L%28%5Ctheta%5E%7B%28i%29%7D%29%2B%5Csum_%7BZ%7DP%28Z%7CY%2C+%5Ctheta%5E%7B%28i%29%7D%29log%5Cfrac%7BP%28Y%7CZ%2C%5Ctheta%29P%28Z%7C%5Ctheta%29%7D%7BP%28Z%7CY%2C+%5Ctheta%5E%7B%28i%29%7D%29P%28Y%7C%5Ctheta%5E%7B%28i%29%7D%29%7D%29%5C%5C+%3Darg%5Cmax_%7B%5Ctheta%7D%28%5Csum_%7BZ%7DP%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%29logP%28Y%7CZ%2C%5Ctheta%29P%28Z%7C%5Ctheta%29%29%5C%5C+%3Darg%5Cmax_%7B%5Ctheta%7D%28%5Csum_%7BZ%7DP%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%29logP%28Y%2CZ%7C%5Ctheta%29%29" alt="\theta^{(EM/pic/equation-20190605135346102)}=arg\max_{\theta}(L(\theta^{(i)})+\sum_{Z}P(Z|Y, \theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y, \theta^{(i)})P(Y|\theta^{(i)})})\\ =arg\max_{\theta}(\sum_{Z}P(Z|Y,\theta^{(i)})logP(Y|Z,\theta)P(Z|\theta))\\ =arg\max_{\theta}(\sum_{Z}P(Z|Y,\theta^{(i)})logP(Y,Z|\theta))"></p>
<p>其中 <img src="https://www.zhihu.com/equation?tex=P%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%29" alt="P(Z|Y,\theta^{(i)})"> 指的是当我们知道模型的参数和样本的分布情况时，此时隐变量的状态。如果模型为高斯分布，那么 <img src="https://www.zhihu.com/equation?tex=P%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%29" alt="P(Z|Y,\theta^{(i)})"> 指的就是样本点属于某个模型的概率； <img src="https://www.zhihu.com/equation?tex=logP%28Y%2CZ%7C%5Ctheta%29" alt="logP(Y,Z|\theta)"> 就是我们要找到 <img src="//schwimmer.github.io/2019/06/05/机器学习和深度学习算法专题/EM/EM算法入门/pic/equation-20190605135345850" alt="\theta"> 使得在当前 <img src="//schwimmer.github.io/2019/06/05/机器学习和深度学习算法专题/EM/EM算法入门/pic/equation-20190605135345859-9714025." alt="Y,Z"> 的情况下，获得函数的一个极大值。</p>
<p>EM算法的流程如下:</p>
<p>(1)随机选择参数 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%280%29%7D" alt="\theta^{(EM/pic/equation-20190605135345872)}"> ,开始迭代</p>
<p>(2)E步:计算 <img src="https://www.zhihu.com/equation?tex=Q%28%5Ctheta%2C%5Ctheta%5E%7B%28i%29%7D%29%3D%5Csum_%7BZ%7DP%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%29logP%28Y%2CZ%7C%5Ctheta%29" alt="Q(EM/pic/equation-20190605135345921)=\sum_{Z}P(Z|Y,\theta^{(i)})logP(Y,Z|\theta)"></p>
<p>(3)M步:最大化 <img src="https://www.zhihu.com/equation?tex=Q%28%5Ctheta%2C%5Ctheta%5E%7B%28i%29%7D%29" alt="Q(EM/pic/equation-20190605135345901)"></p>
<p>(4)重复(2),(3)步直到收敛</p>
<p>对高斯混合模型使用EM算法估计参数，其中第一个高斯分布均值为3，方差为4，系数为0.7；第二个高斯分布均值为0，方差为1，系数为0.3。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_mix_guassian_theta</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># u = 3, siga = 4, alpha = 0.7</span></span><br><span class="line">    f1 = <span class="number">3</span>+<span class="number">2</span>*np.random.randn(<span class="number">700</span>,<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># u = 0, siga = 1, alpha = 0.3 </span></span><br><span class="line">    f2 = <span class="number">1</span>+np.random.rand(<span class="number">300</span>,<span class="number">1</span>)</span><br><span class="line">    f = np.concatenate((f1,f2),axis=<span class="number">0</span>)</span><br><span class="line">    n,m = f.shape</span><br><span class="line"></span><br><span class="line">    alpha = np.random.rand(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">    alpha = alpha/np.sum(alpha, axis=<span class="number">1</span>)</span><br><span class="line">    u = np.random.rand(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">    siga = np.random.rand(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># EM算法求解</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">        <span class="comment">#1.E步</span></span><br><span class="line">        gamma1 = gaussian(f, u[<span class="number">0</span>][<span class="number">0</span>], siga[<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line">        gamma2 = gaussian(f, u[<span class="number">0</span>][<span class="number">1</span>], siga[<span class="number">0</span>][<span class="number">1</span>])</span><br><span class="line">        gamma = np.concatenate((gamma1, gamma2), axis = <span class="number">1</span>)</span><br><span class="line">        gamma = alpha*gamma/np.sum(alpha*gamma, axis = <span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">        <span class="comment">#2.M步</span></span><br><span class="line">        u = np.sum(gamma*f, axis = <span class="number">0</span>, keepdims=<span class="keyword">True</span>)/np.sum(gamma, axis = <span class="number">0</span>,keepdims=<span class="keyword">True</span>)</span><br><span class="line">        siga = np.sum(gamma*((f-u)**<span class="number">2</span>), axis = <span class="number">0</span>, keepdims=<span class="keyword">True</span>)/np.sum(gamma, axis = <span class="number">0</span>,keepdims=<span class="keyword">True</span>)</span><br><span class="line">        siga = siga**(<span class="number">1</span>/<span class="number">2</span>)</span><br><span class="line">        alpha = np.sum(gamma, axis = <span class="number">0</span>, keepdims=<span class="keyword">True</span>)/n</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"alpha1:  %lf,alpha2:    %lf"</span>%(alpha[<span class="number">0</span>][<span class="number">0</span>], alpha[<span class="number">0</span>][<span class="number">1</span>]))</span><br><span class="line">    print(<span class="string">"u1:  %lf,u2:    %lf"</span>%(u[<span class="number">0</span>][<span class="number">0</span>], u[<span class="number">0</span>][<span class="number">1</span>]))</span><br><span class="line">    print(<span class="string">"siga1:  %lf,siga2:    %lf"</span>%(siga[<span class="number">0</span>][<span class="number">0</span>], siga[<span class="number">0</span>][<span class="number">1</span>]))</span><br></pre></td></tr></table></figure>
<p>算法的估计值如下:</p>
<p><img src="//schwimmer.github.io/2019/06/05/机器学习和深度学习算法专题/EM/EM算法入门/pic/v2-c18d437f580e44cf99175d651a705650_hd.png" alt="img"></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/05/机器学习和深度学习算法专题/Transformer/Attention/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/05/机器学习和深度学习算法专题/Transformer/Attention/" class="post-title-link" itemprop="url">未命名</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-05 13:50:50 / 修改时间：13:55:00" itemprop="dateCreated datePublished" datetime="2019-06-05T13:50:50+08:00">2019-06-05</time>
            </span>
          

          
            

            
          

          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/05/机器学习和深度学习算法专题/Transformer/Attention/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/05/机器学习和深度学习算法专题/Transformer/Attention/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://kexue.fm/archives/4765" target="_blank" rel="noopener">《Attention is All You Need》浅读（简介+代码）</a></p>
<p>2017年中，有两篇类似同时也是笔者非常欣赏的论文，分别是FaceBook的《Convolutional Sequence to Sequence Learning》和Google的《Attention is All You Need》，它们都算是Seq2Seq上的创新，本质上来说，都是抛弃了RNN结构来做Seq2Seq任务。</p>
<p>这篇博文中，笔者对<a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">《Attention is All You Need》</a>做一点简单的分析。当然，这两篇论文本身就比较火，因此网上已经有很多解读了（不过很多解读都是直接翻译论文的，鲜有自己的理解），因此这里尽可能多自己的文字，尽量不重复网上各位大佬已经说过的内容。</p>
<h2 id="序列编码"><a href="#序列编码" class="headerlink" title="序列编码 #"></a>序列编码<a href="https://kexue.fm/archives/4765#序列编码" target="_blank" rel="noopener"> #</a></h2><p>深度学习做NLP的方法，基本上都是先将句子分词，然后每个词转化为对应的词向量序列。这样一来，每个句子都对应的是一个矩阵<strong><em>X</em></strong>=(<strong><em>x</em></strong>1,<strong><em>x</em></strong>2,…,<strong><em>x</em></strong>t)X=(x1,x2,…,xt)，其中<strong><em>x</em></strong>ixi都代表着第ii个词的词向量（行向量），维度为dd维，故<strong><em>X</em></strong>∈ℝn×dX∈Rn×d。这样的话，问题就变成了编码这些序列了。</p>
<p>第一个基本的思路是RNN层，RNN的方案很简单，递归式进行：</p>
<p><strong><em>y</em></strong>t=f(<strong><em>y</em></strong>t−1,<strong><em>x</em></strong>t)yt=f(yt−1,xt)</p>
<p>不管是已经被广泛使用的LSTM、GRU还是最近的SRU，都并未脱离这个递归框架。RNN结构本身比较简单，也很适合序列建模，但RNN的明显缺点之一就是无法并行，因此速度较慢，这是递归的天然缺陷。另外我个人觉得</p>
<p>RNN无法很好地学习到全局的结构信息，因为它本质是一个马尔科夫决策过程。</p>
<p>第二个思路是CNN层，其实CNN的方案也是很自然的，窗口式遍历，比如尺寸为3的卷积，就是</p>
<p><strong><em>y</em></strong>t=f(<strong><em>x</em></strong>t−1,<strong><em>x</em></strong>t,<strong><em>x</em></strong>t+1)yt=f(xt−1,xt,xt+1)</p>
<p>在FaceBook的论文中，纯粹使用卷积也完成了Seq2Seq的学习，是卷积的一个精致且极致的使用案例，热衷卷积的读者必须得好好读读这篇文论。</p>
<p>CNN方便并行，而且容易捕捉到一些全局的结构信息，笔者本身是比较偏爱CNN的，在目前的工作或竞赛模型中，我都已经尽量用CNN来代替已有的RNN模型了，并形成了自己的一套使用经验</p>
<p>，这部分我们以后再谈。</p>
<p>Google的大作提供了第三个思路：<strong>纯Attention！单靠注意力就可以！</strong>RNN要逐步递归才能获得全局信息，因此一般要双向RNN才比较好；CNN事实上只能获取局部信息，是通过层叠来增大感受野；Attention的思路最为粗暴，它一步到位获取了全局信息！它的解决方案是：</p>
<p><strong><em>y</em></strong>t=f(<strong><em>x</em></strong>t,<strong><em>A</em></strong>,<strong><em>B</em></strong>)yt=f(xt,A,B)</p>
<p>其中</p>
<p><strong><em>A</em></strong>,<strong><em>B</em></strong>A,B</p>
<p>是另外一个序列（矩阵）。如果都取</p>
<p><strong><em>A</em></strong>=<strong><em>B</em></strong>=<strong><em>X</em></strong>A=B=X</p>
<p>，那么就称为Self Attention，</p>
<p>它的意思是直接将<strong><em>x</em></strong>txt与原来的每个词进行比较，最后算出<strong><em>y</em></strong>tyt</p>
<p>！</p>
<h2 id="Attention层"><a href="#Attention层" class="headerlink" title="Attention层 #"></a>Attention层<a href="https://kexue.fm/archives/4765#Attention层" target="_blank" rel="noopener"> #</a></h2>
          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/04/机器学习和深度学习算法专题/优质git资源/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/04/机器学习和深度学习算法专题/优质git资源/" class="post-title-link" itemprop="url">未命名</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-04 13:00:50 / 修改时间：13:01:34" itemprop="dateCreated datePublished" datetime="2019-06-04T13:00:50+08:00">2019-06-04</time>
            </span>
          

          
            

            
          

          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/04/机器学习和深度学习算法专题/优质git资源/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/04/机器学习和深度学习算法专题/优质git资源/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="AI-Learning"><a href="#AI-Learning" class="headerlink" title="AI Learning"></a>AI Learning</h1><p>汇集了30多名贡献者的集体智慧，把学习机器学习的路线图、视频、电子书、学习建议等中文资料全部都整理好了。</p>
<p><a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/04/深度学习笔记/Pytorch训练营/D2 序列化模型，torch接口/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/04/深度学习笔记/Pytorch训练营/D2 序列化模型，torch接口/" class="post-title-link" itemprop="url">未命名</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-04 09:22:46" itemprop="dateCreated datePublished" datetime="2019-06-04T09:22:46+08:00">2019-06-04</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-08 11:57:16" itemprop="dateModified" datetime="2019-06-08T11:57:16+08:00">2019-06-08</time>
              </span>
            
          

          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/04/深度学习笔记/Pytorch训练营/D2 序列化模型，torch接口/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/04/深度学习笔记/Pytorch训练营/D2 序列化模型，torch接口/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong>任务：Pytorch官方文档（参考资料：Pytorch官方文档 5.序列化模型；6.torch接口）</strong></p>
<p><strong>任务简介</strong>：《Pytorch官方文档》</p>
<p><strong>学习时长：</strong>6/4</p>
<p><strong>详细说明</strong>：</p>
<p>本节任务资料包下载：</p>
<p>链接：<a href="https://pan.baidu.com/s/1U7KBXRv4V5rkvMjWQqULnw" target="_blank" rel="noopener">https://pan.baidu.com/s/1U7KBXRv4V5rkvMjWQqULnw</a> </p>
<p>提取码：0b3u </p>
<p>本节内容包括如何保存和载入模型，我们一般情况下载训练阶段保存在预测阶段载入，同时需要了解两种方法保存模型的差异。下面是pytorch最重要的部分，对Tensor的操作，由于本节内容较多，我们分为七个部分讲解，今天主要是熟悉torch接口中1~9页的API,能够知道如何使用，知道每个函数的意义和参数的意义。</p>
<p><strong>作业名称（详解）：</strong>保存模型的两种形式以及他们的区别？手敲今天所学API三遍</p>
<p><strong>作业提交形式</strong>：打卡提交文字或图片，不少于20字</p>
<p><strong>打卡截止时间：</strong>6/5</p>
<h1 id="1、保存模型的两种形式以及他们的区别？"><a href="#1、保存模型的两种形式以及他们的区别？" class="headerlink" title="1、保存模型的两种形式以及他们的区别？"></a>1、保存模型的两种形式以及他们的区别？</h1><p>第一种，只保存和加载模型参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.save(the_model.state_dict(), PATH)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载时</span></span><br><span class="line">the_model = TheModelClass(*args, **kwargs)</span><br><span class="line">the_model.load_state_dict(torch.load(PATH))</span><br></pre></td></tr></table></figure>
<p>第二种，保存和加载整个模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 没有state_dict</span></span><br><span class="line">torch.save(the_model, PATH)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载时</span></span><br><span class="line">the_model = torch.load(PATH)</span><br></pre></td></tr></table></figure>
<p>第二种的不足是，序列化的数据被绑定到特定的类和固定的目录结构。缺少灵活性，可能有各种break的隐患。</p>
<h1 id="2、Pytorch各种API"><a href="#2、Pytorch各种API" class="headerlink" title="2、Pytorch各种API"></a>2、Pytorch各种API</h1><h2 id="张量操作"><a href="#张量操作" class="headerlink" title="张量操作"></a>张量操作</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 若obj是一个pytorch的tensor，则返回true</span></span><br><span class="line">torch.is_tensor(obj)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 判断obj是否是storage</span></span><br><span class="line"><span class="comment"># torch.Storage是单个数据类型的连续的一维数组，每个torch.Tensor都具有相同数据类型的相应存储。他是torch.tensor底层数据结构,他除了像Tensor一样定义数值，还可以直接把文件映射到内存中进行操作</span></span><br><span class="line">torch.is_storage(obj)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置默认的tensor的数据类型</span></span><br><span class="line">torch.set_default_tensor_type(torch.DoubleTensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回input张量中的元素个数</span></span><br><span class="line">torch.numel(input) -&gt; int</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置打印选项，与numpy一致</span></span><br><span class="line">torch.set_printoptions(precision=<span class="keyword">None</span>, threshold=<span class="keyword">None</span>, edgeitems=<span class="keyword">None</span>, linewidth=<span class="keyword">None</span>, profile=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<h2 id="创建操作Creation-Ops"><a href="#创建操作Creation-Ops" class="headerlink" title="创建操作Creation Ops"></a>创建操作Creation Ops</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 返回一个二维张量，对角线都是1，其他是0</span></span><br><span class="line">torch.eye(n, m=<span class="keyword">None</span>, out=<span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用一个值填充tensor</span></span><br><span class="line">end = torch.Tensor(<span class="number">4</span>).fill_(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string"><span class="meta">&gt;&gt;&gt; </span>end</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> 10</span></span><br><span class="line"><span class="string"> 10</span></span><br><span class="line"><span class="string"> 10</span></span><br><span class="line"><span class="string"> 10</span></span><br><span class="line"><span class="string">[torch.FloatTensor of size 4]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将numpy.ndarray转为tensor，共享同一内存空间，修改一个会导致修改另一个。</span></span><br><span class="line">torch.from_numpy(ndarray) -&gt; Tensor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回一维张量，包含在start和end上均匀间隔的steps个点, out是结果张量</span></span><br><span class="line">torch.linspace(start, end, steps=100, out=None) -&gt; Tensor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回一维张量，包含在区间10^start和10^end上以对数刻度均匀间隔的steps个点。</span></span><br><span class="line">torch.logspace(start, end, steps=100, out=None) -&gt; Tensor</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.logspace(start=<span class="number">-10</span>, end=<span class="number">10</span>, steps=<span class="number">5</span>)</span><br><span class="line">        <span class="number">1.0000e-10</span></span><br><span class="line">        <span class="number">1.0000e-05</span></span><br><span class="line">        <span class="number">1.0000e+00</span></span><br><span class="line">        <span class="number">1.0000e+05</span></span><br><span class="line">        <span class="number">1.0000e+10</span></span><br><span class="line">       [torch.FloatTensor of size <span class="number">5</span>]</span><br><span class="line">      </span><br><span class="line"><span class="comment"># 返回一个全为1的张量，形状由可变参数sizes定义</span></span><br><span class="line">torch.ones(*size, out=None) -&gt; Tensor</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.ones(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="number">111</span></span><br><span class="line"><span class="number">111</span></span><br><span class="line">[torch.FloatTensor of size <span class="number">2</span>x3]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从区间(0,1)的均匀分布中抽取一组随机数</span></span><br><span class="line">torch.rand(*sizes, out=None) -&gt; Tensor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面函数也可以实现这个效果</span></span><br><span class="line">torch.Tensor(<span class="number">3</span>, <span class="number">3</span>).uniform_(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 等同于</span></span><br><span class="line">torch.rand(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从标准正态分布（均值为0，方差为1，即高斯白噪声）中抽取一组随机数。</span></span><br><span class="line">torch.randn(*size, out=None) -&gt; Tensor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 给定参数n，返回一个从0到n-1的随机整数排列</span></span><br><span class="line">torch.randperm(n, out=None) -&gt; LongTensor</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.randperm(<span class="number">4</span>)</span><br><span class="line"><span class="number">2</span> <span class="number">1</span> <span class="number">3</span> <span class="number">0</span></span><br><span class="line">[torch.LongTensor of size <span class="number">4</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回一维张量，长度为floor((end-start)/step)。包含从start到end，以step为步长的</span></span><br><span class="line">torch.arange(start, end, step=1, out=None) -&gt; Tensor</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.arange(<span class="number">1</span>, <span class="number">2.5</span>, <span class="number">0.5</span>)</span><br><span class="line"> <span class="number">1.0000</span></span><br><span class="line"> <span class="number">1.5000</span></span><br><span class="line"> <span class="number">2.0000</span></span><br><span class="line">[torch.FloatTensor of size <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 有floor((end-start)/step)+1个元素，包含在[start, end)</span></span><br><span class="line"><span class="comment"># WARNING: 建议使用函数torch.arange</span></span><br><span class="line">torch.range(start, end, step=1, out=None) -&gt; Tensor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回全为标量0的张量</span></span><br><span class="line">torch.zeros(*size, out=<span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.zeros(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="number">000</span></span><br><span class="line"><span class="number">000</span></span><br><span class="line">[torch.FloatTensor of size <span class="number">2</span>x3]</span><br></pre></td></tr></table></figure>
<h2 id="数学操作"><a href="#数学操作" class="headerlink" title="数学操作"></a>数学操作</h2><h3 id="torch-log1p"><a href="#torch-log1p" class="headerlink" title="torch.log1p"></a>torch.log1p</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.log1p(input, out=None) → Tensor</span><br></pre></td></tr></table></figure>
<p>计算 input+1input+1的自然对数 yi=log(xi+1)yi=log(xi+1)</p>
<p>注意：对值比较小的输入，此函数比<code>torch.log()</code>更准确。</p>
<p><a href="https://blog.csdn.net/qq_36523839/article/details/82422865" target="_blank" rel="noopener">https://blog.csdn.net/qq_36523839/article/details/82422865</a></p>
<p>优点：</p>
<ul>
<li><p>在数据预处理时首先可以对偏度比较大的数据用log1p函数进行转化，使其更加服从高斯分布，此步处理可能会使我们后续的分类结果得到一个更好的结果；</p>
</li>
<li><p>平滑处理很容易被忽略掉，导致模型的结果总是达不到一定的标准，同样使用逼格更高的log1p能避免复值得问题——复值指一个自变量对应多个因变量；</p>
</li>
</ul>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/04/深度学习笔记/Pytorch笔记/函数学习/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/04/深度学习笔记/Pytorch笔记/函数学习/" class="post-title-link" itemprop="url">未命名</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-04 08:30:40 / 修改时间：08:30:52" itemprop="dateCreated datePublished" datetime="2019-06-04T08:30:40+08:00">2019-06-04</time>
            </span>
          

          
            

            
          

          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/04/深度学习笔记/Pytorch笔记/函数学习/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/04/深度学习笔记/Pytorch笔记/函数学习/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>torch.ones</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/03/总结与思考/如何向客户描述产品/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/03/总结与思考/如何向客户描述产品/" class="post-title-link" itemprop="url">未命名</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-03 22:49:31 / 修改时间：22:49:48" itemprop="dateCreated datePublished" datetime="2019-06-03T22:49:31+08:00">2019-06-03</time>
            </span>
          

          
            

            
          

          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/03/总结与思考/如何向客户描述产品/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/03/总结与思考/如何向客户描述产品/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>对于一个系统，如何从业务的角度去描述它，能讲一个故事让用户听懂，让用户感兴趣，而不是从模型到模型。</p>
<p>—首先，这个系统服务于什么领域</p>
<p>​    目前这个领域存在什么问题，</p>
<p>​    有哪些因素影响到这些问题，</p>
<p>​    我们的系统能够做到什么。</p>
<p>这样了解业务逻辑。自己清楚，也能跟别人讲。</p>
<p>了解不同类型的客户，知道这些客户关注的是什么问题。吃透行业标准。总结项目中吸收到的新的知识和方法。</p>
<p>跟客户谈的时候：</p>
<p>—先说当前存在什么问题</p>
<p>​    我们有什么方法；</p>
<p>​    能支持，能推动，能帮你们，能做到……；</p>
<p>不能让客户觉得系统就是个模型的入口跟数据的出口。</p>
<p>对于项目中不确定的模糊的问题，要尽快跟客户商定，不能成为项目隐藏的风险！</p>
<p>跟客户探口风，把握需求的上限。哪些功能要做，哪些不要做，要有理有据。</p>
<p>思考客户提出需求的深层次原因，比如说领导重视。</p>
<p>沟通时，如想表达这个功能可以实现，但是因为钱不够不想做，可以说：这个系统的实现上，重点和难点在于XX。我们可以花工作量在XX功能，但是这会对最关键的内容产生影响。如果能给我们更多一些时间和预算，我们可以保质保量把这个新提出的功能做好。</p>
<p>同样的预算，要让不断提出新需求的客户认识到，做A还是做B，不能盲目接受客户需求，也不能因为合同没写或钱不够而直接拒绝。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/03/工具和环境/colab使用/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/03/工具和环境/colab使用/" class="post-title-link" itemprop="url">Colab使用</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-03 07:59:37" itemprop="dateCreated datePublished" datetime="2019-06-03T07:59:37+08:00">2019-06-03</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-07-05 16:26:42" itemprop="dateModified" datetime="2019-07-05T16:26:42+08:00">2019-07-05</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/工具和环境/" itemprop="url" rel="index"><span itemprop="name">工具和环境</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/03/工具和环境/colab使用/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/03/工具和环境/colab使用/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="挂载google-driver"><a href="#挂载google-driver" class="headerlink" title="挂载google driver"></a>挂载google driver</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from google.colab import drive</span><br><span class="line">drive.mount(&apos;/content/gdrive&apos;)</span><br></pre></td></tr></table></figure>
<p>运行之后：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&amp;redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&amp;scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&amp;response_type=code</span><br><span class="line"></span><br><span class="line">Enter your authorization code:</span><br></pre></td></tr></table></figure>
<p>登录后获得授权码，填写后</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Mounted at /content/gdrive</span><br></pre></td></tr></table></figure>
<p>桌面版工具</p>
<p><a href="https://www.insynchq.com/" target="_blank" rel="noopener">https://www.insynchq.com/</a></p>
<h1 id="跑fast-ai"><a href="#跑fast-ai" class="headerlink" title="跑fast.ai"></a>跑fast.ai</h1><p><a href="https://segmentfault.com/a/1190000018580340?utm_source=tag-newest" target="_blank" rel="noopener">https://segmentfault.com/a/1190000018580340?utm_source=tag-newest</a></p>
<p>fast.ai B站教程<a href="https://www.bilibili.com/video/av42079986?from=search&amp;seid=1436832162873131397" target="_blank" rel="noopener">https://www.bilibili.com/video/av42079986?from=search&amp;seid=1436832162873131397</a></p>
<h1 id="公众号文章-colab汇总"><a href="#公众号文章-colab汇总" class="headerlink" title="公众号文章-colab汇总"></a>公众号文章-colab汇总</h1><p><strong>3 Colab的缺点？</strong></p>
<p>12小时连续连接的限制（可以用 暂存深度学习权重的方法来解决）；</p>
<p>需要科学上网，请查看KK大佬编写的视频教材</p>
<p>“如何优雅地使用Google系产品？”</p>
<p><a href="https://www.bilibili.com/video/av43034822?from=search&amp;seid=14859222855758483769" target="_blank" rel="noopener">https://www.bilibili.com/video/av43034822?from=search&amp;seid=14859222855758483769</a></p>
<p>或者  更简单的方法  如果你有朋友已经搭建好了  你可以蹭一下^_^</p>
<p><strong>4 Colab中 如何装包？</strong></p>
<p>!pip</p>
<p><strong>5 如何导入数据？</strong></p>
<p>使用Google Drive,方法见:</p>
<p><a href="https://colab.research.google.com/notebooks/io.ipynb" target="_blank" rel="noopener">https://colab.research.google.com/notebooks/io.ipynb</a></p>
<p>用！wget直接从数据源读入；</p>
<p>用!git clone直接从Github读取整个项目文件（这个方法用于学习或者测试  特别赞）</p>
<p>提示： Google Drive只有15G  不适合放置大量数据，我的经验是：</p>
<p>数据从源文件处直接用!wget读取，但是将程序文件 和结果文件保存在网盘（特别是前面提到的 暂存的中间权重数据，以便后期恢复继续执行的文件  都要存入网盘 ，否则12小时限制到了  这些数据就没有了）</p>
<p><strong>6 能否直接在Colab执行.py文件？</strong></p>
<p>可以   用!run 或者!python3</p>
<p>但是要注意  这样执行文件如果设计到模块的调用  非常容易出错</p>
<p>另外  在Colab中调用.py文件中定义的模块  也是很方便的</p>
<p><strong>7 如果12小时  无法训练完整的结果，如何暂存结果  并在重新连接服务器后继续恢复执行？</strong></p>
<p>见</p>
<h2 id="如何在TensorFlow中保存和恢复深度学习模型训练"><a href="#如何在TensorFlow中保存和恢复深度学习模型训练" class="headerlink" title="如何在TensorFlow中保存和恢复深度学习模型训练"></a><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjExMDc0Ng==&amp;mid=2247484034&amp;idx=1&amp;sn=89c7c8dcdfa3771fc262f1ebfbffef02&amp;chksm=cef50d77f9828461bf8978918a65eb85d12edc93fb53cff03cd1c9229f6baee74c11f55808ee&amp;token=456076747&amp;lang=zh_CN&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">如何在TensorFlow中保存和恢复深度学习模型训练</a></h2><p><strong>8 能否跑比较大的数据集？</strong></p>
<p>测试过wikidata,这个数据集  对于学习阶段来说  算比较大的数据了</p>
<p>见</p>
<h2 id="在Google-Colab复现一篇EMNLP-2017会议论文中的源代码"><a href="#在Google-Colab复现一篇EMNLP-2017会议论文中的源代码" class="headerlink" title="在Google Colab复现一篇EMNLP 2017会议论文中的源代码"></a><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjExMDc0Ng==&amp;mid=2247483681&amp;idx=1&amp;sn=77ea7b2d334c1755a4d04424973a3d76&amp;chksm=cef50ed4f98287c23a8a55424e19c2c58dc7528db249bff4d2d7c89a17e289dc71285e5260a6&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">在Google Colab复现一篇EMNLP 2017会议论文中的源代码</a></h2><p>当然 这个程序无法在12小时内完成  需要结合前面提到的暂存权重参数  重新恢复训练的方式；</p>
<p>个人觉得  学习阶段的使用Google Colab完全可以满足需求</p>
<p><strong>9 Colab对于知名公开课的支持有哪些？</strong></p>
<p>见</p>
<h2 id="怎样用Google-Colab完成Stanford-CS231n的作业1"><a href="#怎样用Google-Colab完成Stanford-CS231n的作业1" class="headerlink" title="怎样用Google Colab完成Stanford CS231n的作业1"></a><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjExMDc0Ng==&amp;mid=2247483662&amp;idx=1&amp;sn=25c41e01928881cb45b101226ae47041&amp;chksm=cef50efbf98287ed7e803a57041a330c478cb01ab580e6f6cda4cc2d25e7a4b2014a89f9733f&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">怎样用Google Colab完成Stanford CS231n的作业1</a></h2><h2 id="怎样用Google-Colab完成Stanford-CS224n的作业"><a href="#怎样用Google-Colab完成Stanford-CS224n的作业" class="headerlink" title="怎样用Google Colab完成Stanford CS224n的作业"></a><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjExMDc0Ng==&amp;mid=2247483674&amp;idx=1&amp;sn=dbc0aded4b20ee5683b45d1b8e56ba0d&amp;chksm=cef50eeff98287f9aba8d03b1bd2dbfd6a20975cb9597c132722f7dcaf516ca8131954ef30d3&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">怎样用Google Colab完成Stanford CS224n的作业</a></h2><h2 id="吴恩达老师又又又出新课了，这次是TensorFlow"><a href="#吴恩达老师又又又出新课了，这次是TensorFlow" class="headerlink" title="吴恩达老师又又又出新课了，这次是TensorFlow"></a><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjExMDc0Ng==&amp;mid=2247483856&amp;idx=1&amp;sn=15f7f8a9ba6b94908f1a82b36b3ae103&amp;chksm=cef50e25f9828733bd6235808a255d97c5e1d5fcb8e5b2481fe33d51e58103859b13df27fae9&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">吴恩达老师又又又出新课了，这次是TensorFlow</a></h2><h2 id="如何从0开始-在Google-Colab上训练一个图片分类的卷积神经网络"><a href="#如何从0开始-在Google-Colab上训练一个图片分类的卷积神经网络" class="headerlink" title="如何从0开始 在Google Colab上训练一个图片分类的卷积神经网络"></a><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjExMDc0Ng==&amp;mid=2247483868&amp;idx=1&amp;sn=cf9cccdfccf4d623604e9890c966905d&amp;chksm=cef50e29f982873fe4e36c5361eaddc6f7d4bb1c730a3f9e0724b3e1ffdf4282cd5c8d7a9c01&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">如何从0开始 在Google Colab上训练一个图片分类的卷积神经网络</a></h2><h2 id="TensorFlow图片分类实战（吴恩达TensorFlow系列新课第一课完结篇）"><a href="#TensorFlow图片分类实战（吴恩达TensorFlow系列新课第一课完结篇）" class="headerlink" title="TensorFlow图片分类实战（吴恩达TensorFlow系列新课第一课完结篇）"></a><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjExMDc0Ng==&amp;mid=2247483878&amp;idx=1&amp;sn=eeef08cd9caefa1787978e62f8b4b55c&amp;chksm=cef50e13f9828705c03fa905e96a236ad6752ecbf731f0d125266547e12817085d8104d2b63a&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">TensorFlow图片分类实战（吴恩达TensorFlow系列新课第一课完结篇）</a></h2><p><strong>10 Colab对于知名教材的支持？</strong></p>
<p>见</p>
<h2 id="在Google-Colab中测试李航《统计学习方法》Python代码"><a href="#在Google-Colab中测试李航《统计学习方法》Python代码" class="headerlink" title="在Google Colab中测试李航《统计学习方法》Python代码"></a><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjExMDc0Ng==&amp;mid=2247483692&amp;idx=1&amp;sn=ccee84ccc7d0882279f82cf7d395685c&amp;chksm=cef50ed9f98287cff587ab1e18d72a6c792ebed5a8c6dcaab0532b5cbf66d8428772d0a392c5&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">在Google Colab中测试李航《统计学习方法》Python代码</a></h2><h2 id="怎样用Google-Colab完成周志华西瓜书的习题"><a href="#怎样用Google-Colab完成周志华西瓜书的习题" class="headerlink" title="怎样用Google Colab完成周志华西瓜书的习题"></a><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjExMDc0Ng==&amp;mid=2247483712&amp;idx=1&amp;sn=ad199c99f8b205f08a260ec4c6fe5647&amp;chksm=cef50eb5f98287a32a60594021bb8f90c043b8f1aaf0d7e421e686ce6d5fe7f657f90d403f4c&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">怎样用Google Colab完成周志华西瓜书的习题</a></h2><p><strong>11 Colab也可以跑竞赛的数据？</strong></p>
<p>见</p>
<h2 id="如何通过参加-数据分析竞赛-来提高算法水平（以”达观杯”-文本挖掘比赛为例）"><a href="#如何通过参加-数据分析竞赛-来提高算法水平（以”达观杯”-文本挖掘比赛为例）" class="headerlink" title="如何通过参加 数据分析竞赛 来提高算法水平（以”达观杯” 文本挖掘比赛为例）"></a><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjExMDc0Ng==&amp;mid=2247483723&amp;idx=1&amp;sn=a63f45bd3715f99b5451e3c2f38a0c67&amp;chksm=cef50ebef98287a8fe6ee1565f728ea465fccf6bb69e72df0921a0308dd667a76ba77ec1e931&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">如何通过参加 数据分析竞赛 来提高算法水平（以”达观杯” 文本挖掘比赛为例）</a></h2><p><strong>12 Colab支持TensorFlow2.0吗？</strong></p>
<p>见</p>
<h2 id="用Google-Colab尝鲜测试TensorFlow-2-0"><a href="#用Google-Colab尝鲜测试TensorFlow-2-0" class="headerlink" title="用Google Colab尝鲜测试TensorFlow 2.0"></a><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjExMDc0Ng==&amp;mid=2247483767&amp;idx=1&amp;sn=59cf407f03eb0306d1570f13427c5367&amp;chksm=cef50e82f98287943717239c372369ea738704381ecc43f05083a853ddf40d9f52db953130a8&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">用Google Colab尝鲜测试TensorFlow 2.0</a></h2><p><strong>13 也可以用Colab进行Python的基本学习吗？</strong></p>
<p>可以  如用Colab学习Matploylib库  见</p>
<h2 id="用matplotlib可视化初步"><a href="#用matplotlib可视化初步" class="headerlink" title="用matplotlib可视化初步"></a><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjExMDc0Ng==&amp;mid=2247483776&amp;idx=1&amp;sn=ad57f82d4e01589974a56a8af9f35fca&amp;chksm=cef50e75f9828763ab8f4acb67c9e2942f973dc3ab1d44a6bb620f6242d5fed487797db4b62f&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">用matplotlib可视化初步</a></h2><p><strong>14 Colab支持GAN吗？</strong></p>
<p>见</p>
<h2 id="如何在Google-Colab运行你的第一个GAN模型"><a href="#如何在Google-Colab运行你的第一个GAN模型" class="headerlink" title="如何在Google Colab运行你的第一个GAN模型"></a><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjExMDc0Ng==&amp;mid=2247483910&amp;idx=1&amp;sn=291de5debbc97b9a6635da6af726ad9c&amp;chksm=cef50df3f98284e5e4011349e3d642d1a243be0d013cf7f6b9a180cfa3a0e3bf5c4bea564019&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">如何在Google Colab运行你的第一个GAN模型</a></h2><p>这里只测试了最简单的GAN模型，Colab也支持更加复杂的GAN模型训练，如</p>
<p><a href="https://github.com/shaoanlu/faceswap-GAN" target="_blank" rel="noopener">https://github.com/shaoanlu/faceswap-GAN</a></p>
<p><strong>15 如何搜索支持Colab的源代码？</strong></p>
<p>Github,Seedbank项目</p>
<p>jupyter notebook格式的均支持</p>
<p><strong>上传并使用数据文件</strong></p>
<p>我们一般都需要在 Colab 笔记本中使用数据，对吧？你可以使用 wget 之类的工具从网络上获取数据，但是如果你有一些本地文件，想上传到你的谷歌硬盘中的 Colab 环境里并使用它们，该怎么做呢？</p>
<p>很简单，只需 3 步即可实现！</p>
<p>首先使用以下命令调用笔记本中的文件选择器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> files</span><br><span class="line">uploaded = files.upload()</span><br></pre></td></tr></table></figure>
<p>运行之后，我们就会发现单元 cell 下出现了“选择文件”按钮：</p>
<p><img src="//schwimmer.github.io/2019/06/03/工具和环境/colab使用/pic/v2-740d65c0b367aa4ecb9c69ed13f70b04_hd.jpg" alt="img"></p>
<p>这样就可以直接选择你想上传的文件啦！</p>
<p>选择文件后，使用以下迭代方法上传文件以查找其键名，命令如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> fn <span class="keyword">in</span> uploaded.keys():</span><br><span class="line"> print(<span class="string">'User uploaded file "&#123;name&#125;" with length &#123;length&#125; bytes'</span>.format(name=fn, length=len(uploaded[fn])))</span><br></pre></td></tr></table></figure>
<p>例如待上传的是 iris.csv 文件，若运行没有问题的话，应该出现类似下面的提示语句：</p>
<blockquote>
<p>User uploaded file “iris.csv” with length 3716 bytes</p>
</blockquote>
<p>最后，就使用以下命令将文件的内容加载到 Pandas 的 DataFrame 中了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> io</span><br><span class="line">df = pd.read_csv(io.StringIO(uploaded[<span class="string">'iris.csv'</span>].decode(<span class="string">'utf-8'</span>)))</span><br><span class="line">print(df)</span><br></pre></td></tr></table></figure>
          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/02/深度学习笔记/Pytorch笔记/Autograd/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/02/深度学习笔记/Pytorch笔记/Autograd/" class="post-title-link" itemprop="url">未命名</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-02 22:10:58 / 修改时间：22:20:53" itemprop="dateCreated datePublished" datetime="2019-06-02T22:10:58+08:00">2019-06-02</time>
            </span>
          

          
            

            
          

          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/02/深度学习笔记/Pytorch笔记/Autograd/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/02/深度学习笔记/Pytorch笔记/Autograd/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://blog.csdn.net/g11d111/article/details/83035270" target="_blank" rel="noopener">PyTorch学习笔记(12)——PyTorch中的Autograd机制介绍</a></p>
<p><a href="https://blog.csdn.net/MiaoB226/article/details/88934561" target="_blank" rel="noopener">Pytorch从入门到放弃（5）——取消测试与验证阶段的梯度</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/02/深度学习笔记/Pytorch莫烦/建造第一个神经网络/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/02/深度学习笔记/Pytorch莫烦/建造第一个神经网络/" class="post-title-link" itemprop="url">未命名</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-02 16:17:50" itemprop="dateCreated datePublished" datetime="2019-06-02T16:17:50+08:00">2019-06-02</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-04 08:25:49" itemprop="dateModified" datetime="2019-06-04T08:25:49+08:00">2019-06-04</time>
              </span>
            
          

          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/02/深度学习笔记/Pytorch莫烦/建造第一个神经网络/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/02/深度学习笔记/Pytorch莫烦/建造第一个神经网络/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://morvanzhou.github.io/tutorials/machine-learning/torch/3-01-regression/" target="_blank" rel="noopener">https://morvanzhou.github.io/tutorials/machine-learning/torch/3-01-regression/</a></p>
<h1 id="关系拟合-回归"><a href="#关系拟合-回归" class="headerlink" title="关系拟合 (回归)"></a>关系拟合 (回归)</h1><h2 id="要点"><a href="#要点" class="headerlink" title="要点"></a>要点</h2><p>我会这次会来见证神经网络是如何通过简单的形式将一群数据用一条线条来表示. 或者说, 是如何在数据当中找到他们的关系, 然后用神经网络模型来建立一个可以代表他们关系的线条.</p>
<h2 id="建立数据集"><a href="#建立数据集" class="headerlink" title="建立数据集"></a>建立数据集</h2><p>我们创建一些假数据来模拟真实的情况. 比如一个一元二次函数: <code>y = a * x^2 + b</code>, 我们给 <code>y</code> 数据加上一点噪声来更加真实的展示它.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">x = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1)  # x data (tensor), shape=(100, 1)</span><br><span class="line">y = x.pow(2) + 0.2*torch.rand(x.size())                 # noisy y data (tensor), shape=(100, 1)</span><br><span class="line"></span><br><span class="line"># 画图</span><br><span class="line">plt.scatter(x.data.numpy(), y.data.numpy())</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="建立神经网络"><a href="#建立神经网络" class="headerlink" title="建立神经网络"></a>建立神经网络</h2><p>建立一个神经网络我们可以直接运用 torch 中的体系. 先定义所有的层属性(<code>__init__()</code>), 然后再一层层搭建(<code>forward(x)</code>)层于层的关系链接. 建立关系的时候, 我们会用到激励函数, 如果还不清楚激励函数用途的同学, 这里有非常好的<a href="https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/3-04-activation-function/" target="_blank" rel="noopener">一篇动画教程</a>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F     <span class="comment"># 激励函数都在这</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(torch.nn.Module)</span>:</span>  <span class="comment"># 继承 torch 的 Module</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_feature, n_hidden, n_output)</span>:</span></span><br><span class="line">        super(Net, self).__init__()     <span class="comment"># 继承 __init__ 功能</span></span><br><span class="line">        <span class="comment"># 定义每层用什么样的形式</span></span><br><span class="line">        self.hidden = torch.nn.Linear(n_feature, n_hidden)   <span class="comment"># 隐藏层线性输出</span></span><br><span class="line">        self.predict = torch.nn.Linear(n_hidden, n_output)   <span class="comment"># 输出层线性输出</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span>   <span class="comment"># 这同时也是 Module 中的 forward 功能</span></span><br><span class="line">        <span class="comment"># 正向传播输入值, 神经网络分析出输出值</span></span><br><span class="line">        x = F.relu(self.hidden(x))      <span class="comment"># 激励函数(隐藏层的线性值)</span></span><br><span class="line">        x = self.predict(x)             <span class="comment"># 输出值</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net = Net(n_feature=<span class="number">1</span>, n_hidden=<span class="number">10</span>, n_output=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">print(net)  <span class="comment"># net 的结构</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Net (</span></span><br><span class="line"><span class="string">  (hidden): Linear (1 -&gt; 10)</span></span><br><span class="line"><span class="string">  (predict): Linear (10 -&gt; 1)</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<h2 id="训练网络"><a href="#训练网络" class="headerlink" title="训练网络"></a>训练网络</h2><p>训练的步骤很简单, 如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># optimizer 是训练的工具</span></span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.2</span>)  <span class="comment"># 传入 net 的所有参数, 学习率</span></span><br><span class="line">loss_func = torch.nn.MSELoss()      <span class="comment"># 预测值和真实值的误差计算公式 (均方差)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    prediction = net(x)     <span class="comment"># 喂给 net 训练数据 x, 输出预测值</span></span><br><span class="line"></span><br><span class="line">    loss = loss_func(prediction, y)     <span class="comment"># 计算两者的误差</span></span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()   <span class="comment"># 清空上一步的残余更新参数值</span></span><br><span class="line">    loss.backward()         <span class="comment"># 误差反向传播, 计算参数更新值</span></span><br><span class="line">    optimizer.step()        <span class="comment"># 将参数更新值施加到 net 的 parameters 上</span></span><br></pre></td></tr></table></figure>
<h2 id="可视化训练过程"><a href="#可视化训练过程" class="headerlink" title="可视化训练过程"></a>可视化训练过程</h2><p>为了可视化整个训练的过程, 更好的理解是如何训练, 我们如下操作:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.ion()   <span class="comment"># 画图</span></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">200</span>):</span><br><span class="line">    prediction = net(x)     <span class="comment"># 喂给 net 训练数据 x, 输出预测值</span></span><br><span class="line">    loss = loss_func(prediction, y)     <span class="comment"># 计算两者的误差</span></span><br><span class="line">    optimizer.zero_grad()   <span class="comment"># 清空上一步的残余更新参数值</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 接着上面来</span></span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># plot and show learning process</span></span><br><span class="line">        plt.cla()</span><br><span class="line">        plt.scatter(x.data.numpy(), y.data.numpy())</span><br><span class="line">        plt.plot(x.data.numpy(), prediction.data.numpy(), <span class="string">'r-'</span>, lw=<span class="number">5</span>)</span><br><span class="line">        plt.text(<span class="number">0.5</span>, <span class="number">0</span>, <span class="string">'Loss=%.4f'</span> % loss.data.numpy(), fontdict=&#123;<span class="string">'size'</span>: <span class="number">20</span>, <span class="string">'color'</span>:  <span class="string">'red'</span>&#125;)</span><br><span class="line">        plt.pause(<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<h1 id="区分类型"><a href="#区分类型" class="headerlink" title="区分类型"></a>区分类型</h1><h2 id="要点-1"><a href="#要点-1" class="headerlink" title="要点"></a>要点</h2><p>这次我们也是用最简单的途径来看看神经网络是怎么进行事物的分类.</p>
<p><a href="https://morvanzhou.github.io/static/results/torch/1-1-3.gif" target="_blank" rel="noopener"><img src="https://morvanzhou.github.io/static/results/torch/1-1-3.gif" alt="区分类型 (建造第一个神经网络/pic/1-1-3.gif)"></a></p>
<h2 id="建立数据集-1"><a href="#建立数据集-1" class="headerlink" title="建立数据集"></a>建立数据集</h2><p>我们创建一些假数据来模拟真实的情况. 比如两个二次分布的数据, 不过他们的均值都不一样.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># torch.manual_seed(1)    # reproducible</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># make fake data</span></span><br><span class="line">n_data = torch.ones(<span class="number">100</span>, <span class="number">2</span>)</span><br><span class="line">x0 = torch.normal(<span class="number">2</span>*n_data, <span class="number">1</span>)      <span class="comment"># class0 x data (tensor), shape=(100, 2)</span></span><br><span class="line">y0 = torch.zeros(<span class="number">100</span>)               <span class="comment"># class0 y data (tensor), shape=(100, 1)</span></span><br><span class="line">x1 = torch.normal(<span class="number">-2</span>*n_data, <span class="number">1</span>)     <span class="comment"># class1 x data (tensor), shape=(100, 2)</span></span><br><span class="line">y1 = torch.ones(<span class="number">100</span>)                <span class="comment"># class1 y data (tensor), shape=(100, 1)</span></span><br><span class="line">x = torch.cat((x0, x1), <span class="number">0</span>).type(torch.FloatTensor)  <span class="comment"># shape (200, 2) FloatTensor = 32-bit floating</span></span><br><span class="line">y = torch.cat((y0, y1), ).type(torch.LongTensor)    <span class="comment"># shape (200,) LongTensor = 64-bit integer</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The code below is deprecated in Pytorch 0.4. Now, autograd directly supports tensors</span></span><br><span class="line"><span class="comment"># x, y = Variable(x), Variable(y)</span></span><br><span class="line"></span><br><span class="line">plt.scatter(x.data.numpy()[:, <span class="number">0</span>], x.data.numpy()[:, <span class="number">1</span>], c=y.data.numpy(), s=<span class="number">100</span>, lw=<span class="number">0</span>, cmap=<span class="string">'RdYlGn'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="建立神经网络-1"><a href="#建立神经网络-1" class="headerlink" title="建立神经网络"></a>建立神经网络</h2><p>建立一个神经网络我们可以直接运用 torch 中的体系. 先定义所有的层属性(<code>__init__()</code>), 然后再一层层搭建(<code>forward(x)</code>)层于层的关系链接. 这个和我们在前面 regression 的时候的神经网络基本没差. 建立关系的时候, 我们会用到激励函数, 如果还不清楚激励函数用途的同学, 这里有非常好的<a href="https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/3-04-activation-function/" target="_blank" rel="noopener">一篇动画教程</a>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F     <span class="comment"># 激励函数都在这</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(torch.nn.Module)</span>:</span>     <span class="comment"># 继承 torch 的 Module</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_feature, n_hidden, n_output)</span>:</span></span><br><span class="line">        super(Net, self).__init__()     <span class="comment"># 继承 __init__ 功能</span></span><br><span class="line">        self.hidden = torch.nn.Linear(n_feature, n_hidden)   <span class="comment"># 隐藏层线性输出</span></span><br><span class="line">        self.out = torch.nn.Linear(n_hidden, n_output)       <span class="comment"># 输出层线性输出</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># 正向传播输入值, 神经网络分析出输出值</span></span><br><span class="line">        x = F.relu(self.hidden(x))      <span class="comment"># 激励函数(隐藏层的线性值)</span></span><br><span class="line">        x = self.out(x)                 <span class="comment"># 输出值, 但是这个不是预测值, 预测值还需要再另外计算</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net = Net(n_feature=<span class="number">2</span>, n_hidden=<span class="number">10</span>, n_output=<span class="number">2</span>) <span class="comment"># 几个类别就几个 output</span></span><br><span class="line"></span><br><span class="line">print(net)  <span class="comment"># net 的结构</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Net (</span></span><br><span class="line"><span class="string">  (hidden): Linear (2 -&gt; 10)</span></span><br><span class="line"><span class="string">  (out): Linear (10 -&gt; 2)</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<h2 id="训练网络-1"><a href="#训练网络-1" class="headerlink" title="训练网络"></a>训练网络</h2><p>训练的步骤很简单, 如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># optimizer 是训练的工具</span></span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.02</span>)  <span class="comment"># 传入 net 的所有参数, 学习率</span></span><br><span class="line"><span class="comment"># 算误差的时候, 注意真实值!不是! one-hot 形式的, 而是1D Tensor, (batch,)</span></span><br><span class="line"><span class="comment"># 但是预测值是2D tensor (batch, n_classes)</span></span><br><span class="line">loss_func = torch.nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    out = net(x)     <span class="comment"># 喂给 net 训练数据 x, 输出分析值</span></span><br><span class="line"></span><br><span class="line">    loss = loss_func(out, y)     <span class="comment"># 计算两者的误差</span></span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()   <span class="comment"># 清空上一步的残余更新参数值</span></span><br><span class="line">    loss.backward()         <span class="comment"># 误差反向传播, 计算参数更新值</span></span><br><span class="line">    optimizer.step()        <span class="comment"># 将参数更新值施加到 net 的 parameters 上</span></span><br></pre></td></tr></table></figure>
<h2 id="可视化训练过程-1"><a href="#可视化训练过程-1" class="headerlink" title="可视化训练过程"></a>可视化训练过程</h2><p>为了可视化整个训练的过程, 更好的理解是如何训练, 我们如下操作:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">plt.ion()   # 画图</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">for t in range(100):</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    # 接着上面来</span><br><span class="line">    if t % 2 == 0:</span><br><span class="line">        plt.cla()</span><br><span class="line">        # 过了一道 softmax 的激励函数后的最大概率才是预测值</span><br><span class="line">        prediction = torch.max(F.softmax(out), 1)[1]</span><br><span class="line">        pred_y = prediction.data.numpy().squeeze()</span><br><span class="line">        target_y = y.data.numpy()</span><br><span class="line">        plt.scatter(x.data.numpy()[:, 0], x.data.numpy()[:, 1], c=pred_y, s=100, lw=0, cmap=&apos;RdYlGn&apos;)</span><br><span class="line">        accuracy = sum(pred_y == target_y)/200.  # 预测中有多少和真实值一样</span><br><span class="line">        plt.text(1.5, -4, &apos;Accuracy=%.2f&apos; % accuracy, fontdict=&#123;&apos;size&apos;: 20, &apos;color&apos;:  &apos;red&apos;&#125;)</span><br><span class="line">        plt.pause(0.1)</span><br><span class="line"></span><br><span class="line">plt.ioff()  # 停止画图</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><a href="https://morvanzhou.github.io/static/results/torch/3-2-1.png" target="_blank" rel="noopener"><img src="https://morvanzhou.github.io/static/results/torch/3-2-1.png" alt="区分类型 (建造第一个神经网络/pic/3-2-1.png)"></a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/02/机器学习/机器学习问题答疑/Adaboost/" class="post-title-link" itemprop="url">未命名</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-02 15:54:20 / 修改时间：16:07:06" itemprop="dateCreated datePublished" datetime="2019-06-02T15:54:20+08:00">2019-06-02</time>
            </span>
          

          
            

            
          

          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/02/机器学习/机器学习问题答疑/Adaboost/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/02/机器学习/机器学习问题答疑/Adaboost/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Adaboost"><a href="#Adaboost" class="headerlink" title="Adaboost"></a>Adaboost</h1><ol>
<li>想问下机器学习实战这里是不是写错了，应该是大于1的是吗？</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:dIJQW2rLt2pya92qYnGwGO7619M=.jpeg" alt="img"></p>
<ul>
<li>你看那个参数threshIneq，它取值lt或gt。lt表示小于阈值的为-1，gt表示大于阈值的为-1</li>
</ul>
<ol>
<li>绘制roc曲线的代码逻辑都能看的懂，该函数接收的是训练样本被分类的概率值，将这些概率值按由小到大的顺序依次输出，如果是正例，则曲线向下移动，反之向左移动。老师能透彻的解释下，为什么这样绘制就是roc曲线吗？roc不是在不同的阈值下真阳率和假阳率的对应关系吗？</li>
</ol>
<ul>
<li>先要了解正阳率和假阳率的定义。正阳率是预测的正类中确实是正类占所有真实正类的概率，假阳率是预测的正类中确实是负类占所有真实负类的概率。曲线横坐标是假阳率，纵坐标是真阳率。注意，书中的代码跟一般的的做法不一样，一般的做法是将概率从大到小排列，曲线从左下角开始画，原理是一样的。</li>
</ul>
<ol>
<li>老师您好，请问在做回归预测的时候，多项式构造选取多少个属性合适?随机森林选取的属性  重要性小于多少的可以摒弃，一般取前几?</li>
</ol>
<ul>
<li>这没有固定的答案，看具体问题能有几个较好的特征，剔除不必要特征。可以看看特征选择算法。随机森林每棵树可以随机选择一部分特征进行训练。</li>
</ul>
<h1 id="kMeans"><a href="#kMeans" class="headerlink" title="kMeans"></a>kMeans</h1><ol>
<li>如何理解kmeans++算法在解决标准kmeans算法执行时初始质心选择的的作用？该算法的第3步该如何理解，1.先从数据库随机挑个随机点当“种子点”2.对于每个点，都计算其和最近的一个“种子点”的距离D(x)并保存在一个数组里，然后把这些距离加起来得到Sum(D(x))。3.然后，再取一个随机值，用权重的方式来取计算下一个“种子点”。这个算法的实现是，先取一个能落在Sum(D(x))中的随机值Random，然后用Random -= D(x)，直到其&lt;=0，此时的点就是下一个“种子点”。4.重复2和3直到k个聚类中心被选出来5.利用这k个初始的聚类中心运行标准的k-means算法</li>
</ol>
<ul>
<li>kmeans选择初始化质心有不同的方法，可以选择批次距离尽可能远的K个点，也可以选用层次聚类算法BIRCH和ROCK或者Canopy。具体的还要看具体文献或出处。</li>
</ul>
<h1 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h1><ol>
<li>请问SVM中的惩罚因子C的作用是什么，如何调节误分类点的个数和距离二者的关系，为什么当C无穷大时，软间隔就等价于硬间隔</li>
</ol>
<ul>
<li>SVM中允许有分类错误的点时引入了参数C，C表示尽可能选择宽边界和尽可能不要犯错两者之间的权衡。C越大表示希望得到更少的分类错误，即不惜选择窄边界也要尽可能把更多点正确分类；C越小表示希望得到更宽的边界，即不惜增加错误点个数也要选择更宽的分类边界。这种处理问题的思路有点类似于正则化。</li>
</ul>
<p>当C无穷大时候，可以想象选择极窄的边界让所有的点都分类正确，也就等价于硬间隔了。也可以从另一方面来看，硬间隔满足 0≤αn，软间隔满足 0≤αn≤C，当C无穷大时，0≤αn≤C等价于0≤αn。</p>
<ol>
<li>想问一下关于alpha的选择问题，首先，条件a两个alpha必须要在间隔边界之外，b这两个alpha还没有进行过区间化处理或者不在边界上。我的问题是，1)区间话处理是什么意思？2)程序中alpha(i)的选择是在边界之外，但alpha(j)的选择确实非i的任意一个，这能保证alpha(j)满足两个条件吗？其次，随着程序的运行alpha(i)也会选到已经更新的alpha(j),这是不是和未区间化的条件向矛盾</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:qSLlCh12DkeIqFnKbUf0J0AQo20=.jpeg" alt="img"></p>
<ul>
<li>书中的做法是先找到违背条件的alpha1，然后alpha2取其误差与alpha1相差较大的那个，这样减少迭代次数。使用最优化解出alpha2后，根据其与alpha1的关系求得其值。</li>
</ul>
<ol>
<li>针对逻辑回归的这个推导看不明白，您能给讲下吗？数学哪方面知识是讲导数参与运算的，您能大体说下或有相关资料吗</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:onJ5Fj7cX0QhiHRNaqzJiL0e0VI=.jpeg" alt="img"></p>
<ul>
<li>逻辑回归的偏导数计算是比较简单的，用到了指数偏导计算。高数书里有基本的偏导数计算方法。</li>
</ul>
<ol>
<li>这边有个项目需要用GBDT去做训练，如果训练样本数大概是60万左右，正负样本比例是1:5左右 很不均衡，如果只是通过下采样使之达到均衡的话，觉得训练样本量20万，有点少，这种情况，一般按照什么方式去处理比较好？</li>
</ol>
<ul>
<li>可以试试上采样构造正样本，或者即使正负样本比例不同，采用不同的类别权重，在损失函数中使用。</li>
</ul>
<ol>
<li>SVM中参数C的调教:C和松弛向量的乘积加入到了目标函数中，如果要使目标函数有最小值，就是要C和松弛向量的乘积越小，那是不是可以看成:C越大，松弛向量越小，那1-ξ就越大，间距就越大，容错就越大。为啥不能看成这样啊？我看答案是C小点儿越好。。。。</li>
</ol>
<ul>
<li>C是权衡犯错率和宽间隔的，C越大表示宁愿间隔小也要分类正确，对错误的容忍度小。小的C值争取获得更宽的边界。</li>
</ul>
<ol>
<li>请问有没有用svm做多分类的代码可以分享给我的</li>
</ol>
<ul>
<li>可以使用OVO方式，没有手写代码，可以使用libSVM库</li>
</ul>
<ol>
<li>请问这个最小值应该在边界上达到是为什么？</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/Fni_QvWIJuY6ZYJNF2p6ySkNc_ne.jpg!thumbnail" alt="img"></p>
<ul>
<li>举个简单的例子，y=(x-1)^2，x在1处取得最小值。但是如果x的取值范围是[2,3]，那么只能在x的取值边界上得到最小值。</li>
</ul>
<ol>
<li>svm里 1 优化目标  maxmin  为什么要换成minmax  是为了引入基变换？方便计算？2  xi的位置是即可以是原始的特征 也可以是转换后由基函数表示吗 ？将不可分映射到高维可分？3  合约页损失函数是岭回归的思想吗</li>
</ol>
<ul>
<li><p>1.我们习惯解决最小化的优化问题，便于使用优化方法求解；</p>
</li>
<li><p>\2. 不太明白意思，xi可以是原始特征，但引入kernel后，可以认为是映射到高维，得到非线性分类面；</p>
</li>
<li><p>\3. 你说的应该是合页损失函数，它与岭回归不同，岭回归是平方误差函数加上了L2正则化项目，用于回归问题而不是分类问题。网上搜一下二者的区别很容易查到。</p>
</li>
</ul>
<ol>
<li>老师，这一步是你的博客上LinearSVM的代码，没看懂，感觉代码的逻辑说不通，并没有提取出每一个测试样本正例的分数</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:n0uDmZaMQkQv_Xa-BLLIUvZbu7s=.jpeg" alt="img"></p>
<ul>
<li>这篇文章来源于之前看cs231n写的笔记。它的LinearSVM使用的Hinge Loss，使用梯度下降算法计算的，比较简单。这里还是以理解传统的SVM为主。</li>
</ul>
<ol>
<li>统计学习超平面是什么？</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:mzqBn8A3XLrH5YVb_BEwHum7Hhg=.jpeg" alt="img"></p>
<ul>
<li>这里的超平面可以通俗理解为分类问题中的分类面，例如二维平面中的分类线，三维空间的分类面，对应到n维，就叫超平面。</li>
</ul>
<ol>
<li>老师，SVM中找到不满足KKT条件公式如图所示。但在实际代码中实现如下。这是为什么。 if ((self.y_train[i] <em> Ei &lt; -self.toler) and (self.alpha[i] &lt; self.C) or   (self.y_train[i] </em> Ei &gt; self.toler) and (self.alpha[i] &gt; 0)):</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:gh_zjTg6Tfb60fYRgKbHYWNrasE=.jpeg" alt="img"></p>
<ul>
<li>你把代码中Ei=y-y_train代入，移位一下就会发现与理论是一样的了。我在之前SVM直播答疑的视频里讲过，你可以去看看。<a href="https://study.163.com/course/courseLearn.htm?courseId=1006284002&amp;share=2&amp;shareId=400000000445063#/learn/video?lessonId=1053809730&amp;courseId=1006284002" target="_blank" rel="noopener">《机器学习实战》书训练营直播间 - 网易云课堂</a></li>
</ul>
<ol>
<li>老师，为什么李航的统计学128页在SVM中提出SMO算法在每个子问题中选择两个变量优化，其中至少一个变量是违反KKT条件的。</li>
</ol>
<ul>
<li><p>选择两个变量的原因是所有的alpha满足下面图片所示的这个条件，为了保证等式成立，必须同时优化两个alpha。违反KKT条件是因为先找到违背KKT条件的点，让其满足条件。若所有的点都满足KKT条件，则优化结束了。</p>
</li>
<li><p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:vKm1Y1htAp00LjzwNeWZMsEA6K4=.jpeg" alt="img"></p>
</li>
<li><p>老师，为什么因为先找到违背KKT条件的点，让其满足条件。这一步不理解</p>
</li>
<li><p>优化的目的是让所有点都满足KKT条件，因为满足KKT条件了，就得到最优化了。使用SMO是把整个优化问题切分成一个个小的优化问题，每次对两个alpha进行优化，让其满足KKT条件。</p>
</li>
<li><p>那找到违背KKT条件的点是不是要把这个点排除掉吗</p>
</li>
<li><p>更新alpha的值，让它满足KKT</p>
</li>
</ul>
<ol>
<li>老师，我想问一下机器学习实战第八章的内容，图中高斯核函数中的距离，指的是x轴的距离吗? 同时书中给的代码，感觉是x轴距离的平方，不知道我理解的对不对。</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:OhBF3MR_p8y4H2_aIvOk09lNHto=.jpeg" alt="img"></p>
<ul>
<li>这里就是高斯核，x表示所有坐标轴，不是单指x轴。以书中代码为准，平方。</li>
</ul>
<ol>
<li>老师您好，机器学习实战svm这章KernelTrans这个函数里A表示什么?kTup[1]又表示的是什么，不是很明白，望解答一下</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:P0qO5dt6CxKVEEwYDogVH_b826E=.jpeg" alt="img"></p>
<ul>
<li>这里的A就是每个训练样本，kTup是一个元组，kTup[0]表示核函数类型，kTup[1]表示高斯函数方差，sigma。</li>
</ul>
<ol>
<li>老师，请问使用svm做回归时，怎样评价模型的好坏</li>
</ol>
<ul>
<li>一般的回归模型评价指标可以是均方误差（MSE）、均方根误差（RMSE）、平均绝对误差（MAE）、校正决定系数（Ajusted R-Squre）等。</li>
</ul>
<ol>
<li>老师你好，我想问一下关于b阈值的理解，有效相等的话b1=b2，除此之外无效取值有什么意义嘛</li>
</ol>
<ul>
<li><p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:7eRb59d61gqR88E-6-r0JTinA4I=.jpeg" alt="img"></p>
</li>
<li><p>第一种情况是该样本点是支持向量时，可以直接计算得到 b，若不是支持向量，b取两个b1和b2的均值。</p>
</li>
</ul>
<h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><ol>
<li>决策树有两个优点是对中间值缺失不敏感，可以处理不相关的特征数据，如何理解</li>
</ol>
<ul>
<li>第一个优点是对缺失值不敏感，因为决策树不是基于距离度量，大部分时候可以在数据有缺失的时候使用。如果涉及到距离度量，缺失数据就变得比较重要。第二个优点是可以处理不相关特征，树形结构并不要求特征之间具有较高相关性。</li>
</ul>
<ol>
<li>请问在决策树中找出最好的数据集划分方式，可不可以理解为以每一列为特征值计算熵，然后找出最小的呢？</li>
</ol>
<ul>
<li>对每个特征计算条件熵，可以理解为条件熵越小，信息增益越大，就以该特征进行划分。</li>
</ul>
<ol>
<li>能把这个注解函数的各个参数详细的说下嘛</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:wbx9TK4z2PsH5un3ikfmwFolV0U=.jpeg" alt="img"></p>
<ul>
<li>这里使用的是python里的annotate，网上资料很多，可以自己搜一下<a href="https://blog.csdn.net/leaf_zizi/article/details/82886755" target="_blank" rel="noopener">https://blog.csdn.net/leaf_zizi/article/details/82886755</a></li>
</ul>
<ol>
<li>一般决策树用于连续值划分的用例多不多，西瓜书上决策树对连续值处理的理论我没太看明白</li>
</ol>
<ul>
<li>你说的应该是CART算法，应用蛮多的，Random Forest、GBDT都会用到决策树。CART部分建议好好看看，西瓜书理论对初学者不太友好，网上搜一搜简洁教程。</li>
</ul>
<ol>
<li>老师、请问这里为什么要分两段（featVec[：axis]和featVec[axis+1:]）添加featVec的信息？</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:QCMkh3xchDHdelnx09vKhcSOgK4=.jpeg" alt="img"></p>
<ul>
<li>因为要把featVec[axis]这个特征删去。第三章决策树里面，它是使用一个特征之后就把该特征删去的，就像我们根据条件判断一样，这个条件使用过了就不会再用了。</li>
</ul>
<h1 id="k-近邻算法"><a href="#k-近邻算法" class="headerlink" title="k-近邻算法"></a>k-近邻算法</h1><ol>
<li>k近临算法的思路，是不是待观测值与训练数据之间求距离，然后寻找距离最短的点，认为与最短距离的点位一类。 那我想不明白的，图片中第一个矩形框的内容为什么这样写？第2框是对距离排序，而labels并没有排序，那怎么确定第三个框中labels是最短距离所对应的label？#k邻近</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:6lmcCaNV0DDQhPhuaYyZAQchvQ0=.jpeg" alt="img"></p>
<ul>
<li><p>sortedDistIndicies存储的就是距离最小对应的label中类的下标。建议每个变量用print函数打印出来看看，加深理解。</p>
</li>
<li><p>首先，第一个框目的是求一个点和所有点的距离差值，这里的写法是把维度匹配上，做一个向量减法。换句话说就是把待测点的坐标复制出很多份（数量由训练数据大小决定），然后做向量减法。第二个框sort的是index不是里面的值，就是根据值sort了label。</p>
</li>
</ul>
<ol>
<li>ax.scatter(returnMat[:,0],returnMat[:,1],15.0<em>np.array(classLabelVector), 15.0</em>np.array(classLabelVector))  请问下，scatter这个函数第三第四个参数为啥都乘以15，而且为啥需要两个相同的参数？</li>
</ol>
<ul>
<li>第三个参数是s表示大小，第四个参数c表示颜色。之所以用label是让每个类别呈现不同的大小和颜色。15.0是变量，你可以调整为其他值看看效果。</li>
</ul>
<ol>
<li>knn算法为什要对数据进行normalization预处理，normalization方法有哪几种？</li>
</ol>
<ul>
<li>一般的机器学习算法都会对输入进行归一化，其主要目的是将各个特征归一化到相似尺度，提高训练精度。如果是梯度下降算法的话还会提高训练速度。常用的归一化有线性归一化，标准差标准化，非线性归一化等。</li>
</ul>
<ol>
<li>kdtree可以实现k邻近的搜索吗？看李航老师的书在讲knn的时候叙述了kdtree，只实现了最邻近。</li>
</ol>
<ul>
<li>当然可以！kdtree只是使用了特殊的存储结构，可以实现最近邻，也可以实现k近邻。而且李航的书中也说了。附图：</li>
</ul>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:Hfhii1JA8e0qsLgWDiYVqG6AmhM=.jpeg" alt="img"></p>
<ul>
<li><p>能请教一下kdtree实现k邻近的思路吗，是需要遍历所有的节点吗，想编程实现来着，但是对于在另一半子树中存在比当前子树中多个点更近的点时，想不出来怎么解决，网上也没看到资料</p>
</li>
<li><p>既然能找到最邻近点，一定能找到k邻近点。例如k=3，设置一个包含3个元素的数组，在往上寻找的时候总是把最邻近的3个点保存下来。最后统计最多所占的类别就好。</p>
</li>
<li><p>但是搜索完根节点的左子树，假设找到了三个点，怎么确定右子树中没有比这三个点更近的点，是需要在走一遍右子树吗，就是这没想通。在李航老师树的42页那个特征空间划分的图中，如果我想查找(6,1)并且k=3，走完左子树后查到三个最近的点，然后右子树还有一个(8，1)，比左子树中部分点更近，这种应该怎么办？</p>
</li>
<li><p>这还是需要在继续便利，直到右子树也遍历完，像你说的这种情况，会让右子树中的小值替换当前3个元素的数组中的大值，直到找出整个树中最小的前3个值。</p>
</li>
</ul>
<ol>
<li>KNN算法中的K是怎样取值的？越大越好？k它有什么意义吗?</li>
</ol>
<ul>
<li>K一般没有固定的取值，根据具体问题具体分析。一般可以使用交叉验证选择最佳的K值。</li>
</ul>
<ol>
<li>实战书的k近邻算法第二章第四小节说，”k近邻算法的另一个缺陷是它无法给出任何数据的基础信息结构，因此我们也无法知晓平均实例样本和典型实例样本具有什么特征。下一章我们将使用概率测量方法处理分类问题，该算法可以解决这个问题”。疑问1:没理解这个缺陷是什么缺陷，样本数据每个纬度值都有了，为什么说无法知晓具有什么特征和基础信息结构？疑问二:为什么说概率测量方法可以解决？</li>
</ol>
<ul>
<li>1、knn是基于实例的学习，训练的时候使用的是样本所有的输入值进行距离计算，例如图片识别中一张图片所有的像素点。整个过程并没有提取样本本身的固有特征。训练过程保持了所有的训练样本。2、决策树使用信息增益寻找最佳划分特征，信息增益是通过训练样本中的概率测量方法得到的。</li>
</ul>
<ol>
<li>knn运行加载学会数据出错，是什么原因呢</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:AGwHVXhYwVL-ybKCq0-_DgRsQ3U=.jpeg" alt="img"></p>
<ul>
<li>看下你的“datingTestSet.txt”数据集，类别标签不是int型，你可以是使用“datingTestSet2.txt”数据集，label是int型的，或者修改file2matrix函数。这些都在训练营的github上：<a href="https://github.com/RedstoneWill/MachineLearningInAction-Camp" target="_blank" rel="noopener">https://github.com/RedstoneWill/MachineLearningInAction-Camp</a></li>
</ul>
<ol>
<li>老师，K近邻算法中如何理解？voteIlabel = labels[sortedDistIndicies[i]] ？为什么不能写成voteIlabel = labels[sortedDistIndicies==i] ？（当i 为0时 取sortedDistIndicies为0的那项）</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:S5fagacN3D9dtMA9iIlXtMsESX4=.jpeg" alt="img"></p>
<ul>
<li>sortedDistIndicies存储的是从大到小排列，距离最近的label下标，即位置。然后找到前k个点，计这k个点属于哪一个类别，统计最多的那一个类别就是预测类别。建议把每行语句打印出来看看，这样理解得比较透彻。</li>
</ul>
<h1 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h1><ol>
<li>如何用极大似然估计法推出概率估计公式（4.8）和（4.9）。也是统计学习方法朴素贝叶斯法中的课后习题。</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:6cGdixxIXfnKD1o-ESQzA2WOllM=.jpeg" alt="img"></p>
<ul>
<li><a href="https://blog.csdn.net/xiaoxiao_wen/article/details/54097917" target="_blank" rel="noopener">https://blog.csdn.net/xiaoxiao_wen/article/details/54097917</a></li>
</ul>
<ol>
<li>最近我在做文本分类的题目，按理说使用word2vec进行模型建立后，效果应该比用countvectorizer好的，但是我使用word2vec反而效果差了一些，不太懂为什么会这样？</li>
</ol>
<ul>
<li>可能跟样本集、算法、模型都有关系，没有说某个模型一定好。</li>
</ul>
<ol>
<li>想问一下图片上是怎么由1式得到的2式啊（记号处）</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:iCXlUW9aTFzVJdZoHTAdKlMKutU=.jpeg" alt="img"></p>
<ul>
<li>这是条件期望的表达式，可以看成是取每个类别的概率乘以当前类别的期望风险，最后所有类别的总和。</li>
</ul>
<ol>
<li>这个的解答，能直接算p(y=1!2,s)和p(y=-1|2,s)的概率，哪个大就代表是哪个类别吗</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:Zk5IB0G4ePgZi7JHn1wMMmhAHOo=.jpeg" alt="img"></p>
<ul>
<li>是的，朴素贝叶斯公式中，由于分母是全概率都是一样的，所以一般比较分子就行了。</li>
</ul>
<ol>
<li>针对4.5.2的公式和例子有两个问题1.例子中的文档和词条，谁是w谁是c?我理解c是词条，w是文档分类2.在4.5.3的classifyNB的计算中并没有除p(w)</li>
</ol>
<ul>
<li><p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:sJrrYT3AYbEp9lZdN63KNELmEWQ=.jpeg" alt="img"></p>
</li>
<li><p>\1. c 意为 class，c 是类别，w 是特征词条。</p>
</li>
<li><p>\2. 之所以不除以 p(w) 是因为计算所有c别可能性的时候，p(w) 都是相同的，比较大小的时候只看分子就行了。</p>
</li>
</ul>
<ol>
<li>请问老师，在书上朴素贝叶斯分类器过滤垃圾邮件中，图中这几行代码该怎么理解？trainMatrix不是文本向量吗？+＝操作是咋回事？</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:bHzNDf79I9crdbq9M4amyQu6-qk=.jpeg" alt="img"></p>
<ul>
<li>p1Num 和 p0Num 中的 += 操作是向量相加，统计的是训练样本两个类别每个单词出现的次数，p1Denom 和 p0Denom 中的 += 操作是数字相加，统计的是训练样本两个类别各自总的单词数。</li>
</ul>
<ol>
<li>老师，我想问下《机器学习实战》71页程序清单4-7这里，为什么要条件概率大于-6.0的单词加入到列表里，-6.0意味着什么</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:t-YVYN4a776AC1xr6-1uHDSBSIs=.jpeg" alt="img"></p>
<ul>
<li>条件概率加了log，可能会出现负值。这里为什么是-6.0可能是个阈值把。这4.7节不是我们需要完成的任务。原始网站数据因为是外网，所以爬不到，就不用看了。</li>
</ul>
<h1 id="其他疑问"><a href="#其他疑问" class="headerlink" title="其他疑问"></a>其他疑问</h1><ol>
<li>想问一下逻辑回归求参数为什么不是直接求导 而是要梯度下降呢 像SVM这种不都是直接求导结合拉格朗日就可以求参数吗</li>
</ol>
<ul>
<li>首先并不是所有函数都可以直接令导数为零求得极值的，有时可以求出导数在每个点的值, 但是直接解方程解不出来。对计算机而言，更加适合用循环迭代的方法来求极值，即梯度下降。SVM的解是二次规划问题，对于二次规划问题，有经典的最速下降法，牛顿法等，并不是简单的直接求导。</li>
</ul>
<ol>
<li>训练数据集经常出现训练数据集里面没有的属性，这种属性训练的有什么用吗？这么利用，比如下图，希望老师解答的详细一点，之前没有接触过这种数据训练</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:lpET1N0KWYLkGEAgF-sqe6LMKSo=.jpeg" alt="img"></p>
<ul>
<li>我看的是训练集里有一些特征，但是在测试集中却并未出现。尝试使用特征工程找出不同特征之间的相互关系。或者简单的方法忽略这些特征，训练的模型在测试集上测试看看效果如何。</li>
</ul>
<ol>
<li>请问训练营的10.1的阅读材料是台大机器学习基石的PLA,  训练营大纲是从knn开始的，1. 请问国庆节后训练营从哪个算法开始呢？2. 我们平时自己学习的时候按照实战的顺序学习吗？3. 那您博客的基石与技法的笔记要不要看呢？毕竟他们的内容不一样 4. 是否老师根据自己的需要安排算法的讲解顺序，并不是完全参考书本，然后讲解的算法内容与台大的笔记相结合学习？ 我自己比较纠结学习顺序，谢谢老师解答\</li>
</ol>
<ul>
<li>你好，1. 训练营的课程大纲是按照《机器学习实战》这本书为基础的，国庆假期的任务是我给大家安排的选修作业，并不在我们的任务要求之内，有兴趣的同学可以做一做。国庆后从决策树开始，具体见知识圈。2. 按照《机器学习实战》顺序。3. 我的个人博客、微信公众号有不错的资源和文章，大家可以作为参考资料看看。4. 目前这一期的训练营我们还是按照《机器学习实战》这本书的章节顺序来学习的，跟着我们大纲的顺序学习就好了。后期如果有调整会告知大家。</li>
</ul>
<ol>
<li>关于PLA算法的实现，我想问一下这里的x1和x2是随便设的嘛，为什么y1和y2要这样算呢</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:RRyQpBB2tqQ1nkkpjh4-QG6D604=.jpeg" alt="img"></p>
<ul>
<li><p>这里是画出当前w对应的分类线，x1和x2选取合适的值就好。y1和y2是根据分类线表达式推导的，见下图：</p>
</li>
<li><p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:5SEqeHWYeZIPyYa2mlKWwJYRfEA=.jpeg" alt="img"></p>
</li>
</ul>
<ol>
<li>有没有什么好的，理论教材，比如最优化，凸优化这些？</li>
</ol>
<ul>
<li>Bubeck的《Convex Optimization: Algorithms and Complexity》。最好根据自己实际情况找到最适合自己的就行。</li>
</ul>
<ol>
<li>能推荐对极大似然估计和softmax解释得通俗易懂的博客？</li>
</ol>
<ul>
<li>没有专门的推荐，网上资料很多，CSDN和博客园的文章都不错。Softmax的有一篇可以看看：<a href="https://mp.weixin.qq.com/s/XBK7T1P7z3rm3o-3BDNeOA" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/XBK7T1P7z3rm3o-3BDNeOA</a></li>
</ul>
<ol>
<li>例如lstm的变体GRU，每个参数在编程中如何体现的，以及W如何设置呢？能写个样例吗？要是能debug到细节更好了！</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/FnXmqd1ACovsuiQ_uMayt8Mt8P3i.jpg!thumbnail" alt="img"></p>
<ul>
<li>原理上与一般神经网络类似，使用梯度下降更新参数。现在多是直接调用深度学习框架来做，自己手写LSTM没啥必要。学有余力可以直接看看深度学习框架中LSTM的实现源码。</li>
</ul>
<ol>
<li>我安装scikit-learn的时候出现这个问题怎么解决呢</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:najgnP2JhLK4Haeyh_MgbPdUNSw=.jpeg" alt="img"></p>
<ul>
<li>换个镜像源试试。建议使用Anaconda自带的conda，直接输入：conda install scikit-learn即可。</li>
</ul>
<ol>
<li>能详细介绍一下lstm的代码实现，以及每个参数的含义吗？最好是能跑起来的程序！现在网上搜的程序规模太大！不适合从浅入深的学习！</li>
</ol>
<ul>
<li>这个问题太大了，现在基本都是使用tensorflow或pytorch等库来实现LSTM。如果是入门的话，建议看看莫烦的教学视频，这里面讲到了LSTM：<a href="https://morvanzhou.github.io/tutorials/machine-learning/torch/" target="_blank" rel="noopener">https://morvanzhou.github.io/tutorials/machine-learning/torch/</a></li>
</ul>
<ol>
<li>机器学习工具包sklearn，有没有比较好的教程，以及还有其他的比较好的工具包吗</li>
</ol>
<ul>
<li><p>最好的教程就是官方文档。ApachenCN翻译了中文版，可以参考：<a href="http://sklearn.apachecn.org/" target="_blank" rel="noopener">http://sklearn.apachecn.org/</a></p>
</li>
<li><p>机器学习最好的就是sklearn了，其它的深度学习如TensorFlow，PyTorch，Keras等</p>
</li>
</ul>
<ol>
<li>老师，请问跟着你学完这门课程，还需要学习哪些东西，才能找到一分机器学习的工作呢？</li>
</ol>
<ul>
<li>这是一个很大又很实际的问题。首先学习这门课只是基础，帮助大家在理解一定机器学习理论的基础上使用python手写各个基本机器学习算法的实现过程。但掌握这些还远远不够。建议从三个方面入手：1. 补充机器学习理论基础知识，这点在找工作笔试、面试的时候是很重要的，例如SVM、AdaBoost、GBDT等等要吃得透一点。2. 提高代码编程能力，掌握对机器学习库的使用，例如scikit-learn、深度学习的tensorflow等。3. 有机会的话做一些项目积累经验，或者了解构建一个完整的机器学习项目的整个流程。</li>
</ul>
<ol>
<li>我这边在训练一个卷积网络的模型，但是我的数据中正样本很多，负样本很少，我觉得这样的数据训练出来的模型可能会有问题，我想请问一下数据集中正负样本的比例多大时训练出来的模型比较好？</li>
</ol>
<ul>
<li>一般是正负样本近似相等的时候比较好。但实际中出现正负样本不均匀的情况，可以使用重采样和欠采样来尽量让其数量接近。</li>
</ul>
<ol>
<li>我把一个训练好的模型用C++进行调用的时候，我发现单次调用的时候用时100ms左右，但是做100次循环求平均用时5ms左右！请问您知道是什么原因导致的吗？</li>
</ol>
<ul>
<li>第一次参数传到模型时耗时比较多。</li>
</ul>
<ol>
<li>老师您好，我今年27了，在一家国企的设计单位做轨道交通线路走向设计工作。我打算现在转行机器学习，是否来得及？因为年龄比较大了。</li>
</ol>
<ul>
<li>你好，任何时候想要转行机器学习都不晚。况且你也才27岁，还算年轻。建议原先的工作先干着，平时多学点机器学习，为找份机器学习工作做准备。我不知道你的基础如何，一般半年时间可以入门了。</li>
</ul>
<ol>
<li>关于PLA算法，我做机器学习基石作业1的第15题的时候，迭代次数一直都是21，网上的答案是45，不知道自己究竟是哪里出错了</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:Ht9wz0Sn519trUPoelmrve2I-Bw=.jpeg" alt="img"></p>
<ul>
<li>PLA得到的分类超平面不是固定的，每次都可能产出新的结果，迭代次数也与初始选择的参数有关。你把最终的分类线在数据集上画出来看一下，正确的话就没有问题。</li>
</ul>
<ol>
<li>老师您好。本来想提一些关于实战书籍里的问题，结果发现琢磨着也很快看懂了。想来我的目的是来学习，并希望找一份相应的工作，请老师指点一下。我是一个走机器学习的转行人员，具备一年的python自学基础，目前毕业3年，软件测试干了2年，也算是IT行业吧。1,在这个深入一点的IT行业，机器学习里面，实际上机器学习会有怎样的使用。(应该不仅仅是kaggle或者天池020刷题一样吧？)2,机器学习涉及到大量数据的处理，我想问一下这些数据也是我自己来获取吗？3,对于面试，我应该准备到多少？(不会面多年经验的，所以就从初等看吧。)4,对于这种面试的情况下，我们应该面试的工资在什么范围？</li>
</ol>
<ul>
<li>你好。1.实际工程项目与打比赛区别还是很大的。比较仅仅考虑的是模型的准确率而不惜使用非常复杂、臃肿的模型。但是再工程应用上，除了考虑性能之外，还要注重速度、资源消耗、成本等各个方面。实际上机器学习有很大应用，比如推荐系统、图像识别等。2.数据不用自己获取，网络上有大量可供下载的数据集。公司里的话，也会有专门的人做数据收集、清洗等工作的。但是机器学习工程师也多少做过数据收集这些事情。3.面试的话多多准备一些机器学习典型问题的知识点，比如SVM、集成学习AdaBoost等，还有你的项目经验。4.这个得根据工作地点、公司、具体什么工作等来确定。可以根据当地IT均资来设个心理价位。</li>
</ul>
<ol>
<li>LSTM中的cell state表示什么意思？</li>
</ol>
<ul>
<li><p>cell state一般是保存模型当前及历史状态。有点像是传输带，它直直地流过整个链，受到轻微的非线性相互作用影响。因此信息可以轻松地沿它流动而不发生改变。可以看下这篇文章：<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">Understanding LSTM Networks — colah’s blog</a></p>
</li>
<li><p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:TxTw6IhA74ue0H40Y8kGSV61lMU=.jpeg" alt="img"></p>
</li>
</ul>
<ol>
<li>老师你好，学习算法需不需要每一步自己去证明它呢?</li>
</ol>
<ul>
<li>其实，对大部分人来说，机器学习算法的每一步详细的数学证明是不需要的。但是我们要感性地理解它的意思和推导方式。就像SVM中涉及的理论推导很多，拉格朗日那块内容每一步推导要大致知道思路和方法，但详细的数学证明可能就不需要了。</li>
</ul>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/5/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><span class="page-number current">6</span><a class="page-number" href="/page/7/">7</a><span class="space">&hellip;</span><a class="page-number" href="/page/31/">31</a><a class="extend next" rel="next" href="/page/7/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Schwimmer</p>
              <div class="site-description motion-element" itemprop="description">Record and Think!</div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">308</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">25</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">61</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          

          

          
          

          
            
          
          

        </div>
      </div>

      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Schwimmer</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.9.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> v7.2.0</div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span>
    <span class="site-uv" title="总访客量">
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="post-meta-divider">|</span>
  

  
    <span class="post-meta-item-icon">
      <i class="fa fa-eye"></i>
    </span>
    <span class="site-pv" title="总访问量">
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    
      <div>
        
  <script>
    window._bd_share_config = {
      "common": {
        "bdText": "",
        "bdMini": "1",
        "bdMiniList": false,
        "bdPic": ""
      },
      "image": {
        "viewList": ["tsina", "douban", "sqq", "qzone", "weixin", "twi", "fbook"],
        "viewText": "分享到：",
        "viewSize": "16"
      },
      "slide": {
        "bdImg": "5",
        "bdPos": "left",
        "bdTop": "100"
      }
    }
  </script>

<script>
  with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='//bdimg.share.baidu.com/static/api/js/share.js?cdnversion='+~(-new Date()/36e5)];
</script>

      </div>
    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/utils.js?v=7.2.0"></script>

  <script src="/js/motion.js?v=7.2.0"></script>



  
  


  <script src="/js/schemes/muse.js?v=7.2.0"></script>




  

  


  <script src="/js/next-boot.js?v=7.2.0"></script>


  

  

  

  
  
<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://https-schwimmer-github-io.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>







  
  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
  

  
  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      document.getElementById(all[i].inputID + '-Frame').parentNode.className += ' has-jax';
    }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

    
  


  

  

  

  

  

  

  

  

  

  

  

</body>
</html>
