<!DOCTYPE html>












  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222"/>






















<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0"/>

<link rel="stylesheet" href="/css/main.css?v=7.2.0"/>


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.2.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.2.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.2.0',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="Record and Think!">
<meta property="og:type" content="website">
<meta property="og:title" content="Schwimmer&#39;s Blog">
<meta property="og:url" content="https://schwimmer.github.io/page/4/index.html">
<meta property="og:site_name" content="Schwimmer&#39;s Blog">
<meta property="og:description" content="Record and Think!">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Schwimmer&#39;s Blog">
<meta name="twitter:description" content="Record and Think!">





  
  
  <link rel="canonical" href="https://schwimmer.github.io/page/4/"/>



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Schwimmer's Blog</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Schwimmer's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Navigationsleiste an/ausschalten">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br/>Startseite</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br/>Schlagwörter</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br/>Kategorien</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br/>Archiv</a>

  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/14/深度学习笔记/Pytorch笔记/完整的一次任务/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/14/深度学习笔记/Pytorch笔记/完整的一次任务/" class="post-title-link" itemprop="url">Unbenannt</a>
              
            
          </h2>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2019-06-14 10:59:11" itemprop="dateCreated datePublished" datetime="2019-06-14T10:59:11+08:00">2019-06-14</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Bearbeitet am</span>
                
                <time title="Geändert am: 2019-06-16 09:16:07" itemprop="dateModified" datetime="2019-06-16T09:16:07+08:00">2019-06-16</time>
              </span>
            
          

          

          
            
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>《PyTorch模型训练实用教程》</p>
<p>代码在：<code>/Users/david/david/code/deep_learning/PyTorch_Tutorial_yuting</code></p>
<p>预先安装</p>
<p>requirement.txt</p>
<p>运行时报错</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">OMP: Error #15: Initializing libiomp5.dylib, but found libomp.dylib already initialized.</div><div class="line">OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://www.intel.com/software/products/support/.</div></pre></td></tr></table></figure>
<p>解决方案</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">os.environ[&quot;KMP_DUPLICATE_LIB_OK&quot;]=&quot;TRUE&quot;</div></pre></td></tr></table></figure>
<h1 id="第一章-数据"><a href="#第一章-数据" class="headerlink" title="第一章 数据"></a>第一章 数据</h1><h2 id="1-1-Cifar10-转-png"><a href="#1-1-Cifar10-转-png" class="headerlink" title="1.1 Cifar10 转 png"></a>1.1 Cifar10 转 png</h2><p>cifar-10 的测试集，10000张图片。</p>
<p>官网:<a href="http://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="noopener">http://www.cs.toronto.edu/~kriz/cifar.html</a></p>
<h3 id="读取并保存为图片"><a href="#读取并保存为图片" class="headerlink" title="读取并保存为图片"></a>读取并保存为图片</h3><p><strong>图片是序列化的，不能直接读取。</strong></p>
<p>运行代码:Code/1_data_prepare/1_1_cifar10_to_png.py</p>
<p>可在文件夹 Data/cifar-10-png/raw_test/下看到 0-9 个文件夹，对应 9 个类别。</p>
<p>将 测试集中的 10000 张图片解压出来，作为原始图片，将从这 10000 张图片中划分出训练集 (train)，验证集(valid)，测试集(test)。 </p>
<h2 id="1-2-训练集、验证集、测试集的划分"><a href="#1-2-训练集、验证集、测试集的划分" class="headerlink" title="1.2 训练集、验证集、测试集的划分"></a>1.2 训练集、验证集、测试集的划分</h2><p>把原始数据按 8:1:1 的比例划分为训练集(train set)、验证集(valid/dev set)和测试集(test set) </p>
<p>运行代码：Code/1_data_prepare/1_2_split_dataset.py</p>
<p>数据划分完毕，下一步是制作存放有图片路径及其标签的 txt。pytorch会根据txt的信息寻找图片，并读取图片数据和标签数据。</p>
<h2 id="1-3-Pytorch读图片数据集"><a href="#1-3-Pytorch读图片数据集" class="headerlink" title="1.3 Pytorch读图片数据集"></a>1.3 Pytorch读图片数据集</h2><h3 id="Dataset类"><a href="#Dataset类" class="headerlink" title="Dataset类"></a>Dataset类</h3><p>PyTorch 读取<strong>图片</strong>，主要是通过 Dataset 类，所以先简单了解一下 Dataset 类。抽象类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dataset</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="string">"""An abstract class representing a Dataset.</span></div><div class="line"></div><div class="line">    All other datasets should subclass it. All subclasses should override</div><div class="line">    ``__len__``, that provides the size of the dataset, and ``__getitem__``,</div><div class="line">    supporting integer indexing in range from 0 to len(self) exclusive.</div><div class="line">    """</div><div class="line">		</div><div class="line">    <span class="comment"># 接收一个 index，然后返回图片数据和标签</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></div><div class="line">        <span class="keyword">raise</span> NotImplementedError</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">raise</span> NotImplementedError</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__add__</span><span class="params">(self, other)</span>:</span></div><div class="line">        <span class="keyword">return</span> ConcatDataset([self, other])</div></pre></td></tr></table></figure>
<p>需要将list放到txt里面，读取txt，获取list。那么读取自己数据的基本流程就是:</p>
<ol>
<li>制作存储了图片的路径和标签信息的 txt</li>
<li>将这些信息转化为 list，该 list 每一个元素对应一个样本</li>
<li>通过 getitem 函数，读取数据和标签，并返回数据和标签</li>
</ol>
<p>在训练代码里是感觉不到这些操作的，只会看到通过 DataLoader 就可以获取一个batch 的数据，其实触发去读取图片这些操作的是 DataLoader 里的<strong>iter</strong>(self)，后面会详细讲解读取过程。在本小节，主要讲 Dataset 子类。</p>
<p>要让 PyTorch 能读取自己的数据集，只需要两步:</p>
<ol>
<li>制作图片数据的索引</li>
<li><strong>构建 Dataset 子类</strong></li>
</ol>
<h3 id="1、制作图片索引"><a href="#1、制作图片索引" class="headerlink" title="1、制作图片索引"></a>1、制作图片索引</h3><p>就是获取图片路径和标签，保存到txt。</p>
<p>运行代码 Code/1_data_prepare/1_3_generate_txt.py</p>
<h3 id="2、构建Dataset子类"><a href="#2、构建Dataset子类" class="headerlink" title="2、构建Dataset子类"></a>2、构建Dataset子类</h3><p>构建了MyDataset类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDataset</span><span class="params">(Dataset)</span>:</span></div><div class="line">  	<span class="comment"># 初始化中，从txt读到imgs对象</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, txt_path, transform = None, target_transform = None)</span>:</span></div><div class="line">        fh = open(txt_path, <span class="string">'r'</span>)</div><div class="line">        imgs = []</div><div class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> fh:</div><div class="line">            line = line.rstrip()</div><div class="line">            words = line.split()</div><div class="line">            imgs.append((words[<span class="number">0</span>], int(words[<span class="number">1</span>])))</div><div class="line"></div><div class="line">        <span class="comment"># 最主要就是要生成这个list， 然后DataLoader中给index，通过getitem读取图片数据</span></div><div class="line">        self.imgs = imgs        </div><div class="line">        <span class="comment"># Compose 类型，里边有一个 list，list定义了对图像的各种操作，如减均值，除标准差，随机裁剪，仿射变换等。</span></div><div class="line">        self.transform = transform</div><div class="line">        self.target_transform = target_transform</div><div class="line"></div><div class="line">    <span class="comment"># python内建的魔法方法，访问索引就会触发</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></div><div class="line">        fn, label = self.imgs[index]</div><div class="line">        img = Image.open(fn).convert(<span class="string">'RGB'</span>)     <span class="comment"># 像素值 0~255，在transfrom.totensor会除以255，使像素值变成 0~1</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> self.transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            img = self.transform(img)   <span class="comment"># 在这里做transform，转为tensor等等</span></div><div class="line"></div><div class="line">        <span class="keyword">return</span> img, label</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> len(self.imgs)</div></pre></td></tr></table></figure>
<p>Pytorch对图片的处理不能生成新图，而是覆盖原图（不是真正的覆盖，就是对象赋值）。当采用randomcrop之类的随机操作时，每个 epoch 输入进来的图片几乎不会是一模一样的，这达到了样本多样性的功能。</p>
<p>当 Mydataset 构建好，剩下的操作就交给 DataLoder，在 DataLoder 中，会触发Mydataset 中的 getiterm 函数读取一张图片的数据和标签，并拼接成一个 batch 返回，作为模型真正的输入。</p>
<h2 id="1-4-DataLoder加载图片"><a href="#1-4-DataLoder加载图片" class="headerlink" title="1.4 DataLoder加载图片"></a>1.4 DataLoder加载图片</h2><p>getitem是在DataLoader中触发的，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 构建MyDataset实例</span></div><div class="line">train_data = MyDataset(txt_path=train_txt_path, transform=trainTransform)</div><div class="line"></div><div class="line"><span class="comment"># 构建DataLoder</span></div><div class="line">train_loader = DataLoader(dataset=train_data, batch_size=train_bs, shuffle=<span class="keyword">True</span>)</div><div class="line"></div><div class="line"><span class="comment"># 这里的data就是__getitem__返回的img, label</span></div><div class="line"><span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(train_loader):</div><div class="line"></div><div class="line"><span class="string">"""</span></div><div class="line">-&gt;     def __iter__(self):</div><div class="line">        return _DataLoaderIter(self)</div><div class="line">        </div><div class="line">    class _DataLoaderIter(object):</div><div class="line">    	def __next__(self):</div><div class="line">    		# collate_fn (callable, optional): merges a list of samples to form a mini-batch.</div><div class="line">        # 这里调用__getitem__</div><div class="line">    		batch = self.collate_fn([self.dataset[i] for i in indices])</div><div class="line">"""</div><div class="line">  </div><div class="line">	<span class="comment"># 获取图片和标签</span></div><div class="line">  inputs, labels = data</div><div class="line">  inputs, labels = Variable(inputs), Variable(labels)</div></pre></td></tr></table></figure>
<blockquote>
<p>图片是通过 Image.open()函数读取进来的，当涉及如下问题：</p>
<p>图片的通道顺序(RGB ? BGR ?)</p>
<p>图片是<code>w*h*c ? c*w*h ?</code></p>
<p>像素值范围[0-1] or [0-255] ?</p>
<p>就要查看 MyDataset()类中 <strong>getitem</strong>()下读取图片用的是什么方法</p>
</blockquote>
<h2 id="1-5-数据增强和数据标准化"><a href="#1-5-数据增强和数据标准化" class="headerlink" title="1.5 数据增强和数据标准化"></a>1.5 数据增强和数据标准化</h2><p>在 PyTorch 中，数据增强方法放在了 transforms.py 文件中。</p>
<p>这一节主要介绍transforms的操作。</p>
<p>1.6 transforms 的二十二个方法</p>
<h1 id="第二章-模型"><a href="#第二章-模型" class="headerlink" title="第二章 模型"></a>第二章 模型</h1><h2 id="2-1-模型的搭建"><a href="#2-1-模型的搭建" class="headerlink" title="2.1 模型的搭建"></a>2.1 模型的搭建</h2><h3 id="2-1-1-模型定义的三要素"><a href="#2-1-1-模型定义的三要素" class="headerlink" title="2.1.1 模型定义的三要素"></a>2.1.1 模型定义的三要素</h3><p>1）首先，必须继承 nn.Module 这个类，要让 PyTorch 知道这个类是一个 Module。</p>
<p>2）其次，在<strong>init</strong>(self)中设置好需要的“组件”(如 conv、pooling、Linear、BatchNorm等）。</p>
<p>3）最后，在 forward(self, x)中用定义好的“组件”进行组装，就像搭积木，把网络结构搭建</p>
<p>在/Code/main<em>training/main.py 中可以看到定义了一个类<code>class Net(nn.Module)</code>，集成了nn.Module，先看<em>_init</em></em>(self)函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">		<span class="comment"># 初始化</span></div><div class="line">    super(Net, self).__init__()</div><div class="line">    <span class="comment"># 定义了一系列组件</span></div><div class="line">    self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</div><div class="line">    self.pool1 = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</div><div class="line">    self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">3</span>)</div><div class="line">    self.pool2 = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</div><div class="line">    self.conv3 = nn.Conv2d(<span class="number">16</span>, <span class="number">64</span>, <span class="number">3</span>)</div><div class="line">    self.pool3=nn.MaxPool2d(<span class="number">2</span>,<span class="number">1</span>)</div><div class="line">    self.fc1 = nn.Linear(<span class="number">64</span> * <span class="number">3</span> * <span class="number">3</span>, <span class="number">120</span>)</div><div class="line">    self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</div><div class="line">    self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</div></pre></td></tr></table></figure>
<p>当这些组件定义好之后，就可以定义 forward()函数，用来搭建网络结构</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">		<span class="comment"># x作为模型的输入，x 经过 conv1，然后经过激活函数 relu，再经过 pool1 操作</span></div><div class="line">    x = self.pool1(F.relu(self.conv1(x)))</div><div class="line">    <span class="comment"># 再做一轮</span></div><div class="line">    x = self.pool2(F.relu(self.conv2(x)))</div><div class="line">    <span class="comment"># 再做一轮</span></div><div class="line">    x=self.pool3(F.relu(self.conv3(x)))</div><div class="line">    <span class="comment"># 将 x 进行 reshape，为了后面做为全连接层的输入</span></div><div class="line">    x = x.view(<span class="number">-1</span>, <span class="number">64</span> * <span class="number">3</span>* <span class="number">3</span>)</div><div class="line">    <span class="comment"># 先经过全连接层 fc，然后经过 relu</span></div><div class="line">    x = F.relu(self.fc1(x))</div><div class="line">    x = F.relu(self.fc2(x))</div><div class="line">    x = self.fc3(x)</div><div class="line">    <span class="keyword">return</span> x</div></pre></td></tr></table></figure>
<h3 id="2-1-2-一个更复杂的模型"><a href="#2-1-2-一个更复杂的模型" class="headerlink" title="2.1.2 一个更复杂的模型"></a>2.1.2 一个更复杂的模型</h3><p>来看一个更复杂的模型，看Resnet网络的定义方法<a href="https://github.com/yuanlairuci110/pytorch-best-practice-master/blob/master/models/ResNet34.py" target="_blank" rel="noopener">https://github.com/yuanlairuci110/pytorch-best-practice-master/blob/master/models/ResNet34.py</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#coding:utf8</span></div><div class="line"><span class="keyword">from</span> .BasicModule <span class="keyword">import</span> BasicModule</div><div class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</div><div class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResidualBlock</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="string">'''</span></div><div class="line">    实现子module: Residual Block</div><div class="line">    '''</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, inchannel, outchannel, stride=<span class="number">1</span>, shortcut=None)</span>:</span></div><div class="line">        super(ResidualBlock, self).__init__()</div><div class="line">        self.left = nn.Sequential(</div><div class="line">                nn.Conv2d(inchannel, outchannel, <span class="number">3</span>, stride, <span class="number">1</span>, bias=<span class="keyword">False</span>),</div><div class="line">                nn.BatchNorm2d(outchannel),</div><div class="line">                nn.ReLU(inplace=<span class="keyword">True</span>),</div><div class="line">                nn.Conv2d(outchannel, outchannel, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, bias=<span class="keyword">False</span>),</div><div class="line">                nn.BatchNorm2d(outchannel) )</div><div class="line">        self.right = shortcut</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        out = self.left(x)</div><div class="line">        residual = x <span class="keyword">if</span> self.right <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">else</span> self.right(x)</div><div class="line">        out += residual</div><div class="line">        <span class="keyword">return</span> F.relu(out)</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet34</span><span class="params">(BasicModule)</span>:</span></div><div class="line">    <span class="string">'''</span></div><div class="line">    实现主module：ResNet34</div><div class="line">    ResNet34包含多个layer，每个layer又包含多个Residual block</div><div class="line">    用子module来实现Residual block，用_make_layer函数来实现layer</div><div class="line">    '''</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_classes=<span class="number">2</span>)</span>:</span></div><div class="line">        super(ResNet34, self).__init__()</div><div class="line">        self.model_name = <span class="string">'resnet34'</span></div><div class="line"></div><div class="line">        <span class="comment"># 前几层: 图像转换</span></div><div class="line">        self.pre = nn.Sequential(</div><div class="line">                nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">3</span>, bias=<span class="keyword">False</span>),</div><div class="line">                nn.BatchNorm2d(<span class="number">64</span>),</div><div class="line">                nn.ReLU(inplace=<span class="keyword">True</span>),</div><div class="line">                nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>))</div><div class="line">        </div><div class="line">        <span class="comment"># 重复的layer，分别有3，4，6，3个residual block</span></div><div class="line">        self.layer1 = self._make_layer( <span class="number">64</span>, <span class="number">128</span>, <span class="number">3</span>)</div><div class="line">        self.layer2 = self._make_layer( <span class="number">128</span>, <span class="number">256</span>, <span class="number">4</span>, stride=<span class="number">2</span>)</div><div class="line">        self.layer3 = self._make_layer( <span class="number">256</span>, <span class="number">512</span>, <span class="number">6</span>, stride=<span class="number">2</span>)</div><div class="line">        self.layer4 = self._make_layer( <span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, stride=<span class="number">2</span>)</div><div class="line"></div><div class="line">        <span class="comment">#分类用的全连接</span></div><div class="line">        self.fc = nn.Linear(<span class="number">512</span>, num_classes)</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_make_layer</span><span class="params">(self,  inchannel, outchannel, block_num, stride=<span class="number">1</span>)</span>:</span></div><div class="line">        <span class="string">'''</span></div><div class="line">        构建layer,包含多个residual block</div><div class="line">        '''</div><div class="line">        shortcut = nn.Sequential(</div><div class="line">                nn.Conv2d(inchannel,outchannel,<span class="number">1</span>,stride, bias=<span class="keyword">False</span>),</div><div class="line">                nn.BatchNorm2d(outchannel))</div><div class="line">        </div><div class="line">        layers = []</div><div class="line">        layers.append(ResidualBlock(inchannel, outchannel, stride, shortcut))</div><div class="line">        </div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, block_num):</div><div class="line">            layers.append(ResidualBlock(outchannel, outchannel))</div><div class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</div><div class="line">        </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        x = self.pre(x)</div><div class="line">        </div><div class="line">        x = self.layer1(x)</div><div class="line">        x = self.layer2(x)</div><div class="line">        x = self.layer3(x)</div><div class="line">        x = self.layer4(x)</div><div class="line"></div><div class="line">        x = F.avg_pool2d(x, <span class="number">7</span>)</div><div class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)</div><div class="line">        <span class="keyword">return</span> self.fc(x)</div></pre></td></tr></table></figure>
<p>这里用到了<code>torch.nn.Sequential</code></p>
<h3 id="2-1-3-nn-Sequential"><a href="#2-1-3-nn-Sequential" class="headerlink" title="2.1.3 nn.Sequential"></a>2.1.3 nn.Sequential</h3><p>这个是Sequential容器，将一系列操作包起来。例如Resnet有很多重复的block，就可以包起来。</p>
<p>官方文档中给了两个例子</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Example of using Sequential model = nn.Sequential(</span></div><div class="line">nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>), nn.ReLU(), nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>), nn.ReLU()</div><div class="line">)</div><div class="line"></div><div class="line"><span class="comment"># Example of using Sequential with OrderedDict</span></div><div class="line">model = nn.Sequential(OrderedDict([</div><div class="line">(<span class="string">'conv1'</span>, nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>)), (<span class="string">'relu1'</span>, nn.ReLU()),</div><div class="line">(<span class="string">'conv2'</span>, nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>)), (<span class="string">'relu2'</span>, nn.ReLU())</div><div class="line">]))</div></pre></td></tr></table></figure>
<p>总结：模型的定义就是先继承，再构建组件，最后组装(forward)。</p>
<h2 id="2-2-权值初始化的十种方法"><a href="#2-2-权值初始化的十种方法" class="headerlink" title="2.2 权值初始化的十种方法"></a>2.2 权值初始化的十种方法</h2><p>初始化方法会直接影响模型的收敛与否</p>
<h3 id="2-2-1-权重初始化流程"><a href="#2-2-1-权重初始化流程" class="headerlink" title="2.2.1 权重初始化流程"></a>2.2.1 权重初始化流程</h3><p>总共两步，</p>
<p>1）先设定什么层用什么初始化方法，初始化方法在torch.nn.init中给出；</p>
<p>2）实例化一个模型之后，执行该函数，即可完成初始化。</p>
<p>重点是第一步，看Main的方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义权值初始化</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize_weights</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</div><div class="line">            <span class="keyword">if</span> isinstance(m, nn.Conv2d):</div><div class="line">                torch.nn.init.xavier_normal_(m.weight.data)</div><div class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">                    <span class="comment"># 若有bias，初始化全为0</span></div><div class="line">                    m.bias.data.zero_()</div><div class="line">            <span class="keyword">elif</span> isinstance(m, nn.BatchNorm2d):</div><div class="line">                m.weight.data.fill_(<span class="number">1</span>)</div><div class="line">                m.bias.data.zero_()</div><div class="line">            <span class="keyword">elif</span> isinstance(m, nn.Linear):</div><div class="line">                torch.nn.init.normal_(m.weight.data, <span class="number">0</span>, <span class="number">0.01</span>)</div><div class="line">                m.bias.data.zero_()</div></pre></td></tr></table></figure>
<h3 id="2-2-2-常用初始化方法"><a href="#2-2-2-常用初始化方法" class="headerlink" title="2.2.2 常用初始化方法"></a>2.2.2 常用初始化方法</h3><p>1）Xavier，kaiming系列</p>
<p>2）其他方法分布</p>
<p>Xavier 初始化方法，论文在《Understanding the difficulty of training deep feedforward neural  networks》 </p>
<p>公式推导是从“方差一致性”出发，初始化的分布有均匀分布和正态分布两种。</p>
<h4 id="1、Xavier均匀分布"><a href="#1、Xavier均匀分布" class="headerlink" title="1、Xavier均匀分布"></a>1、Xavier均匀分布</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torch.nn.init.xavier_uniform_(tensor, gain=1)</div></pre></td></tr></table></figure>
<p>服从均匀分布U(-a, a)，分布的参数$a=gain * sqrt(6/fan_in+fan_out)$</p>
<p>这里有一个gain，增益的大小是依据激活函数类型来设定。如</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">nn.init.xavier_uniform_(w, gain=nn.init.calculate_gain(&apos;relu&apos;))</div></pre></td></tr></table></figure>
<p>上述方法也成为Glorot initialization</p>
<h4 id="2、Xavier正态分布"><a href="#2、Xavier正态分布" class="headerlink" title="2、Xavier正态分布"></a>2、Xavier正态分布</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torch.nn.init.xavier_normal_(tensor, gain=1)</div></pre></td></tr></table></figure>
<h4 id="3、kaiming均匀分布"><a href="#3、kaiming均匀分布" class="headerlink" title="3、kaiming均匀分布"></a>3、kaiming均匀分布</h4><p>论文在《 Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification》。公式推导同样从“方差一致性”出法，kaiming是针对 xavier 初始化方法在 relu 这一类激活函数表现不佳而提出的改进</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torch.nn.init.kaiming_uniform_(tensor, a=0, mode=&apos;fan_in&apos;, nonlinearity=&apos;leaky_relu&apos;)</div></pre></td></tr></table></figure>
<p>其中，a是激活函数的负半轴的斜率，relu是0</p>
<p>mode可选为fan_in或fan_out，前者使正向传播时方差一致，后者使反向传播时方差一致。</p>
<p>nonlinearity可选relu和leaky_relu，默认值为leaky_relu。</p>
<p>4、kaiming正态分布</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torch.nn.init.kaiming_normal_(tensor, a=0, mode=&apos;fan_in&apos;, nonlinearity=&apos;leaky_relu&apos;)</div></pre></td></tr></table></figure>
<p>5、其他方法</p>
<p>参见文档</p>
<p><strong>权值初始化杂谈</strong></p>
<p>1、从代码中发现，即使不进行初始化，模型的权重也不为空，而是有值的，这些值是什么时候赋给的呢？</p>
<blockquote>
<p>其实，在创建网络实例的过程中，一旦调用nn.Conv2d的时候就会对权值进行初始化。</p>
<p>初始化过程是在Conv2d的基类_ConvNd中进行的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&gt; class Conv2d(_ConvNd):</div><div class="line">&gt; --&gt; 在_ConvNd 中:</div><div class="line">&gt; --&gt; self.reset_parameters()</div><div class="line">&gt; ---&gt; def reset_parameters(self)</div><div class="line">&gt; ---&gt; self.weight.data.uniform_(-stdv, stdv)</div><div class="line">&gt;</div></pre></td></tr></table></figure>
</blockquote>
<p>&gt;</p>
<blockquote>
<p>可以看出这里是均匀分布，其中-stdv与kernel的size有关。</p>
<p>补充：在Pytorch1.0版本中，这里改用了kaiming<em>uniform</em>()进行初始化。</p>
</blockquote>
<p>2、按需定义初始化方法，例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">if isinstance(m, nn.Conv2d):</div><div class="line">	n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels 		</div><div class="line">	m.weight.data.normal_(0, math.sqrt(2. / n))</div></pre></td></tr></table></figure>
<h2 id="2-3-模型Finetune"><a href="#2-3-模型Finetune" class="headerlink" title="2.3 模型Finetune"></a>2.3 模型Finetune</h2><p>实际应用中，通常采用一个已经训练模型的权值参数作为我们模型的初始化参数，也称之为finetune—迁移学习。迁移学习中的 Finetune 技术，本质上就是让我们新构建的模型，拥有一个较好的权值初始值。</p>
<p>finetune 权值初始化三步曲，finetune 就相当于给模型进行初始化，其流程共用三步:<br>第一步:保存模型，拥有一个预训练模型;<br>第二步:加载模型，把预训练模型中的权值取出来;<br>第三步:初始化，将权值对应的“放”到新模型中</p>
<h3 id="2-3-1-权值初始化"><a href="#2-3-1-权值初始化" class="headerlink" title="2.3.1 权值初始化"></a>2.3.1 权值初始化</h3><p>在进行 finetune 之前我们需要拥有一个模型或者是模型参数，因此需要了解如何保存 模型。官方文档中介绍了两种保存模型的方法，一种是保存整个模型，另外一种是仅保存 模型参数(官方推荐用这种方法)，这里采用官方推荐的方法。 </p>
<h4 id="1、保存模型参数"><a href="#1、保存模型参数" class="headerlink" title="1、保存模型参数"></a>1、保存模型参数</h4><p>若拥有模型参数，可跳过这一步。<br>假设创建了一个 net = Net()，并且经过训练，通过以下方式保存:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torch.save(net.state_dict(), &apos;net_params.pkl&apos;)</div></pre></td></tr></table></figure>
<h4 id="2、加载模型"><a href="#2、加载模型" class="headerlink" title="2、加载模型"></a>2、加载模型</h4><p>进行三步曲中的第二步，加载模型，这里只是加载模型的参数: </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pretrained_dict = torch.load(&apos;net_params.pkl&apos;)</div></pre></td></tr></table></figure>
<h4 id="3、初始化"><a href="#3、初始化" class="headerlink" title="3、初始化"></a>3、初始化</h4><p>进行三步曲中的第三步，将取到的权值，对应的放到新模型中: 首先我们创建新模型，并且获取新模型的参数字典 net_state_dict: </p>
<p>net=Net()# 创建net</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">net_state_dict = net.state_dict() # 获取已创建 net 的 state_dict</div></pre></td></tr></table></figure>
<p>接着将 pretrained_dict 里不属于 net_state_dict 的键<strong>剔除掉</strong>: </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pretrained_dict_1 = &#123;k: v for k, v in pretrained_dict.items() if k in net_state_dict&#125;</div></pre></td></tr></table></figure>
<p>然后，用预训练模型的参数字典 对 新模型的参数字典 net_state_dict 进行更新: </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">net_state_dict.update(pretrained_dict_1)</div></pre></td></tr></table></figure>
<p>最后，将更新了参数的字典 “放”回到网络中: </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">net.load_state_dict(net_state_dict)</div></pre></td></tr></table></figure>
<p>采用 finetune 的训练过程中，<strong>有时候希望前面层的学习率低一些，改变不要太大，而 后面的全连接层的学习率相对大一些</strong>。这时就需要对不同的层设置不同的学习率，下面就 介绍如何为不同层配置不同的学习率。 </p>
<h3 id="2-3-2-不同层设置不同学习率"><a href="#2-3-2-不同层设置不同学习率" class="headerlink" title="2.3.2 不同层设置不同学习率"></a>2.3.2 不同层设置不同学习率</h3><p>在利用 pre-trained model 的参数做初始化之后，我们可能想让 fc 层更新相对快一些，而希望前面的权值更新小一些，这就可以通过为不同的层设置不同的学习率来达到此目的。</p>
<p>为不同层设置不同的学习率，主要通过优化器对多个参数组进行设置不同的参数。所以，只需要将原始的参数组，划分成两个，甚至更多的参数组，然后分别进行设置学习率。</p>
<p>这里将原始参数“切分”成 fc3 层参数和其余参数，为 fc3 层设置更大的学习率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 返回的是 parameters 的 内存地址 </span></div><div class="line"><span class="comment"># 将fc3层的参数从原始参数net.parameters中剥离出来。</span></div><div class="line"><span class="comment"># base_params是剥离了fc3层参数的其他参数。</span></div><div class="line">ignored_params = list(map(id, net.fc3.parameters())) </div><div class="line">base_params = filter(<span class="keyword">lambda</span> p: id(p) <span class="keyword">not</span> <span class="keyword">in</span> 	ignored_params, net.parameters())</div><div class="line"><span class="comment"># 然后优化器中为fc3的单独设置学习率</span></div><div class="line">optimizer = optim.SGD([</div><div class="line">  &#123;<span class="string">'params'</span>: base_params&#125;,</div><div class="line">	&#123;<span class="string">'params'</span>: net.fc3.parameters(), <span class="string">'lr'</span>: <span class="number">0.001</span>*<span class="number">10</span>&#125;], <span class="number">0.001</span>, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">1e-4</span>)</div><div class="line">])</div></pre></td></tr></table></figure>
<p>完整代码在<code>/Code/2_model/2_finetune.py</code></p>
<h1 id="第三章-损失函数和优化器"><a href="#第三章-损失函数和优化器" class="headerlink" title="第三章 损失函数和优化器"></a>第三章 损失函数和优化器</h1><p>Pytorch中十七个损失函数，十个优化器和六个学习率调整方法。</p>
<h2 id="3-1-十七个损失函数"><a href="#3-1-十七个损失函数" class="headerlink" title="3.1 十七个损失函数"></a>3.1 十七个损失函数</h2><p>我们所说的优化，即优化网络权值使得损失函数值变小。但是，损失函数值变小是否能代表模型的分类/回归精度变高呢?那么多种损失函数，应该如何选择呢?请来了解PyTorch 中给出的十七种损失函数吧。</p>
<h3 id="3-1-1-L1loss"><a href="#3-1-1-L1loss" class="headerlink" title="3.1.1 L1loss"></a>3.1.1 L1loss</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">class torch.nn.L1Loss(size_average=None, reduce=None)</div></pre></td></tr></table></figure>
<p>官方文档中仍有 reduction=’elementwise_mean’参数，但代码实现中已经删除该参数</p>
<p><strong>功能：</strong></p>
<p>计算output和target之差的绝对值，可选返回同维度的tensor或一个标量。</p>
<p><strong>公式：</strong></p>
<p><img src="/2019/06/14/深度学习笔记/Pytorch笔记/完整的一次任务/pic/image-20190616085944731.png" alt="image-20190616085944731"></p>
<p><strong>参数:</strong> </p>
<p>reduce(bool)- 返回值是否为标量，默认为 True<br>size_average(bool)- 当 reduce=True 时有效。为 True 时，返回的 loss 为平均值;为 False 时，返回的各样本的 loss 之和。<br><strong>实例:</strong> </p>
<p><code>/Code/3_optimizer/3_1_lossFunction/1_L1Loss.py</code></p>
<h1 id="第四章-监控模型-可视化"><a href="#第四章-监控模型-可视化" class="headerlink" title="第四章 监控模型-可视化"></a>第四章 监控模型-可视化</h1><h2 id="4-1-TensorBoardX"><a href="#4-1-TensorBoardX" class="headerlink" title="4.1 TensorBoardX"></a>4.1 TensorBoardX</h2><p>流行的有两种方法，本文重点介绍第二种。</p>
<p>1、构建Logger类</p>
<p>Logger 类中“包”了 tf.summary.FileWriter ，截至目前(2018.10.17)，只有三种操作，分别是 scalar_summary(), image_summary(), histo_summary()。</p>
<p>优点:轻便，可满足大部分需求</p>
<p>参考github：<a href="https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/04-utils/tensorboard" target="_blank" rel="noopener">https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/04-utils/tensorboard</a></p>
<p>2、借助TensorBoardX包</p>
<p>TensorBoardX 包的功能就比较全，截至目前(2018.10.17)，支持除<br>beholder 之外的所有 tensorboard 的记录类型。</p>
<p>github：<a href="https://github.com/lanpa/tensorboardX" target="_blank" rel="noopener">https://github.com/lanpa/tensorboardX</a></p>
<p>API文档：<a href="https://tensorboard-pytorch.readthedocs.io/en/latest/tutorial_zh.html" target="_blank" rel="noopener">https://tensorboard-pytorch.readthedocs.io/en/latest/tutorial_zh.html</a></p>
<p><strong>代码实现:</strong></p>
<p>tensorboardX 提供 13 个函数，可以记录标量、图像、语音、文字等等，功能十分丰富。<br>本节将对这些函数进行介绍，所用代码为 tensorboardX 的官方 demo.py，放在：</p>
<p><code>/Code/4_viewer/1_tensorboardX_demo.py</code></p>
<p>运行该文件，再打开一个terminal，进入/Result/，执行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tensorboard --logdir=runs</div></pre></td></tr></table></figure>
<p>然后浏览器打开</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">localhost:6006</div></pre></td></tr></table></figure>
<p>可以看到显示界面如下：</p>
<h2 id="4-2-TensorBoardX的函数"><a href="#4-2-TensorBoardX的函数" class="headerlink" title="4.2 TensorBoardX的函数"></a>4.2 TensorBoardX的函数</h2><h3 id="4-2-1-add-scalar"><a href="#4-2-1-add-scalar" class="headerlink" title="4.2.1 add_scalar()"></a>4.2.1 add_scalar()</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">add_scalar(tag, scalar_value, global_step=None, walltime=None)</div></pre></td></tr></table></figure>
<p><strong>功能：</strong></p>
<p>在一个图表中记录一个标量的变化，常用于Loss和Accuracy曲线的记录。</p>
<p><strong>参数:</strong> </p>
<p>tag(string)- 该图的标签，类似于 polt.title。 </p>
<p>scalar_value(float or string/blobname)- 用于存储的值，曲线图的 y 坐标 </p>
<p>global_step(int)- 曲线图的 x 坐标<br> walltime(float)- 为 event 文件的文件名设置时间，默认为 time.time() 运行 demo 中的: </p>
<p>用 github 首页 demo 运行这一行:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">writer.add_scalar(&apos;data/scalar1&apos;, dummy_s1[0], n_iter)</div></pre></td></tr></table></figure>
<p> 可以得到下图: </p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/12/深度学习笔记/Pytorch训练营/D4 数据读取、数据扩增/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/12/深度学习笔记/Pytorch训练营/D4 数据读取、数据扩增/" class="post-title-link" itemprop="url">Unbenannt</a>
              
            
          </h2>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2019-06-12 22:25:33" itemprop="dateCreated datePublished" datetime="2019-06-12T22:25:33+08:00">2019-06-12</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Bearbeitet am</span>
                
                <time title="Geändert am: 2019-06-15 10:44:06" itemprop="dateModified" datetime="2019-06-15T10:44:06+08:00">2019-06-15</time>
              </span>
            
          

          

          
            
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong>任务名称</strong>：1.数据读取；2.数据扩增[参考资料：(1)PyTorch<em>tutorial_0.0.5</em>余霆嵩文档1~16页(2)pytorch官方文档16.torch.utils.data;17.torch.utils.model_zoo;18.torchvision.datasets;19.torchvision.models;20.torchvision.transforms;21.torchvision.utils]</p>
<p><strong>任务简介</strong>：数据读取和自定义数据集的读取操作；数据集的扩增的方法；</p>
<p><strong>详细说明</strong>：</p>
<p>Day9~Day10的任务是数据的读取和数据的扩增，在学习之前需要我们把官方文档16~21的资料学习，然后学习余霆嵩大神整理的资料，这里包括了数据读取和数据扩增的方法基本操作的API。数据扩增在我们数据有限的情况下可以通过数据扩增得到更多的数据，一方面可以抑制过拟合一方面可以提高模型泛化性，pytorch中封装了22种数据扩增的方法，基本包括了我们常用扩增方法，可以根据需求调用各自方法。</p>
<p>作业资料包下载链接：</p>
<p>链接：<a href="https://pan.baidu.com/s/17xW8rfG-14nu6vc9vjeBlQ" target="_blank" rel="noopener">https://pan.baidu.com/s/17xW8rfG-14nu6vc9vjeBlQ</a> </p>
<p>提取码：34k3 </p>
<p><strong>作业名称（详解）：</strong>（1）使用提供的网络模型读取自己数据进行训练；（2）使用22中数据扩增的方法进行组合，测试其效果；</p>
<p><strong>作业提交形式</strong>：打卡提交文字或图片，不少于20字</p>
<p><strong>打卡截止时间：</strong>6/12</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/12/深度学习笔记/colab faceswap/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/12/深度学习笔记/colab faceswap/" class="post-title-link" itemprop="url">Unbenannt</a>
              
            
          </h2>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2019-06-12 08:22:59 / Geändert am: 15:40:59" itemprop="dateCreated datePublished" datetime="2019-06-12T08:22:59+08:00">2019-06-12</time>
            </span>
          

          
            

            
          

          

          
            
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="报错"><a href="#报错" class="headerlink" title="报错"></a>报错</h1><p>1、</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">!python tools.py effmpeg -a extract -i ./video/zhubo.mp4 -o ./video/zhubo20190611/</div></pre></td></tr></table></figure>
<p>报错</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">Traceback (most recent call last):</div><div class="line">  File &quot;tools.py&quot;, line 5, in &lt;module&gt;</div><div class="line">    import tools.cli as cli</div><div class="line">  File &quot;/content/gdrive/My Drive/tensorflow/faceswap-master/tools.py&quot;, line 5, in &lt;module&gt;</div><div class="line">    import tools.cli as cli</div><div class="line">ModuleNotFoundError: No module named &apos;tools.cli&apos;; &apos;tools&apos; is not a package</div></pre></td></tr></table></figure>
<p>2、</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">!python faceswap.py extract -i video/wangwei2/ -o  data/wangwei3</div></pre></td></tr></table></figure>
<p>报错</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">Traceback (most recent call last):</div><div class="line">  File &quot;faceswap.py&quot;, line 36, in &lt;module&gt;</div><div class="line">    ARGUMENTS.func(ARGUMENTS)</div><div class="line">  File &quot;/content/gdrive/My Drive/tensorflow/faceswap-master/lib/cli.py&quot;, line 105, in execute_script</div><div class="line">    log_setup(arguments.loglevel, arguments.logfile, self.command)</div><div class="line">  File &quot;/content/gdrive/My Drive/tensorflow/faceswap-master/lib/logger.py&quot;, line 93, in log_setup</div><div class="line">    f_handler = file_handler(numeric_loglevel, logfile, log_format, command)</div><div class="line">  File &quot;/content/gdrive/My Drive/tensorflow/faceswap-master/lib/logger.py&quot;, line 115, in file_handler</div><div class="line">    log_file.doRollover()</div><div class="line">  File &quot;/usr/lib/python3.6/logging/handlers.py&quot;, line 172, in doRollover</div><div class="line">    os.remove(dfn)</div><div class="line">PermissionError: [Errno 1] Operation not permitted: &apos;/content/gdrive/My Drive/tensorflow/faceswap-master/faceswap.log.1&apos;</div></pre></td></tr></table></figure>
<p>参考<a href="https://www.deepfakescn.com/?p=295" target="_blank" rel="noopener">https://www.deepfakescn.com/?p=295</a></p>
<p><a href="https://github.com/dream80/DeepFaceLab_Colab" target="_blank" rel="noopener">https://github.com/dream80/DeepFaceLab_Colab</a></p>
<p><a href="https://github.com/dream80/DeepFaceLab_Colab/blob/master/DeepFaceLab_Colab_zh_cn.ipynb" target="_blank" rel="noopener">https://github.com/dream80/DeepFaceLab_Colab/blob/master/DeepFaceLab_Colab_zh_cn.ipynb</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/10/深度学习笔记/Pytorch训练营/D3 storage和cuda操作/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/10/深度学习笔记/Pytorch训练营/D3 storage和cuda操作/" class="post-title-link" itemprop="url">Unbenannt</a>
              
            
          </h2>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2019-06-10 22:17:03 / Geändert am: 22:55:01" itemprop="dateCreated datePublished" datetime="2019-06-10T22:17:03+08:00">2019-06-10</time>
            </span>
          

          
            

            
          

          

          
            
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong>任务名称</strong>：<strong>Pytorch官方文档(参考资料：Pytorch官方文档</strong> 8.torch.Storage操作；14.torch.cuda操作）</p>
<p><strong>任务简介</strong>：torch.Storage 主要是Tensor数据类型的转换；torch.cuda；</p>
<p><strong>详细说明</strong>：</p>
<p>Day8的任务两块，第一块是Storage操作，主要包括数据类型转换的接口；第二块是如何使用判断是否有显卡，以及如何把数据和模型在显卡上运行。</p>
<p>作业资料包下载链接：</p>
<p>链接：<a href="https://pan.baidu.com/s/17xW8rfG-14nu6vc9vjeBlQ" target="_blank" rel="noopener">https://pan.baidu.com/s/17xW8rfG-14nu6vc9vjeBlQ</a> </p>
<p>提取码：34k3 </p>
<p><strong>作业名称（详解）：</strong>（1）自己练习数据类型之间的转换方法（2）测试在显卡上训练和在cpu上训练的速度差多少倍；</p>
<p><strong>作业提交形式</strong>：打卡提交文字或图片</p>
<p><strong>打卡截止时间：</strong>6/11</p>
<p><a href="https://blog.csdn.net/Mr_JP/article/details/81906616" target="_blank" rel="noopener">https://blog.csdn.net/Mr_JP/article/details/81906616</a></p>
<h1 id="torch-Storage"><a href="#torch-Storage" class="headerlink" title="torch.Storage"></a>torch.Storage</h1><p><em>torch.Storage 是一个连续的，一维的，单一数据类型的数组。每一个torch.Tensor有一个对应的torch.storage,并且二者都有相同的数据类型。</em></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">a = torch.FloatTensor([1, 2, 3])</div><div class="line">b = torch.FloatStorage([1, 2, 3])</div><div class="line"></div><div class="line">print (a)</div><div class="line">print (type(a))</div><div class="line">print (type(a[0]))</div><div class="line">print (a.shape)</div><div class="line"></div><div class="line">print (b)</div><div class="line">print (type(b))</div><div class="line">print (type(b[0]))</div><div class="line">#print (b.shape) 报错</div></pre></td></tr></table></figure>
<blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">&gt; tensor([1., 2., 3.])</div><div class="line">&gt; &lt;class &apos;torch.Tensor&apos;&gt;</div><div class="line">&gt; &lt;class &apos;torch.Tensor&apos;&gt;</div><div class="line">&gt; torch.Size([3])</div><div class="line">&gt;  1.0</div><div class="line">&gt;  2.0</div><div class="line">&gt;  3.0</div><div class="line">&gt; [torch.FloatStorage of size 3]</div><div class="line">&gt; &lt;class &apos;torch.FloatStorage&apos;&gt;</div><div class="line">&gt; &lt;class &apos;float&apos;&gt;</div><div class="line">&gt;</div></pre></td></tr></table></figure>
</blockquote>
<p>FloatTensor可以接受FloatStorage类型进行初始化，若对FloatStorage进行修改时，FloatTensor也会被修改:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">b = torch.FloatStorage([1, 2, 3])</div><div class="line">a = torch.FloatTensor(b)</div><div class="line"></div><div class="line">b[0] = 10</div><div class="line">print(b)</div><div class="line">print(a)</div></pre></td></tr></table></figure>
<blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&gt;  10.0</div><div class="line">&gt;  2.0</div><div class="line">&gt;  3.0</div><div class="line">&gt; [torch.FloatStorage of size 3]</div><div class="line">&gt; tensor([10.,  2.,  3.])</div><div class="line">&gt;</div></pre></td></tr></table></figure>
</blockquote>
<p><strong>torch.from_numpy</strong><br>torch.from_numpy与用FloatStorage初始化类似，不是直接复制数据初始化。修改numpy类型数据时，tensor数据会发生改变.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">a = np.array([1,2,3])</div><div class="line">b = torch.from_numpy(a)</div><div class="line">a[0] = 10</div><div class="line">print (b)</div></pre></td></tr></table></figure>
<p>输出内容：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tensor([10,  2,  3], dtype=torch.int32)</div></pre></td></tr></table></figure>
<p><strong>结论</strong><br>当张量需要被多处共享使用时，使用FloatStorage初始化FloatTensor.<br>torch.from_numpy和torch.tensor(FloatStorage)都是浅拷贝。</p>
<h1 id="torch-cuda"><a href="#torch-cuda" class="headerlink" title="torch.cuda"></a>torch.cuda</h1>
          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/10/编程语言学习/PYTHON/pyspark各种操作/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/10/编程语言学习/PYTHON/pyspark各种操作/" class="post-title-link" itemprop="url">Unbenannt</a>
              
            
          </h2>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2019-06-10 16:28:44" itemprop="dateCreated datePublished" datetime="2019-06-10T16:28:44+08:00">2019-06-10</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Bearbeitet am</span>
                
                <time title="Geändert am: 2019-06-11 10:47:27" itemprop="dateModified" datetime="2019-06-11T10:47:27+08:00">2019-06-11</time>
              </span>
            
          

          

          
            
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="pyspark-sql-functions"><a href="#pyspark-sql-functions" class="headerlink" title="pyspark.sql.functions"></a>pyspark.sql.functions</h1><p><a href="http://spark.apache.org/docs/1.5.2/api/python/_modules/pyspark/sql/functions.html" target="_blank" rel="noopener">http://spark.apache.org/docs/1.5.2/api/python/_modules/pyspark/sql/functions.html</a></p>
<p><a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.to_json" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.to_json</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div></pre></td><td class="code"><pre><div class="line">_functions = &#123;</div><div class="line">    &apos;lit&apos;: &apos;Creates a :class:`Column` of literal value.&apos;,</div><div class="line">    &apos;col&apos;: &apos;Returns a :class:`Column` based on the given column name.&apos;,</div><div class="line">    &apos;column&apos;: &apos;Returns a :class:`Column` based on the given column name.&apos;,</div><div class="line">    &apos;asc&apos;: &apos;Returns a sort expression based on the ascending order of the given column name.&apos;,</div><div class="line">    &apos;desc&apos;: &apos;Returns a sort expression based on the descending order of the given column name.&apos;,</div><div class="line"></div><div class="line">    &apos;upper&apos;: &apos;Converts a string expression to upper case.&apos;,</div><div class="line">    &apos;lower&apos;: &apos;Converts a string expression to upper case.&apos;,</div><div class="line">    &apos;sqrt&apos;: &apos;Computes the square root of the specified float value.&apos;,</div><div class="line">    &apos;abs&apos;: &apos;Computes the absolute value.&apos;,</div><div class="line"></div><div class="line">    &apos;max&apos;: &apos;Aggregate function: returns the maximum value of the expression in a group.&apos;,</div><div class="line">    &apos;min&apos;: &apos;Aggregate function: returns the minimum value of the expression in a group.&apos;,</div><div class="line">    &apos;first&apos;: &apos;Aggregate function: returns the first value in a group.&apos;,</div><div class="line">    &apos;last&apos;: &apos;Aggregate function: returns the last value in a group.&apos;,</div><div class="line">    &apos;count&apos;: &apos;Aggregate function: returns the number of items in a group.&apos;,</div><div class="line">    &apos;sum&apos;: &apos;Aggregate function: returns the sum of all values in the expression.&apos;,</div><div class="line">    &apos;avg&apos;: &apos;Aggregate function: returns the average of the values in a group.&apos;,</div><div class="line">    &apos;mean&apos;: &apos;Aggregate function: returns the average of the values in a group.&apos;,</div><div class="line">    &apos;sumDistinct&apos;: &apos;Aggregate function: returns the sum of distinct values in the expression.&apos;,</div><div class="line">&#125;</div><div class="line"></div><div class="line">_functions_1_4 = &#123;</div><div class="line">    # unary math functions</div><div class="line">    &apos;acos&apos;: &apos;Computes the cosine inverse of the given value; the returned angle is in the range&apos; +</div><div class="line">            &apos;0.0 through pi.&apos;,</div><div class="line">    &apos;asin&apos;: &apos;Computes the sine inverse of the given value; the returned angle is in the range&apos; +</div><div class="line">            &apos;-pi/2 through pi/2.&apos;,</div><div class="line">    &apos;atan&apos;: &apos;Computes the tangent inverse of the given value.&apos;,</div><div class="line">    &apos;cbrt&apos;: &apos;Computes the cube-root of the given value.&apos;,</div><div class="line">    &apos;ceil&apos;: &apos;Computes the ceiling of the given value.&apos;,</div><div class="line">    &apos;cos&apos;: &apos;Computes the cosine of the given value.&apos;,</div><div class="line">    &apos;cosh&apos;: &apos;Computes the hyperbolic cosine of the given value.&apos;,</div><div class="line">    &apos;exp&apos;: &apos;Computes the exponential of the given value.&apos;,</div><div class="line">    &apos;expm1&apos;: &apos;Computes the exponential of the given value minus one.&apos;,</div><div class="line">    &apos;floor&apos;: &apos;Computes the floor of the given value.&apos;,</div><div class="line">    &apos;log&apos;: &apos;Computes the natural logarithm of the given value.&apos;,</div><div class="line">    &apos;log10&apos;: &apos;Computes the logarithm of the given value in Base 10.&apos;,</div><div class="line">    &apos;log1p&apos;: &apos;Computes the natural logarithm of the given value plus one.&apos;,</div><div class="line">    &apos;rint&apos;: &apos;Returns the double value that is closest in value to the argument and&apos; +</div><div class="line">            &apos; is equal to a mathematical integer.&apos;,</div><div class="line">    &apos;signum&apos;: &apos;Computes the signum of the given value.&apos;,</div><div class="line">    &apos;sin&apos;: &apos;Computes the sine of the given value.&apos;,</div><div class="line">    &apos;sinh&apos;: &apos;Computes the hyperbolic sine of the given value.&apos;,</div><div class="line">    &apos;tan&apos;: &apos;Computes the tangent of the given value.&apos;,</div><div class="line">    &apos;tanh&apos;: &apos;Computes the hyperbolic tangent of the given value.&apos;,</div><div class="line">    &apos;toDegrees&apos;: &apos;Converts an angle measured in radians to an approximately equivalent angle &apos; +</div><div class="line">                 &apos;measured in degrees.&apos;,</div><div class="line">    &apos;toRadians&apos;: &apos;Converts an angle measured in degrees to an approximately equivalent angle &apos; +</div><div class="line">                 &apos;measured in radians.&apos;,</div><div class="line"></div><div class="line">    &apos;bitwiseNOT&apos;: &apos;Computes bitwise not.&apos;,</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>sparkDF.columns</p>
<h1 id="把某一行转成json"><a href="#把某一行转成json" class="headerlink" title="把某一行转成json"></a>把某一行转成json</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">row_2_json</span><span class="params">(row)</span>:</span></div><div class="line">    <span class="keyword">return</span> json.dumps(row.asDict(),ensure_ascii=<span class="keyword">False</span>,indent=<span class="number">2</span>)</div><div class="line"></div><div class="line">row_2_json = F.udf(row_2_json, StringType())</div><div class="line"></div><div class="line">spark_df = hiveCtx.createDataFrame(install_df)</div><div class="line">    event_df = spark_df.withColumn(<span class="string">'evnet_id'</span>, F.lit(<span class="number">6</span>)) \</div><div class="line">                 .withColumn(<span class="string">'event_name'</span>, F.lit(<span class="string">'安装信息异常'</span>)) \</div><div class="line">                 .withColumn(<span class="string">'event_lonlat'</span>, F.col(<span class="string">'install_lonlat'</span>)) \</div><div class="line">                 .withColumn(<span class="string">'anomaly_json'</span>, row_2_json(F.struct([F.col(x) <span class="keyword">for</span> x <span class="keyword">in</span> spark_df.columns]))) \</div><div class="line">                 .withColumn(<span class="string">'data_date'</span>, F.lit(data_param)) \</div><div class="line">                 .select(<span class="string">'app_code'</span>, <span class="string">'evnet_id'</span>, <span class="string">'event_name'</span>, <span class="string">'event_lonlat'</span>, <span class="string">'anomaly_json'</span>, <span class="string">'data_date'</span>)</div></pre></td></tr></table></figure>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/10/编程语言学习/PYTHON/pandas转spark处理/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/10/编程语言学习/PYTHON/pandas转spark处理/" class="post-title-link" itemprop="url">Unbenannt</a>
              
            
          </h2>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2019-06-10 15:53:39 / Geändert am: 16:01:15" itemprop="dateCreated datePublished" datetime="2019-06-10T15:53:39+08:00">2019-06-10</time>
            </span>
          

          
            

            
          

          

          
            
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="spark跟pandas数据转换"><a href="#spark跟pandas数据转换" class="headerlink" title="spark跟pandas数据转换"></a>spark跟pandas数据转换</h1><p>不支持spark 1.6</p>
<p>pandas dataframe转 spark dataframe,</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">import pandas as pd</div><div class="line">from pyspark.sql import SparkSession</div><div class="line"></div><div class="line"></div><div class="line">#pandas读取cvs,形成dataframe,</div><div class="line">userDF = pd.read_csv(&quot;src/main/resources/upload.csv&quot;)</div><div class="line"></div><div class="line">#启动spark</div><div class="line">spark = SparkSession \</div><div class="line">        .builder \</div><div class="line">        .appName(&quot;Python Spark SQL Hive integration example&quot;) \</div><div class="line">        .enableHiveSupport() \</div><div class="line">        .getOrCreate()</div><div class="line"></div><div class="line">#spark读取pandas dataframe,形成spark dataframe</div><div class="line">sparkDF = spark.createDataFrame(userDF)</div><div class="line">sparkDF.show()</div></pre></td></tr></table></figure>
<p>spark dataframe 转 pandas data,download.py</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">from pyspark.sql import SparkSession</div><div class="line"></div><div class="line">spark = SparkSession \</div><div class="line">        .builder \</div><div class="line">        .appName(&quot;Python Spark SQL Hive integration example&quot;) \</div><div class="line">        .enableHiveSupport() \</div><div class="line">        .getOrCreate()</div><div class="line"></div><div class="line">spark.sql(&quot;CREATE TABLE IF NOT EXISTS user (userid int, name string)&quot;)</div><div class="line">spark.sql(&quot;LOAD DATA LOCAL INPATH &apos;src/main/resources/user.txt&apos; INTO TABLE user&quot;)</div><div class="line"></div><div class="line">userSparkDF = spark.sql(&quot;select * from user&quot;)</div><div class="line"></div><div class="line"></div><div class="line">userPandasDF = userSparkDF.toPandas()</div><div class="line"></div><div class="line">print userPandasDF</div><div class="line"></div><div class="line">spark.stop()</div></pre></td></tr></table></figure>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/08/机器学习和深度学习算法理论/SoftMax/输出层的Softmax/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/08/机器学习和深度学习算法理论/SoftMax/输出层的Softmax/" class="post-title-link" itemprop="url">Unbenannt</a>
              
            
          </h2>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2019-06-08 12:06:17 / Geändert am: 12:07:26" itemprop="dateCreated datePublished" datetime="2019-06-08T12:06:17+08:00">2019-06-08</time>
            </span>
          

          
            

            
          

          

          
            
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://cloud.tencent.com/developer/news/307323" target="_blank" rel="noopener">https://cloud.tencent.com/developer/news/307323</a></p>
<p>Softmax是假设不同特征是相互独立的</p>
<p>然而，这可能在许多情况下不成立，因为特征之间可能存在协同作用或冗余，种协同或者作用会直接影响输出概率。</p>
<p>解决方案可以是：</p>
<p>1）去除有协同作用或冗余的特征，如x3 =X1⋅x2x3=x1⋅x2（但是如果我们不知道哪些特征值是相关的，我们可能会引入更多无用的特征！</p>
<p>2）当两个特征经常一起被激活时，训练过程将学习较小的权重W1和W2，使得它们的联合效果更接近真实效果</p>
<blockquote>
<p>如何判断两个特征是否同时被激活</p>
</blockquote>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/05/机器学习和深度学习算法理论/EM/EM算法入门/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/05/机器学习和深度学习算法理论/EM/EM算法入门/" class="post-title-link" itemprop="url">Unbenannt</a>
              
            
          </h2>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2019-06-05 13:52:55 / Geändert am: 13:54:49" itemprop="dateCreated datePublished" datetime="2019-06-05T13:52:55+08:00">2019-06-05</time>
            </span>
          

          
            

            
          

          

          
            
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://zhuanlan.zhihu.com/p/61768577?utm_source=ZHShareTargetIDMore&amp;utm_medium=social&amp;utm_oi=28206795063296" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/61768577?utm_source=ZHShareTargetIDMore&amp;utm_medium=social&amp;utm_oi=28206795063296</a></p>
<p>这个推导啥子的也太难了把。不过经过我不停不停不停不停的看这个算法，到今天我突然觉得自己好像明白了，然后我决定把我的理解写成一篇文章，毕竟只有给别人讲明白了才能算自己真正的明白。那么就进入我们这篇文章的主题:<strong>EM算法。</strong></p>
<p>我们先讲一下极大似然估计法，然后再引申出EM算法</p>
<p><strong>1.极大似然估计法</strong></p>
<p>假设我们有如下的一维高斯分布</p>
<p><img src="https://www.zhihu.com/equation?tex=X%5Csim+N%3D%28%5Cmu%2C+%5Csigma%5E%7B2%7D%29+" alt="X\sim N=(EM/pic/equation-20190605135301319) "></p>
<p>X的概率密度函数为:</p>
<p><img src="https://www.zhihu.com/equation?tex=f%28x%3B%5Cmu%2C%5Csigma%5E%7B2%7D%29%3D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%5Csigma%7D%7De%5E%7B-%5Cfrac%7B%28x-%5Cmu%29%5E%7B2%7D%7D%7B2%5Csigma%5E%7B2%7D%7D%7D" alt="f(EM/pic/equation-20190605135301359)=\frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}}"></p>
<p>其似然函数为</p>
<p><img src="https://www.zhihu.com/equation?tex=L%28%5Cmu%2C%5Csigma%5E%7B%5E%7B2%7D%7D%29%3D%5Cprod_%7Bi%3D1%7D%5E%7Bn%7D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%5Csigma%7D%7De%5E%7B-%5Cfrac%7B%28x-%5Cmu%29%5E%7B2%7D%7D%7B2%5Csigma%5E%7B2%7D%7D%7D" alt="L(EM/pic/equation-20190605135332617)=\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}}"></p>
<p>求对数为</p>
<p><img src="https://www.zhihu.com/equation?tex=lnL%3D-%5Cfrac%7Bn%7D%7B2%7Dln%5Cpi-%5Cfrac%7Bn%7D%7B2%7Dln%5Csigma%5E%7B2%7D-%5Cfrac%7B1%7D%7B2%5Csigma%5E%7B2%7D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7B%28x_%7Bi%7D-%5Cmu%29%5E%7B2%7D%7D%7D" alt="lnL=-\frac{n}{2}ln\pi-\frac{n}{2}ln\sigma^{2}-\frac{1}{2\sigma^{2}\sum_{i=1}^{n}{(EM/pic/equation-20190605135332570)^{2}}}"></p>
<p>对其求导，可以得到如下似然方程组</p>
<p><img src="/2019/06/05/机器学习和深度学习算法理论/EM/EM算法入门/pic/image-20190605135436560.png" alt="image-20190605135436560"></p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cdelta%7D%7B%5Cdelta%5Csigma%5E%7B2%7D%7D%3D-%5Cfrac%7Bn%7D%7B2%5Csigma%5E%7B2%7D%7D%2B%5Cfrac%7B1%7D%7B2%28%5Csigma%5E%7B2%7D%29%5E%7B2%7D%7D%5B%5Csum_%7Bi%3D1%7D%5E%7Bn%7Dx_%7Bi%7D-n%5Cmu%5D" alt="\frac{\delta}{\delta\sigma^{2}}=-\frac{n}{2\sigma^{2}}+\frac{1}{2(EM/pic/equation-20190605135332601)^{2}}[\sum_{i=1}^{n}x_{i}-n\mu]"></p>
<p>我们可以使用</p>
<ul>
<li>梯度下降法</li>
<li>极大似然估计法</li>
</ul>
<p>这两种方法来根据样本估计高斯分布的参数，具体代码如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_guassian_theta</span><span class="params">()</span>:</span></div><div class="line">    <span class="comment"># u=1 siga=4</span></div><div class="line">    y = <span class="number">1</span> + <span class="number">2</span>*np.random.randn(<span class="number">1000</span>,<span class="number">1</span>)</div><div class="line">    n,m = y.shape</div><div class="line"></div><div class="line">    <span class="comment"># 梯度下降</span></div><div class="line">    u1 = np.random.randn(<span class="number">1</span>)[<span class="number">0</span>]</div><div class="line">    siga1 = np.random.randn(<span class="number">1</span>)[<span class="number">0</span>]</div><div class="line">    lr = <span class="number">0.001</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</div><div class="line">        lu = (np.sum(y,axis=<span class="number">0</span>) - n*u1)/(siga1)</div><div class="line">        lsiga = (np.sum((y-u1)**<span class="number">2</span>, axis=<span class="number">0</span>)/siga1 - n)/(<span class="number">2</span>*siga1)</div><div class="line">        u1 = u1+lr*lu</div><div class="line">        siga1 = siga1+lr*lsiga</div><div class="line">    print(<span class="string">"u1:   %lf, siga1:   %lf"</span>%(u1, siga1))</div><div class="line"></div><div class="line">    <span class="comment"># 解析解</span></div><div class="line">    u2 = np.sum(y, axis = <span class="number">0</span>)/n</div><div class="line">    siga2 = np.sum((y-u2)**<span class="number">2</span>, axis = <span class="number">0</span>)/n</div><div class="line">    print(<span class="string">"u2:   %lf, siga2:   %lf"</span>%(u2, siga2))</div></pre></td></tr></table></figure>
<p>我们使用均值为1，标准差为2的高斯分布随机生成了1000个样本，然后分别使用梯度下降和极大似然估计法两种方式来估计参数，得到的参数如下:</p>
<p><img src="/2019/06/05/机器学习和深度学习算法理论/EM/EM算法入门/EM/pic/v2-4d49b54725f875723438ba87a94020e2_hd.png" alt="img"></p>
<p>两种方法得到的结果还是挺不错的。</p>
<p><strong>2. EM算法</strong></p>
<p><strong>极大似然算法确实可以很方便的根据样本估算模型的参数，如果样本来自一个以上的模型，我们又不知道某个样本点到底是来自某个模型的</strong>，那么此时极大似然算法就无能为力了。</p>
<p>我们依旧用高斯分布来举例子，混合高斯分布的模型如下:</p>
<p><img src="https://www.zhihu.com/equation?tex=P%28y%7C%5Ctheta%29%3D%5Csum_%7Bk%3D1%7D%5E%7BK%7D%5Calpha_%7Bk%7D%5Cphi%28y%7C%5Ctheta_%7Bk%7D%29" alt="P(EM/pic/equation-20190605135345806)=\sum_{k=1}^{K}\alpha_{k}\phi(y|\theta_{k})"></p>
<p>其中 <img src="https://www.zhihu.com/equation?tex=%5Calpha_%7Bk%7D" alt="\alpha_{k}"> 是系数， <img src="https://www.zhihu.com/equation?tex=%5Calpha_%7Bk%7D%5Cgeq0" alt="\alpha_{k}\geq0"> ，同时 <img src="/2019/06/05/机器学习和深度学习算法理论/EM/EM算法入门/EM/pic/equation-20190605135345792" alt="\sum_{k=1}^{K}a_{k}=1"> ； <img src="https://www.zhihu.com/equation?tex=%5Cphi%28y%7C%5Ctheta_%7Bk%7D%29" alt="\phi(y|\theta_{k})"> 是高斯分布密度函数， <img src="https://www.zhihu.com/equation?tex=%5Ctheta_%7Bk%7D%3D%28u_%7Bk%7D%2C+%5Csigma_%7Bk%7D%5E%7B2%7D%29" alt="\theta_{k}=(u_{k}, \sigma_{k}^{2})"> 。</p>
<p>这个时候我们需要估计的参数有</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Ctheta%3D%28%5Calpha_%7B1%7D%2C+...%2C+%5Calpha_%7Bk%7D%3B%5Ctheta_%7B1%7D%2C+.....%2C%5Ctheta_%7Bk%7D%29" alt="\theta=(EM/pic/equation?tex=%5Ctheta%3D%28%5Calpha_%7B1%7D%2C+...%2C+%5Calpha_%7Bk%7D%3B%5Ctheta_%7B1%7D%2C+.....%2C%5Ctheta_%7Bk%7D%29)"></p>
<p><strong>此时阻挡我们使用极大似然法的原因就是:我们不知道到底哪些样本点由哪个模型生成。</strong></p>
<p>现在假设我们有1000个样本点，由两个独立的高斯分布生成。我们知道其中第一类有300个，第二类有700个，那么我们就可以对两个高斯分布分别使用极大似然法估计他们的参数了。</p>
<p>但事实上我们知道的只有一堆样本点以及其可能的类别数 <img src="/2019/06/05/机器学习和深度学习算法理论/EM/EM算法入门/EM/pic/equation-20190605135345838" alt="K"> ，至于某个样本到底属于那个模型我们是不知道。此时就要到EM算法登场的时候了，<strong>EM算法的主要思想如下：</strong></p>
<ul>
<li><strong>E步:先随便设置一下各种参数<img src="https://www.zhihu.com/equation?tex=%5Ctheta%3D%28%5Calpha_%7B1%7D%2C+...%2C+%5Calpha_%7Bk%7D%3B%5Ctheta_%7B1%7D%2C+.....%2C%5Ctheta_%7Bk%7D%29" alt="\theta=(\alpha_{1}, ..., \alpha_{k};\theta_{1}, .....,\theta_{k})">，然后再算一下在当前情况下每个样本点属于哪一个模型的概率值；</strong></li>
<li><strong>M步:此时我们知道了一个样本点属于某个模型的概率，然后再次计算各个模型的参数（具体计算方法在下面）；然后返回上一步，直至算法收敛。</strong></li>
</ul>
<p>现在我们知道了EM算法的思想，那么EM算法是怎么在第二步估算出各个模型的参数呢。</p>
<p>我们先介绍一些概念：用 <img src="/2019/06/05/机器学习和深度学习算法理论/EM/EM算法入门/EM/pic/equation-20190605135345838-9714025." alt="Y"> 来表示观测随机变量的数据， <img src="/2019/06/05/机器学习和深度学习算法理论/EM/EM算法入门/EM/pic/equation-20190605135345835" alt="Z"> 表示隐随机变量的数据（比如上述混合高斯分布样本点属于某个模型的概率）， <img src="/2019/06/05/机器学习和深度学习算法理论/EM/EM算法入门/EM/pic/equation-20190605135345838-9714025." alt="Y"> 和 <img src="/2019/06/05/机器学习和深度学习算法理论/EM/EM算法入门/EM/pic/equation-20190605135345837" alt="Z "> 连在一起称为<strong>完全数据(这个我们是没法知道的)，</strong>观测数据 <img src="/2019/06/05/机器学习和深度学习算法理论/EM/EM算法入门/EM/pic/equation-20190605135345838-9714025." alt="Y"> 也被称为<strong>不完全数据(这个我们知道)，</strong> <img src="/2019/06/05/机器学习和深度学习算法理论/EM/EM算法入门/EM/pic/equation-20190605135345835" alt="Z"> 被称为<strong>隐变量(我们不知道)</strong>，假定给定观测数据 <img src="/2019/06/05/机器学习和深度学习算法理论/EM/EM算法入门/EM/pic/equation-20190605135345838-9714025." alt="Y"> ，其概率分布是 <img src="https://www.zhihu.com/equation?tex=P%28Y%7C%5Ctheta%29" alt="P(Y|\theta)"> ，其中 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="\theta"> 是需要估计的模型参数，那么不完全数据 <img src="/2019/06/05/机器学习和深度学习算法理论/EM/EM算法入门/EM/pic/equation-20190605135345838-9714025." alt="Y"> 的似然函数是 <img src="https://www.zhihu.com/equation?tex=P%28Y%7C%5Ctheta%29" alt="P(Y|\theta)"> ，对数似然函数为 <img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%29%3DlogP%28Y%7C%5Ctheta%29" alt="L(\theta)=logP(Y|\theta)"> ；<img src="/2019/06/05/机器学习和深度学习算法理论/EM/EM算法入门/EM/pic/equation-20190605135345838-9714025." alt="Y"> 和 <img src="/2019/06/05/机器学习和深度学习算法理论/EM/EM算法入门/EM/pic/equation-20190605135345837" alt="Z "> 的联合概率分布是 <img src="https://www.zhihu.com/equation?tex=P%28Y%2CZ%7C%5Ctheta%29" alt="P(Y,Z|\theta)"> ，其对数似然函数是 <img src="https://www.zhihu.com/equation?tex=logP%28Y%2CZ%7C%5Ctheta%29" alt="logP(Y,Z|\theta)"> 。</p>
<p>EM算法是通过迭代来求 <img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%29%3DlogP%28Y%7C%5Ctheta%29" alt="L(EM/pic/equation-20190605135345863)=logP(Y|\theta)">的极大似然估计，也就是在估计出来的参数条件下，模型产生给定样本点的概率最大~。因此我们要最大化下式。</p>
<p><img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%29%3DlogP%28Y%7C%5Ctheta%29%3Dlog%5Csum_%7BZ%7DP%28Y%2CZ%7C%5Ctheta%29%3Dlog%28%5Csum_%7BZ%7DP%28Y%7CZ%2C%5Ctheta%29P%28Z%7C%5Ctheta%29%29" alt="L(EM/pic/equation-20190605135345914)=logP(Y|\theta)=log\sum_{Z}P(Y,Z|\theta)=log(\sum_{Z}P(Y|Z,\theta)P(Z|\theta))"></p>
<p><strong>上式中最右边的 <img src="https://www.zhihu.com/equation?tex=P%28Z%7C%5Ctheta%29" alt="P(EM/pic/equation-20190605135345874)"> 指的的一个模型被选择的概率， <img src="https://www.zhihu.com/equation?tex=P%28Y%7CZ%2C%5Ctheta%29" alt="P(Y|Z,\theta)"> 是指我们选定了一个模型，此模型产生这个样本点的概率</strong>。</p>
<p><img src="/2019/06/05/机器学习和深度学习算法理论/EM/EM算法入门/EM/pic/v2-dcb858e6f913ffd51cf8219a57f96122_hd.jpg" alt="img"></p>
<p>上图是一维高斯混合分布，黄色的那个高斯分布均值为0，方差为1，被选择的概率为0.3；红色的那个高斯分布均值为3，方差为4，被选择的概率为0.7。</p>
<p>(下面部分参考《李航统计学习》P.159，推导更详细了一点)</p>
<p>EM是通过迭代的方法来逐步逼近近似极大化 <img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%29" alt="L(EM/pic/equation-20190605135345840)"> ，假设某一次我们得到了模型的参数估计值为 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%28i%29%7D" alt="\theta^{(i)}"> (是一个我们知道的值)，我们要求估计新的 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="\theta">可以时 <img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%29" alt="L(EM/pic/equation-20190605135345840)"> 增大，即 <img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta+%29%3EL%28%5Ctheta%5E%7B%28i%29%7D%29" alt="L(\theta )&gt;L(\theta^{(i)})"> 。我们计算两者的差</p>
<p><img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%29-L%28%5Ctheta%5E%7B%28i%29%7D%29%3Dlog%28%5Csum_%7BZ%7DP%28Y%7CZ%2C%5Ctheta%29P%28Z%7C%5Ctheta%29%29-logP%28Y%7C%5Ctheta%5E%7B%28i%29%7D%29" alt="L(\theta)-L(\theta^{(i)})=log(\sum_{Z}P(Y|Z,\theta)P(Z|\theta))-logP(Y|\theta^{(i)})"></p>
<p>利用Jenson不等式</p>
<p><img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%29-L%28%5Ctheta%5E%7Bi%7D%29%3Dlog%28%5Csum_%7BZ%7DP%28Y%7CZ%2C+%5Ctheta%5E%7B%28i%29%7D%29%5Cfrac%7BP%28Y%7CZ%2C%5Ctheta%29P%28Z%7C%5Ctheta%29%7D%7BP%28Y%7CZ%2C+%5Ctheta%5E%7B%28i%29%7D%29%7D%29-logP%28Y%7C%5Ctheta%5E%7B%28i%29%7D%29%5C%5C+%5Cgeq+%5Csum_%7BZ%7DP%28Z%7CY%2C+%5Ctheta%5E%7B%28i%29%7D%29log%5Cfrac%7BP%28Y%7CZ%2C%5Ctheta%29P%28Z%7C%5Ctheta%29%7D%7BP%28Z%7CY%2C+%5Ctheta%5E%7B%28i%29%7D%29%7D-logP%28Y%7C%5Ctheta%5E%7B%28i%29%7D%29%5C%5C+%3D%5Csum_%7BZ%7DP%28Z%7CY%2C+%5Ctheta%5E%7B%28i%29%7D%29log%5Cfrac%7BP%28Y%7CZ%2C%5Ctheta%29P%28Z%7C%5Ctheta%29%7D%7BP%28Z%7CY%2C+%5Ctheta%5E%7B%28i%29%7D%29%7D-logP%28Y%7C%5Ctheta%5E%7B%28i%29%7D%29%5Csum_%7BZ%7DP%28Z%7CY%2C+%5Ctheta%5E%7B%28i%29%7D%29%5C%5C+%3D%5Csum_%7BZ%7DP%28Z%7CY%2C+%5Ctheta%5E%7B%28i%29%7D%29log%5Cfrac%7BP%28Y%7CZ%2C%5Ctheta%29P%28Z%7C%5Ctheta%29%7D%7BP%28Z%7CY%2C+%5Ctheta%5E%7B%28i%29%7D%29P%28Y%7C%5Ctheta%5E%7B%28i%29%7D%29%7D" alt="L(EM/pic/equation-20190605135346171)-L(\theta^{i})=log(\sum_{Z}P(Y|Z, \theta^{(i)})\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Y|Z, \theta^{(i)})})-logP(Y|\theta^{(i)})\\ \geq \sum_{Z}P(Z|Y, \theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y, \theta^{(i)})}-logP(Y|\theta^{(i)})\\ =\sum_{Z}P(Z|Y, \theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y, \theta^{(i)})}-logP(Y|\theta^{(i)})\sum_{Z}P(Z|Y, \theta^{(i)})\\ =\sum_{Z}P(Z|Y, \theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y, \theta^{(i)})P(Y|\theta^{(i)})}"></p>
<p>第一步到第二步除了使用了Jenson不等式，还使用了 <img src="https://www.zhihu.com/equation?tex=P%28X%7CY%29%3D%5Cfrac%7BP%28Y%7CX%29P%28X%29%7D%7BP%28Y%29%7D" alt="P(EM/pic/equation-20190605135345893)=\frac{P(Y|X)P(X)}{P(Y)}"> ，其中的 <img src="https://www.zhihu.com/equation?tex=P%28X%29%2CP%28Y%29" alt="P(X),P(Y)"> 都被约去。</p>
<p>令</p>
<p><img src="https://www.zhihu.com/equation?tex=B%28%5Ctheta%2C%5Ctheta%5E%7B%28i%29%7D%29%3DL%28%5Ctheta%5E%7B%28i%29%7D%29%2B%5Csum_%7BZ%7DP%28Z%7CY%2C+%5Ctheta%5E%7B%28i%29%7D%29log%5Cfrac%7BP%28Y%7CZ%2C%5Ctheta%29P%28Z%7C%5Ctheta%29%7D%7BP%28Z%7CY%2C+%5Ctheta%5E%7B%28i%29%7D%29P%28Y%7C%5Ctheta%5E%7B%28i%29%7D%29%7D" alt="B(EM/pic/equation-20190605135346027)=L(\theta^{(i)})+\sum_{Z}P(Z|Y, \theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y, \theta^{(i)})P(Y|\theta^{(i)})}"></p>
<p>则有 <img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%29+%5Cgeq+B%28%5Ctheta%2C+%5Ctheta%5E%7B%28i%29%7D%29" alt="L(EM/pic/equation-20190605135345897) \geq B(\theta, \theta^{(i)})"> ，为了使 <img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%29" alt="L(EM/pic/equation-20190605135345840)"> 尽可能的大，我们应该选择 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%28i%2B1%29%7D" alt="\theta^{(i+1)}"> 使 <img src="https://www.zhihu.com/equation?tex=+B%28%5Ctheta%2C+%5Ctheta%5E%7B%28i%29%7D%29" alt=" B(\theta, \theta^{(i)})"> 达到极大值。即 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%28i%2B1%29%7D%3Darg%5Cmax_%7B%5Ctheta%7DB%28%5Ctheta%2C%5Ctheta%5E%7B%28i%29%7D%29" alt="\theta^{(i+1)}=arg\max_{\theta}B(\theta,\theta^{(i)})"></p>
<p>通过省去对 <img src="/2019/06/05/机器学习和深度学习算法理论/EM/EM算法入门/EM/pic/equation-20190605135345850" alt="\theta"> 极大化是常数的项。</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%28i%2B1%29%7D%3Darg%5Cmax_%7B%5Ctheta%7D%28L%28%5Ctheta%5E%7B%28i%29%7D%29%2B%5Csum_%7BZ%7DP%28Z%7CY%2C+%5Ctheta%5E%7B%28i%29%7D%29log%5Cfrac%7BP%28Y%7CZ%2C%5Ctheta%29P%28Z%7C%5Ctheta%29%7D%7BP%28Z%7CY%2C+%5Ctheta%5E%7B%28i%29%7D%29P%28Y%7C%5Ctheta%5E%7B%28i%29%7D%29%7D%29%5C%5C+%3Darg%5Cmax_%7B%5Ctheta%7D%28%5Csum_%7BZ%7DP%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%29logP%28Y%7CZ%2C%5Ctheta%29P%28Z%7C%5Ctheta%29%29%5C%5C+%3Darg%5Cmax_%7B%5Ctheta%7D%28%5Csum_%7BZ%7DP%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%29logP%28Y%2CZ%7C%5Ctheta%29%29" alt="\theta^{(EM/pic/equation-20190605135346102)}=arg\max_{\theta}(L(\theta^{(i)})+\sum_{Z}P(Z|Y, \theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y, \theta^{(i)})P(Y|\theta^{(i)})})\\ =arg\max_{\theta}(\sum_{Z}P(Z|Y,\theta^{(i)})logP(Y|Z,\theta)P(Z|\theta))\\ =arg\max_{\theta}(\sum_{Z}P(Z|Y,\theta^{(i)})logP(Y,Z|\theta))"></p>
<p>其中 <img src="https://www.zhihu.com/equation?tex=P%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%29" alt="P(Z|Y,\theta^{(i)})"> 指的是当我们知道模型的参数和样本的分布情况时，此时隐变量的状态。如果模型为高斯分布，那么 <img src="https://www.zhihu.com/equation?tex=P%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%29" alt="P(Z|Y,\theta^{(i)})"> 指的就是样本点属于某个模型的概率； <img src="https://www.zhihu.com/equation?tex=logP%28Y%2CZ%7C%5Ctheta%29" alt="logP(Y,Z|\theta)"> 就是我们要找到 <img src="/2019/06/05/机器学习和深度学习算法理论/EM/EM算法入门/EM/pic/equation-20190605135345850" alt="\theta"> 使得在当前 <img src="/2019/06/05/机器学习和深度学习算法理论/EM/EM算法入门/EM/pic/equation-20190605135345859-9714025." alt="Y,Z"> 的情况下，获得函数的一个极大值。</p>
<p>EM算法的流程如下:</p>
<p>(1)随机选择参数 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%280%29%7D" alt="\theta^{(EM/pic/equation-20190605135345872)}"> ,开始迭代</p>
<p>(2)E步:计算 <img src="https://www.zhihu.com/equation?tex=Q%28%5Ctheta%2C%5Ctheta%5E%7B%28i%29%7D%29%3D%5Csum_%7BZ%7DP%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%29logP%28Y%2CZ%7C%5Ctheta%29" alt="Q(EM/pic/equation-20190605135345921)=\sum_{Z}P(Z|Y,\theta^{(i)})logP(Y,Z|\theta)"></p>
<p>(3)M步:最大化 <img src="https://www.zhihu.com/equation?tex=Q%28%5Ctheta%2C%5Ctheta%5E%7B%28i%29%7D%29" alt="Q(EM/pic/equation-20190605135345901)"></p>
<p>(4)重复(2),(3)步直到收敛</p>
<p>对高斯混合模型使用EM算法估计参数，其中第一个高斯分布均值为3，方差为4，系数为0.7；第二个高斯分布均值为0，方差为1，系数为0.3。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_mix_guassian_theta</span><span class="params">()</span>:</span></div><div class="line">    <span class="comment"># u = 3, siga = 4, alpha = 0.7</span></div><div class="line">    f1 = <span class="number">3</span>+<span class="number">2</span>*np.random.randn(<span class="number">700</span>,<span class="number">1</span>)</div><div class="line">    <span class="comment"># u = 0, siga = 1, alpha = 0.3 </span></div><div class="line">    f2 = <span class="number">1</span>+np.random.rand(<span class="number">300</span>,<span class="number">1</span>)</div><div class="line">    f = np.concatenate((f1,f2),axis=<span class="number">0</span>)</div><div class="line">    n,m = f.shape</div><div class="line"></div><div class="line">    alpha = np.random.rand(<span class="number">1</span>,<span class="number">2</span>)</div><div class="line">    alpha = alpha/np.sum(alpha, axis=<span class="number">1</span>)</div><div class="line">    u = np.random.rand(<span class="number">1</span>,<span class="number">2</span>)</div><div class="line">    siga = np.random.rand(<span class="number">1</span>,<span class="number">2</span>)</div><div class="line"></div><div class="line">    <span class="comment"># EM算法求解</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</div><div class="line">        <span class="comment">#1.E步</span></div><div class="line">        gamma1 = gaussian(f, u[<span class="number">0</span>][<span class="number">0</span>], siga[<span class="number">0</span>][<span class="number">0</span>])</div><div class="line">        gamma2 = gaussian(f, u[<span class="number">0</span>][<span class="number">1</span>], siga[<span class="number">0</span>][<span class="number">1</span>])</div><div class="line">        gamma = np.concatenate((gamma1, gamma2), axis = <span class="number">1</span>)</div><div class="line">        gamma = alpha*gamma/np.sum(alpha*gamma, axis = <span class="number">1</span>, keepdims=<span class="keyword">True</span>)</div><div class="line">        <span class="comment">#2.M步</span></div><div class="line">        u = np.sum(gamma*f, axis = <span class="number">0</span>, keepdims=<span class="keyword">True</span>)/np.sum(gamma, axis = <span class="number">0</span>,keepdims=<span class="keyword">True</span>)</div><div class="line">        siga = np.sum(gamma*((f-u)**<span class="number">2</span>), axis = <span class="number">0</span>, keepdims=<span class="keyword">True</span>)/np.sum(gamma, axis = <span class="number">0</span>,keepdims=<span class="keyword">True</span>)</div><div class="line">        siga = siga**(<span class="number">1</span>/<span class="number">2</span>)</div><div class="line">        alpha = np.sum(gamma, axis = <span class="number">0</span>, keepdims=<span class="keyword">True</span>)/n</div><div class="line"></div><div class="line">    print(<span class="string">"alpha1:  %lf,alpha2:    %lf"</span>%(alpha[<span class="number">0</span>][<span class="number">0</span>], alpha[<span class="number">0</span>][<span class="number">1</span>]))</div><div class="line">    print(<span class="string">"u1:  %lf,u2:    %lf"</span>%(u[<span class="number">0</span>][<span class="number">0</span>], u[<span class="number">0</span>][<span class="number">1</span>]))</div><div class="line">    print(<span class="string">"siga1:  %lf,siga2:    %lf"</span>%(siga[<span class="number">0</span>][<span class="number">0</span>], siga[<span class="number">0</span>][<span class="number">1</span>]))</div></pre></td></tr></table></figure>
<p>算法的估计值如下:</p>
<p><img src="/2019/06/05/机器学习和深度学习算法理论/EM/EM算法入门/EM/pic/v2-c18d437f580e44cf99175d651a705650_hd.png" alt="img"></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/05/机器学习和深度学习算法理论/Transformer/Attention/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/05/机器学习和深度学习算法理论/Transformer/Attention/" class="post-title-link" itemprop="url">Unbenannt</a>
              
            
          </h2>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2019-06-05 13:50:50 / Geändert am: 13:55:00" itemprop="dateCreated datePublished" datetime="2019-06-05T13:50:50+08:00">2019-06-05</time>
            </span>
          

          
            

            
          

          

          
            
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://kexue.fm/archives/4765" target="_blank" rel="noopener">《Attention is All You Need》浅读（简介+代码）</a></p>
<p>2017年中，有两篇类似同时也是笔者非常欣赏的论文，分别是FaceBook的《Convolutional Sequence to Sequence Learning》和Google的《Attention is All You Need》，它们都算是Seq2Seq上的创新，本质上来说，都是抛弃了RNN结构来做Seq2Seq任务。</p>
<p>这篇博文中，笔者对<a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">《Attention is All You Need》</a>做一点简单的分析。当然，这两篇论文本身就比较火，因此网上已经有很多解读了（不过很多解读都是直接翻译论文的，鲜有自己的理解），因此这里尽可能多自己的文字，尽量不重复网上各位大佬已经说过的内容。</p>
<h2 id="序列编码"><a href="#序列编码" class="headerlink" title="序列编码 #"></a>序列编码<a href="https://kexue.fm/archives/4765#序列编码" target="_blank" rel="noopener"> #</a></h2><p>深度学习做NLP的方法，基本上都是先将句子分词，然后每个词转化为对应的词向量序列。这样一来，每个句子都对应的是一个矩阵<strong><em>X</em></strong>=(<strong><em>x</em></strong>1,<strong><em>x</em></strong>2,…,<strong><em>x</em></strong>t)X=(x1,x2,…,xt)，其中<strong><em>x</em></strong>ixi都代表着第ii个词的词向量（行向量），维度为dd维，故<strong><em>X</em></strong>∈ℝn×dX∈Rn×d。这样的话，问题就变成了编码这些序列了。</p>
<p>第一个基本的思路是RNN层，RNN的方案很简单，递归式进行：</p>
<p><strong><em>y</em></strong>t=f(<strong><em>y</em></strong>t−1,<strong><em>x</em></strong>t)yt=f(yt−1,xt)</p>
<p>不管是已经被广泛使用的LSTM、GRU还是最近的SRU，都并未脱离这个递归框架。RNN结构本身比较简单，也很适合序列建模，但RNN的明显缺点之一就是无法并行，因此速度较慢，这是递归的天然缺陷。另外我个人觉得</p>
<p>RNN无法很好地学习到全局的结构信息，因为它本质是一个马尔科夫决策过程。</p>
<p>第二个思路是CNN层，其实CNN的方案也是很自然的，窗口式遍历，比如尺寸为3的卷积，就是</p>
<p><strong><em>y</em></strong>t=f(<strong><em>x</em></strong>t−1,<strong><em>x</em></strong>t,<strong><em>x</em></strong>t+1)yt=f(xt−1,xt,xt+1)</p>
<p>在FaceBook的论文中，纯粹使用卷积也完成了Seq2Seq的学习，是卷积的一个精致且极致的使用案例，热衷卷积的读者必须得好好读读这篇文论。</p>
<p>CNN方便并行，而且容易捕捉到一些全局的结构信息，笔者本身是比较偏爱CNN的，在目前的工作或竞赛模型中，我都已经尽量用CNN来代替已有的RNN模型了，并形成了自己的一套使用经验</p>
<p>，这部分我们以后再谈。</p>
<p>Google的大作提供了第三个思路：<strong>纯Attention！单靠注意力就可以！</strong>RNN要逐步递归才能获得全局信息，因此一般要双向RNN才比较好；CNN事实上只能获取局部信息，是通过层叠来增大感受野；Attention的思路最为粗暴，它一步到位获取了全局信息！它的解决方案是：</p>
<p><strong><em>y</em></strong>t=f(<strong><em>x</em></strong>t,<strong><em>A</em></strong>,<strong><em>B</em></strong>)yt=f(xt,A,B)</p>
<p>其中</p>
<p><strong><em>A</em></strong>,<strong><em>B</em></strong>A,B</p>
<p>是另外一个序列（矩阵）。如果都取</p>
<p><strong><em>A</em></strong>=<strong><em>B</em></strong>=<strong><em>X</em></strong>A=B=X</p>
<p>，那么就称为Self Attention，</p>
<p>它的意思是直接将<strong><em>x</em></strong>txt与原来的每个词进行比较，最后算出<strong><em>y</em></strong>tyt</p>
<p>！</p>
<h2 id="Attention层"><a href="#Attention层" class="headerlink" title="Attention层 #"></a>Attention层<a href="https://kexue.fm/archives/4765#Attention层" target="_blank" rel="noopener"> #</a></h2>
          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/04/机器学习和深度学习算法理论/优质git资源/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/04/机器学习和深度学习算法理论/优质git资源/" class="post-title-link" itemprop="url">Unbenannt</a>
              
            
          </h2>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2019-06-04 13:00:50 / Geändert am: 13:01:34" itemprop="dateCreated datePublished" datetime="2019-06-04T13:00:50+08:00">2019-06-04</time>
            </span>
          

          
            

            
          

          

          
            
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="AI-Learning"><a href="#AI-Learning" class="headerlink" title="AI Learning"></a>AI Learning</h1><p>汇集了30多名贡献者的集体智慧，把学习机器学习的路线图、视频、电子书、学习建议等中文资料全部都整理好了。</p>
<p><a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/3/"><i class="fa fa-angle-left" aria-label="Vorherige Seite"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/32/">32</a><a class="extend next" rel="next" href="/page/5/"><i class="fa fa-angle-right" aria-label="Nächste Seite"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Schwimmer</p>
              <div class="site-description motion-element" itemprop="description">Record and Think!</div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">315</span>
                    <span class="site-state-item-name">Artikel</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">17</span>
                    <span class="site-state-item-name">Kategorien</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">55</span>
                    <span class="site-state-item-name">schlagwörter</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          

          

          
          

          
            
          
          

        </div>
      </div>

      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Schwimmer</span>

  

  
</div>


  <div class="powered-by">Erstellt mit  <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.5.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Design – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.2.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/utils.js?v=7.2.0"></script>

  <script src="/js/motion.js?v=7.2.0"></script>



  
  


  <script src="/js/affix.js?v=7.2.0"></script>

  <script src="/js/schemes/pisces.js?v=7.2.0"></script>



  

  


  <script src="/js/next-boot.js?v=7.2.0"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


  

  

  

  



  




  

  

  

  

  

  

  

  

  

  

  

  

  

  

</body>
</html>
