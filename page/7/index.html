<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">






















<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">

<link rel="stylesheet" href="/css/main.css?v=7.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.2.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.2.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '7.2.0',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="Record and Think!">
<meta property="og:type" content="website">
<meta property="og:title" content="Schwimmer&#39;s Blog">
<meta property="og:url" content="https://schwimmer.github.io/page/7/index.html">
<meta property="og:site_name" content="Schwimmer&#39;s Blog">
<meta property="og:description" content="Record and Think!">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Schwimmer&#39;s Blog">
<meta name="twitter:description" content="Record and Think!">





  
  
  <link rel="canonical" href="https://schwimmer.github.io/page/7/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Schwimmer's Blog</title>
  




  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-143240576-1"></script>
  <script>
    var host = window.location.hostname;
    if (host !== "localhost" || !true) {
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-143240576-1');
    }
  </script>









  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Schwimmer's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
        </li>
      
    </ul>
  

  
    

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/04/深度学习笔记/Pytorch训练营/D2 序列化模型，torch接口/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/04/深度学习笔记/Pytorch训练营/D2 序列化模型，torch接口/" class="post-title-link" itemprop="url">未命名</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-04 09:22:46" itemprop="dateCreated datePublished" datetime="2019-06-04T09:22:46+08:00">2019-06-04</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-08 11:57:16" itemprop="dateModified" datetime="2019-06-08T11:57:16+08:00">2019-06-08</time>
              </span>
            
          

          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/04/深度学习笔记/Pytorch训练营/D2 序列化模型，torch接口/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/04/深度学习笔记/Pytorch训练营/D2 序列化模型，torch接口/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong>任务：Pytorch官方文档（参考资料：Pytorch官方文档 5.序列化模型；6.torch接口）</strong></p>
<p><strong>任务简介</strong>：《Pytorch官方文档》</p>
<p><strong>学习时长：</strong>6/4</p>
<p><strong>详细说明</strong>：</p>
<p>本节任务资料包下载：</p>
<p>链接：<a href="https://pan.baidu.com/s/1U7KBXRv4V5rkvMjWQqULnw" target="_blank" rel="noopener">https://pan.baidu.com/s/1U7KBXRv4V5rkvMjWQqULnw</a> </p>
<p>提取码：0b3u </p>
<p>本节内容包括如何保存和载入模型，我们一般情况下载训练阶段保存在预测阶段载入，同时需要了解两种方法保存模型的差异。下面是pytorch最重要的部分，对Tensor的操作，由于本节内容较多，我们分为七个部分讲解，今天主要是熟悉torch接口中1~9页的API,能够知道如何使用，知道每个函数的意义和参数的意义。</p>
<p><strong>作业名称（详解）：</strong>保存模型的两种形式以及他们的区别？手敲今天所学API三遍</p>
<p><strong>作业提交形式</strong>：打卡提交文字或图片，不少于20字</p>
<p><strong>打卡截止时间：</strong>6/5</p>
<h1 id="1、保存模型的两种形式以及他们的区别？"><a href="#1、保存模型的两种形式以及他们的区别？" class="headerlink" title="1、保存模型的两种形式以及他们的区别？"></a>1、保存模型的两种形式以及他们的区别？</h1><p>第一种，只保存和加载模型参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.save(the_model.state_dict(), PATH)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载时</span></span><br><span class="line">the_model = TheModelClass(*args, **kwargs)</span><br><span class="line">the_model.load_state_dict(torch.load(PATH))</span><br></pre></td></tr></table></figure>
<p>第二种，保存和加载整个模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 没有state_dict</span></span><br><span class="line">torch.save(the_model, PATH)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载时</span></span><br><span class="line">the_model = torch.load(PATH)</span><br></pre></td></tr></table></figure>
<p>第二种的不足是，序列化的数据被绑定到特定的类和固定的目录结构。缺少灵活性，可能有各种break的隐患。</p>
<h1 id="2、Pytorch各种API"><a href="#2、Pytorch各种API" class="headerlink" title="2、Pytorch各种API"></a>2、Pytorch各种API</h1><h2 id="张量操作"><a href="#张量操作" class="headerlink" title="张量操作"></a>张量操作</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 若obj是一个pytorch的tensor，则返回true</span></span><br><span class="line">torch.is_tensor(obj)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 判断obj是否是storage</span></span><br><span class="line"><span class="comment"># torch.Storage是单个数据类型的连续的一维数组，每个torch.Tensor都具有相同数据类型的相应存储。他是torch.tensor底层数据结构,他除了像Tensor一样定义数值，还可以直接把文件映射到内存中进行操作</span></span><br><span class="line">torch.is_storage(obj)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置默认的tensor的数据类型</span></span><br><span class="line">torch.set_default_tensor_type(torch.DoubleTensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回input张量中的元素个数</span></span><br><span class="line">torch.numel(input) -&gt; int</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置打印选项，与numpy一致</span></span><br><span class="line">torch.set_printoptions(precision=<span class="keyword">None</span>, threshold=<span class="keyword">None</span>, edgeitems=<span class="keyword">None</span>, linewidth=<span class="keyword">None</span>, profile=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<h2 id="创建操作Creation-Ops"><a href="#创建操作Creation-Ops" class="headerlink" title="创建操作Creation Ops"></a>创建操作Creation Ops</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 返回一个二维张量，对角线都是1，其他是0</span></span><br><span class="line">torch.eye(n, m=<span class="keyword">None</span>, out=<span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用一个值填充tensor</span></span><br><span class="line">end = torch.Tensor(<span class="number">4</span>).fill_(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string"><span class="meta">&gt;&gt;&gt; </span>end</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> 10</span></span><br><span class="line"><span class="string"> 10</span></span><br><span class="line"><span class="string"> 10</span></span><br><span class="line"><span class="string"> 10</span></span><br><span class="line"><span class="string">[torch.FloatTensor of size 4]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将numpy.ndarray转为tensor，共享同一内存空间，修改一个会导致修改另一个。</span></span><br><span class="line">torch.from_numpy(ndarray) -&gt; Tensor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回一维张量，包含在start和end上均匀间隔的steps个点, out是结果张量</span></span><br><span class="line">torch.linspace(start, end, steps=100, out=None) -&gt; Tensor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回一维张量，包含在区间10^start和10^end上以对数刻度均匀间隔的steps个点。</span></span><br><span class="line">torch.logspace(start, end, steps=100, out=None) -&gt; Tensor</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.logspace(start=<span class="number">-10</span>, end=<span class="number">10</span>, steps=<span class="number">5</span>)</span><br><span class="line">        <span class="number">1.0000e-10</span></span><br><span class="line">        <span class="number">1.0000e-05</span></span><br><span class="line">        <span class="number">1.0000e+00</span></span><br><span class="line">        <span class="number">1.0000e+05</span></span><br><span class="line">        <span class="number">1.0000e+10</span></span><br><span class="line">       [torch.FloatTensor of size <span class="number">5</span>]</span><br><span class="line">      </span><br><span class="line"><span class="comment"># 返回一个全为1的张量，形状由可变参数sizes定义</span></span><br><span class="line">torch.ones(*size, out=None) -&gt; Tensor</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.ones(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="number">111</span></span><br><span class="line"><span class="number">111</span></span><br><span class="line">[torch.FloatTensor of size <span class="number">2</span>x3]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从区间(0,1)的均匀分布中抽取一组随机数</span></span><br><span class="line">torch.rand(*sizes, out=None) -&gt; Tensor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面函数也可以实现这个效果</span></span><br><span class="line">torch.Tensor(<span class="number">3</span>, <span class="number">3</span>).uniform_(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 等同于</span></span><br><span class="line">torch.rand(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从标准正态分布（均值为0，方差为1，即高斯白噪声）中抽取一组随机数。</span></span><br><span class="line">torch.randn(*size, out=None) -&gt; Tensor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 给定参数n，返回一个从0到n-1的随机整数排列</span></span><br><span class="line">torch.randperm(n, out=None) -&gt; LongTensor</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.randperm(<span class="number">4</span>)</span><br><span class="line"><span class="number">2</span> <span class="number">1</span> <span class="number">3</span> <span class="number">0</span></span><br><span class="line">[torch.LongTensor of size <span class="number">4</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回一维张量，长度为floor((end-start)/step)。包含从start到end，以step为步长的</span></span><br><span class="line">torch.arange(start, end, step=1, out=None) -&gt; Tensor</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.arange(<span class="number">1</span>, <span class="number">2.5</span>, <span class="number">0.5</span>)</span><br><span class="line"> <span class="number">1.0000</span></span><br><span class="line"> <span class="number">1.5000</span></span><br><span class="line"> <span class="number">2.0000</span></span><br><span class="line">[torch.FloatTensor of size <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 有floor((end-start)/step)+1个元素，包含在[start, end)</span></span><br><span class="line"><span class="comment"># WARNING: 建议使用函数torch.arange</span></span><br><span class="line">torch.range(start, end, step=1, out=None) -&gt; Tensor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回全为标量0的张量</span></span><br><span class="line">torch.zeros(*size, out=<span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.zeros(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="number">000</span></span><br><span class="line"><span class="number">000</span></span><br><span class="line">[torch.FloatTensor of size <span class="number">2</span>x3]</span><br></pre></td></tr></table></figure>
<h2 id="数学操作"><a href="#数学操作" class="headerlink" title="数学操作"></a>数学操作</h2><h3 id="torch-log1p"><a href="#torch-log1p" class="headerlink" title="torch.log1p"></a>torch.log1p</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.log1p(input, out=None) → Tensor</span><br></pre></td></tr></table></figure>
<p>计算 input+1input+1的自然对数 yi=log(xi+1)yi=log(xi+1)</p>
<p>注意：对值比较小的输入，此函数比<code>torch.log()</code>更准确。</p>
<p><a href="https://blog.csdn.net/qq_36523839/article/details/82422865" target="_blank" rel="noopener">https://blog.csdn.net/qq_36523839/article/details/82422865</a></p>
<p>优点：</p>
<ul>
<li><p>在数据预处理时首先可以对偏度比较大的数据用log1p函数进行转化，使其更加服从高斯分布，此步处理可能会使我们后续的分类结果得到一个更好的结果；</p>
</li>
<li><p>平滑处理很容易被忽略掉，导致模型的结果总是达不到一定的标准，同样使用逼格更高的log1p能避免复值得问题——复值指一个自变量对应多个因变量；</p>
</li>
</ul>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/04/深度学习笔记/Pytorch笔记/函数学习/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/04/深度学习笔记/Pytorch笔记/函数学习/" class="post-title-link" itemprop="url">未命名</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-04 08:30:40 / 修改时间：08:30:52" itemprop="dateCreated datePublished" datetime="2019-06-04T08:30:40+08:00">2019-06-04</time>
            </span>
          

          
            

            
          

          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/04/深度学习笔记/Pytorch笔记/函数学习/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/04/深度学习笔记/Pytorch笔记/函数学习/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>torch.ones</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/03/总结与思考/如何向客户描述产品/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/03/总结与思考/如何向客户描述产品/" class="post-title-link" itemprop="url">未命名</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-03 22:49:31 / 修改时间：22:49:48" itemprop="dateCreated datePublished" datetime="2019-06-03T22:49:31+08:00">2019-06-03</time>
            </span>
          

          
            

            
          

          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/03/总结与思考/如何向客户描述产品/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/03/总结与思考/如何向客户描述产品/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>对于一个系统，如何从业务的角度去描述它，能讲一个故事让用户听懂，让用户感兴趣，而不是从模型到模型。</p>
<p>—首先，这个系统服务于什么领域</p>
<p>​    目前这个领域存在什么问题，</p>
<p>​    有哪些因素影响到这些问题，</p>
<p>​    我们的系统能够做到什么。</p>
<p>这样了解业务逻辑。自己清楚，也能跟别人讲。</p>
<p>了解不同类型的客户，知道这些客户关注的是什么问题。吃透行业标准。总结项目中吸收到的新的知识和方法。</p>
<p>跟客户谈的时候：</p>
<p>—先说当前存在什么问题</p>
<p>​    我们有什么方法；</p>
<p>​    能支持，能推动，能帮你们，能做到……；</p>
<p>不能让客户觉得系统就是个模型的入口跟数据的出口。</p>
<p>对于项目中不确定的模糊的问题，要尽快跟客户商定，不能成为项目隐藏的风险！</p>
<p>跟客户探口风，把握需求的上限。哪些功能要做，哪些不要做，要有理有据。</p>
<p>思考客户提出需求的深层次原因，比如说领导重视。</p>
<p>沟通时，如想表达这个功能可以实现，但是因为钱不够不想做，可以说：这个系统的实现上，重点和难点在于XX。我们可以花工作量在XX功能，但是这会对最关键的内容产生影响。如果能给我们更多一些时间和预算，我们可以保质保量把这个新提出的功能做好。</p>
<p>同样的预算，要让不断提出新需求的客户认识到，做A还是做B，不能盲目接受客户需求，也不能因为合同没写或钱不够而直接拒绝。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/03/工具和环境/colab使用/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/03/工具和环境/colab使用/" class="post-title-link" itemprop="url">Colab使用</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-03 07:59:37" itemprop="dateCreated datePublished" datetime="2019-06-03T07:59:37+08:00">2019-06-03</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-07-05 16:26:42" itemprop="dateModified" datetime="2019-07-05T16:26:42+08:00">2019-07-05</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/工具和环境/" itemprop="url" rel="index"><span itemprop="name">工具和环境</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/03/工具和环境/colab使用/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/03/工具和环境/colab使用/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="挂载google-driver"><a href="#挂载google-driver" class="headerlink" title="挂载google driver"></a>挂载google driver</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from google.colab import drive</span><br><span class="line">drive.mount(&apos;/content/gdrive&apos;)</span><br></pre></td></tr></table></figure>
<p>运行之后：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&amp;redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&amp;scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&amp;response_type=code</span><br><span class="line"></span><br><span class="line">Enter your authorization code:</span><br></pre></td></tr></table></figure>
<p>登录后获得授权码，填写后</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Mounted at /content/gdrive</span><br></pre></td></tr></table></figure>
<p>桌面版工具</p>
<p><a href="https://www.insynchq.com/" target="_blank" rel="noopener">https://www.insynchq.com/</a></p>
<h1 id="跑fast-ai"><a href="#跑fast-ai" class="headerlink" title="跑fast.ai"></a>跑fast.ai</h1><p><a href="https://segmentfault.com/a/1190000018580340?utm_source=tag-newest" target="_blank" rel="noopener">https://segmentfault.com/a/1190000018580340?utm_source=tag-newest</a></p>
<p>fast.ai B站教程<a href="https://www.bilibili.com/video/av42079986?from=search&amp;seid=1436832162873131397" target="_blank" rel="noopener">https://www.bilibili.com/video/av42079986?from=search&amp;seid=1436832162873131397</a></p>
<h1 id="公众号文章-colab汇总"><a href="#公众号文章-colab汇总" class="headerlink" title="公众号文章-colab汇总"></a>公众号文章-colab汇总</h1><p><strong>3 Colab的缺点？</strong></p>
<p>12小时连续连接的限制（可以用 暂存深度学习权重的方法来解决）；</p>
<p>需要科学上网，请查看KK大佬编写的视频教材</p>
<p>“如何优雅地使用Google系产品？”</p>
<p><a href="https://www.bilibili.com/video/av43034822?from=search&amp;seid=14859222855758483769" target="_blank" rel="noopener">https://www.bilibili.com/video/av43034822?from=search&amp;seid=14859222855758483769</a></p>
<p>或者  更简单的方法  如果你有朋友已经搭建好了  你可以蹭一下^_^</p>
<p><strong>4 Colab中 如何装包？</strong></p>
<p>!pip</p>
<p><strong>5 如何导入数据？</strong></p>
<p>使用Google Drive,方法见:</p>
<p><a href="https://colab.research.google.com/notebooks/io.ipynb" target="_blank" rel="noopener">https://colab.research.google.com/notebooks/io.ipynb</a></p>
<p>用！wget直接从数据源读入；</p>
<p>用!git clone直接从Github读取整个项目文件（这个方法用于学习或者测试  特别赞）</p>
<p>提示： Google Drive只有15G  不适合放置大量数据，我的经验是：</p>
<p>数据从源文件处直接用!wget读取，但是将程序文件 和结果文件保存在网盘（特别是前面提到的 暂存的中间权重数据，以便后期恢复继续执行的文件  都要存入网盘 ，否则12小时限制到了  这些数据就没有了）</p>
<p><strong>6 能否直接在Colab执行.py文件？</strong></p>
<p>可以   用!run 或者!python3</p>
<p>但是要注意  这样执行文件如果设计到模块的调用  非常容易出错</p>
<p>另外  在Colab中调用.py文件中定义的模块  也是很方便的</p>
<p><strong>7 如果12小时  无法训练完整的结果，如何暂存结果  并在重新连接服务器后继续恢复执行？</strong></p>
<p>见</p>
<h2 id="如何在TensorFlow中保存和恢复深度学习模型训练"><a href="#如何在TensorFlow中保存和恢复深度学习模型训练" class="headerlink" title="如何在TensorFlow中保存和恢复深度学习模型训练"></a><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjExMDc0Ng==&amp;mid=2247484034&amp;idx=1&amp;sn=89c7c8dcdfa3771fc262f1ebfbffef02&amp;chksm=cef50d77f9828461bf8978918a65eb85d12edc93fb53cff03cd1c9229f6baee74c11f55808ee&amp;token=456076747&amp;lang=zh_CN&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">如何在TensorFlow中保存和恢复深度学习模型训练</a></h2><p><strong>8 能否跑比较大的数据集？</strong></p>
<p>测试过wikidata,这个数据集  对于学习阶段来说  算比较大的数据了</p>
<p>见</p>
<h2 id="在Google-Colab复现一篇EMNLP-2017会议论文中的源代码"><a href="#在Google-Colab复现一篇EMNLP-2017会议论文中的源代码" class="headerlink" title="在Google Colab复现一篇EMNLP 2017会议论文中的源代码"></a><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjExMDc0Ng==&amp;mid=2247483681&amp;idx=1&amp;sn=77ea7b2d334c1755a4d04424973a3d76&amp;chksm=cef50ed4f98287c23a8a55424e19c2c58dc7528db249bff4d2d7c89a17e289dc71285e5260a6&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">在Google Colab复现一篇EMNLP 2017会议论文中的源代码</a></h2><p>当然 这个程序无法在12小时内完成  需要结合前面提到的暂存权重参数  重新恢复训练的方式；</p>
<p>个人觉得  学习阶段的使用Google Colab完全可以满足需求</p>
<p><strong>9 Colab对于知名公开课的支持有哪些？</strong></p>
<p>见</p>
<h2 id="怎样用Google-Colab完成Stanford-CS231n的作业1"><a href="#怎样用Google-Colab完成Stanford-CS231n的作业1" class="headerlink" title="怎样用Google Colab完成Stanford CS231n的作业1"></a><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjExMDc0Ng==&amp;mid=2247483662&amp;idx=1&amp;sn=25c41e01928881cb45b101226ae47041&amp;chksm=cef50efbf98287ed7e803a57041a330c478cb01ab580e6f6cda4cc2d25e7a4b2014a89f9733f&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">怎样用Google Colab完成Stanford CS231n的作业1</a></h2><h2 id="怎样用Google-Colab完成Stanford-CS224n的作业"><a href="#怎样用Google-Colab完成Stanford-CS224n的作业" class="headerlink" title="怎样用Google Colab完成Stanford CS224n的作业"></a><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjExMDc0Ng==&amp;mid=2247483674&amp;idx=1&amp;sn=dbc0aded4b20ee5683b45d1b8e56ba0d&amp;chksm=cef50eeff98287f9aba8d03b1bd2dbfd6a20975cb9597c132722f7dcaf516ca8131954ef30d3&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">怎样用Google Colab完成Stanford CS224n的作业</a></h2><h2 id="吴恩达老师又又又出新课了，这次是TensorFlow"><a href="#吴恩达老师又又又出新课了，这次是TensorFlow" class="headerlink" title="吴恩达老师又又又出新课了，这次是TensorFlow"></a><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjExMDc0Ng==&amp;mid=2247483856&amp;idx=1&amp;sn=15f7f8a9ba6b94908f1a82b36b3ae103&amp;chksm=cef50e25f9828733bd6235808a255d97c5e1d5fcb8e5b2481fe33d51e58103859b13df27fae9&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">吴恩达老师又又又出新课了，这次是TensorFlow</a></h2><h2 id="如何从0开始-在Google-Colab上训练一个图片分类的卷积神经网络"><a href="#如何从0开始-在Google-Colab上训练一个图片分类的卷积神经网络" class="headerlink" title="如何从0开始 在Google Colab上训练一个图片分类的卷积神经网络"></a><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjExMDc0Ng==&amp;mid=2247483868&amp;idx=1&amp;sn=cf9cccdfccf4d623604e9890c966905d&amp;chksm=cef50e29f982873fe4e36c5361eaddc6f7d4bb1c730a3f9e0724b3e1ffdf4282cd5c8d7a9c01&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">如何从0开始 在Google Colab上训练一个图片分类的卷积神经网络</a></h2><h2 id="TensorFlow图片分类实战（吴恩达TensorFlow系列新课第一课完结篇）"><a href="#TensorFlow图片分类实战（吴恩达TensorFlow系列新课第一课完结篇）" class="headerlink" title="TensorFlow图片分类实战（吴恩达TensorFlow系列新课第一课完结篇）"></a><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjExMDc0Ng==&amp;mid=2247483878&amp;idx=1&amp;sn=eeef08cd9caefa1787978e62f8b4b55c&amp;chksm=cef50e13f9828705c03fa905e96a236ad6752ecbf731f0d125266547e12817085d8104d2b63a&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">TensorFlow图片分类实战（吴恩达TensorFlow系列新课第一课完结篇）</a></h2><p><strong>10 Colab对于知名教材的支持？</strong></p>
<p>见</p>
<h2 id="在Google-Colab中测试李航《统计学习方法》Python代码"><a href="#在Google-Colab中测试李航《统计学习方法》Python代码" class="headerlink" title="在Google Colab中测试李航《统计学习方法》Python代码"></a><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjExMDc0Ng==&amp;mid=2247483692&amp;idx=1&amp;sn=ccee84ccc7d0882279f82cf7d395685c&amp;chksm=cef50ed9f98287cff587ab1e18d72a6c792ebed5a8c6dcaab0532b5cbf66d8428772d0a392c5&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">在Google Colab中测试李航《统计学习方法》Python代码</a></h2><h2 id="怎样用Google-Colab完成周志华西瓜书的习题"><a href="#怎样用Google-Colab完成周志华西瓜书的习题" class="headerlink" title="怎样用Google Colab完成周志华西瓜书的习题"></a><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjExMDc0Ng==&amp;mid=2247483712&amp;idx=1&amp;sn=ad199c99f8b205f08a260ec4c6fe5647&amp;chksm=cef50eb5f98287a32a60594021bb8f90c043b8f1aaf0d7e421e686ce6d5fe7f657f90d403f4c&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">怎样用Google Colab完成周志华西瓜书的习题</a></h2><p><strong>11 Colab也可以跑竞赛的数据？</strong></p>
<p>见</p>
<h2 id="如何通过参加-数据分析竞赛-来提高算法水平（以”达观杯”-文本挖掘比赛为例）"><a href="#如何通过参加-数据分析竞赛-来提高算法水平（以”达观杯”-文本挖掘比赛为例）" class="headerlink" title="如何通过参加 数据分析竞赛 来提高算法水平（以”达观杯” 文本挖掘比赛为例）"></a><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjExMDc0Ng==&amp;mid=2247483723&amp;idx=1&amp;sn=a63f45bd3715f99b5451e3c2f38a0c67&amp;chksm=cef50ebef98287a8fe6ee1565f728ea465fccf6bb69e72df0921a0308dd667a76ba77ec1e931&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">如何通过参加 数据分析竞赛 来提高算法水平（以”达观杯” 文本挖掘比赛为例）</a></h2><p><strong>12 Colab支持TensorFlow2.0吗？</strong></p>
<p>见</p>
<h2 id="用Google-Colab尝鲜测试TensorFlow-2-0"><a href="#用Google-Colab尝鲜测试TensorFlow-2-0" class="headerlink" title="用Google Colab尝鲜测试TensorFlow 2.0"></a><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjExMDc0Ng==&amp;mid=2247483767&amp;idx=1&amp;sn=59cf407f03eb0306d1570f13427c5367&amp;chksm=cef50e82f98287943717239c372369ea738704381ecc43f05083a853ddf40d9f52db953130a8&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">用Google Colab尝鲜测试TensorFlow 2.0</a></h2><p><strong>13 也可以用Colab进行Python的基本学习吗？</strong></p>
<p>可以  如用Colab学习Matploylib库  见</p>
<h2 id="用matplotlib可视化初步"><a href="#用matplotlib可视化初步" class="headerlink" title="用matplotlib可视化初步"></a><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjExMDc0Ng==&amp;mid=2247483776&amp;idx=1&amp;sn=ad57f82d4e01589974a56a8af9f35fca&amp;chksm=cef50e75f9828763ab8f4acb67c9e2942f973dc3ab1d44a6bb620f6242d5fed487797db4b62f&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">用matplotlib可视化初步</a></h2><p><strong>14 Colab支持GAN吗？</strong></p>
<p>见</p>
<h2 id="如何在Google-Colab运行你的第一个GAN模型"><a href="#如何在Google-Colab运行你的第一个GAN模型" class="headerlink" title="如何在Google Colab运行你的第一个GAN模型"></a><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjExMDc0Ng==&amp;mid=2247483910&amp;idx=1&amp;sn=291de5debbc97b9a6635da6af726ad9c&amp;chksm=cef50df3f98284e5e4011349e3d642d1a243be0d013cf7f6b9a180cfa3a0e3bf5c4bea564019&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">如何在Google Colab运行你的第一个GAN模型</a></h2><p>这里只测试了最简单的GAN模型，Colab也支持更加复杂的GAN模型训练，如</p>
<p><a href="https://github.com/shaoanlu/faceswap-GAN" target="_blank" rel="noopener">https://github.com/shaoanlu/faceswap-GAN</a></p>
<p><strong>15 如何搜索支持Colab的源代码？</strong></p>
<p>Github,Seedbank项目</p>
<p>jupyter notebook格式的均支持</p>
<p><strong>上传并使用数据文件</strong></p>
<p>我们一般都需要在 Colab 笔记本中使用数据，对吧？你可以使用 wget 之类的工具从网络上获取数据，但是如果你有一些本地文件，想上传到你的谷歌硬盘中的 Colab 环境里并使用它们，该怎么做呢？</p>
<p>很简单，只需 3 步即可实现！</p>
<p>首先使用以下命令调用笔记本中的文件选择器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> files</span><br><span class="line">uploaded = files.upload()</span><br></pre></td></tr></table></figure>
<p>运行之后，我们就会发现单元 cell 下出现了“选择文件”按钮：</p>
<p><img src="//schwimmer.github.io/2019/06/03/工具和环境/colab使用/pic/v2-740d65c0b367aa4ecb9c69ed13f70b04_hd.jpg" alt="img"></p>
<p>这样就可以直接选择你想上传的文件啦！</p>
<p>选择文件后，使用以下迭代方法上传文件以查找其键名，命令如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> fn <span class="keyword">in</span> uploaded.keys():</span><br><span class="line"> print(<span class="string">'User uploaded file "&#123;name&#125;" with length &#123;length&#125; bytes'</span>.format(name=fn, length=len(uploaded[fn])))</span><br></pre></td></tr></table></figure>
<p>例如待上传的是 iris.csv 文件，若运行没有问题的话，应该出现类似下面的提示语句：</p>
<blockquote>
<p>User uploaded file “iris.csv” with length 3716 bytes</p>
</blockquote>
<p>最后，就使用以下命令将文件的内容加载到 Pandas 的 DataFrame 中了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> io</span><br><span class="line">df = pd.read_csv(io.StringIO(uploaded[<span class="string">'iris.csv'</span>].decode(<span class="string">'utf-8'</span>)))</span><br><span class="line">print(df)</span><br></pre></td></tr></table></figure>
          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/02/深度学习笔记/Pytorch笔记/Autograd/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/02/深度学习笔记/Pytorch笔记/Autograd/" class="post-title-link" itemprop="url">未命名</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-02 22:10:58 / 修改时间：22:20:53" itemprop="dateCreated datePublished" datetime="2019-06-02T22:10:58+08:00">2019-06-02</time>
            </span>
          

          
            

            
          

          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/02/深度学习笔记/Pytorch笔记/Autograd/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/02/深度学习笔记/Pytorch笔记/Autograd/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://blog.csdn.net/g11d111/article/details/83035270" target="_blank" rel="noopener">PyTorch学习笔记(12)——PyTorch中的Autograd机制介绍</a></p>
<p><a href="https://blog.csdn.net/MiaoB226/article/details/88934561" target="_blank" rel="noopener">Pytorch从入门到放弃（5）——取消测试与验证阶段的梯度</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/02/深度学习笔记/Pytorch莫烦/建造第一个神经网络/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/02/深度学习笔记/Pytorch莫烦/建造第一个神经网络/" class="post-title-link" itemprop="url">未命名</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-02 16:17:50" itemprop="dateCreated datePublished" datetime="2019-06-02T16:17:50+08:00">2019-06-02</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-04 08:25:49" itemprop="dateModified" datetime="2019-06-04T08:25:49+08:00">2019-06-04</time>
              </span>
            
          

          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/02/深度学习笔记/Pytorch莫烦/建造第一个神经网络/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/02/深度学习笔记/Pytorch莫烦/建造第一个神经网络/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://morvanzhou.github.io/tutorials/machine-learning/torch/3-01-regression/" target="_blank" rel="noopener">https://morvanzhou.github.io/tutorials/machine-learning/torch/3-01-regression/</a></p>
<h1 id="关系拟合-回归"><a href="#关系拟合-回归" class="headerlink" title="关系拟合 (回归)"></a>关系拟合 (回归)</h1><h2 id="要点"><a href="#要点" class="headerlink" title="要点"></a>要点</h2><p>我会这次会来见证神经网络是如何通过简单的形式将一群数据用一条线条来表示. 或者说, 是如何在数据当中找到他们的关系, 然后用神经网络模型来建立一个可以代表他们关系的线条.</p>
<h2 id="建立数据集"><a href="#建立数据集" class="headerlink" title="建立数据集"></a>建立数据集</h2><p>我们创建一些假数据来模拟真实的情况. 比如一个一元二次函数: <code>y = a * x^2 + b</code>, 我们给 <code>y</code> 数据加上一点噪声来更加真实的展示它.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">x = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1)  # x data (tensor), shape=(100, 1)</span><br><span class="line">y = x.pow(2) + 0.2*torch.rand(x.size())                 # noisy y data (tensor), shape=(100, 1)</span><br><span class="line"></span><br><span class="line"># 画图</span><br><span class="line">plt.scatter(x.data.numpy(), y.data.numpy())</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="建立神经网络"><a href="#建立神经网络" class="headerlink" title="建立神经网络"></a>建立神经网络</h2><p>建立一个神经网络我们可以直接运用 torch 中的体系. 先定义所有的层属性(<code>__init__()</code>), 然后再一层层搭建(<code>forward(x)</code>)层于层的关系链接. 建立关系的时候, 我们会用到激励函数, 如果还不清楚激励函数用途的同学, 这里有非常好的<a href="https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/3-04-activation-function/" target="_blank" rel="noopener">一篇动画教程</a>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F     <span class="comment"># 激励函数都在这</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(torch.nn.Module)</span>:</span>  <span class="comment"># 继承 torch 的 Module</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_feature, n_hidden, n_output)</span>:</span></span><br><span class="line">        super(Net, self).__init__()     <span class="comment"># 继承 __init__ 功能</span></span><br><span class="line">        <span class="comment"># 定义每层用什么样的形式</span></span><br><span class="line">        self.hidden = torch.nn.Linear(n_feature, n_hidden)   <span class="comment"># 隐藏层线性输出</span></span><br><span class="line">        self.predict = torch.nn.Linear(n_hidden, n_output)   <span class="comment"># 输出层线性输出</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span>   <span class="comment"># 这同时也是 Module 中的 forward 功能</span></span><br><span class="line">        <span class="comment"># 正向传播输入值, 神经网络分析出输出值</span></span><br><span class="line">        x = F.relu(self.hidden(x))      <span class="comment"># 激励函数(隐藏层的线性值)</span></span><br><span class="line">        x = self.predict(x)             <span class="comment"># 输出值</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net = Net(n_feature=<span class="number">1</span>, n_hidden=<span class="number">10</span>, n_output=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">print(net)  <span class="comment"># net 的结构</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Net (</span></span><br><span class="line"><span class="string">  (hidden): Linear (1 -&gt; 10)</span></span><br><span class="line"><span class="string">  (predict): Linear (10 -&gt; 1)</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<h2 id="训练网络"><a href="#训练网络" class="headerlink" title="训练网络"></a>训练网络</h2><p>训练的步骤很简单, 如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># optimizer 是训练的工具</span></span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.2</span>)  <span class="comment"># 传入 net 的所有参数, 学习率</span></span><br><span class="line">loss_func = torch.nn.MSELoss()      <span class="comment"># 预测值和真实值的误差计算公式 (均方差)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    prediction = net(x)     <span class="comment"># 喂给 net 训练数据 x, 输出预测值</span></span><br><span class="line"></span><br><span class="line">    loss = loss_func(prediction, y)     <span class="comment"># 计算两者的误差</span></span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()   <span class="comment"># 清空上一步的残余更新参数值</span></span><br><span class="line">    loss.backward()         <span class="comment"># 误差反向传播, 计算参数更新值</span></span><br><span class="line">    optimizer.step()        <span class="comment"># 将参数更新值施加到 net 的 parameters 上</span></span><br></pre></td></tr></table></figure>
<h2 id="可视化训练过程"><a href="#可视化训练过程" class="headerlink" title="可视化训练过程"></a>可视化训练过程</h2><p>为了可视化整个训练的过程, 更好的理解是如何训练, 我们如下操作:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.ion()   <span class="comment"># 画图</span></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">200</span>):</span><br><span class="line">    prediction = net(x)     <span class="comment"># 喂给 net 训练数据 x, 输出预测值</span></span><br><span class="line">    loss = loss_func(prediction, y)     <span class="comment"># 计算两者的误差</span></span><br><span class="line">    optimizer.zero_grad()   <span class="comment"># 清空上一步的残余更新参数值</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 接着上面来</span></span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># plot and show learning process</span></span><br><span class="line">        plt.cla()</span><br><span class="line">        plt.scatter(x.data.numpy(), y.data.numpy())</span><br><span class="line">        plt.plot(x.data.numpy(), prediction.data.numpy(), <span class="string">'r-'</span>, lw=<span class="number">5</span>)</span><br><span class="line">        plt.text(<span class="number">0.5</span>, <span class="number">0</span>, <span class="string">'Loss=%.4f'</span> % loss.data.numpy(), fontdict=&#123;<span class="string">'size'</span>: <span class="number">20</span>, <span class="string">'color'</span>:  <span class="string">'red'</span>&#125;)</span><br><span class="line">        plt.pause(<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<h1 id="区分类型"><a href="#区分类型" class="headerlink" title="区分类型"></a>区分类型</h1><h2 id="要点-1"><a href="#要点-1" class="headerlink" title="要点"></a>要点</h2><p>这次我们也是用最简单的途径来看看神经网络是怎么进行事物的分类.</p>
<p><a href="https://morvanzhou.github.io/static/results/torch/1-1-3.gif" target="_blank" rel="noopener"><img src="https://morvanzhou.github.io/static/results/torch/1-1-3.gif" alt="区分类型 (建造第一个神经网络/pic/1-1-3.gif)"></a></p>
<h2 id="建立数据集-1"><a href="#建立数据集-1" class="headerlink" title="建立数据集"></a>建立数据集</h2><p>我们创建一些假数据来模拟真实的情况. 比如两个二次分布的数据, 不过他们的均值都不一样.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># torch.manual_seed(1)    # reproducible</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># make fake data</span></span><br><span class="line">n_data = torch.ones(<span class="number">100</span>, <span class="number">2</span>)</span><br><span class="line">x0 = torch.normal(<span class="number">2</span>*n_data, <span class="number">1</span>)      <span class="comment"># class0 x data (tensor), shape=(100, 2)</span></span><br><span class="line">y0 = torch.zeros(<span class="number">100</span>)               <span class="comment"># class0 y data (tensor), shape=(100, 1)</span></span><br><span class="line">x1 = torch.normal(<span class="number">-2</span>*n_data, <span class="number">1</span>)     <span class="comment"># class1 x data (tensor), shape=(100, 2)</span></span><br><span class="line">y1 = torch.ones(<span class="number">100</span>)                <span class="comment"># class1 y data (tensor), shape=(100, 1)</span></span><br><span class="line">x = torch.cat((x0, x1), <span class="number">0</span>).type(torch.FloatTensor)  <span class="comment"># shape (200, 2) FloatTensor = 32-bit floating</span></span><br><span class="line">y = torch.cat((y0, y1), ).type(torch.LongTensor)    <span class="comment"># shape (200,) LongTensor = 64-bit integer</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The code below is deprecated in Pytorch 0.4. Now, autograd directly supports tensors</span></span><br><span class="line"><span class="comment"># x, y = Variable(x), Variable(y)</span></span><br><span class="line"></span><br><span class="line">plt.scatter(x.data.numpy()[:, <span class="number">0</span>], x.data.numpy()[:, <span class="number">1</span>], c=y.data.numpy(), s=<span class="number">100</span>, lw=<span class="number">0</span>, cmap=<span class="string">'RdYlGn'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="建立神经网络-1"><a href="#建立神经网络-1" class="headerlink" title="建立神经网络"></a>建立神经网络</h2><p>建立一个神经网络我们可以直接运用 torch 中的体系. 先定义所有的层属性(<code>__init__()</code>), 然后再一层层搭建(<code>forward(x)</code>)层于层的关系链接. 这个和我们在前面 regression 的时候的神经网络基本没差. 建立关系的时候, 我们会用到激励函数, 如果还不清楚激励函数用途的同学, 这里有非常好的<a href="https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/3-04-activation-function/" target="_blank" rel="noopener">一篇动画教程</a>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F     <span class="comment"># 激励函数都在这</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(torch.nn.Module)</span>:</span>     <span class="comment"># 继承 torch 的 Module</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_feature, n_hidden, n_output)</span>:</span></span><br><span class="line">        super(Net, self).__init__()     <span class="comment"># 继承 __init__ 功能</span></span><br><span class="line">        self.hidden = torch.nn.Linear(n_feature, n_hidden)   <span class="comment"># 隐藏层线性输出</span></span><br><span class="line">        self.out = torch.nn.Linear(n_hidden, n_output)       <span class="comment"># 输出层线性输出</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># 正向传播输入值, 神经网络分析出输出值</span></span><br><span class="line">        x = F.relu(self.hidden(x))      <span class="comment"># 激励函数(隐藏层的线性值)</span></span><br><span class="line">        x = self.out(x)                 <span class="comment"># 输出值, 但是这个不是预测值, 预测值还需要再另外计算</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net = Net(n_feature=<span class="number">2</span>, n_hidden=<span class="number">10</span>, n_output=<span class="number">2</span>) <span class="comment"># 几个类别就几个 output</span></span><br><span class="line"></span><br><span class="line">print(net)  <span class="comment"># net 的结构</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Net (</span></span><br><span class="line"><span class="string">  (hidden): Linear (2 -&gt; 10)</span></span><br><span class="line"><span class="string">  (out): Linear (10 -&gt; 2)</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<h2 id="训练网络-1"><a href="#训练网络-1" class="headerlink" title="训练网络"></a>训练网络</h2><p>训练的步骤很简单, 如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># optimizer 是训练的工具</span></span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.02</span>)  <span class="comment"># 传入 net 的所有参数, 学习率</span></span><br><span class="line"><span class="comment"># 算误差的时候, 注意真实值!不是! one-hot 形式的, 而是1D Tensor, (batch,)</span></span><br><span class="line"><span class="comment"># 但是预测值是2D tensor (batch, n_classes)</span></span><br><span class="line">loss_func = torch.nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    out = net(x)     <span class="comment"># 喂给 net 训练数据 x, 输出分析值</span></span><br><span class="line"></span><br><span class="line">    loss = loss_func(out, y)     <span class="comment"># 计算两者的误差</span></span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()   <span class="comment"># 清空上一步的残余更新参数值</span></span><br><span class="line">    loss.backward()         <span class="comment"># 误差反向传播, 计算参数更新值</span></span><br><span class="line">    optimizer.step()        <span class="comment"># 将参数更新值施加到 net 的 parameters 上</span></span><br></pre></td></tr></table></figure>
<h2 id="可视化训练过程-1"><a href="#可视化训练过程-1" class="headerlink" title="可视化训练过程"></a>可视化训练过程</h2><p>为了可视化整个训练的过程, 更好的理解是如何训练, 我们如下操作:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">plt.ion()   # 画图</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">for t in range(100):</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    # 接着上面来</span><br><span class="line">    if t % 2 == 0:</span><br><span class="line">        plt.cla()</span><br><span class="line">        # 过了一道 softmax 的激励函数后的最大概率才是预测值</span><br><span class="line">        prediction = torch.max(F.softmax(out), 1)[1]</span><br><span class="line">        pred_y = prediction.data.numpy().squeeze()</span><br><span class="line">        target_y = y.data.numpy()</span><br><span class="line">        plt.scatter(x.data.numpy()[:, 0], x.data.numpy()[:, 1], c=pred_y, s=100, lw=0, cmap=&apos;RdYlGn&apos;)</span><br><span class="line">        accuracy = sum(pred_y == target_y)/200.  # 预测中有多少和真实值一样</span><br><span class="line">        plt.text(1.5, -4, &apos;Accuracy=%.2f&apos; % accuracy, fontdict=&#123;&apos;size&apos;: 20, &apos;color&apos;:  &apos;red&apos;&#125;)</span><br><span class="line">        plt.pause(0.1)</span><br><span class="line"></span><br><span class="line">plt.ioff()  # 停止画图</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><a href="https://morvanzhou.github.io/static/results/torch/3-2-1.png" target="_blank" rel="noopener"><img src="https://morvanzhou.github.io/static/results/torch/3-2-1.png" alt="区分类型 (建造第一个神经网络/pic/3-2-1.png)"></a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/02/机器学习/机器学习问题答疑/Adaboost/" class="post-title-link" itemprop="url">未命名</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-02 15:54:20 / 修改时间：16:07:06" itemprop="dateCreated datePublished" datetime="2019-06-02T15:54:20+08:00">2019-06-02</time>
            </span>
          

          
            

            
          

          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/02/机器学习/机器学习问题答疑/Adaboost/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/02/机器学习/机器学习问题答疑/Adaboost/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Adaboost"><a href="#Adaboost" class="headerlink" title="Adaboost"></a>Adaboost</h1><ol>
<li>想问下机器学习实战这里是不是写错了，应该是大于1的是吗？</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:dIJQW2rLt2pya92qYnGwGO7619M=.jpeg" alt="img"></p>
<ul>
<li>你看那个参数threshIneq，它取值lt或gt。lt表示小于阈值的为-1，gt表示大于阈值的为-1</li>
</ul>
<ol>
<li>绘制roc曲线的代码逻辑都能看的懂，该函数接收的是训练样本被分类的概率值，将这些概率值按由小到大的顺序依次输出，如果是正例，则曲线向下移动，反之向左移动。老师能透彻的解释下，为什么这样绘制就是roc曲线吗？roc不是在不同的阈值下真阳率和假阳率的对应关系吗？</li>
</ol>
<ul>
<li>先要了解正阳率和假阳率的定义。正阳率是预测的正类中确实是正类占所有真实正类的概率，假阳率是预测的正类中确实是负类占所有真实负类的概率。曲线横坐标是假阳率，纵坐标是真阳率。注意，书中的代码跟一般的的做法不一样，一般的做法是将概率从大到小排列，曲线从左下角开始画，原理是一样的。</li>
</ul>
<ol>
<li>老师您好，请问在做回归预测的时候，多项式构造选取多少个属性合适?随机森林选取的属性  重要性小于多少的可以摒弃，一般取前几?</li>
</ol>
<ul>
<li>这没有固定的答案，看具体问题能有几个较好的特征，剔除不必要特征。可以看看特征选择算法。随机森林每棵树可以随机选择一部分特征进行训练。</li>
</ul>
<h1 id="kMeans"><a href="#kMeans" class="headerlink" title="kMeans"></a>kMeans</h1><ol>
<li>如何理解kmeans++算法在解决标准kmeans算法执行时初始质心选择的的作用？该算法的第3步该如何理解，1.先从数据库随机挑个随机点当“种子点”2.对于每个点，都计算其和最近的一个“种子点”的距离D(x)并保存在一个数组里，然后把这些距离加起来得到Sum(D(x))。3.然后，再取一个随机值，用权重的方式来取计算下一个“种子点”。这个算法的实现是，先取一个能落在Sum(D(x))中的随机值Random，然后用Random -= D(x)，直到其&lt;=0，此时的点就是下一个“种子点”。4.重复2和3直到k个聚类中心被选出来5.利用这k个初始的聚类中心运行标准的k-means算法</li>
</ol>
<ul>
<li>kmeans选择初始化质心有不同的方法，可以选择批次距离尽可能远的K个点，也可以选用层次聚类算法BIRCH和ROCK或者Canopy。具体的还要看具体文献或出处。</li>
</ul>
<h1 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h1><ol>
<li>请问SVM中的惩罚因子C的作用是什么，如何调节误分类点的个数和距离二者的关系，为什么当C无穷大时，软间隔就等价于硬间隔</li>
</ol>
<ul>
<li>SVM中允许有分类错误的点时引入了参数C，C表示尽可能选择宽边界和尽可能不要犯错两者之间的权衡。C越大表示希望得到更少的分类错误，即不惜选择窄边界也要尽可能把更多点正确分类；C越小表示希望得到更宽的边界，即不惜增加错误点个数也要选择更宽的分类边界。这种处理问题的思路有点类似于正则化。</li>
</ul>
<p>当C无穷大时候，可以想象选择极窄的边界让所有的点都分类正确，也就等价于硬间隔了。也可以从另一方面来看，硬间隔满足 0≤αn，软间隔满足 0≤αn≤C，当C无穷大时，0≤αn≤C等价于0≤αn。</p>
<ol>
<li>想问一下关于alpha的选择问题，首先，条件a两个alpha必须要在间隔边界之外，b这两个alpha还没有进行过区间化处理或者不在边界上。我的问题是，1)区间话处理是什么意思？2)程序中alpha(i)的选择是在边界之外，但alpha(j)的选择确实非i的任意一个，这能保证alpha(j)满足两个条件吗？其次，随着程序的运行alpha(i)也会选到已经更新的alpha(j),这是不是和未区间化的条件向矛盾</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:qSLlCh12DkeIqFnKbUf0J0AQo20=.jpeg" alt="img"></p>
<ul>
<li>书中的做法是先找到违背条件的alpha1，然后alpha2取其误差与alpha1相差较大的那个，这样减少迭代次数。使用最优化解出alpha2后，根据其与alpha1的关系求得其值。</li>
</ul>
<ol>
<li>针对逻辑回归的这个推导看不明白，您能给讲下吗？数学哪方面知识是讲导数参与运算的，您能大体说下或有相关资料吗</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:onJ5Fj7cX0QhiHRNaqzJiL0e0VI=.jpeg" alt="img"></p>
<ul>
<li>逻辑回归的偏导数计算是比较简单的，用到了指数偏导计算。高数书里有基本的偏导数计算方法。</li>
</ul>
<ol>
<li>这边有个项目需要用GBDT去做训练，如果训练样本数大概是60万左右，正负样本比例是1:5左右 很不均衡，如果只是通过下采样使之达到均衡的话，觉得训练样本量20万，有点少，这种情况，一般按照什么方式去处理比较好？</li>
</ol>
<ul>
<li>可以试试上采样构造正样本，或者即使正负样本比例不同，采用不同的类别权重，在损失函数中使用。</li>
</ul>
<ol>
<li>SVM中参数C的调教:C和松弛向量的乘积加入到了目标函数中，如果要使目标函数有最小值，就是要C和松弛向量的乘积越小，那是不是可以看成:C越大，松弛向量越小，那1-ξ就越大，间距就越大，容错就越大。为啥不能看成这样啊？我看答案是C小点儿越好。。。。</li>
</ol>
<ul>
<li>C是权衡犯错率和宽间隔的，C越大表示宁愿间隔小也要分类正确，对错误的容忍度小。小的C值争取获得更宽的边界。</li>
</ul>
<ol>
<li>请问有没有用svm做多分类的代码可以分享给我的</li>
</ol>
<ul>
<li>可以使用OVO方式，没有手写代码，可以使用libSVM库</li>
</ul>
<ol>
<li>请问这个最小值应该在边界上达到是为什么？</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/Fni_QvWIJuY6ZYJNF2p6ySkNc_ne.jpg!thumbnail" alt="img"></p>
<ul>
<li>举个简单的例子，y=(x-1)^2，x在1处取得最小值。但是如果x的取值范围是[2,3]，那么只能在x的取值边界上得到最小值。</li>
</ul>
<ol>
<li>svm里 1 优化目标  maxmin  为什么要换成minmax  是为了引入基变换？方便计算？2  xi的位置是即可以是原始的特征 也可以是转换后由基函数表示吗 ？将不可分映射到高维可分？3  合约页损失函数是岭回归的思想吗</li>
</ol>
<ul>
<li><p>1.我们习惯解决最小化的优化问题，便于使用优化方法求解；</p>
</li>
<li><p>\2. 不太明白意思，xi可以是原始特征，但引入kernel后，可以认为是映射到高维，得到非线性分类面；</p>
</li>
<li><p>\3. 你说的应该是合页损失函数，它与岭回归不同，岭回归是平方误差函数加上了L2正则化项目，用于回归问题而不是分类问题。网上搜一下二者的区别很容易查到。</p>
</li>
</ul>
<ol>
<li>老师，这一步是你的博客上LinearSVM的代码，没看懂，感觉代码的逻辑说不通，并没有提取出每一个测试样本正例的分数</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:n0uDmZaMQkQv_Xa-BLLIUvZbu7s=.jpeg" alt="img"></p>
<ul>
<li>这篇文章来源于之前看cs231n写的笔记。它的LinearSVM使用的Hinge Loss，使用梯度下降算法计算的，比较简单。这里还是以理解传统的SVM为主。</li>
</ul>
<ol>
<li>统计学习超平面是什么？</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:mzqBn8A3XLrH5YVb_BEwHum7Hhg=.jpeg" alt="img"></p>
<ul>
<li>这里的超平面可以通俗理解为分类问题中的分类面，例如二维平面中的分类线，三维空间的分类面，对应到n维，就叫超平面。</li>
</ul>
<ol>
<li>老师，SVM中找到不满足KKT条件公式如图所示。但在实际代码中实现如下。这是为什么。 if ((self.y_train[i] <em> Ei &lt; -self.toler) and (self.alpha[i] &lt; self.C) or   (self.y_train[i] </em> Ei &gt; self.toler) and (self.alpha[i] &gt; 0)):</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:gh_zjTg6Tfb60fYRgKbHYWNrasE=.jpeg" alt="img"></p>
<ul>
<li>你把代码中Ei=y-y_train代入，移位一下就会发现与理论是一样的了。我在之前SVM直播答疑的视频里讲过，你可以去看看。<a href="https://study.163.com/course/courseLearn.htm?courseId=1006284002&amp;share=2&amp;shareId=400000000445063#/learn/video?lessonId=1053809730&amp;courseId=1006284002" target="_blank" rel="noopener">《机器学习实战》书训练营直播间 - 网易云课堂</a></li>
</ul>
<ol>
<li>老师，为什么李航的统计学128页在SVM中提出SMO算法在每个子问题中选择两个变量优化，其中至少一个变量是违反KKT条件的。</li>
</ol>
<ul>
<li><p>选择两个变量的原因是所有的alpha满足下面图片所示的这个条件，为了保证等式成立，必须同时优化两个alpha。违反KKT条件是因为先找到违背KKT条件的点，让其满足条件。若所有的点都满足KKT条件，则优化结束了。</p>
</li>
<li><p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:vKm1Y1htAp00LjzwNeWZMsEA6K4=.jpeg" alt="img"></p>
</li>
<li><p>老师，为什么因为先找到违背KKT条件的点，让其满足条件。这一步不理解</p>
</li>
<li><p>优化的目的是让所有点都满足KKT条件，因为满足KKT条件了，就得到最优化了。使用SMO是把整个优化问题切分成一个个小的优化问题，每次对两个alpha进行优化，让其满足KKT条件。</p>
</li>
<li><p>那找到违背KKT条件的点是不是要把这个点排除掉吗</p>
</li>
<li><p>更新alpha的值，让它满足KKT</p>
</li>
</ul>
<ol>
<li>老师，我想问一下机器学习实战第八章的内容，图中高斯核函数中的距离，指的是x轴的距离吗? 同时书中给的代码，感觉是x轴距离的平方，不知道我理解的对不对。</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:OhBF3MR_p8y4H2_aIvOk09lNHto=.jpeg" alt="img"></p>
<ul>
<li>这里就是高斯核，x表示所有坐标轴，不是单指x轴。以书中代码为准，平方。</li>
</ul>
<ol>
<li>老师您好，机器学习实战svm这章KernelTrans这个函数里A表示什么?kTup[1]又表示的是什么，不是很明白，望解答一下</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:P0qO5dt6CxKVEEwYDogVH_b826E=.jpeg" alt="img"></p>
<ul>
<li>这里的A就是每个训练样本，kTup是一个元组，kTup[0]表示核函数类型，kTup[1]表示高斯函数方差，sigma。</li>
</ul>
<ol>
<li>老师，请问使用svm做回归时，怎样评价模型的好坏</li>
</ol>
<ul>
<li>一般的回归模型评价指标可以是均方误差（MSE）、均方根误差（RMSE）、平均绝对误差（MAE）、校正决定系数（Ajusted R-Squre）等。</li>
</ul>
<ol>
<li>老师你好，我想问一下关于b阈值的理解，有效相等的话b1=b2，除此之外无效取值有什么意义嘛</li>
</ol>
<ul>
<li><p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:7eRb59d61gqR88E-6-r0JTinA4I=.jpeg" alt="img"></p>
</li>
<li><p>第一种情况是该样本点是支持向量时，可以直接计算得到 b，若不是支持向量，b取两个b1和b2的均值。</p>
</li>
</ul>
<h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><ol>
<li>决策树有两个优点是对中间值缺失不敏感，可以处理不相关的特征数据，如何理解</li>
</ol>
<ul>
<li>第一个优点是对缺失值不敏感，因为决策树不是基于距离度量，大部分时候可以在数据有缺失的时候使用。如果涉及到距离度量，缺失数据就变得比较重要。第二个优点是可以处理不相关特征，树形结构并不要求特征之间具有较高相关性。</li>
</ul>
<ol>
<li>请问在决策树中找出最好的数据集划分方式，可不可以理解为以每一列为特征值计算熵，然后找出最小的呢？</li>
</ol>
<ul>
<li>对每个特征计算条件熵，可以理解为条件熵越小，信息增益越大，就以该特征进行划分。</li>
</ul>
<ol>
<li>能把这个注解函数的各个参数详细的说下嘛</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:wbx9TK4z2PsH5un3ikfmwFolV0U=.jpeg" alt="img"></p>
<ul>
<li>这里使用的是python里的annotate，网上资料很多，可以自己搜一下<a href="https://blog.csdn.net/leaf_zizi/article/details/82886755" target="_blank" rel="noopener">https://blog.csdn.net/leaf_zizi/article/details/82886755</a></li>
</ul>
<ol>
<li>一般决策树用于连续值划分的用例多不多，西瓜书上决策树对连续值处理的理论我没太看明白</li>
</ol>
<ul>
<li>你说的应该是CART算法，应用蛮多的，Random Forest、GBDT都会用到决策树。CART部分建议好好看看，西瓜书理论对初学者不太友好，网上搜一搜简洁教程。</li>
</ul>
<ol>
<li>老师、请问这里为什么要分两段（featVec[：axis]和featVec[axis+1:]）添加featVec的信息？</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:QCMkh3xchDHdelnx09vKhcSOgK4=.jpeg" alt="img"></p>
<ul>
<li>因为要把featVec[axis]这个特征删去。第三章决策树里面，它是使用一个特征之后就把该特征删去的，就像我们根据条件判断一样，这个条件使用过了就不会再用了。</li>
</ul>
<h1 id="k-近邻算法"><a href="#k-近邻算法" class="headerlink" title="k-近邻算法"></a>k-近邻算法</h1><ol>
<li>k近临算法的思路，是不是待观测值与训练数据之间求距离，然后寻找距离最短的点，认为与最短距离的点位一类。 那我想不明白的，图片中第一个矩形框的内容为什么这样写？第2框是对距离排序，而labels并没有排序，那怎么确定第三个框中labels是最短距离所对应的label？#k邻近</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:6lmcCaNV0DDQhPhuaYyZAQchvQ0=.jpeg" alt="img"></p>
<ul>
<li><p>sortedDistIndicies存储的就是距离最小对应的label中类的下标。建议每个变量用print函数打印出来看看，加深理解。</p>
</li>
<li><p>首先，第一个框目的是求一个点和所有点的距离差值，这里的写法是把维度匹配上，做一个向量减法。换句话说就是把待测点的坐标复制出很多份（数量由训练数据大小决定），然后做向量减法。第二个框sort的是index不是里面的值，就是根据值sort了label。</p>
</li>
</ul>
<ol>
<li>ax.scatter(returnMat[:,0],returnMat[:,1],15.0<em>np.array(classLabelVector), 15.0</em>np.array(classLabelVector))  请问下，scatter这个函数第三第四个参数为啥都乘以15，而且为啥需要两个相同的参数？</li>
</ol>
<ul>
<li>第三个参数是s表示大小，第四个参数c表示颜色。之所以用label是让每个类别呈现不同的大小和颜色。15.0是变量，你可以调整为其他值看看效果。</li>
</ul>
<ol>
<li>knn算法为什要对数据进行normalization预处理，normalization方法有哪几种？</li>
</ol>
<ul>
<li>一般的机器学习算法都会对输入进行归一化，其主要目的是将各个特征归一化到相似尺度，提高训练精度。如果是梯度下降算法的话还会提高训练速度。常用的归一化有线性归一化，标准差标准化，非线性归一化等。</li>
</ul>
<ol>
<li>kdtree可以实现k邻近的搜索吗？看李航老师的书在讲knn的时候叙述了kdtree，只实现了最邻近。</li>
</ol>
<ul>
<li>当然可以！kdtree只是使用了特殊的存储结构，可以实现最近邻，也可以实现k近邻。而且李航的书中也说了。附图：</li>
</ul>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:Hfhii1JA8e0qsLgWDiYVqG6AmhM=.jpeg" alt="img"></p>
<ul>
<li><p>能请教一下kdtree实现k邻近的思路吗，是需要遍历所有的节点吗，想编程实现来着，但是对于在另一半子树中存在比当前子树中多个点更近的点时，想不出来怎么解决，网上也没看到资料</p>
</li>
<li><p>既然能找到最邻近点，一定能找到k邻近点。例如k=3，设置一个包含3个元素的数组，在往上寻找的时候总是把最邻近的3个点保存下来。最后统计最多所占的类别就好。</p>
</li>
<li><p>但是搜索完根节点的左子树，假设找到了三个点，怎么确定右子树中没有比这三个点更近的点，是需要在走一遍右子树吗，就是这没想通。在李航老师树的42页那个特征空间划分的图中，如果我想查找(6,1)并且k=3，走完左子树后查到三个最近的点，然后右子树还有一个(8，1)，比左子树中部分点更近，这种应该怎么办？</p>
</li>
<li><p>这还是需要在继续便利，直到右子树也遍历完，像你说的这种情况，会让右子树中的小值替换当前3个元素的数组中的大值，直到找出整个树中最小的前3个值。</p>
</li>
</ul>
<ol>
<li>KNN算法中的K是怎样取值的？越大越好？k它有什么意义吗?</li>
</ol>
<ul>
<li>K一般没有固定的取值，根据具体问题具体分析。一般可以使用交叉验证选择最佳的K值。</li>
</ul>
<ol>
<li>实战书的k近邻算法第二章第四小节说，”k近邻算法的另一个缺陷是它无法给出任何数据的基础信息结构，因此我们也无法知晓平均实例样本和典型实例样本具有什么特征。下一章我们将使用概率测量方法处理分类问题，该算法可以解决这个问题”。疑问1:没理解这个缺陷是什么缺陷，样本数据每个纬度值都有了，为什么说无法知晓具有什么特征和基础信息结构？疑问二:为什么说概率测量方法可以解决？</li>
</ol>
<ul>
<li>1、knn是基于实例的学习，训练的时候使用的是样本所有的输入值进行距离计算，例如图片识别中一张图片所有的像素点。整个过程并没有提取样本本身的固有特征。训练过程保持了所有的训练样本。2、决策树使用信息增益寻找最佳划分特征，信息增益是通过训练样本中的概率测量方法得到的。</li>
</ul>
<ol>
<li>knn运行加载学会数据出错，是什么原因呢</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:AGwHVXhYwVL-ybKCq0-_DgRsQ3U=.jpeg" alt="img"></p>
<ul>
<li>看下你的“datingTestSet.txt”数据集，类别标签不是int型，你可以是使用“datingTestSet2.txt”数据集，label是int型的，或者修改file2matrix函数。这些都在训练营的github上：<a href="https://github.com/RedstoneWill/MachineLearningInAction-Camp" target="_blank" rel="noopener">https://github.com/RedstoneWill/MachineLearningInAction-Camp</a></li>
</ul>
<ol>
<li>老师，K近邻算法中如何理解？voteIlabel = labels[sortedDistIndicies[i]] ？为什么不能写成voteIlabel = labels[sortedDistIndicies==i] ？（当i 为0时 取sortedDistIndicies为0的那项）</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:S5fagacN3D9dtMA9iIlXtMsESX4=.jpeg" alt="img"></p>
<ul>
<li>sortedDistIndicies存储的是从大到小排列，距离最近的label下标，即位置。然后找到前k个点，计这k个点属于哪一个类别，统计最多的那一个类别就是预测类别。建议把每行语句打印出来看看，这样理解得比较透彻。</li>
</ul>
<h1 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h1><ol>
<li>如何用极大似然估计法推出概率估计公式（4.8）和（4.9）。也是统计学习方法朴素贝叶斯法中的课后习题。</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:6cGdixxIXfnKD1o-ESQzA2WOllM=.jpeg" alt="img"></p>
<ul>
<li><a href="https://blog.csdn.net/xiaoxiao_wen/article/details/54097917" target="_blank" rel="noopener">https://blog.csdn.net/xiaoxiao_wen/article/details/54097917</a></li>
</ul>
<ol>
<li>最近我在做文本分类的题目，按理说使用word2vec进行模型建立后，效果应该比用countvectorizer好的，但是我使用word2vec反而效果差了一些，不太懂为什么会这样？</li>
</ol>
<ul>
<li>可能跟样本集、算法、模型都有关系，没有说某个模型一定好。</li>
</ul>
<ol>
<li>想问一下图片上是怎么由1式得到的2式啊（记号处）</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:iCXlUW9aTFzVJdZoHTAdKlMKutU=.jpeg" alt="img"></p>
<ul>
<li>这是条件期望的表达式，可以看成是取每个类别的概率乘以当前类别的期望风险，最后所有类别的总和。</li>
</ul>
<ol>
<li>这个的解答，能直接算p(y=1!2,s)和p(y=-1|2,s)的概率，哪个大就代表是哪个类别吗</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:Zk5IB0G4ePgZi7JHn1wMMmhAHOo=.jpeg" alt="img"></p>
<ul>
<li>是的，朴素贝叶斯公式中，由于分母是全概率都是一样的，所以一般比较分子就行了。</li>
</ul>
<ol>
<li>针对4.5.2的公式和例子有两个问题1.例子中的文档和词条，谁是w谁是c?我理解c是词条，w是文档分类2.在4.5.3的classifyNB的计算中并没有除p(w)</li>
</ol>
<ul>
<li><p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:sJrrYT3AYbEp9lZdN63KNELmEWQ=.jpeg" alt="img"></p>
</li>
<li><p>\1. c 意为 class，c 是类别，w 是特征词条。</p>
</li>
<li><p>\2. 之所以不除以 p(w) 是因为计算所有c别可能性的时候，p(w) 都是相同的，比较大小的时候只看分子就行了。</p>
</li>
</ul>
<ol>
<li>请问老师，在书上朴素贝叶斯分类器过滤垃圾邮件中，图中这几行代码该怎么理解？trainMatrix不是文本向量吗？+＝操作是咋回事？</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:bHzNDf79I9crdbq9M4amyQu6-qk=.jpeg" alt="img"></p>
<ul>
<li>p1Num 和 p0Num 中的 += 操作是向量相加，统计的是训练样本两个类别每个单词出现的次数，p1Denom 和 p0Denom 中的 += 操作是数字相加，统计的是训练样本两个类别各自总的单词数。</li>
</ul>
<ol>
<li>老师，我想问下《机器学习实战》71页程序清单4-7这里，为什么要条件概率大于-6.0的单词加入到列表里，-6.0意味着什么</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:t-YVYN4a776AC1xr6-1uHDSBSIs=.jpeg" alt="img"></p>
<ul>
<li>条件概率加了log，可能会出现负值。这里为什么是-6.0可能是个阈值把。这4.7节不是我们需要完成的任务。原始网站数据因为是外网，所以爬不到，就不用看了。</li>
</ul>
<h1 id="其他疑问"><a href="#其他疑问" class="headerlink" title="其他疑问"></a>其他疑问</h1><ol>
<li>想问一下逻辑回归求参数为什么不是直接求导 而是要梯度下降呢 像SVM这种不都是直接求导结合拉格朗日就可以求参数吗</li>
</ol>
<ul>
<li>首先并不是所有函数都可以直接令导数为零求得极值的，有时可以求出导数在每个点的值, 但是直接解方程解不出来。对计算机而言，更加适合用循环迭代的方法来求极值，即梯度下降。SVM的解是二次规划问题，对于二次规划问题，有经典的最速下降法，牛顿法等，并不是简单的直接求导。</li>
</ul>
<ol>
<li>训练数据集经常出现训练数据集里面没有的属性，这种属性训练的有什么用吗？这么利用，比如下图，希望老师解答的详细一点，之前没有接触过这种数据训练</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:lpET1N0KWYLkGEAgF-sqe6LMKSo=.jpeg" alt="img"></p>
<ul>
<li>我看的是训练集里有一些特征，但是在测试集中却并未出现。尝试使用特征工程找出不同特征之间的相互关系。或者简单的方法忽略这些特征，训练的模型在测试集上测试看看效果如何。</li>
</ul>
<ol>
<li>请问训练营的10.1的阅读材料是台大机器学习基石的PLA,  训练营大纲是从knn开始的，1. 请问国庆节后训练营从哪个算法开始呢？2. 我们平时自己学习的时候按照实战的顺序学习吗？3. 那您博客的基石与技法的笔记要不要看呢？毕竟他们的内容不一样 4. 是否老师根据自己的需要安排算法的讲解顺序，并不是完全参考书本，然后讲解的算法内容与台大的笔记相结合学习？ 我自己比较纠结学习顺序，谢谢老师解答\</li>
</ol>
<ul>
<li>你好，1. 训练营的课程大纲是按照《机器学习实战》这本书为基础的，国庆假期的任务是我给大家安排的选修作业，并不在我们的任务要求之内，有兴趣的同学可以做一做。国庆后从决策树开始，具体见知识圈。2. 按照《机器学习实战》顺序。3. 我的个人博客、微信公众号有不错的资源和文章，大家可以作为参考资料看看。4. 目前这一期的训练营我们还是按照《机器学习实战》这本书的章节顺序来学习的，跟着我们大纲的顺序学习就好了。后期如果有调整会告知大家。</li>
</ul>
<ol>
<li>关于PLA算法的实现，我想问一下这里的x1和x2是随便设的嘛，为什么y1和y2要这样算呢</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:RRyQpBB2tqQ1nkkpjh4-QG6D604=.jpeg" alt="img"></p>
<ul>
<li><p>这里是画出当前w对应的分类线，x1和x2选取合适的值就好。y1和y2是根据分类线表达式推导的，见下图：</p>
</li>
<li><p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:5SEqeHWYeZIPyYa2mlKWwJYRfEA=.jpeg" alt="img"></p>
</li>
</ul>
<ol>
<li>有没有什么好的，理论教材，比如最优化，凸优化这些？</li>
</ol>
<ul>
<li>Bubeck的《Convex Optimization: Algorithms and Complexity》。最好根据自己实际情况找到最适合自己的就行。</li>
</ul>
<ol>
<li>能推荐对极大似然估计和softmax解释得通俗易懂的博客？</li>
</ol>
<ul>
<li>没有专门的推荐，网上资料很多，CSDN和博客园的文章都不错。Softmax的有一篇可以看看：<a href="https://mp.weixin.qq.com/s/XBK7T1P7z3rm3o-3BDNeOA" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/XBK7T1P7z3rm3o-3BDNeOA</a></li>
</ul>
<ol>
<li>例如lstm的变体GRU，每个参数在编程中如何体现的，以及W如何设置呢？能写个样例吗？要是能debug到细节更好了！</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/FnXmqd1ACovsuiQ_uMayt8Mt8P3i.jpg!thumbnail" alt="img"></p>
<ul>
<li>原理上与一般神经网络类似，使用梯度下降更新参数。现在多是直接调用深度学习框架来做，自己手写LSTM没啥必要。学有余力可以直接看看深度学习框架中LSTM的实现源码。</li>
</ul>
<ol>
<li>我安装scikit-learn的时候出现这个问题怎么解决呢</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:najgnP2JhLK4Haeyh_MgbPdUNSw=.jpeg" alt="img"></p>
<ul>
<li>换个镜像源试试。建议使用Anaconda自带的conda，直接输入：conda install scikit-learn即可。</li>
</ul>
<ol>
<li>能详细介绍一下lstm的代码实现，以及每个参数的含义吗？最好是能跑起来的程序！现在网上搜的程序规模太大！不适合从浅入深的学习！</li>
</ol>
<ul>
<li>这个问题太大了，现在基本都是使用tensorflow或pytorch等库来实现LSTM。如果是入门的话，建议看看莫烦的教学视频，这里面讲到了LSTM：<a href="https://morvanzhou.github.io/tutorials/machine-learning/torch/" target="_blank" rel="noopener">https://morvanzhou.github.io/tutorials/machine-learning/torch/</a></li>
</ul>
<ol>
<li>机器学习工具包sklearn，有没有比较好的教程，以及还有其他的比较好的工具包吗</li>
</ol>
<ul>
<li><p>最好的教程就是官方文档。ApachenCN翻译了中文版，可以参考：<a href="http://sklearn.apachecn.org/" target="_blank" rel="noopener">http://sklearn.apachecn.org/</a></p>
</li>
<li><p>机器学习最好的就是sklearn了，其它的深度学习如TensorFlow，PyTorch，Keras等</p>
</li>
</ul>
<ol>
<li>老师，请问跟着你学完这门课程，还需要学习哪些东西，才能找到一分机器学习的工作呢？</li>
</ol>
<ul>
<li>这是一个很大又很实际的问题。首先学习这门课只是基础，帮助大家在理解一定机器学习理论的基础上使用python手写各个基本机器学习算法的实现过程。但掌握这些还远远不够。建议从三个方面入手：1. 补充机器学习理论基础知识，这点在找工作笔试、面试的时候是很重要的，例如SVM、AdaBoost、GBDT等等要吃得透一点。2. 提高代码编程能力，掌握对机器学习库的使用，例如scikit-learn、深度学习的tensorflow等。3. 有机会的话做一些项目积累经验，或者了解构建一个完整的机器学习项目的整个流程。</li>
</ul>
<ol>
<li>我这边在训练一个卷积网络的模型，但是我的数据中正样本很多，负样本很少，我觉得这样的数据训练出来的模型可能会有问题，我想请问一下数据集中正负样本的比例多大时训练出来的模型比较好？</li>
</ol>
<ul>
<li>一般是正负样本近似相等的时候比较好。但实际中出现正负样本不均匀的情况，可以使用重采样和欠采样来尽量让其数量接近。</li>
</ul>
<ol>
<li>我把一个训练好的模型用C++进行调用的时候，我发现单次调用的时候用时100ms左右，但是做100次循环求平均用时5ms左右！请问您知道是什么原因导致的吗？</li>
</ol>
<ul>
<li>第一次参数传到模型时耗时比较多。</li>
</ul>
<ol>
<li>老师您好，我今年27了，在一家国企的设计单位做轨道交通线路走向设计工作。我打算现在转行机器学习，是否来得及？因为年龄比较大了。</li>
</ol>
<ul>
<li>你好，任何时候想要转行机器学习都不晚。况且你也才27岁，还算年轻。建议原先的工作先干着，平时多学点机器学习，为找份机器学习工作做准备。我不知道你的基础如何，一般半年时间可以入门了。</li>
</ul>
<ol>
<li>关于PLA算法，我做机器学习基石作业1的第15题的时候，迭代次数一直都是21，网上的答案是45，不知道自己究竟是哪里出错了</li>
</ol>
<p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:Ht9wz0Sn519trUPoelmrve2I-Bw=.jpeg" alt="img"></p>
<ul>
<li>PLA得到的分类超平面不是固定的，每次都可能产出新的结果，迭代次数也与初始选择的参数有关。你把最终的分类线在数据集上画出来看一下，正确的话就没有问题。</li>
</ul>
<ol>
<li>老师您好。本来想提一些关于实战书籍里的问题，结果发现琢磨着也很快看懂了。想来我的目的是来学习，并希望找一份相应的工作，请老师指点一下。我是一个走机器学习的转行人员，具备一年的python自学基础，目前毕业3年，软件测试干了2年，也算是IT行业吧。1,在这个深入一点的IT行业，机器学习里面，实际上机器学习会有怎样的使用。(应该不仅仅是kaggle或者天池020刷题一样吧？)2,机器学习涉及到大量数据的处理，我想问一下这些数据也是我自己来获取吗？3,对于面试，我应该准备到多少？(不会面多年经验的，所以就从初等看吧。)4,对于这种面试的情况下，我们应该面试的工资在什么范围？</li>
</ol>
<ul>
<li>你好。1.实际工程项目与打比赛区别还是很大的。比较仅仅考虑的是模型的准确率而不惜使用非常复杂、臃肿的模型。但是再工程应用上，除了考虑性能之外，还要注重速度、资源消耗、成本等各个方面。实际上机器学习有很大应用，比如推荐系统、图像识别等。2.数据不用自己获取，网络上有大量可供下载的数据集。公司里的话，也会有专门的人做数据收集、清洗等工作的。但是机器学习工程师也多少做过数据收集这些事情。3.面试的话多多准备一些机器学习典型问题的知识点，比如SVM、集成学习AdaBoost等，还有你的项目经验。4.这个得根据工作地点、公司、具体什么工作等来确定。可以根据当地IT均资来设个心理价位。</li>
</ul>
<ol>
<li>LSTM中的cell state表示什么意思？</li>
</ol>
<ul>
<li><p>cell state一般是保存模型当前及历史状态。有点像是传输带，它直直地流过整个链，受到轻微的非线性相互作用影响。因此信息可以轻松地沿它流动而不发生改变。可以看下这篇文章：<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">Understanding LSTM Networks — colah’s blog</a></p>
</li>
<li><p><img src="//schwimmer.github.io/2019/06/02/机器学习/机器学习问题答疑/Adaboost/pic/75&amp;e=1874736000&amp;token=kIxbL07-8jAj8w1n4s9zv64FuZZNEATmlU_Vm6zD:TxTw6IhA74ue0H40Y8kGSV61lMU=.jpeg" alt="img"></p>
</li>
</ul>
<ol>
<li>老师你好，学习算法需不需要每一步自己去证明它呢?</li>
</ol>
<ul>
<li>其实，对大部分人来说，机器学习算法的每一步详细的数学证明是不需要的。但是我们要感性地理解它的意思和推导方式。就像SVM中涉及的理论推导很多，拉格朗日那块内容每一步推导要大致知道思路和方法，但详细的数学证明可能就不需要了。</li>
</ul>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/02/深度学习笔记/Pytorch莫烦/pytorch基础/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/02/深度学习笔记/Pytorch莫烦/pytorch基础/" class="post-title-link" itemprop="url">未命名</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-02 15:07:08 / 修改时间：16:09:19" itemprop="dateCreated datePublished" datetime="2019-06-02T15:07:08+08:00">2019-06-02</time>
            </span>
          

          
            

            
          

          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/02/深度学习笔记/Pytorch莫烦/pytorch基础/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/02/深度学习笔记/Pytorch莫烦/pytorch基础/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://morvanzhou.github.io/tutorials/machine-learning/torch/2-01-torch-numpy/" target="_blank" rel="noopener">https://morvanzhou.github.io/tutorials/machine-learning/torch/2-01-torch-numpy/</a></p>
<p>基于0.1.11的版本</p>
<h2 id="用-Numpy-还是-Torch"><a href="#用-Numpy-还是-Torch" class="headerlink" title="用 Numpy 还是 Torch"></a>用 Numpy 还是 Torch</h2><p>我们对 Numpy 还是爱不释手的, 因为我们太习惯 numpy 的形式了. 不过 torch 看出来我们的喜爱, 他把 torch 做的和 numpy 能很好的兼容. 比如这样就能自由地转换 numpy array 和 torch tensor 了:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">np_data = np.arange(<span class="number">6</span>).reshape((<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">torch_data = torch.from_numpy(np_data)</span><br><span class="line">tensor2array = torch_data.numpy()</span><br><span class="line">print(</span><br><span class="line">    <span class="string">'\nnumpy array:'</span>, np_data,          <span class="comment"># [[0 1 2], [3 4 5]]</span></span><br><span class="line">    <span class="string">'\ntorch tensor:'</span>, torch_data,      <span class="comment">#  0  1  2 \n 3  4  5    [torch.LongTensor of size 2x3]</span></span><br><span class="line">    <span class="string">'\ntensor to array:'</span>, tensor2array, <span class="comment"># [[0 1 2], [3 4 5]]</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h2 id="Torch-中的数学运算"><a href="#Torch-中的数学运算" class="headerlink" title="Torch 中的数学运算"></a>Torch 中的数学运算</h2><p>其实 torch 中 tensor 的运算和 numpy array 的如出一辙, 我们就以对比的形式来看. 如果想了解 torch 中其它更多有用的运算符, <a href="http://pytorch.org/docs/torch.html#math-operations" target="_blank" rel="noopener">API就是你要去的地方</a>.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># abs 绝对值计算</span><br><span class="line">data = [-1, -2, 1, 2]</span><br><span class="line">tensor = torch.FloatTensor(data)  # 转换成32位浮点 tensor</span><br><span class="line">print(</span><br><span class="line">    &apos;\nabs&apos;,</span><br><span class="line">    &apos;\nnumpy: &apos;, np.abs(data),          # [1 2 1 2]</span><br><span class="line">    &apos;\ntorch: &apos;, torch.abs(tensor)      # [1 2 1 2]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># sin   三角函数 sin</span><br><span class="line">print(</span><br><span class="line">    &apos;\nsin&apos;,</span><br><span class="line">    &apos;\nnumpy: &apos;, np.sin(data),      # [-0.84147098 -0.90929743  0.84147098  0.90929743]</span><br><span class="line">    &apos;\ntorch: &apos;, torch.sin(tensor)  # [-0.8415 -0.9093  0.8415  0.9093]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># mean  均值</span><br><span class="line">print(</span><br><span class="line">    &apos;\nmean&apos;,</span><br><span class="line">    &apos;\nnumpy: &apos;, np.mean(data),         # 0.0</span><br><span class="line">    &apos;\ntorch: &apos;, torch.mean(tensor)     # 0.0</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>除了简单的计算, 矩阵运算才是神经网络中最重要的部分. 所以我们展示下矩阵的乘法. 注意一下包含了一个 numpy 中可行, 但是 torch 中不可行的方式.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># matrix multiplication 矩阵点乘</span><br><span class="line">data = [[1,2], [3,4]]</span><br><span class="line">tensor = torch.FloatTensor(data)  # 转换成32位浮点 tensor</span><br><span class="line"># correct method</span><br><span class="line">print(</span><br><span class="line">    &apos;\nmatrix multiplication (matmul)&apos;,</span><br><span class="line">    &apos;\nnumpy: &apos;, np.matmul(data, data),     # [[7, 10], [15, 22]]</span><br><span class="line">    &apos;\ntorch: &apos;, torch.mm(tensor, tensor)   # [[7, 10], [15, 22]]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># !!!!  下面是错误的方法 !!!!</span><br><span class="line">data = np.array(data)</span><br><span class="line">print(</span><br><span class="line">    &apos;\nmatrix multiplication (dot)&apos;,</span><br><span class="line">    &apos;\nnumpy: &apos;, data.dot(data),        # [[7, 10], [15, 22]] 在numpy 中可行</span><br><span class="line">    &apos;\ntorch: &apos;, tensor.dot(tensor)     # torch 会转换成 [1,2,3,4].dot([1,2,3,4) = 30.0</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>新版本中(&gt;=0.3.0), 关于 <code>tensor.dot()</code> 有了新的改变, 它<a href="http://pytorch.org/docs/master/torch.html" target="_blank" rel="noopener">只能</a>针对于一维的数组. 所以上面的有所改变.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor.dot(tensor)     # torch 会转换成 [1,2,3,4].dot([1,2,3,4) = 30.0</span><br><span class="line"></span><br><span class="line"># 变为</span><br><span class="line">torch.dot(tensor.dot(tensor)</span><br></pre></td></tr></table></figure>
<h1 id="变量-Variable"><a href="#变量-Variable" class="headerlink" title="变量 (Variable)"></a>变量 (Variable)</h1><h2 id="什么是-Variable"><a href="#什么是-Variable" class="headerlink" title="什么是 Variable"></a>什么是 Variable</h2><p>在 Torch 中的 Variable 就是一个存放会变化的值的地理位置. 里面的值会不停的变化. 就像一个裝鸡蛋的篮子, 鸡蛋数会不停变动. 那谁是里面的鸡蛋呢, 自然就是 Torch 的 Tensor 咯. 如果用一个 Variable 进行计算, 那返回的也是一个同类型的 Variable.</p>
<p>我们定义一个 Variable:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable <span class="comment"># torch 中 Variable 模块</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 先生鸡蛋</span></span><br><span class="line">tensor = torch.FloatTensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line"><span class="comment"># 把鸡蛋放到篮子里, requires_grad是参不参与误差反向传播, 要不要计算梯度</span></span><br><span class="line">variable = Variable(tensor, requires_grad=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">print(tensor)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string"> 1  2</span></span><br><span class="line"><span class="string"> 3  4</span></span><br><span class="line"><span class="string">[torch.FloatTensor of size 2x2]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">print(variable)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Variable containing:</span></span><br><span class="line"><span class="string"> 1  2</span></span><br><span class="line"><span class="string"> 3  4</span></span><br><span class="line"><span class="string">[torch.FloatTensor of size 2x2]</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<h2 id="Variable-计算-梯度"><a href="#Variable-计算-梯度" class="headerlink" title="Variable 计算, 梯度"></a>Variable 计算, 梯度</h2><p>我们再对比一下 tensor 的计算和 variable 的计算.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">t_out = torch.mean(tensor*tensor)       # x^2</span><br><span class="line">v_out = torch.mean(variable*variable)   # x^2</span><br><span class="line">print(t_out)</span><br><span class="line">print(v_out)    # 7.5</span><br></pre></td></tr></table></figure>
<p>到目前为止, 我们看不出什么不同, <strong>但是时刻记住, Variable 计算时, 它在背景幕布后面一步步默默地搭建着一个庞大的系统, 叫做计算图, computational graph. 这个图是用来干嘛的? 原来是将所有的计算步骤 (节点) 都连接起来, 最后进行误差反向传递的时候, 一次性将所有 variable 里面的修改幅度 (梯度) 都计算出来, 而 tensor 就没有这个能力啦.</strong></p>
<p><code>v_out = torch.mean(variable*variable)</code> 就是在计算图中添加的一个计算步骤, 计算误差反向传递的时候有他一份功劳, 我们就来举个例子:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">v_out.backward()    # 模拟 v_out 的误差反向传递</span><br><span class="line"></span><br><span class="line"># 下面两步看不懂没关系, 只要知道 Variable 是计算图的一部分, 可以用来传递误差就好.</span><br><span class="line"># v_out = 1/4 * sum(variable*variable) 这是计算图中的 v_out 计算步骤</span><br><span class="line"># 针对于 v_out 的梯度就是, d(v_out)/d(variable) = 1/4*2*variable = variable/2</span><br><span class="line"></span><br><span class="line">print(variable.grad)    # 初始 Variable 的梯度</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"> 0.5000  1.0000</span><br><span class="line"> 1.5000  2.0000</span><br><span class="line">&apos;&apos;&apos;</span><br></pre></td></tr></table></figure>
<h2 id="获取-Variable-里面的数据"><a href="#获取-Variable-里面的数据" class="headerlink" title="获取 Variable 里面的数据"></a>获取 Variable 里面的数据</h2><p>直接<code>print(variable)</code>只会输出 Variable 形式的数据, 在很多时候是用不了的(比如想要用 plt 画图), 所以我们要转换一下, 将它变成 tensor 形式.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">print(variable)     #  Variable 形式</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">Variable containing:</span><br><span class="line"> 1  2</span><br><span class="line"> 3  4</span><br><span class="line">[torch.FloatTensor of size 2x2]</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">print(variable.data)    # tensor 形式</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"> 1  2</span><br><span class="line"> 3  4</span><br><span class="line">[torch.FloatTensor of size 2x2]</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">print(variable.data.numpy())    # numpy 形式</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">[[ 1.  2.]</span><br><span class="line"> [ 3.  4.]]</span><br><span class="line">&quot;&quot;&quot;</span><br></pre></td></tr></table></figure>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/01/深度学习笔记/Pytorch训练营/D1 pytorch入门/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/01/深度学习笔记/Pytorch训练营/D1 pytorch入门/" class="post-title-link" itemprop="url">未命名</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-01 22:59:25" itemprop="dateCreated datePublished" datetime="2019-06-01T22:59:25+08:00">2019-06-01</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-19 10:23:49" itemprop="dateModified" datetime="2019-06-19T10:23:49+08:00">2019-06-19</time>
              </span>
            
          

          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/01/深度学习笔记/Pytorch训练营/D1 pytorch入门/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/01/深度学习笔记/Pytorch训练营/D1 pytorch入门/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>笔记见 幕布</p>
<p>pytorch和其他框架的区别</p>
<ul>
<li>pytorch：接口简洁，面向对象设计中最优雅的一个。底层c</li>
<li>tensorflow：接口复杂，且为静态图机制。底层c</li>
<li>Mxnet：小众框架</li>
<li>Keras：高度封装，最容易上手，但不能自定义函数。底层是python，速度慢</li>
<li>Caffe：缺少灵活性。</li>
</ul>
<p>智慧课堂项目</p>
<ul>
<li><p>面向全国学生，识别学生的姿态，举手/睡觉/看书</p>
</li>
<li><p>困难</p>
<ul>
<li>图片背景复杂，衣着、动作、体态都不一样</li>
<li>数据采集不均衡，比如举手的人很少，而看书/听课的人很多</li>
<li>有些动作差异不大，难以量化。比如举手和托腮</li>
</ul>
</li>
<li><p>方案</p>
<ul>
<li><p>骨架提取OpenPose。把背景复杂度降到最低</p>
<p><img src="//schwimmer.github.io/2019/06/01/深度学习笔记/Pytorch训练营/D1 pytorch入门/pic/7b1a3454-982c-449c-8303-82ecd2394b4d-2148887.jpg" alt="img"></p>
</li>
<li><p>备选方案有目标检测、视频行为检测。很多具体项目使用单一方案很难达到预期效果，必须使用某些算法的组合，比如人体行为识别的项目，使用openpose提取骨架图+图像分类</p>
</li>
</ul>
</li>
</ul>
<h1 id="打卡要求"><a href="#打卡要求" class="headerlink" title="打卡要求"></a>打卡要求</h1><p><strong>打卡要求：</strong>在训练和测试时自动求导的区别？如何调用CUDA？程序中如何使用多进程？</p>
<p><strong>打卡内容：</strong>文字或图片拍照提交，文字要求最少50字，图片要求最少3张</p>
<p><strong>打卡截止时间：</strong>6/3</p>
<h1 id="1、在训练和测试时自动求导的区别？"><a href="#1、在训练和测试时自动求导的区别？" class="headerlink" title="1、在训练和测试时自动求导的区别？"></a>1、在训练和测试时自动求导的区别？</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable </span><br><span class="line"></span><br><span class="line">x = Variable(torch.randn(<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">z = Variable(torch.randn(<span class="number">5</span>,<span class="number">5</span>), requires_grad=<span class="keyword">True</span>)</span><br><span class="line">b = x + z</span><br><span class="line">b.requires_grad</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">True</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<p>训练时，需要开启自动求导，这样backward就可以自动计算所有的导数，更新权重。</p>
<p>而测试时，因为不需要更新权重，所以不用保存梯度，而pytorch默认是保存的，这就会导致测试时消耗无谓的资源。这个时候需要通过显式的设置<code>requires_grad=False</code>，这样测试和验证阶段就不会保存梯度了。</p>
<h2 id="2、如何调用CUDA？"><a href="#2、如何调用CUDA？" class="headerlink" title="2、如何调用CUDA？"></a>2、如何调用CUDA？</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一种方法是定义时就在CUDA</span></span><br><span class="line">x = torch.cuda.FloatTensor(<span class="number">1</span>)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">x.get_device() == 0</span></span><br><span class="line"><span class="string">此时会默认使用GPU 0也就是第一张显卡来进行操作</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 也可以指定用哪一个显卡</span></span><br><span class="line"><span class="keyword">with</span> torch.cuda.device(<span class="number">1</span>):</span><br><span class="line">  a = torch.cuda.FloatTensor(<span class="number">1</span>)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">a.get_device() == 1</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">  </span><br><span class="line"><span class="comment"># 另一种方法是从CPU复制到GPU，直接通过.cuda()</span></span><br><span class="line">ten1 = torch.FloatTensor(<span class="number">2</span>)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">  ten1 = ten1.cuda()</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">6.1101e+24</span></span><br><span class="line"><span class="string">4.5659e-41</span></span><br><span class="line"><span class="string">[torch.cuda.FloatTensor of size 2 (GPU 0)]</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<h1 id="3、程序中如何使用多进程？"><a href="#3、程序中如何使用多进程？" class="headerlink" title="3、程序中如何使用多进程？"></a>3、程序中如何使用多进程？</h1><p>通过<code>torch.multiprocessing</code>使用多进程，扩展了python的<code>multiprocessing</code>，可以通过<code>multiprocessing.Queue</code>移动所有tensor的数据到共享内存中。</p>
<p>这种方式可以异步训练模型，参数可以一直共享，也可以定期同步。</p>
<h1 id="pytorch安装"><a href="#pytorch安装" class="headerlink" title="pytorch安装"></a>pytorch安装</h1><p>官网选择版本</p>
<p><a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener">https://pytorch.org/get-started/locally/</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install torch torchvision</span><br></pre></td></tr></table></figure>
<p>目前稳定版是1.1版</p>
<p>安装后，<code>import torch</code>报错</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import torch</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;</span><br><span class="line">  File &quot;/Users/david/anaconda3/lib/python3.6/site-packages/torch/__init__.py&quot;, line 79, in &lt;module&gt;</span><br><span class="line">    from torch._C import *</span><br><span class="line">ImportError: dlopen(/Users/david/anaconda3/lib/python3.6/site-packages/torch/_C.cpython-36m-darwin.so, 9): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib</span><br><span class="line">  Referenced from: /Users/david/anaconda3/lib/python3.6/site-packages/torch/lib/libshm.dylib</span><br><span class="line">  Reason: image not found</span><br></pre></td></tr></table></figure>
<p>解决：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install libomp</span><br></pre></td></tr></table></figure>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/05/21/深度学习笔记/tensorflow/基础-莫烦教程/5-2 CNN/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/05/21/深度学习笔记/tensorflow/基础-莫烦教程/5-2 CNN/" class="post-title-link" itemprop="url">未命名</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-05-21 23:17:28" itemprop="dateCreated datePublished" datetime="2019-05-21T23:17:28+08:00">2019-05-21</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-22 09:30:32" itemprop="dateModified" datetime="2019-05-22T09:30:32+08:00">2019-05-22</time>
              </span>
            
          

          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/05/21/深度学习笔记/tensorflow/基础-莫烦教程/5-2 CNN/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/05/21/深度学习笔记/tensorflow/基础-莫烦教程/5-2 CNN/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/5-03-A-CNN/" target="_blank" rel="noopener">https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/5-03-A-CNN/</a></p>
<h2 id="流行的-CNN-结构"><a href="#流行的-CNN-结构" class="headerlink" title="流行的 CNN 结构"></a>流行的 CNN 结构</h2><p>比较流行的一种搭建结构是这样, 从下到上的顺序, 首先是输入的图片(image), 经过一层卷积层 (convolution), 然后在用池化(pooling)方式处理卷积的信息, 这里使用的是 max pooling 的方式. 然后在经过一次同样的处理, 把得到的第二次处理的信息传入两层全连接的神经层 (fully connected),这也是一般的两层神经网络层,最后在接上一个分类器(classifier)进行分类预测</p>
<p>我们在代码中实现一个基于MNIST数据集的例子</p>
<h2 id="定义卷积层的-weight-bias"><a href="#定义卷积层的-weight-bias" class="headerlink" title="定义卷积层的 weight bias"></a>定义卷积层的 weight bias</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">python from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line"></span><br><span class="line">mnist=input_data.read_data_sets(&apos;MNIST_data&apos;,one_hot=true)</span><br></pre></td></tr></table></figure>
<p>定义<code>Weight</code>变量，输入<code>shape</code>，返回变量的参数。其中我们使用<code>tf.truncted_normal</code>产生随机变量来进行初始化:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def weight_variable(shape): </span><br><span class="line">	inital=tf.truncted_normal(shape,stddev=0.1)</span><br><span class="line">	return tf.Variable(initial)</span><br></pre></td></tr></table></figure>
<p>同样的定义<code>biase</code>变量，输入<code>shape</code> ,返回变量的一些参数。其中我们使用<code>tf.constant</code>常量函数来进行初始化:</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/6/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><span class="page-number current">7</span><a class="page-number" href="/page/8/">8</a><span class="space">&hellip;</span><a class="page-number" href="/page/32/">32</a><a class="extend next" rel="next" href="/page/8/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Schwimmer</p>
              <div class="site-description motion-element" itemprop="description">Record and Think!</div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">315</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">25</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">62</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          

          

          
          

          
            
          
          

        </div>
      </div>

      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Schwimmer</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.9.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> v7.2.0</div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span>
    <span class="site-uv" title="总访客量">
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="post-meta-divider">|</span>
  

  
    <span class="post-meta-item-icon">
      <i class="fa fa-eye"></i>
    </span>
    <span class="site-pv" title="总访问量">
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    
      <div>
        
  <script>
    window._bd_share_config = {
      "common": {
        "bdText": "",
        "bdMini": "1",
        "bdMiniList": false,
        "bdPic": ""
      },
      "image": {
        "viewList": ["tsina", "douban", "sqq", "qzone", "weixin", "twi", "fbook"],
        "viewText": "分享到：",
        "viewSize": "16"
      },
      "slide": {
        "bdImg": "5",
        "bdPos": "left",
        "bdTop": "100"
      }
    }
  </script>

<script>
  with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='//bdimg.share.baidu.com/static/api/js/share.js?cdnversion='+~(-new Date()/36e5)];
</script>

      </div>
    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/utils.js?v=7.2.0"></script>

  <script src="/js/motion.js?v=7.2.0"></script>



  
  


  <script src="/js/schemes/muse.js?v=7.2.0"></script>




  

  


  <script src="/js/next-boot.js?v=7.2.0"></script>


  

  

  

  
  
<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://https-schwimmer-github-io.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>







  
  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
  

  
  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      document.getElementById(all[i].inputID + '-Frame').parentNode.className += ' has-jax';
    }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

    
  


  

  

  

  

  

  

  

  

  

  

  

</body>
</html>
