<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">






















<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">

<link rel="stylesheet" href="/css/main.css?v=7.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.2.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.2.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '7.2.0',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="Record and Think!">
<meta property="og:type" content="website">
<meta property="og:title" content="Schwimmer&#39;s Blog">
<meta property="og:url" content="https://schwimmer.github.io/page/5/index.html">
<meta property="og:site_name" content="Schwimmer&#39;s Blog">
<meta property="og:description" content="Record and Think!">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Schwimmer&#39;s Blog">
<meta name="twitter:description" content="Record and Think!">





  
  
  <link rel="canonical" href="https://schwimmer.github.io/page/5/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Schwimmer's Blog</title>
  




  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-143240576-1"></script>
  <script>
    var host = window.location.hostname;
    if (host !== "localhost" || !true) {
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-143240576-1');
    }
  </script>









  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Schwimmer's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
        </li>
      
    </ul>
  

  
    

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/08/机器学习和深度学习算法理论/SoftMax/输出层的Softmax/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/08/机器学习和深度学习算法理论/SoftMax/输出层的Softmax/" class="post-title-link" itemprop="url">未命名</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-08 12:06:17 / 修改时间：12:07:26" itemprop="dateCreated datePublished" datetime="2019-06-08T12:06:17+08:00">2019-06-08</time>
            </span>
          

          
            

            
          

          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/08/机器学习和深度学习算法理论/SoftMax/输出层的Softmax/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/08/机器学习和深度学习算法理论/SoftMax/输出层的Softmax/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://cloud.tencent.com/developer/news/307323" target="_blank" rel="noopener">https://cloud.tencent.com/developer/news/307323</a></p>
<p>Softmax是假设不同特征是相互独立的</p>
<p>然而，这可能在许多情况下不成立，因为特征之间可能存在协同作用或冗余，种协同或者作用会直接影响输出概率。</p>
<p>解决方案可以是：</p>
<p>1）去除有协同作用或冗余的特征，如x3 =X1⋅x2x3=x1⋅x2（但是如果我们不知道哪些特征值是相关的，我们可能会引入更多无用的特征！</p>
<p>2）当两个特征经常一起被激活时，训练过程将学习较小的权重W1和W2，使得它们的联合效果更接近真实效果</p>
<blockquote>
<p>如何判断两个特征是否同时被激活</p>
</blockquote>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/05/机器学习和深度学习算法理论/EM/EM算法入门/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/05/机器学习和深度学习算法理论/EM/EM算法入门/" class="post-title-link" itemprop="url">未命名</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-05 13:52:55 / 修改时间：13:54:49" itemprop="dateCreated datePublished" datetime="2019-06-05T13:52:55+08:00">2019-06-05</time>
            </span>
          

          
            

            
          

          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/05/机器学习和深度学习算法理论/EM/EM算法入门/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/05/机器学习和深度学习算法理论/EM/EM算法入门/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://zhuanlan.zhihu.com/p/61768577?utm_source=ZHShareTargetIDMore&amp;utm_medium=social&amp;utm_oi=28206795063296" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/61768577?utm_source=ZHShareTargetIDMore&amp;utm_medium=social&amp;utm_oi=28206795063296</a></p>
<p>这个推导啥子的也太难了把。不过经过我不停不停不停不停的看这个算法，到今天我突然觉得自己好像明白了，然后我决定把我的理解写成一篇文章，毕竟只有给别人讲明白了才能算自己真正的明白。那么就进入我们这篇文章的主题:<strong>EM算法。</strong></p>
<p>我们先讲一下极大似然估计法，然后再引申出EM算法</p>
<p><strong>1.极大似然估计法</strong></p>
<p>假设我们有如下的一维高斯分布</p>
<p><img src="https://www.zhihu.com/equation?tex=X%5Csim+N%3D%28%5Cmu%2C+%5Csigma%5E%7B2%7D%29+" alt="X\sim N=(EM/pic/equation-20190605135301319) "></p>
<p>X的概率密度函数为:</p>
<p><img src="https://www.zhihu.com/equation?tex=f%28x%3B%5Cmu%2C%5Csigma%5E%7B2%7D%29%3D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%5Csigma%7D%7De%5E%7B-%5Cfrac%7B%28x-%5Cmu%29%5E%7B2%7D%7D%7B2%5Csigma%5E%7B2%7D%7D%7D" alt="f(EM/pic/equation-20190605135301359)=\frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}}"></p>
<p>其似然函数为</p>
<p><img src="https://www.zhihu.com/equation?tex=L%28%5Cmu%2C%5Csigma%5E%7B%5E%7B2%7D%7D%29%3D%5Cprod_%7Bi%3D1%7D%5E%7Bn%7D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%5Csigma%7D%7De%5E%7B-%5Cfrac%7B%28x-%5Cmu%29%5E%7B2%7D%7D%7B2%5Csigma%5E%7B2%7D%7D%7D" alt="L(EM/pic/equation-20190605135332617)=\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}}"></p>
<p>求对数为</p>
<p><img src="https://www.zhihu.com/equation?tex=lnL%3D-%5Cfrac%7Bn%7D%7B2%7Dln%5Cpi-%5Cfrac%7Bn%7D%7B2%7Dln%5Csigma%5E%7B2%7D-%5Cfrac%7B1%7D%7B2%5Csigma%5E%7B2%7D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7B%28x_%7Bi%7D-%5Cmu%29%5E%7B2%7D%7D%7D" alt="lnL=-\frac{n}{2}ln\pi-\frac{n}{2}ln\sigma^{2}-\frac{1}{2\sigma^{2}\sum_{i=1}^{n}{(EM/pic/equation-20190605135332570)^{2}}}"></p>
<p>对其求导，可以得到如下似然方程组</p>
<p><img src="/.io//pic/image-20190605135436560.png" alt="image-20190605135436560"></p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cdelta%7D%7B%5Cdelta%5Csigma%5E%7B2%7D%7D%3D-%5Cfrac%7Bn%7D%7B2%5Csigma%5E%7B2%7D%7D%2B%5Cfrac%7B1%7D%7B2%28%5Csigma%5E%7B2%7D%29%5E%7B2%7D%7D%5B%5Csum_%7Bi%3D1%7D%5E%7Bn%7Dx_%7Bi%7D-n%5Cmu%5D" alt="\frac{\delta}{\delta\sigma^{2}}=-\frac{n}{2\sigma^{2}}+\frac{1}{2(EM/pic/equation-20190605135332601)^{2}}[\sum_{i=1}^{n}x_{i}-n\mu]"></p>
<p>我们可以使用</p>
<ul>
<li>梯度下降法</li>
<li>极大似然估计法</li>
</ul>
<p>这两种方法来根据样本估计高斯分布的参数，具体代码如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_guassian_theta</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># u=1 siga=4</span></span><br><span class="line">    y = <span class="number">1</span> + <span class="number">2</span>*np.random.randn(<span class="number">1000</span>,<span class="number">1</span>)</span><br><span class="line">    n,m = y.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 梯度下降</span></span><br><span class="line">    u1 = np.random.randn(<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">    siga1 = np.random.randn(<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">    lr = <span class="number">0.001</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">        lu = (np.sum(y,axis=<span class="number">0</span>) - n*u1)/(siga1)</span><br><span class="line">        lsiga = (np.sum((y-u1)**<span class="number">2</span>, axis=<span class="number">0</span>)/siga1 - n)/(<span class="number">2</span>*siga1)</span><br><span class="line">        u1 = u1+lr*lu</span><br><span class="line">        siga1 = siga1+lr*lsiga</span><br><span class="line">    print(<span class="string">"u1:   %lf, siga1:   %lf"</span>%(u1, siga1))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 解析解</span></span><br><span class="line">    u2 = np.sum(y, axis = <span class="number">0</span>)/n</span><br><span class="line">    siga2 = np.sum((y-u2)**<span class="number">2</span>, axis = <span class="number">0</span>)/n</span><br><span class="line">    print(<span class="string">"u2:   %lf, siga2:   %lf"</span>%(u2, siga2))</span><br></pre></td></tr></table></figure>
<p>我们使用均值为1，标准差为2的高斯分布随机生成了1000个样本，然后分别使用梯度下降和极大似然估计法两种方式来估计参数，得到的参数如下:</p>
<p><img src="/.io//pic/v2-4d49b54725f875723438ba87a94020e2_hd.png" alt="img"></p>
<p>两种方法得到的结果还是挺不错的。</p>
<p><strong>2. EM算法</strong></p>
<p><strong>极大似然算法确实可以很方便的根据样本估算模型的参数，如果样本来自一个以上的模型，我们又不知道某个样本点到底是来自某个模型的</strong>，那么此时极大似然算法就无能为力了。</p>
<p>我们依旧用高斯分布来举例子，混合高斯分布的模型如下:</p>
<p><img src="https://www.zhihu.com/equation?tex=P%28y%7C%5Ctheta%29%3D%5Csum_%7Bk%3D1%7D%5E%7BK%7D%5Calpha_%7Bk%7D%5Cphi%28y%7C%5Ctheta_%7Bk%7D%29" alt="P(EM/pic/equation-20190605135345806)=\sum_{k=1}^{K}\alpha_{k}\phi(y|\theta_{k})"></p>
<p>其中 <img src="https://www.zhihu.com/equation?tex=%5Calpha_%7Bk%7D" alt="\alpha_{k}"> 是系数， <img src="https://www.zhihu.com/equation?tex=%5Calpha_%7Bk%7D%5Cgeq0" alt="\alpha_{k}\geq0"> ，同时 <img src="/.io//pic/equation-20190605135345792" alt="\sum_{k=1}^{K}a_{k}=1"> ； <img src="https://www.zhihu.com/equation?tex=%5Cphi%28y%7C%5Ctheta_%7Bk%7D%29" alt="\phi(y|\theta_{k})"> 是高斯分布密度函数， <img src="https://www.zhihu.com/equation?tex=%5Ctheta_%7Bk%7D%3D%28u_%7Bk%7D%2C+%5Csigma_%7Bk%7D%5E%7B2%7D%29" alt="\theta_{k}=(u_{k}, \sigma_{k}^{2})"> 。</p>
<p>这个时候我们需要估计的参数有</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Ctheta%3D%28%5Calpha_%7B1%7D%2C+...%2C+%5Calpha_%7Bk%7D%3B%5Ctheta_%7B1%7D%2C+.....%2C%5Ctheta_%7Bk%7D%29" alt="\theta=(EM/pic/equation?tex=%5Ctheta%3D%28%5Calpha_%7B1%7D%2C+...%2C+%5Calpha_%7Bk%7D%3B%5Ctheta_%7B1%7D%2C+.....%2C%5Ctheta_%7Bk%7D%29)"></p>
<p><strong>此时阻挡我们使用极大似然法的原因就是:我们不知道到底哪些样本点由哪个模型生成。</strong></p>
<p>现在假设我们有1000个样本点，由两个独立的高斯分布生成。我们知道其中第一类有300个，第二类有700个，那么我们就可以对两个高斯分布分别使用极大似然法估计他们的参数了。</p>
<p>但事实上我们知道的只有一堆样本点以及其可能的类别数 <img src="/.io//pic/equation-20190605135345838" alt="K"> ，至于某个样本到底属于那个模型我们是不知道。此时就要到EM算法登场的时候了，<strong>EM算法的主要思想如下：</strong></p>
<ul>
<li><strong>E步:先随便设置一下各种参数<img src="https://www.zhihu.com/equation?tex=%5Ctheta%3D%28%5Calpha_%7B1%7D%2C+...%2C+%5Calpha_%7Bk%7D%3B%5Ctheta_%7B1%7D%2C+.....%2C%5Ctheta_%7Bk%7D%29" alt="\theta=(\alpha_{1}, ..., \alpha_{k};\theta_{1}, .....,\theta_{k})">，然后再算一下在当前情况下每个样本点属于哪一个模型的概率值；</strong></li>
<li><strong>M步:此时我们知道了一个样本点属于某个模型的概率，然后再次计算各个模型的参数（具体计算方法在下面）；然后返回上一步，直至算法收敛。</strong></li>
</ul>
<p>现在我们知道了EM算法的思想，那么EM算法是怎么在第二步估算出各个模型的参数呢。</p>
<p>我们先介绍一些概念：用 <img src="/.io//pic/equation-20190605135345838-9714025." alt="Y"> 来表示观测随机变量的数据， <img src="/.io//pic/equation-20190605135345835" alt="Z"> 表示隐随机变量的数据（比如上述混合高斯分布样本点属于某个模型的概率）， <img src="/.io//pic/equation-20190605135345838-9714025." alt="Y"> 和 <img src="/.io//pic/equation-20190605135345837" alt="Z "> 连在一起称为<strong>完全数据(这个我们是没法知道的)，</strong>观测数据 <img src="/.io//pic/equation-20190605135345838-9714025." alt="Y"> 也被称为<strong>不完全数据(这个我们知道)，</strong> <img src="/.io//pic/equation-20190605135345835" alt="Z"> 被称为<strong>隐变量(我们不知道)</strong>，假定给定观测数据 <img src="/.io//pic/equation-20190605135345838-9714025." alt="Y"> ，其概率分布是 <img src="https://www.zhihu.com/equation?tex=P%28Y%7C%5Ctheta%29" alt="P(Y|\theta)"> ，其中 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="\theta"> 是需要估计的模型参数，那么不完全数据 <img src="/.io//pic/equation-20190605135345838-9714025." alt="Y"> 的似然函数是 <img src="https://www.zhihu.com/equation?tex=P%28Y%7C%5Ctheta%29" alt="P(Y|\theta)"> ，对数似然函数为 <img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%29%3DlogP%28Y%7C%5Ctheta%29" alt="L(\theta)=logP(Y|\theta)"> ；<img src="/.io//pic/equation-20190605135345838-9714025." alt="Y"> 和 <img src="/.io//pic/equation-20190605135345837" alt="Z "> 的联合概率分布是 <img src="https://www.zhihu.com/equation?tex=P%28Y%2CZ%7C%5Ctheta%29" alt="P(Y,Z|\theta)"> ，其对数似然函数是 <img src="https://www.zhihu.com/equation?tex=logP%28Y%2CZ%7C%5Ctheta%29" alt="logP(Y,Z|\theta)"> 。</p>
<p>EM算法是通过迭代来求 <img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%29%3DlogP%28Y%7C%5Ctheta%29" alt="L(EM/pic/equation-20190605135345863)=logP(Y|\theta)">的极大似然估计，也就是在估计出来的参数条件下，模型产生给定样本点的概率最大~。因此我们要最大化下式。</p>
<p><img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%29%3DlogP%28Y%7C%5Ctheta%29%3Dlog%5Csum_%7BZ%7DP%28Y%2CZ%7C%5Ctheta%29%3Dlog%28%5Csum_%7BZ%7DP%28Y%7CZ%2C%5Ctheta%29P%28Z%7C%5Ctheta%29%29" alt="L(EM/pic/equation-20190605135345914)=logP(Y|\theta)=log\sum_{Z}P(Y,Z|\theta)=log(\sum_{Z}P(Y|Z,\theta)P(Z|\theta))"></p>
<p><strong>上式中最右边的 <img src="https://www.zhihu.com/equation?tex=P%28Z%7C%5Ctheta%29" alt="P(EM/pic/equation-20190605135345874)"> 指的的一个模型被选择的概率， <img src="https://www.zhihu.com/equation?tex=P%28Y%7CZ%2C%5Ctheta%29" alt="P(Y|Z,\theta)"> 是指我们选定了一个模型，此模型产生这个样本点的概率</strong>。</p>
<p><img src="/.io//pic/v2-dcb858e6f913ffd51cf8219a57f96122_hd.jpg" alt="img"></p>
<p>上图是一维高斯混合分布，黄色的那个高斯分布均值为0，方差为1，被选择的概率为0.3；红色的那个高斯分布均值为3，方差为4，被选择的概率为0.7。</p>
<p>(下面部分参考《李航统计学习》P.159，推导更详细了一点)</p>
<p>EM是通过迭代的方法来逐步逼近近似极大化 <img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%29" alt="L(EM/pic/equation-20190605135345840)"> ，假设某一次我们得到了模型的参数估计值为 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%28i%29%7D" alt="\theta^{(i)}"> (是一个我们知道的值)，我们要求估计新的 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="\theta">可以时 <img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%29" alt="L(EM/pic/equation-20190605135345840)"> 增大，即 <img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta+%29%3EL%28%5Ctheta%5E%7B%28i%29%7D%29" alt="L(\theta )&gt;L(\theta^{(i)})"> 。我们计算两者的差</p>
<p><img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%29-L%28%5Ctheta%5E%7B%28i%29%7D%29%3Dlog%28%5Csum_%7BZ%7DP%28Y%7CZ%2C%5Ctheta%29P%28Z%7C%5Ctheta%29%29-logP%28Y%7C%5Ctheta%5E%7B%28i%29%7D%29" alt="L(\theta)-L(\theta^{(i)})=log(\sum_{Z}P(Y|Z,\theta)P(Z|\theta))-logP(Y|\theta^{(i)})"></p>
<p>利用Jenson不等式</p>
<p><img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%29-L%28%5Ctheta%5E%7Bi%7D%29%3Dlog%28%5Csum_%7BZ%7DP%28Y%7CZ%2C+%5Ctheta%5E%7B%28i%29%7D%29%5Cfrac%7BP%28Y%7CZ%2C%5Ctheta%29P%28Z%7C%5Ctheta%29%7D%7BP%28Y%7CZ%2C+%5Ctheta%5E%7B%28i%29%7D%29%7D%29-logP%28Y%7C%5Ctheta%5E%7B%28i%29%7D%29%5C%5C+%5Cgeq+%5Csum_%7BZ%7DP%28Z%7CY%2C+%5Ctheta%5E%7B%28i%29%7D%29log%5Cfrac%7BP%28Y%7CZ%2C%5Ctheta%29P%28Z%7C%5Ctheta%29%7D%7BP%28Z%7CY%2C+%5Ctheta%5E%7B%28i%29%7D%29%7D-logP%28Y%7C%5Ctheta%5E%7B%28i%29%7D%29%5C%5C+%3D%5Csum_%7BZ%7DP%28Z%7CY%2C+%5Ctheta%5E%7B%28i%29%7D%29log%5Cfrac%7BP%28Y%7CZ%2C%5Ctheta%29P%28Z%7C%5Ctheta%29%7D%7BP%28Z%7CY%2C+%5Ctheta%5E%7B%28i%29%7D%29%7D-logP%28Y%7C%5Ctheta%5E%7B%28i%29%7D%29%5Csum_%7BZ%7DP%28Z%7CY%2C+%5Ctheta%5E%7B%28i%29%7D%29%5C%5C+%3D%5Csum_%7BZ%7DP%28Z%7CY%2C+%5Ctheta%5E%7B%28i%29%7D%29log%5Cfrac%7BP%28Y%7CZ%2C%5Ctheta%29P%28Z%7C%5Ctheta%29%7D%7BP%28Z%7CY%2C+%5Ctheta%5E%7B%28i%29%7D%29P%28Y%7C%5Ctheta%5E%7B%28i%29%7D%29%7D" alt="L(EM/pic/equation-20190605135346171)-L(\theta^{i})=log(\sum_{Z}P(Y|Z, \theta^{(i)})\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Y|Z, \theta^{(i)})})-logP(Y|\theta^{(i)})\\ \geq \sum_{Z}P(Z|Y, \theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y, \theta^{(i)})}-logP(Y|\theta^{(i)})\\ =\sum_{Z}P(Z|Y, \theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y, \theta^{(i)})}-logP(Y|\theta^{(i)})\sum_{Z}P(Z|Y, \theta^{(i)})\\ =\sum_{Z}P(Z|Y, \theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y, \theta^{(i)})P(Y|\theta^{(i)})}"></p>
<p>第一步到第二步除了使用了Jenson不等式，还使用了 <img src="https://www.zhihu.com/equation?tex=P%28X%7CY%29%3D%5Cfrac%7BP%28Y%7CX%29P%28X%29%7D%7BP%28Y%29%7D" alt="P(EM/pic/equation-20190605135345893)=\frac{P(Y|X)P(X)}{P(Y)}"> ，其中的 <img src="https://www.zhihu.com/equation?tex=P%28X%29%2CP%28Y%29" alt="P(X),P(Y)"> 都被约去。</p>
<p>令</p>
<p><img src="https://www.zhihu.com/equation?tex=B%28%5Ctheta%2C%5Ctheta%5E%7B%28i%29%7D%29%3DL%28%5Ctheta%5E%7B%28i%29%7D%29%2B%5Csum_%7BZ%7DP%28Z%7CY%2C+%5Ctheta%5E%7B%28i%29%7D%29log%5Cfrac%7BP%28Y%7CZ%2C%5Ctheta%29P%28Z%7C%5Ctheta%29%7D%7BP%28Z%7CY%2C+%5Ctheta%5E%7B%28i%29%7D%29P%28Y%7C%5Ctheta%5E%7B%28i%29%7D%29%7D" alt="B(EM/pic/equation-20190605135346027)=L(\theta^{(i)})+\sum_{Z}P(Z|Y, \theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y, \theta^{(i)})P(Y|\theta^{(i)})}"></p>
<p>则有 <img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%29+%5Cgeq+B%28%5Ctheta%2C+%5Ctheta%5E%7B%28i%29%7D%29" alt="L(EM/pic/equation-20190605135345897) \geq B(\theta, \theta^{(i)})"> ，为了使 <img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%29" alt="L(EM/pic/equation-20190605135345840)"> 尽可能的大，我们应该选择 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%28i%2B1%29%7D" alt="\theta^{(i+1)}"> 使 <img src="https://www.zhihu.com/equation?tex=+B%28%5Ctheta%2C+%5Ctheta%5E%7B%28i%29%7D%29" alt=" B(\theta, \theta^{(i)})"> 达到极大值。即 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%28i%2B1%29%7D%3Darg%5Cmax_%7B%5Ctheta%7DB%28%5Ctheta%2C%5Ctheta%5E%7B%28i%29%7D%29" alt="\theta^{(i+1)}=arg\max_{\theta}B(\theta,\theta^{(i)})"></p>
<p>通过省去对 <img src="/.io//pic/equation-20190605135345850" alt="\theta"> 极大化是常数的项。</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%28i%2B1%29%7D%3Darg%5Cmax_%7B%5Ctheta%7D%28L%28%5Ctheta%5E%7B%28i%29%7D%29%2B%5Csum_%7BZ%7DP%28Z%7CY%2C+%5Ctheta%5E%7B%28i%29%7D%29log%5Cfrac%7BP%28Y%7CZ%2C%5Ctheta%29P%28Z%7C%5Ctheta%29%7D%7BP%28Z%7CY%2C+%5Ctheta%5E%7B%28i%29%7D%29P%28Y%7C%5Ctheta%5E%7B%28i%29%7D%29%7D%29%5C%5C+%3Darg%5Cmax_%7B%5Ctheta%7D%28%5Csum_%7BZ%7DP%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%29logP%28Y%7CZ%2C%5Ctheta%29P%28Z%7C%5Ctheta%29%29%5C%5C+%3Darg%5Cmax_%7B%5Ctheta%7D%28%5Csum_%7BZ%7DP%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%29logP%28Y%2CZ%7C%5Ctheta%29%29" alt="\theta^{(EM/pic/equation-20190605135346102)}=arg\max_{\theta}(L(\theta^{(i)})+\sum_{Z}P(Z|Y, \theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y, \theta^{(i)})P(Y|\theta^{(i)})})\\ =arg\max_{\theta}(\sum_{Z}P(Z|Y,\theta^{(i)})logP(Y|Z,\theta)P(Z|\theta))\\ =arg\max_{\theta}(\sum_{Z}P(Z|Y,\theta^{(i)})logP(Y,Z|\theta))"></p>
<p>其中 <img src="https://www.zhihu.com/equation?tex=P%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%29" alt="P(Z|Y,\theta^{(i)})"> 指的是当我们知道模型的参数和样本的分布情况时，此时隐变量的状态。如果模型为高斯分布，那么 <img src="https://www.zhihu.com/equation?tex=P%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%29" alt="P(Z|Y,\theta^{(i)})"> 指的就是样本点属于某个模型的概率； <img src="https://www.zhihu.com/equation?tex=logP%28Y%2CZ%7C%5Ctheta%29" alt="logP(Y,Z|\theta)"> 就是我们要找到 <img src="/.io//pic/equation-20190605135345850" alt="\theta"> 使得在当前 <img src="/.io//pic/equation-20190605135345859-9714025." alt="Y,Z"> 的情况下，获得函数的一个极大值。</p>
<p>EM算法的流程如下:</p>
<p>(1)随机选择参数 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%280%29%7D" alt="\theta^{(EM/pic/equation-20190605135345872)}"> ,开始迭代</p>
<p>(2)E步:计算 <img src="https://www.zhihu.com/equation?tex=Q%28%5Ctheta%2C%5Ctheta%5E%7B%28i%29%7D%29%3D%5Csum_%7BZ%7DP%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%29logP%28Y%2CZ%7C%5Ctheta%29" alt="Q(EM/pic/equation-20190605135345921)=\sum_{Z}P(Z|Y,\theta^{(i)})logP(Y,Z|\theta)"></p>
<p>(3)M步:最大化 <img src="https://www.zhihu.com/equation?tex=Q%28%5Ctheta%2C%5Ctheta%5E%7B%28i%29%7D%29" alt="Q(EM/pic/equation-20190605135345901)"></p>
<p>(4)重复(2),(3)步直到收敛</p>
<p>对高斯混合模型使用EM算法估计参数，其中第一个高斯分布均值为3，方差为4，系数为0.7；第二个高斯分布均值为0，方差为1，系数为0.3。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_mix_guassian_theta</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># u = 3, siga = 4, alpha = 0.7</span></span><br><span class="line">    f1 = <span class="number">3</span>+<span class="number">2</span>*np.random.randn(<span class="number">700</span>,<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># u = 0, siga = 1, alpha = 0.3 </span></span><br><span class="line">    f2 = <span class="number">1</span>+np.random.rand(<span class="number">300</span>,<span class="number">1</span>)</span><br><span class="line">    f = np.concatenate((f1,f2),axis=<span class="number">0</span>)</span><br><span class="line">    n,m = f.shape</span><br><span class="line"></span><br><span class="line">    alpha = np.random.rand(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">    alpha = alpha/np.sum(alpha, axis=<span class="number">1</span>)</span><br><span class="line">    u = np.random.rand(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">    siga = np.random.rand(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># EM算法求解</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">        <span class="comment">#1.E步</span></span><br><span class="line">        gamma1 = gaussian(f, u[<span class="number">0</span>][<span class="number">0</span>], siga[<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line">        gamma2 = gaussian(f, u[<span class="number">0</span>][<span class="number">1</span>], siga[<span class="number">0</span>][<span class="number">1</span>])</span><br><span class="line">        gamma = np.concatenate((gamma1, gamma2), axis = <span class="number">1</span>)</span><br><span class="line">        gamma = alpha*gamma/np.sum(alpha*gamma, axis = <span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">        <span class="comment">#2.M步</span></span><br><span class="line">        u = np.sum(gamma*f, axis = <span class="number">0</span>, keepdims=<span class="keyword">True</span>)/np.sum(gamma, axis = <span class="number">0</span>,keepdims=<span class="keyword">True</span>)</span><br><span class="line">        siga = np.sum(gamma*((f-u)**<span class="number">2</span>), axis = <span class="number">0</span>, keepdims=<span class="keyword">True</span>)/np.sum(gamma, axis = <span class="number">0</span>,keepdims=<span class="keyword">True</span>)</span><br><span class="line">        siga = siga**(<span class="number">1</span>/<span class="number">2</span>)</span><br><span class="line">        alpha = np.sum(gamma, axis = <span class="number">0</span>, keepdims=<span class="keyword">True</span>)/n</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"alpha1:  %lf,alpha2:    %lf"</span>%(alpha[<span class="number">0</span>][<span class="number">0</span>], alpha[<span class="number">0</span>][<span class="number">1</span>]))</span><br><span class="line">    print(<span class="string">"u1:  %lf,u2:    %lf"</span>%(u[<span class="number">0</span>][<span class="number">0</span>], u[<span class="number">0</span>][<span class="number">1</span>]))</span><br><span class="line">    print(<span class="string">"siga1:  %lf,siga2:    %lf"</span>%(siga[<span class="number">0</span>][<span class="number">0</span>], siga[<span class="number">0</span>][<span class="number">1</span>]))</span><br></pre></td></tr></table></figure>
<p>算法的估计值如下:</p>
<p><img src="/.io//pic/v2-c18d437f580e44cf99175d651a705650_hd.png" alt="img"></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/05/机器学习和深度学习算法理论/Transformer/Attention/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/05/机器学习和深度学习算法理论/Transformer/Attention/" class="post-title-link" itemprop="url">未命名</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-05 13:50:50 / 修改时间：13:55:00" itemprop="dateCreated datePublished" datetime="2019-06-05T13:50:50+08:00">2019-06-05</time>
            </span>
          

          
            

            
          

          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/05/机器学习和深度学习算法理论/Transformer/Attention/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/05/机器学习和深度学习算法理论/Transformer/Attention/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://kexue.fm/archives/4765" target="_blank" rel="noopener">《Attention is All You Need》浅读（简介+代码）</a></p>
<p>2017年中，有两篇类似同时也是笔者非常欣赏的论文，分别是FaceBook的《Convolutional Sequence to Sequence Learning》和Google的《Attention is All You Need》，它们都算是Seq2Seq上的创新，本质上来说，都是抛弃了RNN结构来做Seq2Seq任务。</p>
<p>这篇博文中，笔者对<a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">《Attention is All You Need》</a>做一点简单的分析。当然，这两篇论文本身就比较火，因此网上已经有很多解读了（不过很多解读都是直接翻译论文的，鲜有自己的理解），因此这里尽可能多自己的文字，尽量不重复网上各位大佬已经说过的内容。</p>
<h2 id="序列编码"><a href="#序列编码" class="headerlink" title="序列编码 #"></a>序列编码<a href="https://kexue.fm/archives/4765#序列编码" target="_blank" rel="noopener"> #</a></h2><p>深度学习做NLP的方法，基本上都是先将句子分词，然后每个词转化为对应的词向量序列。这样一来，每个句子都对应的是一个矩阵<strong><em>X</em></strong>=(<strong><em>x</em></strong>1,<strong><em>x</em></strong>2,…,<strong><em>x</em></strong>t)X=(x1,x2,…,xt)，其中<strong><em>x</em></strong>ixi都代表着第ii个词的词向量（行向量），维度为dd维，故<strong><em>X</em></strong>∈ℝn×dX∈Rn×d。这样的话，问题就变成了编码这些序列了。</p>
<p>第一个基本的思路是RNN层，RNN的方案很简单，递归式进行：</p>
<p><strong><em>y</em></strong>t=f(<strong><em>y</em></strong>t−1,<strong><em>x</em></strong>t)yt=f(yt−1,xt)</p>
<p>不管是已经被广泛使用的LSTM、GRU还是最近的SRU，都并未脱离这个递归框架。RNN结构本身比较简单，也很适合序列建模，但RNN的明显缺点之一就是无法并行，因此速度较慢，这是递归的天然缺陷。另外我个人觉得</p>
<p>RNN无法很好地学习到全局的结构信息，因为它本质是一个马尔科夫决策过程。</p>
<p>第二个思路是CNN层，其实CNN的方案也是很自然的，窗口式遍历，比如尺寸为3的卷积，就是</p>
<p><strong><em>y</em></strong>t=f(<strong><em>x</em></strong>t−1,<strong><em>x</em></strong>t,<strong><em>x</em></strong>t+1)yt=f(xt−1,xt,xt+1)</p>
<p>在FaceBook的论文中，纯粹使用卷积也完成了Seq2Seq的学习，是卷积的一个精致且极致的使用案例，热衷卷积的读者必须得好好读读这篇文论。</p>
<p>CNN方便并行，而且容易捕捉到一些全局的结构信息，笔者本身是比较偏爱CNN的，在目前的工作或竞赛模型中，我都已经尽量用CNN来代替已有的RNN模型了，并形成了自己的一套使用经验</p>
<p>，这部分我们以后再谈。</p>
<p>Google的大作提供了第三个思路：<strong>纯Attention！单靠注意力就可以！</strong>RNN要逐步递归才能获得全局信息，因此一般要双向RNN才比较好；CNN事实上只能获取局部信息，是通过层叠来增大感受野；Attention的思路最为粗暴，它一步到位获取了全局信息！它的解决方案是：</p>
<p><strong><em>y</em></strong>t=f(<strong><em>x</em></strong>t,<strong><em>A</em></strong>,<strong><em>B</em></strong>)yt=f(xt,A,B)</p>
<p>其中</p>
<p><strong><em>A</em></strong>,<strong><em>B</em></strong>A,B</p>
<p>是另外一个序列（矩阵）。如果都取</p>
<p><strong><em>A</em></strong>=<strong><em>B</em></strong>=<strong><em>X</em></strong>A=B=X</p>
<p>，那么就称为Self Attention，</p>
<p>它的意思是直接将<strong><em>x</em></strong>txt与原来的每个词进行比较，最后算出<strong><em>y</em></strong>tyt</p>
<p>！</p>
<h2 id="Attention层"><a href="#Attention层" class="headerlink" title="Attention层 #"></a>Attention层<a href="https://kexue.fm/archives/4765#Attention层" target="_blank" rel="noopener"> #</a></h2>
          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/04/机器学习和深度学习算法理论/优质git资源/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/04/机器学习和深度学习算法理论/优质git资源/" class="post-title-link" itemprop="url">未命名</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-04 13:00:50 / 修改时间：13:01:34" itemprop="dateCreated datePublished" datetime="2019-06-04T13:00:50+08:00">2019-06-04</time>
            </span>
          

          
            

            
          

          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/04/机器学习和深度学习算法理论/优质git资源/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/04/机器学习和深度学习算法理论/优质git资源/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="AI-Learning"><a href="#AI-Learning" class="headerlink" title="AI Learning"></a>AI Learning</h1><p>汇集了30多名贡献者的集体智慧，把学习机器学习的路线图、视频、电子书、学习建议等中文资料全部都整理好了。</p>
<p><a href="https://github.com/apachecn/AiLearning" target="_blank" rel="noopener">https://github.com/apachecn/AiLearning</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/04/深度学习笔记/Pytorch训练营/D2 序列化模型，torch接口/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/04/深度学习笔记/Pytorch训练营/D2 序列化模型，torch接口/" class="post-title-link" itemprop="url">未命名</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-04 09:22:46" itemprop="dateCreated datePublished" datetime="2019-06-04T09:22:46+08:00">2019-06-04</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-08 11:57:16" itemprop="dateModified" datetime="2019-06-08T11:57:16+08:00">2019-06-08</time>
              </span>
            
          

          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/04/深度学习笔记/Pytorch训练营/D2 序列化模型，torch接口/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/04/深度学习笔记/Pytorch训练营/D2 序列化模型，torch接口/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong>任务：Pytorch官方文档（参考资料：Pytorch官方文档 5.序列化模型；6.torch接口）</strong></p>
<p><strong>任务简介</strong>：《Pytorch官方文档》</p>
<p><strong>学习时长：</strong>6/4</p>
<p><strong>详细说明</strong>：</p>
<p>本节任务资料包下载：</p>
<p>链接：<a href="https://pan.baidu.com/s/1U7KBXRv4V5rkvMjWQqULnw" target="_blank" rel="noopener">https://pan.baidu.com/s/1U7KBXRv4V5rkvMjWQqULnw</a> </p>
<p>提取码：0b3u </p>
<p>本节内容包括如何保存和载入模型，我们一般情况下载训练阶段保存在预测阶段载入，同时需要了解两种方法保存模型的差异。下面是pytorch最重要的部分，对Tensor的操作，由于本节内容较多，我们分为七个部分讲解，今天主要是熟悉torch接口中1~9页的API,能够知道如何使用，知道每个函数的意义和参数的意义。</p>
<p><strong>作业名称（详解）：</strong>保存模型的两种形式以及他们的区别？手敲今天所学API三遍</p>
<p><strong>作业提交形式</strong>：打卡提交文字或图片，不少于20字</p>
<p><strong>打卡截止时间：</strong>6/5</p>
<h1 id="1、保存模型的两种形式以及他们的区别？"><a href="#1、保存模型的两种形式以及他们的区别？" class="headerlink" title="1、保存模型的两种形式以及他们的区别？"></a>1、保存模型的两种形式以及他们的区别？</h1><p>第一种，只保存和加载模型参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.save(the_model.state_dict(), PATH)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载时</span></span><br><span class="line">the_model = TheModelClass(*args, **kwargs)</span><br><span class="line">the_model.load_state_dict(torch.load(PATH))</span><br></pre></td></tr></table></figure>
<p>第二种，保存和加载整个模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 没有state_dict</span></span><br><span class="line">torch.save(the_model, PATH)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载时</span></span><br><span class="line">the_model = torch.load(PATH)</span><br></pre></td></tr></table></figure>
<p>第二种的不足是，序列化的数据被绑定到特定的类和固定的目录结构。缺少灵活性，可能有各种break的隐患。</p>
<h1 id="2、Pytorch各种API"><a href="#2、Pytorch各种API" class="headerlink" title="2、Pytorch各种API"></a>2、Pytorch各种API</h1><h2 id="张量操作"><a href="#张量操作" class="headerlink" title="张量操作"></a>张量操作</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 若obj是一个pytorch的tensor，则返回true</span></span><br><span class="line">torch.is_tensor(obj)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 判断obj是否是storage</span></span><br><span class="line"><span class="comment"># torch.Storage是单个数据类型的连续的一维数组，每个torch.Tensor都具有相同数据类型的相应存储。他是torch.tensor底层数据结构,他除了像Tensor一样定义数值，还可以直接把文件映射到内存中进行操作</span></span><br><span class="line">torch.is_storage(obj)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置默认的tensor的数据类型</span></span><br><span class="line">torch.set_default_tensor_type(torch.DoubleTensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回input张量中的元素个数</span></span><br><span class="line">torch.numel(input) -&gt; int</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置打印选项，与numpy一致</span></span><br><span class="line">torch.set_printoptions(precision=<span class="keyword">None</span>, threshold=<span class="keyword">None</span>, edgeitems=<span class="keyword">None</span>, linewidth=<span class="keyword">None</span>, profile=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<h2 id="创建操作Creation-Ops"><a href="#创建操作Creation-Ops" class="headerlink" title="创建操作Creation Ops"></a>创建操作Creation Ops</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 返回一个二维张量，对角线都是1，其他是0</span></span><br><span class="line">torch.eye(n, m=<span class="keyword">None</span>, out=<span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用一个值填充tensor</span></span><br><span class="line">end = torch.Tensor(<span class="number">4</span>).fill_(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string"><span class="meta">&gt;&gt;&gt; </span>end</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> 10</span></span><br><span class="line"><span class="string"> 10</span></span><br><span class="line"><span class="string"> 10</span></span><br><span class="line"><span class="string"> 10</span></span><br><span class="line"><span class="string">[torch.FloatTensor of size 4]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将numpy.ndarray转为tensor，共享同一内存空间，修改一个会导致修改另一个。</span></span><br><span class="line">torch.from_numpy(ndarray) -&gt; Tensor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回一维张量，包含在start和end上均匀间隔的steps个点, out是结果张量</span></span><br><span class="line">torch.linspace(start, end, steps=100, out=None) -&gt; Tensor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回一维张量，包含在区间10^start和10^end上以对数刻度均匀间隔的steps个点。</span></span><br><span class="line">torch.logspace(start, end, steps=100, out=None) -&gt; Tensor</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.logspace(start=<span class="number">-10</span>, end=<span class="number">10</span>, steps=<span class="number">5</span>)</span><br><span class="line">        <span class="number">1.0000e-10</span></span><br><span class="line">        <span class="number">1.0000e-05</span></span><br><span class="line">        <span class="number">1.0000e+00</span></span><br><span class="line">        <span class="number">1.0000e+05</span></span><br><span class="line">        <span class="number">1.0000e+10</span></span><br><span class="line">       [torch.FloatTensor of size <span class="number">5</span>]</span><br><span class="line">      </span><br><span class="line"><span class="comment"># 返回一个全为1的张量，形状由可变参数sizes定义</span></span><br><span class="line">torch.ones(*size, out=None) -&gt; Tensor</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.ones(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="number">111</span></span><br><span class="line"><span class="number">111</span></span><br><span class="line">[torch.FloatTensor of size <span class="number">2</span>x3]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从区间(0,1)的均匀分布中抽取一组随机数</span></span><br><span class="line">torch.rand(*sizes, out=None) -&gt; Tensor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面函数也可以实现这个效果</span></span><br><span class="line">torch.Tensor(<span class="number">3</span>, <span class="number">3</span>).uniform_(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 等同于</span></span><br><span class="line">torch.rand(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从标准正态分布（均值为0，方差为1，即高斯白噪声）中抽取一组随机数。</span></span><br><span class="line">torch.randn(*size, out=None) -&gt; Tensor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 给定参数n，返回一个从0到n-1的随机整数排列</span></span><br><span class="line">torch.randperm(n, out=None) -&gt; LongTensor</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.randperm(<span class="number">4</span>)</span><br><span class="line"><span class="number">2</span> <span class="number">1</span> <span class="number">3</span> <span class="number">0</span></span><br><span class="line">[torch.LongTensor of size <span class="number">4</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回一维张量，长度为floor((end-start)/step)。包含从start到end，以step为步长的</span></span><br><span class="line">torch.arange(start, end, step=1, out=None) -&gt; Tensor</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.arange(<span class="number">1</span>, <span class="number">2.5</span>, <span class="number">0.5</span>)</span><br><span class="line"> <span class="number">1.0000</span></span><br><span class="line"> <span class="number">1.5000</span></span><br><span class="line"> <span class="number">2.0000</span></span><br><span class="line">[torch.FloatTensor of size <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 有floor((end-start)/step)+1个元素，包含在[start, end)</span></span><br><span class="line"><span class="comment"># WARNING: 建议使用函数torch.arange</span></span><br><span class="line">torch.range(start, end, step=1, out=None) -&gt; Tensor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回全为标量0的张量</span></span><br><span class="line">torch.zeros(*size, out=<span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.zeros(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="number">000</span></span><br><span class="line"><span class="number">000</span></span><br><span class="line">[torch.FloatTensor of size <span class="number">2</span>x3]</span><br></pre></td></tr></table></figure>
<h2 id="数学操作"><a href="#数学操作" class="headerlink" title="数学操作"></a>数学操作</h2><h3 id="torch-log1p"><a href="#torch-log1p" class="headerlink" title="torch.log1p"></a>torch.log1p</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.log1p(input, out=None) → Tensor</span><br></pre></td></tr></table></figure>
<p>计算 input+1input+1的自然对数 yi=log(xi+1)yi=log(xi+1)</p>
<p>注意：对值比较小的输入，此函数比<code>torch.log()</code>更准确。</p>
<p><a href="https://blog.csdn.net/qq_36523839/article/details/82422865" target="_blank" rel="noopener">https://blog.csdn.net/qq_36523839/article/details/82422865</a></p>
<p>优点：</p>
<ul>
<li><p>在数据预处理时首先可以对偏度比较大的数据用log1p函数进行转化，使其更加服从高斯分布，此步处理可能会使我们后续的分类结果得到一个更好的结果；</p>
</li>
<li><p>平滑处理很容易被忽略掉，导致模型的结果总是达不到一定的标准，同样使用逼格更高的log1p能避免复值得问题——复值指一个自变量对应多个因变量；</p>
</li>
</ul>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/04/深度学习笔记/Pytorch笔记/函数学习/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/04/深度学习笔记/Pytorch笔记/函数学习/" class="post-title-link" itemprop="url">未命名</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-04 08:30:40 / 修改时间：08:30:52" itemprop="dateCreated datePublished" datetime="2019-06-04T08:30:40+08:00">2019-06-04</time>
            </span>
          

          
            

            
          

          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/04/深度学习笔记/Pytorch笔记/函数学习/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/04/深度学习笔记/Pytorch笔记/函数学习/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>torch.ones</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/03/总结与思考/如何向客户描述产品/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/03/总结与思考/如何向客户描述产品/" class="post-title-link" itemprop="url">未命名</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-03 22:49:31 / 修改时间：22:49:48" itemprop="dateCreated datePublished" datetime="2019-06-03T22:49:31+08:00">2019-06-03</time>
            </span>
          

          
            

            
          

          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/03/总结与思考/如何向客户描述产品/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/03/总结与思考/如何向客户描述产品/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>对于一个系统，如何从业务的角度去描述它，能讲一个故事让用户听懂，让用户感兴趣，而不是从模型到模型。</p>
<p>—首先，这个系统服务于什么领域</p>
<p>​    目前这个领域存在什么问题，</p>
<p>​    有哪些因素影响到这些问题，</p>
<p>​    我们的系统能够做到什么。</p>
<p>这样了解业务逻辑。自己清楚，也能跟别人讲。</p>
<p>了解不同类型的客户，知道这些客户关注的是什么问题。吃透行业标准。总结项目中吸收到的新的知识和方法。</p>
<p>跟客户谈的时候：</p>
<p>—先说当前存在什么问题</p>
<p>​    我们有什么方法；</p>
<p>​    能支持，能推动，能帮你们，能做到……；</p>
<p>不能让客户觉得系统就是个模型的入口跟数据的出口。</p>
<p>对于项目中不确定的模糊的问题，要尽快跟客户商定，不能成为项目隐藏的风险！</p>
<p>跟客户探口风，把握需求的上限。哪些功能要做，哪些不要做，要有理有据。</p>
<p>思考客户提出需求的深层次原因，比如说领导重视。</p>
<p>沟通时，如想表达这个功能可以实现，但是因为钱不够不想做，可以说：这个系统的实现上，重点和难点在于XX。我们可以花工作量在XX功能，但是这会对最关键的内容产生影响。如果能给我们更多一些时间和预算，我们可以保质保量把这个新提出的功能做好。</p>
<p>同样的预算，要让不断提出新需求的客户认识到，做A还是做B，不能盲目接受客户需求，也不能因为合同没写或钱不够而直接拒绝。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/03/工具和环境/colab使用/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/03/工具和环境/colab使用/" class="post-title-link" itemprop="url">未命名</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-03 07:59:37" itemprop="dateCreated datePublished" datetime="2019-06-03T07:59:37+08:00">2019-06-03</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-12 09:09:00" itemprop="dateModified" datetime="2019-06-12T09:09:00+08:00">2019-06-12</time>
              </span>
            
          

          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/03/工具和环境/colab使用/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/03/工具和环境/colab使用/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="挂载google-driver"><a href="#挂载google-driver" class="headerlink" title="挂载google driver"></a>挂载google driver</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from google.colab import drive</span><br><span class="line">drive.mount(&apos;/content/gdrive&apos;)</span><br></pre></td></tr></table></figure>
<p>运行之后：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&amp;redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&amp;scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&amp;response_type=code</span><br><span class="line"></span><br><span class="line">Enter your authorization code:</span><br></pre></td></tr></table></figure>
<p>登录后获得授权码，填写后</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Mounted at /content/gdrive</span><br></pre></td></tr></table></figure>
<p>桌面版工具</p>
<p><a href="https://www.insynchq.com/" target="_blank" rel="noopener">https://www.insynchq.com/</a></p>
<h1 id="跑fast-ai"><a href="#跑fast-ai" class="headerlink" title="跑fast.ai"></a>跑fast.ai</h1><p><a href="https://segmentfault.com/a/1190000018580340?utm_source=tag-newest" target="_blank" rel="noopener">https://segmentfault.com/a/1190000018580340?utm_source=tag-newest</a></p>
<p>fast.ai B站教程<a href="https://www.bilibili.com/video/av42079986?from=search&amp;seid=1436832162873131397" target="_blank" rel="noopener">https://www.bilibili.com/video/av42079986?from=search&amp;seid=1436832162873131397</a></p>
<h1 id="公众号文章-colab汇总"><a href="#公众号文章-colab汇总" class="headerlink" title="公众号文章-colab汇总"></a>公众号文章-colab汇总</h1><p><strong>3 Colab的缺点？</strong></p>
<p>12小时连续连接的限制（可以用 暂存深度学习权重的方法来解决）；</p>
<p>需要科学上网，请查看KK大佬编写的视频教材</p>
<p>“如何优雅地使用Google系产品？”</p>
<p><a href="https://www.bilibili.com/video/av43034822?from=search&amp;seid=14859222855758483769" target="_blank" rel="noopener">https://www.bilibili.com/video/av43034822?from=search&amp;seid=14859222855758483769</a></p>
<p>或者  更简单的方法  如果你有朋友已经搭建好了  你可以蹭一下^_^</p>
<p><strong>4 Colab中 如何装包？</strong></p>
<p>!pip</p>
<p><strong>5 如何导入数据？</strong></p>
<p>使用Google Drive,方法见:</p>
<p><a href="https://colab.research.google.com/notebooks/io.ipynb" target="_blank" rel="noopener">https://colab.research.google.com/notebooks/io.ipynb</a></p>
<p>用！wget直接从数据源读入；</p>
<p>用!git clone直接从Github读取整个项目文件（这个方法用于学习或者测试  特别赞）</p>
<p>提示： Google Drive只有15G  不适合放置大量数据，我的经验是：</p>
<p>数据从源文件处直接用!wget读取，但是将程序文件 和结果文件保存在网盘（特别是前面提到的 暂存的中间权重数据，以便后期恢复继续执行的文件  都要存入网盘 ，否则12小时限制到了  这些数据就没有了）</p>
<p><strong>6 能否直接在Colab执行.py文件？</strong></p>
<p>可以   用!run 或者!python3</p>
<p>但是要注意  这样执行文件如果设计到模块的调用  非常容易出错</p>
<p>另外  在Colab中调用.py文件中定义的模块  也是很方便的</p>
<p><strong>7 如果12小时  无法训练完整的结果，如何暂存结果  并在重新连接服务器后继续恢复执行？</strong></p>
<p>见</p>
<h2 id="如何在TensorFlow中保存和恢复深度学习模型训练"><a href="#如何在TensorFlow中保存和恢复深度学习模型训练" class="headerlink" title="如何在TensorFlow中保存和恢复深度学习模型训练"></a><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjExMDc0Ng==&amp;mid=2247484034&amp;idx=1&amp;sn=89c7c8dcdfa3771fc262f1ebfbffef02&amp;chksm=cef50d77f9828461bf8978918a65eb85d12edc93fb53cff03cd1c9229f6baee74c11f55808ee&amp;token=456076747&amp;lang=zh_CN&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">如何在TensorFlow中保存和恢复深度学习模型训练</a></h2><p><strong>8 能否跑比较大的数据集？</strong></p>
<p>测试过wikidata,这个数据集  对于学习阶段来说  算比较大的数据了</p>
<p>见</p>
<h2 id="在Google-Colab复现一篇EMNLP-2017会议论文中的源代码"><a href="#在Google-Colab复现一篇EMNLP-2017会议论文中的源代码" class="headerlink" title="在Google Colab复现一篇EMNLP 2017会议论文中的源代码"></a><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjExMDc0Ng==&amp;mid=2247483681&amp;idx=1&amp;sn=77ea7b2d334c1755a4d04424973a3d76&amp;chksm=cef50ed4f98287c23a8a55424e19c2c58dc7528db249bff4d2d7c89a17e289dc71285e5260a6&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">在Google Colab复现一篇EMNLP 2017会议论文中的源代码</a></h2><p>当然 这个程序无法在12小时内完成  需要结合前面提到的暂存权重参数  重新恢复训练的方式；</p>
<p>个人觉得  学习阶段的使用Google Colab完全可以满足需求</p>
<p><strong>9 Colab对于知名公开课的支持有哪些？</strong></p>
<p>见</p>
<h2 id="怎样用Google-Colab完成Stanford-CS231n的作业1"><a href="#怎样用Google-Colab完成Stanford-CS231n的作业1" class="headerlink" title="怎样用Google Colab完成Stanford CS231n的作业1"></a><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjExMDc0Ng==&amp;mid=2247483662&amp;idx=1&amp;sn=25c41e01928881cb45b101226ae47041&amp;chksm=cef50efbf98287ed7e803a57041a330c478cb01ab580e6f6cda4cc2d25e7a4b2014a89f9733f&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">怎样用Google Colab完成Stanford CS231n的作业1</a></h2><h2 id="怎样用Google-Colab完成Stanford-CS224n的作业"><a href="#怎样用Google-Colab完成Stanford-CS224n的作业" class="headerlink" title="怎样用Google Colab完成Stanford CS224n的作业"></a><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjExMDc0Ng==&amp;mid=2247483674&amp;idx=1&amp;sn=dbc0aded4b20ee5683b45d1b8e56ba0d&amp;chksm=cef50eeff98287f9aba8d03b1bd2dbfd6a20975cb9597c132722f7dcaf516ca8131954ef30d3&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">怎样用Google Colab完成Stanford CS224n的作业</a></h2><h2 id="吴恩达老师又又又出新课了，这次是TensorFlow"><a href="#吴恩达老师又又又出新课了，这次是TensorFlow" class="headerlink" title="吴恩达老师又又又出新课了，这次是TensorFlow"></a><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjExMDc0Ng==&amp;mid=2247483856&amp;idx=1&amp;sn=15f7f8a9ba6b94908f1a82b36b3ae103&amp;chksm=cef50e25f9828733bd6235808a255d97c5e1d5fcb8e5b2481fe33d51e58103859b13df27fae9&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">吴恩达老师又又又出新课了，这次是TensorFlow</a></h2><h2 id="如何从0开始-在Google-Colab上训练一个图片分类的卷积神经网络"><a href="#如何从0开始-在Google-Colab上训练一个图片分类的卷积神经网络" class="headerlink" title="如何从0开始 在Google Colab上训练一个图片分类的卷积神经网络"></a><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjExMDc0Ng==&amp;mid=2247483868&amp;idx=1&amp;sn=cf9cccdfccf4d623604e9890c966905d&amp;chksm=cef50e29f982873fe4e36c5361eaddc6f7d4bb1c730a3f9e0724b3e1ffdf4282cd5c8d7a9c01&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">如何从0开始 在Google Colab上训练一个图片分类的卷积神经网络</a></h2><h2 id="TensorFlow图片分类实战（吴恩达TensorFlow系列新课第一课完结篇）"><a href="#TensorFlow图片分类实战（吴恩达TensorFlow系列新课第一课完结篇）" class="headerlink" title="TensorFlow图片分类实战（吴恩达TensorFlow系列新课第一课完结篇）"></a><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjExMDc0Ng==&amp;mid=2247483878&amp;idx=1&amp;sn=eeef08cd9caefa1787978e62f8b4b55c&amp;chksm=cef50e13f9828705c03fa905e96a236ad6752ecbf731f0d125266547e12817085d8104d2b63a&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">TensorFlow图片分类实战（吴恩达TensorFlow系列新课第一课完结篇）</a></h2><p><strong>10 Colab对于知名教材的支持？</strong></p>
<p>见</p>
<h2 id="在Google-Colab中测试李航《统计学习方法》Python代码"><a href="#在Google-Colab中测试李航《统计学习方法》Python代码" class="headerlink" title="在Google Colab中测试李航《统计学习方法》Python代码"></a><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjExMDc0Ng==&amp;mid=2247483692&amp;idx=1&amp;sn=ccee84ccc7d0882279f82cf7d395685c&amp;chksm=cef50ed9f98287cff587ab1e18d72a6c792ebed5a8c6dcaab0532b5cbf66d8428772d0a392c5&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">在Google Colab中测试李航《统计学习方法》Python代码</a></h2><h2 id="怎样用Google-Colab完成周志华西瓜书的习题"><a href="#怎样用Google-Colab完成周志华西瓜书的习题" class="headerlink" title="怎样用Google Colab完成周志华西瓜书的习题"></a><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjExMDc0Ng==&amp;mid=2247483712&amp;idx=1&amp;sn=ad199c99f8b205f08a260ec4c6fe5647&amp;chksm=cef50eb5f98287a32a60594021bb8f90c043b8f1aaf0d7e421e686ce6d5fe7f657f90d403f4c&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">怎样用Google Colab完成周志华西瓜书的习题</a></h2><p><strong>11 Colab也可以跑竞赛的数据？</strong></p>
<p>见</p>
<h2 id="如何通过参加-数据分析竞赛-来提高算法水平（以”达观杯”-文本挖掘比赛为例）"><a href="#如何通过参加-数据分析竞赛-来提高算法水平（以”达观杯”-文本挖掘比赛为例）" class="headerlink" title="如何通过参加 数据分析竞赛 来提高算法水平（以”达观杯” 文本挖掘比赛为例）"></a><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjExMDc0Ng==&amp;mid=2247483723&amp;idx=1&amp;sn=a63f45bd3715f99b5451e3c2f38a0c67&amp;chksm=cef50ebef98287a8fe6ee1565f728ea465fccf6bb69e72df0921a0308dd667a76ba77ec1e931&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">如何通过参加 数据分析竞赛 来提高算法水平（以”达观杯” 文本挖掘比赛为例）</a></h2><p><strong>12 Colab支持TensorFlow2.0吗？</strong></p>
<p>见</p>
<h2 id="用Google-Colab尝鲜测试TensorFlow-2-0"><a href="#用Google-Colab尝鲜测试TensorFlow-2-0" class="headerlink" title="用Google Colab尝鲜测试TensorFlow 2.0"></a><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjExMDc0Ng==&amp;mid=2247483767&amp;idx=1&amp;sn=59cf407f03eb0306d1570f13427c5367&amp;chksm=cef50e82f98287943717239c372369ea738704381ecc43f05083a853ddf40d9f52db953130a8&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">用Google Colab尝鲜测试TensorFlow 2.0</a></h2><p><strong>13 也可以用Colab进行Python的基本学习吗？</strong></p>
<p>可以  如用Colab学习Matploylib库  见</p>
<h2 id="用matplotlib可视化初步"><a href="#用matplotlib可视化初步" class="headerlink" title="用matplotlib可视化初步"></a><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjExMDc0Ng==&amp;mid=2247483776&amp;idx=1&amp;sn=ad57f82d4e01589974a56a8af9f35fca&amp;chksm=cef50e75f9828763ab8f4acb67c9e2942f973dc3ab1d44a6bb620f6242d5fed487797db4b62f&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">用matplotlib可视化初步</a></h2><p><strong>14 Colab支持GAN吗？</strong></p>
<p>见</p>
<h2 id="如何在Google-Colab运行你的第一个GAN模型"><a href="#如何在Google-Colab运行你的第一个GAN模型" class="headerlink" title="如何在Google Colab运行你的第一个GAN模型"></a><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MjExMDc0Ng==&amp;mid=2247483910&amp;idx=1&amp;sn=291de5debbc97b9a6635da6af726ad9c&amp;chksm=cef50df3f98284e5e4011349e3d642d1a243be0d013cf7f6b9a180cfa3a0e3bf5c4bea564019&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">如何在Google Colab运行你的第一个GAN模型</a></h2><p>这里只测试了最简单的GAN模型，Colab也支持更加复杂的GAN模型训练，如</p>
<p><a href="https://github.com/shaoanlu/faceswap-GAN" target="_blank" rel="noopener">https://github.com/shaoanlu/faceswap-GAN</a></p>
<p><strong>15 如何搜索支持Colab的源代码？</strong></p>
<p>Github,Seedbank项目</p>
<p>jupyter notebook格式的均支持</p>
<p><strong>上传并使用数据文件</strong></p>
<p>我们一般都需要在 Colab 笔记本中使用数据，对吧？你可以使用 wget 之类的工具从网络上获取数据，但是如果你有一些本地文件，想上传到你的谷歌硬盘中的 Colab 环境里并使用它们，该怎么做呢？</p>
<p>很简单，只需 3 步即可实现！</p>
<p>首先使用以下命令调用笔记本中的文件选择器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> files</span><br><span class="line">uploaded = files.upload()</span><br></pre></td></tr></table></figure>
<p>运行之后，我们就会发现单元 cell 下出现了“选择文件”按钮：</p>
<p><img src="/.io//pic/v2-740d65c0b367aa4ecb9c69ed13f70b04_hd.jpg" alt="img"></p>
<p>这样就可以直接选择你想上传的文件啦！</p>
<p>选择文件后，使用以下迭代方法上传文件以查找其键名，命令如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> fn <span class="keyword">in</span> uploaded.keys():</span><br><span class="line"> print(<span class="string">'User uploaded file "&#123;name&#125;" with length &#123;length&#125; bytes'</span>.format(name=fn, length=len(uploaded[fn])))</span><br></pre></td></tr></table></figure>
<p>例如待上传的是 iris.csv 文件，若运行没有问题的话，应该出现类似下面的提示语句：</p>
<blockquote>
<p>User uploaded file “iris.csv” with length 3716 bytes</p>
</blockquote>
<p>最后，就使用以下命令将文件的内容加载到 Pandas 的 DataFrame 中了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> io</span><br><span class="line">df = pd.read_csv(io.StringIO(uploaded[<span class="string">'iris.csv'</span>].decode(<span class="string">'utf-8'</span>)))</span><br><span class="line">print(df)</span><br></pre></td></tr></table></figure>
          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/02/深度学习笔记/Pytorch笔记/Autograd/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/02/深度学习笔记/Pytorch笔记/Autograd/" class="post-title-link" itemprop="url">未命名</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-02 22:10:58 / 修改时间：22:20:53" itemprop="dateCreated datePublished" datetime="2019-06-02T22:10:58+08:00">2019-06-02</time>
            </span>
          

          
            

            
          

          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/02/深度学习笔记/Pytorch笔记/Autograd/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/02/深度学习笔记/Pytorch笔记/Autograd/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://blog.csdn.net/g11d111/article/details/83035270" target="_blank" rel="noopener">PyTorch学习笔记(12)——PyTorch中的Autograd机制介绍</a></p>
<p><a href="https://blog.csdn.net/MiaoB226/article/details/88934561" target="_blank" rel="noopener">Pytorch从入门到放弃（5）——取消测试与验证阶段的梯度</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/02/深度学习笔记/Pytorch莫烦/建造第一个神经网络/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/02/深度学习笔记/Pytorch莫烦/建造第一个神经网络/" class="post-title-link" itemprop="url">未命名</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-02 16:17:50" itemprop="dateCreated datePublished" datetime="2019-06-02T16:17:50+08:00">2019-06-02</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-04 08:25:49" itemprop="dateModified" datetime="2019-06-04T08:25:49+08:00">2019-06-04</time>
              </span>
            
          

          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/02/深度学习笔记/Pytorch莫烦/建造第一个神经网络/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/02/深度学习笔记/Pytorch莫烦/建造第一个神经网络/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://morvanzhou.github.io/tutorials/machine-learning/torch/3-01-regression/" target="_blank" rel="noopener">https://morvanzhou.github.io/tutorials/machine-learning/torch/3-01-regression/</a></p>
<h1 id="关系拟合-回归"><a href="#关系拟合-回归" class="headerlink" title="关系拟合 (回归)"></a>关系拟合 (回归)</h1><h2 id="要点"><a href="#要点" class="headerlink" title="要点"></a>要点</h2><p>我会这次会来见证神经网络是如何通过简单的形式将一群数据用一条线条来表示. 或者说, 是如何在数据当中找到他们的关系, 然后用神经网络模型来建立一个可以代表他们关系的线条.</p>
<h2 id="建立数据集"><a href="#建立数据集" class="headerlink" title="建立数据集"></a>建立数据集</h2><p>我们创建一些假数据来模拟真实的情况. 比如一个一元二次函数: <code>y = a * x^2 + b</code>, 我们给 <code>y</code> 数据加上一点噪声来更加真实的展示它.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">x = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1)  # x data (tensor), shape=(100, 1)</span><br><span class="line">y = x.pow(2) + 0.2*torch.rand(x.size())                 # noisy y data (tensor), shape=(100, 1)</span><br><span class="line"></span><br><span class="line"># 画图</span><br><span class="line">plt.scatter(x.data.numpy(), y.data.numpy())</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="建立神经网络"><a href="#建立神经网络" class="headerlink" title="建立神经网络"></a>建立神经网络</h2><p>建立一个神经网络我们可以直接运用 torch 中的体系. 先定义所有的层属性(<code>__init__()</code>), 然后再一层层搭建(<code>forward(x)</code>)层于层的关系链接. 建立关系的时候, 我们会用到激励函数, 如果还不清楚激励函数用途的同学, 这里有非常好的<a href="https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/3-04-activation-function/" target="_blank" rel="noopener">一篇动画教程</a>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F     <span class="comment"># 激励函数都在这</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(torch.nn.Module)</span>:</span>  <span class="comment"># 继承 torch 的 Module</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_feature, n_hidden, n_output)</span>:</span></span><br><span class="line">        super(Net, self).__init__()     <span class="comment"># 继承 __init__ 功能</span></span><br><span class="line">        <span class="comment"># 定义每层用什么样的形式</span></span><br><span class="line">        self.hidden = torch.nn.Linear(n_feature, n_hidden)   <span class="comment"># 隐藏层线性输出</span></span><br><span class="line">        self.predict = torch.nn.Linear(n_hidden, n_output)   <span class="comment"># 输出层线性输出</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span>   <span class="comment"># 这同时也是 Module 中的 forward 功能</span></span><br><span class="line">        <span class="comment"># 正向传播输入值, 神经网络分析出输出值</span></span><br><span class="line">        x = F.relu(self.hidden(x))      <span class="comment"># 激励函数(隐藏层的线性值)</span></span><br><span class="line">        x = self.predict(x)             <span class="comment"># 输出值</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net = Net(n_feature=<span class="number">1</span>, n_hidden=<span class="number">10</span>, n_output=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">print(net)  <span class="comment"># net 的结构</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Net (</span></span><br><span class="line"><span class="string">  (hidden): Linear (1 -&gt; 10)</span></span><br><span class="line"><span class="string">  (predict): Linear (10 -&gt; 1)</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<h2 id="训练网络"><a href="#训练网络" class="headerlink" title="训练网络"></a>训练网络</h2><p>训练的步骤很简单, 如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># optimizer 是训练的工具</span></span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.2</span>)  <span class="comment"># 传入 net 的所有参数, 学习率</span></span><br><span class="line">loss_func = torch.nn.MSELoss()      <span class="comment"># 预测值和真实值的误差计算公式 (均方差)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    prediction = net(x)     <span class="comment"># 喂给 net 训练数据 x, 输出预测值</span></span><br><span class="line"></span><br><span class="line">    loss = loss_func(prediction, y)     <span class="comment"># 计算两者的误差</span></span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()   <span class="comment"># 清空上一步的残余更新参数值</span></span><br><span class="line">    loss.backward()         <span class="comment"># 误差反向传播, 计算参数更新值</span></span><br><span class="line">    optimizer.step()        <span class="comment"># 将参数更新值施加到 net 的 parameters 上</span></span><br></pre></td></tr></table></figure>
<h2 id="可视化训练过程"><a href="#可视化训练过程" class="headerlink" title="可视化训练过程"></a>可视化训练过程</h2><p>为了可视化整个训练的过程, 更好的理解是如何训练, 我们如下操作:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.ion()   <span class="comment"># 画图</span></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">200</span>):</span><br><span class="line">    prediction = net(x)     <span class="comment"># 喂给 net 训练数据 x, 输出预测值</span></span><br><span class="line">    loss = loss_func(prediction, y)     <span class="comment"># 计算两者的误差</span></span><br><span class="line">    optimizer.zero_grad()   <span class="comment"># 清空上一步的残余更新参数值</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 接着上面来</span></span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># plot and show learning process</span></span><br><span class="line">        plt.cla()</span><br><span class="line">        plt.scatter(x.data.numpy(), y.data.numpy())</span><br><span class="line">        plt.plot(x.data.numpy(), prediction.data.numpy(), <span class="string">'r-'</span>, lw=<span class="number">5</span>)</span><br><span class="line">        plt.text(<span class="number">0.5</span>, <span class="number">0</span>, <span class="string">'Loss=%.4f'</span> % loss.data.numpy(), fontdict=&#123;<span class="string">'size'</span>: <span class="number">20</span>, <span class="string">'color'</span>:  <span class="string">'red'</span>&#125;)</span><br><span class="line">        plt.pause(<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<h1 id="区分类型"><a href="#区分类型" class="headerlink" title="区分类型"></a>区分类型</h1><h2 id="要点-1"><a href="#要点-1" class="headerlink" title="要点"></a>要点</h2><p>这次我们也是用最简单的途径来看看神经网络是怎么进行事物的分类.</p>
<p><a href="https://morvanzhou.github.io/static/results/torch/1-1-3.gif" target="_blank" rel="noopener"><img src="https://morvanzhou.github.io/static/results/torch/1-1-3.gif" alt="区分类型 (建造第一个神经网络/pic/1-1-3.gif)"></a></p>
<h2 id="建立数据集-1"><a href="#建立数据集-1" class="headerlink" title="建立数据集"></a>建立数据集</h2><p>我们创建一些假数据来模拟真实的情况. 比如两个二次分布的数据, 不过他们的均值都不一样.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># torch.manual_seed(1)    # reproducible</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># make fake data</span></span><br><span class="line">n_data = torch.ones(<span class="number">100</span>, <span class="number">2</span>)</span><br><span class="line">x0 = torch.normal(<span class="number">2</span>*n_data, <span class="number">1</span>)      <span class="comment"># class0 x data (tensor), shape=(100, 2)</span></span><br><span class="line">y0 = torch.zeros(<span class="number">100</span>)               <span class="comment"># class0 y data (tensor), shape=(100, 1)</span></span><br><span class="line">x1 = torch.normal(<span class="number">-2</span>*n_data, <span class="number">1</span>)     <span class="comment"># class1 x data (tensor), shape=(100, 2)</span></span><br><span class="line">y1 = torch.ones(<span class="number">100</span>)                <span class="comment"># class1 y data (tensor), shape=(100, 1)</span></span><br><span class="line">x = torch.cat((x0, x1), <span class="number">0</span>).type(torch.FloatTensor)  <span class="comment"># shape (200, 2) FloatTensor = 32-bit floating</span></span><br><span class="line">y = torch.cat((y0, y1), ).type(torch.LongTensor)    <span class="comment"># shape (200,) LongTensor = 64-bit integer</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The code below is deprecated in Pytorch 0.4. Now, autograd directly supports tensors</span></span><br><span class="line"><span class="comment"># x, y = Variable(x), Variable(y)</span></span><br><span class="line"></span><br><span class="line">plt.scatter(x.data.numpy()[:, <span class="number">0</span>], x.data.numpy()[:, <span class="number">1</span>], c=y.data.numpy(), s=<span class="number">100</span>, lw=<span class="number">0</span>, cmap=<span class="string">'RdYlGn'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="建立神经网络-1"><a href="#建立神经网络-1" class="headerlink" title="建立神经网络"></a>建立神经网络</h2><p>建立一个神经网络我们可以直接运用 torch 中的体系. 先定义所有的层属性(<code>__init__()</code>), 然后再一层层搭建(<code>forward(x)</code>)层于层的关系链接. 这个和我们在前面 regression 的时候的神经网络基本没差. 建立关系的时候, 我们会用到激励函数, 如果还不清楚激励函数用途的同学, 这里有非常好的<a href="https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/3-04-activation-function/" target="_blank" rel="noopener">一篇动画教程</a>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F     <span class="comment"># 激励函数都在这</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(torch.nn.Module)</span>:</span>     <span class="comment"># 继承 torch 的 Module</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_feature, n_hidden, n_output)</span>:</span></span><br><span class="line">        super(Net, self).__init__()     <span class="comment"># 继承 __init__ 功能</span></span><br><span class="line">        self.hidden = torch.nn.Linear(n_feature, n_hidden)   <span class="comment"># 隐藏层线性输出</span></span><br><span class="line">        self.out = torch.nn.Linear(n_hidden, n_output)       <span class="comment"># 输出层线性输出</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># 正向传播输入值, 神经网络分析出输出值</span></span><br><span class="line">        x = F.relu(self.hidden(x))      <span class="comment"># 激励函数(隐藏层的线性值)</span></span><br><span class="line">        x = self.out(x)                 <span class="comment"># 输出值, 但是这个不是预测值, 预测值还需要再另外计算</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net = Net(n_feature=<span class="number">2</span>, n_hidden=<span class="number">10</span>, n_output=<span class="number">2</span>) <span class="comment"># 几个类别就几个 output</span></span><br><span class="line"></span><br><span class="line">print(net)  <span class="comment"># net 的结构</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Net (</span></span><br><span class="line"><span class="string">  (hidden): Linear (2 -&gt; 10)</span></span><br><span class="line"><span class="string">  (out): Linear (10 -&gt; 2)</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<h2 id="训练网络-1"><a href="#训练网络-1" class="headerlink" title="训练网络"></a>训练网络</h2><p>训练的步骤很简单, 如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># optimizer 是训练的工具</span></span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.02</span>)  <span class="comment"># 传入 net 的所有参数, 学习率</span></span><br><span class="line"><span class="comment"># 算误差的时候, 注意真实值!不是! one-hot 形式的, 而是1D Tensor, (batch,)</span></span><br><span class="line"><span class="comment"># 但是预测值是2D tensor (batch, n_classes)</span></span><br><span class="line">loss_func = torch.nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    out = net(x)     <span class="comment"># 喂给 net 训练数据 x, 输出分析值</span></span><br><span class="line"></span><br><span class="line">    loss = loss_func(out, y)     <span class="comment"># 计算两者的误差</span></span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()   <span class="comment"># 清空上一步的残余更新参数值</span></span><br><span class="line">    loss.backward()         <span class="comment"># 误差反向传播, 计算参数更新值</span></span><br><span class="line">    optimizer.step()        <span class="comment"># 将参数更新值施加到 net 的 parameters 上</span></span><br></pre></td></tr></table></figure>
<h2 id="可视化训练过程-1"><a href="#可视化训练过程-1" class="headerlink" title="可视化训练过程"></a>可视化训练过程</h2><p>为了可视化整个训练的过程, 更好的理解是如何训练, 我们如下操作:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">plt.ion()   # 画图</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">for t in range(100):</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    # 接着上面来</span><br><span class="line">    if t % 2 == 0:</span><br><span class="line">        plt.cla()</span><br><span class="line">        # 过了一道 softmax 的激励函数后的最大概率才是预测值</span><br><span class="line">        prediction = torch.max(F.softmax(out), 1)[1]</span><br><span class="line">        pred_y = prediction.data.numpy().squeeze()</span><br><span class="line">        target_y = y.data.numpy()</span><br><span class="line">        plt.scatter(x.data.numpy()[:, 0], x.data.numpy()[:, 1], c=pred_y, s=100, lw=0, cmap=&apos;RdYlGn&apos;)</span><br><span class="line">        accuracy = sum(pred_y == target_y)/200.  # 预测中有多少和真实值一样</span><br><span class="line">        plt.text(1.5, -4, &apos;Accuracy=%.2f&apos; % accuracy, fontdict=&#123;&apos;size&apos;: 20, &apos;color&apos;:  &apos;red&apos;&#125;)</span><br><span class="line">        plt.pause(0.1)</span><br><span class="line"></span><br><span class="line">plt.ioff()  # 停止画图</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><a href="https://morvanzhou.github.io/static/results/torch/3-2-1.png" target="_blank" rel="noopener"><img src="https://morvanzhou.github.io/static/results/torch/3-2-1.png" alt="区分类型 (建造第一个神经网络/pic/3-2-1.png)"></a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/4/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/32/">32</a><a class="extend next" rel="next" href="/page/6/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Schwimmer</p>
              <div class="site-description motion-element" itemprop="description">Record and Think!</div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">314</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">22</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">59</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          

          

          
          

          
            
          
          

        </div>
      </div>

      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Schwimmer</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.9.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> v7.2.0</div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span>
    <span class="site-uv" title="总访客量">
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="post-meta-divider">|</span>
  

  
    <span class="post-meta-item-icon">
      <i class="fa fa-eye"></i>
    </span>
    <span class="site-pv" title="总访问量">
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    
      <div>
        
  <script>
    window._bd_share_config = {
      "common": {
        "bdText": "",
        "bdMini": "1",
        "bdMiniList": false,
        "bdPic": ""
      },
      "image": {
        "viewList": ["tsina", "douban", "sqq", "qzone", "weixin", "twi", "fbook"],
        "viewText": "分享到：",
        "viewSize": "16"
      },
      "slide": {
        "bdImg": "5",
        "bdPos": "left",
        "bdTop": "100"
      }
    }
  </script>

<script>
  with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='//bdimg.share.baidu.com/static/api/js/share.js?cdnversion='+~(-new Date()/36e5)];
</script>

      </div>
    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/utils.js?v=7.2.0"></script>

  <script src="/js/motion.js?v=7.2.0"></script>



  
  


  <script src="/js/schemes/muse.js?v=7.2.0"></script>




  

  


  <script src="/js/next-boot.js?v=7.2.0"></script>


  

  

  

  
  
<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://https-schwimmer-github-io.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>







  
  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
  

  
  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      document.getElementById(all[i].inputID + '-Frame').parentNode.className += ' has-jax';
    }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

    
  


  

  

  

  

  

  

  

  

  

  

  

</body>
</html>
