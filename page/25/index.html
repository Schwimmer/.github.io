<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222"/>






















<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0"/>

<link rel="stylesheet" href="/css/main.css?v=7.2.0"/>


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.2.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.2.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '7.2.0',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="Record and Think!">
<meta property="og:type" content="website">
<meta property="og:title" content="Schwimmer&#39;s Blog">
<meta property="og:url" content="https://schwimmer.github.io/page/25/index.html">
<meta property="og:site_name" content="Schwimmer&#39;s Blog">
<meta property="og:description" content="Record and Think!">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Schwimmer&#39;s Blog">
<meta name="twitter:description" content="Record and Think!">





  
  
  <link rel="canonical" href="https://schwimmer.github.io/page/25/"/>



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Schwimmer's Blog</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Schwimmer's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br/>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br/>Tags</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br/>Categories</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br/>Archives</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-sitemap">

    
    
      
    

    

    <a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br/>Sitemap</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-commonweal">

    
    
      
    

    

    <a href="/404/" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br/>Commonweal 404</a>

  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2018/03/17/机器学习/sklearn的一些总结/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/03/17/机器学习/sklearn的一些总结/" class="post-title-link" itemprop="url">sklearn的一些总结</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-03-17 20:50:31" itemprop="dateCreated datePublished" datetime="2018-03-17T20:50:31+08:00">2018-03-17</time>
            </span>
          

          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>转自 <a href="http://blog.csdn.net/wang1127248268/article/details/53264041" target="_blank" rel="noopener">sklearn的一些总结</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2018/03/17/机器学习/人群画像/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/03/17/机器学习/人群画像/" class="post-title-link" itemprop="url">人群画像</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-03-17 20:50:31" itemprop="dateCreated datePublished" datetime="2018-03-17T20:50:31+08:00">2018-03-17</time>
            </span>
          

          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://zhuanlan.zhihu.com/p/27126412" target="_blank" rel="noopener">比你更了解你，浅谈用户画像</a></p>
<p>爱点击的性别预测模型</p>
<p>为什么用朴素贝叶斯？</p>
<p>如何选择特征？</p>
<blockquote>
<p>去除覆盖率低的，去除</p>
</blockquote>
<p>如何解决特征有依赖关系的问题？</p>
<blockquote>
<p>假设，对于同一个一级域名，下面的N级域名中男女分布比例在接近的合并为同一个特征。</p>
</blockquote>
<p>训练集和测试集？</p>
<p>线上效果</p>
<blockquote>
<p>鼎盛时期，平均每个cookie有5个url</p>
</blockquote>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2018/03/17/机器学习/BP神经网络/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/03/17/机器学习/BP神经网络/" class="post-title-link" itemprop="url">BP神经网络</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-03-17 20:50:31" itemprop="dateCreated datePublished" datetime="2018-03-17T20:50:31+08:00">2018-03-17</time>
            </span>
          

          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://www.cnblogs.com/wjy-lulu/p/6511616.html" target="_blank" rel="noopener">https://www.cnblogs.com/wjy-lulu/p/6511616.html</a></p>
<p><a href="https://www.cnblogs.com/biaoyu/archive/2015/06/20/4591304.html" target="_blank" rel="noopener">https://www.cnblogs.com/biaoyu/archive/2015/06/20/4591304.html</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2018/03/17/机器学习/SVD/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/03/17/机器学习/SVD/" class="post-title-link" itemprop="url">SVD</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-03-17 20:50:31" itemprop="dateCreated datePublished" datetime="2018-03-17T20:50:31+08:00">2018-03-17</time>
            </span>
          

          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>整理自：</p>
<p><a href="http://www.cnblogs.com/pinard/p/6251584.html" target="_blank" rel="noopener">奇异值分解(SVD)原理与在降维中的应用</a></p>
<p><a href="http://blog.sciencenet.cn/blog-696950-699380.html" target="_blank" rel="noopener">奇异值分解(SVD) —- 线性变换几何意义</a></p>
<p><a href="http://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html" target="_blank" rel="noopener">机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用</a></p>
<h1 id="特征值和特征分解"><a href="#特征值和特征分解" class="headerlink" title="特征值和特征分解"></a>特征值和特征分解</h1><script type="math/tex; mode=display">
Ax=\lambda x</script><p>其中A是一个n×n的矩阵，x是一个n维向量，则我们说<strong>λ</strong>是矩阵A的一个<strong>特征值</strong>，而<strong>x</strong>是矩阵A的特征值λ所对应的<strong>特征向量</strong>。</p>
<p>而从几何上看，A相当于对向量x进行了拉伸，λ是拉伸的尺度。</p>
<p>前提是A是一个对称矩阵。</p>
<blockquote>
<p><strong>对称矩阵</strong></p>
<p>转置后与原矩阵相等。任意矩阵乘以它的转置也是对称矩阵。</p>
</blockquote>
<p>特征分解时，A必须是方阵。</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170105115457425-1545975626.png" alt=""></p>
<p>SVD之后，对于奇异值,它跟我们特征分解中的特征值类似，在奇异值矩阵中也是按照从大到小排列，而且奇异值的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上的比例。也就是说，我们也可以用最大的k个的奇异值和对应的左右奇异向量来近似描述矩阵。</p>
<script type="math/tex; mode=display">
A_{m \times n} = U_{m \times m}\Sigma_{m \times n} V^T_{n \times n} \approx U_{m \times k}\Sigma_{k \times k} V^T_{k \times n}</script><p>这样，矩阵A就可以近似的表示为</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170105140822191-1774139119.png" alt=""></p>
<blockquote>
<p>（文本分析中）</p>
<p>三个矩阵有非常清楚的物理含义：</p>
<ul>
<li>第一个矩阵X中的每一行表示意思相关的一类词，其中的每个非零元素表示这类词中每个词的重要性（或者说相关性），数值越大越相关。</li>
<li>第三个矩阵Y中的每一列表示同一主题一类文章，其中每个元素表示这类文章中每篇文章的相关性。</li>
<li>第二个矩阵B则表示类词和文章之间的相关性。因此，我们只要对关联矩阵A进行一次奇异值分解，我们就可以同时完成了近义词分类和文章的分类。（同时得到每类文章和每类词的相关性）。</li>
</ul>
</blockquote>
<p>SVD的性质</p>
<p>降维</p>
<p>对于奇异值,它跟我们特征分解中的特征值类似，在奇异值矩阵中也是按照从大到小排列，而且奇异值的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上的比例。也就是说，我们也可以用最大的k个的奇异值和对应的左右奇异向量来近似描述矩阵。也就是说：</p>
<script type="math/tex; mode=display">
A_{m \times n} = U_{m \times m}\Sigma_{m \times n} V^T_{n \times n} \approx U_{m \times k}\Sigma_{k \times k} V^T_{k \times n}</script>
          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2018/03/17/机器学习/SVM/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/03/17/机器学习/SVM/" class="post-title-link" itemprop="url">SVM</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-03-17 20:50:31" itemprop="dateCreated datePublished" datetime="2018-03-17T20:50:31+08:00">2018-03-17</time>
            </span>
          

          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>线性可分的推导<a href="http://blog.sina.com.cn/s/blog_4298002e010144k8.html" target="_blank" rel="noopener">http://blog.sina.com.cn/s/blog_4298002e010144k8.html</a></p>
<p>线性不可分的时候<a href="http://blog.csdn.net/american199062/article/details/51322852" target="_blank" rel="noopener">http://blog.csdn.net/american199062/article/details/51322852</a></p>
<p>松弛系数</p>
<p><img src="https://pic4.zhimg.com/80/ca45458396bf807868674316793205b7_hd.jpg" alt=""></p>
<p>允许错误的分类，但要付出代价。错分的苹果是大于1，在margin当中但分类正确的在0,1之间。</p>
<p>对于整体的惩罚力度，要另外使用一个参数C来衡量惩罚的程度。</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cmin+%5Cfrac%7B1%7D%7B2%7D%7C%7Cw%7C%7C%5E2+%2B+%5Cfrac%7B%5Cgamma%7D%7B2%7D%5Csum_%7Bn%3D1%7D%5E%7BN%7D%7B%5Cxi_n%5E2%7D+" alt=""></p>
<p>通过核函数可以以低的计算复杂度构造更复杂的分类器，而不用在低维映射到高维。</p>
<p>SMO优化 <a href="https://www.cnblogs.com/pinard/p/6111471.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6111471.html</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2018/03/17/机器学习/CTR预测专题/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/03/17/机器学习/CTR预测专题/" class="post-title-link" itemprop="url">CTR预测专题</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-03-17 20:50:31" itemprop="dateCreated datePublished" datetime="2018-03-17T20:50:31+08:00">2018-03-17</time>
            </span>
          

          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://www.zhihu.com/question/54009615" target="_blank" rel="noopener">如何评价CTR预估效果？</a></p>
<p><a href="https://www.zhihu.com/question/23652394" target="_blank" rel="noopener">为什么LR可以用来做CTR预估？</a></p>
<p><a href="http://blog.csdn.net/gexyz/article/details/78432192" target="_blank" rel="noopener">关于CTR预测的一个总结</a></p>
<p><a href="http://www.cnblogs.com/iloveyouforever/p/4353491.html" target="_blank" rel="noopener">为什么CTR预估使用AUC来评估模型？</a></p>
<p><a href="http://blog.csdn.net/cserchen/article/details/7535182" target="_blank" rel="noopener">广告计算中的AUC和ROC曲线</a></p>
<p><a href="http://www.cnblogs.com/qcloud1001/p/7513982.html" target="_blank" rel="noopener">常见计算广告点击率预估算法总结</a></p>
<p>关于贝叶斯平滑</p>
<p><a href="http://blog.csdn.net/z363115269/article/details/78637702" target="_blank" rel="noopener">http://blog.csdn.net/z363115269/article/details/78637702</a></p>
<p><a href="http://blog.csdn.net/google19890102/article/details/50492787" target="_blank" rel="noopener">http://blog.csdn.net/google19890102/article/details/50492787</a></p>
<p><a href="http://blog.csdn.net/jinping_shi/article/details/78334362" target="_blank" rel="noopener">http://blog.csdn.net/jinping_shi/article/details/78334362</a></p>
<p><a href="http://blog.csdn.net/wwqwkg6e/article/details/55000216" target="_blank" rel="noopener">http://blog.csdn.net/wwqwkg6e/article/details/55000216</a></p>
<p>贝叶斯平滑的思想是给CTR预设一个<strong>经验初始值</strong>，再通过当前的点击量和曝光量来修正这个初始值。如果某商品的点击量和曝光量都是0，那么该商品的CTR就是这个经验初始值；如果商品A和商品B的曝光量差别很大，那么可以通过这个经验初始值来修正，使得曝光量大的商品的权重增大。</p>
<p>贝叶斯平滑就是确定这个经验值的过程。贝叶斯平滑是基于贝叶斯统计推断的，因此经验值计算的过程依赖于数据的分布情况。</p>
<p>贝叶斯平滑的推导涉及贝叶斯参数估计</p>
<p>ctr能不能加入id类特征？</p>
<p>这里没看懂？为什么ctr越高的分段上权重越大？</p>
<blockquote>
<p>假设一个最简单的问题，预估广告的点击率CTR。为了便于讨论，假设你只有一个特征，就是每次展现广告在过去一个时间窗内的历史点击率ctr，现在目标是预测下一次点击的ctr。简单起见，不妨假设系统中只有两条候选广告。</p>
<p>显而易见，预测分数是和ctr正相关的。如果你使用的是离散LR，那么在分段之后，显然ctr越高的分段上权重越大。这个模型实际跑起来就是最简单的“热门广告”的效果。</p>
</blockquote>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2018/03/17/总结与思考/从内容-用户画像到如何做算法研发/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/03/17/总结与思考/从内容-用户画像到如何做算法研发/" class="post-title-link" itemprop="url">从内容-用户画像到如何做算法研发</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-03-17 20:50:31" itemprop="dateCreated datePublished" datetime="2018-03-17T20:50:31+08:00">2018-03-17</time>
            </span>
          

          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/总结与思考/" itemprop="url" rel="index"><span itemprop="name">总结与思考</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>转自<a href="http://blog.csdn.net/bitcarmanlee/article/details/77574371" target="_blank" rel="noopener">http://blog.csdn.net/bitcarmanlee/article/details/77574371</a></p>
<p>要求以Spark的Mlib为载体，尽量所有人共用一个算法平台。这样做的好处是大家信息共享会更快，同一个平台也更好维护。比如，算法工程师写了一个巨牛逼的算法原型，然后他需要先给工程师讲懂这个算法，工程师看个人水平，先不说能否将算法实现，实现所花的时间，以及是否真的有时间和精力去帮着实现，实现的是不是有问题就是一个很大的问题了。来回一折腾，两个人都会比较累。</p>
<p><strong>怎么才算对算法有了真正的理解。</strong></p>
<p>首先我们看什么场景用什么算法，但实际用起来，效果并不是那么好。这个时候我们至少需要了解两方面：</p>
<p>算法的核心是什么，有什么潜在的需求？比如是不是对数据的分布做了什么假设么? </p>
<p>特征和数据集的情况是如何的？</p>
<p>而且很多算法做了很多很粗暴的假设，这种假设会导致算法存在一些固有的问题，如果你不了解其内部的这些假设，你会以为这些是他的一个特性，其实是一个缺点。比如Gini Importance，如果你不去了解的内部思想，你在理解数据时，就会造成误解，导致错误的认为先被选中的特征是很重要的，而其余的特征是不重要的，但实际上这些特征对响应变量的作用确实非常接近的。</p>
<p>我有时候觉得，引用算法工程师最流行的一个话，就是tricky。 中文我不知道怎么翻译更合适，很多时候是需要悟性和对事物本质的了解，才能了解一个算法的，绝对不是靠几个公式就能搞定的。</p>
<p>协同算法是我们应用的比较广泛的一个算法。 但是我觉得协同不应该算是一个算法，而是一种模式。 我们常见的很多模型，最后都是协同模式。举个例子来说，是不是个A1用户推荐文章B1,我们可能是这么做的：</p>
<p>把用户用向量做表征，文章也是<br>观察大量的用户A2,A3…AN 是不是有点击该B1<br>使用逻辑回归/SVM等分类算法训练模型<br>把A1,B1丢进模型，得到是否推荐。<br>但事实上这套算法，用的就是协同。为啥的？本质上还是相近的用户做的选择互相推荐。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2018/03/17/机器学习/NLP/Word2vec-C代码/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/03/17/机器学习/NLP/Word2vec-C代码/" class="post-title-link" itemprop="url">Word2Vec源码解读</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-03-17 20:50:31" itemprop="dateCreated datePublished" datetime="2018-03-17T20:50:31+08:00">2018-03-17</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2018-03-25 17:23:53" itemprop="dateModified" datetime="2018-03-25T17:23:53+08:00">2018-03-25</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="http://blog.csdn.net/google19890102/article/details/51887344" target="_blank" rel="noopener">机器学习算法实现解析——word2vec源码解析</a></p>
<p>源码在<code>/home/david/code/nlp/word2vec/word2vecC/word2vec.c</code></p>
<p>代码的主要工作包括：</p>
<ul>
<li>预处理。变量声明，全局变量遍历；</li>
<li>构建词库。包括文本处理，以及是否需要有指定词库。</li>
<li>初始化网络结构。参数初始化，Huffman编码的生成。</li>
<li>多线程模型训练。</li>
<li>最终结果的处理。</li>
</ul>
<p>以上的过程，可以用下图表示：</p>
<p><img src="/2018/03/17/机器学习/NLP/Word2vec-C代码/Word2Vec-CBOW和Skip-Gram模型/算法流程.png" alt="算法流程"></p>
<h1 id="输入参数"><a href="#输入参数" class="headerlink" title="输入参数"></a>输入参数</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">-train text8 表示的是输入文件是text8</div><div class="line"></div><div class="line">-output vectors.bin 输出文件是vectors.bin</div><div class="line"></div><div class="line">-cbow 1 表示使用cbow模型，默认为Skip-Gram模型</div><div class="line"></div><div class="line">-size 200 每个单词的向量维度是200</div><div class="line"></div><div class="line">-window 8 训练的窗口大小为5就是考虑一个词前八个和后八个词语（实际代码中还有一个随机选窗口的过程，窗口大小小于等于8）</div><div class="line"></div><div class="line">-negative 0 使用ns的时候采样的样本数，默认0，通常是5-10</div><div class="line"></div><div class="line">-save-vocab 词汇表存储文件</div><div class="line"></div><div class="line">-read-vocab 词汇表加载文件</div><div class="line"></div><div class="line">-classes 输出单词类别数，默认为0，即不输出单词</div><div class="line"></div><div class="line">-hs 1不使用NEG方法，使用HS方法。-</div><div class="line"></div><div class="line">-sample 亚采样拒绝概率的参数</div><div class="line">指的是采样的阈值，如果一个词语在训练样本中出现的频率越大，那么就越会被采样。</div><div class="line"></div><div class="line">-binary为1指的是结果二进制存储，为0是普通存储（普通存储的时候是可以打开看到词语和对应的向量的）</div><div class="line"></div><div class="line">-iter 15 迭代次数</div></pre></td></tr></table></figure>
<h1 id="全局变量"><a href="#全局变量" class="headerlink" title="全局变量"></a>全局变量</h1><p><code>int *vocab_hash</code> </p>
<p>词在词库中的index，在构建词库时先初始化为-1</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">// 词的结构体</div><div class="line">struct vocab_word &#123;</div><div class="line">        long long cn; // 出现的次数</div><div class="line">        int *point; // 从根结点到叶子节点的路径</div><div class="line">        char *word, *code, codelen;// 分别对应着词，Huffman编码，编码长度</div><div class="line">&#125;;</div></pre></td></tr></table></figure>
<p>vocab_word是词的结构体</p>
<p><code>vocab = (struct vocab_word *)calloc(vocab_max_size, sizeof(struct vocab_word));</code></p>
<p>vocab存储词</p>
<p><code>vocab_size</code> ：词汇表的总量</p>
<p><code>syn0</code> ：上下文词</p>
<p><code>syn1</code> ：$\theta_{j-1}^w$</p>
<p><code>neu1</code> ：映射层的向量，就是输入层的向量之和</p>
<p><code>neu1e</code> ：对应伪代码中的e</p>
<h1 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h1><p><img src="/2018/03/17/机器学习/NLP/Word2vec-C代码/sigmoid1.png" alt=""></p>
<p>在预处理部分，对word2vec需要使用的参数进行初始化，在word2vec中是利用传入的方式对参数进行初始化的。</p>
<p>在预处理部分，实现了sigmoid函数值的近似计算。</p>
<p>如果每一次都请求计算sigmoid值，对性能将会有一定的影响，当sigmoid的值对精度的要求并不是非常严格时，可以采用近似计算。在word2vec中，将区间[−6,6]（设置的参数<strong>MAX_EXP</strong>为6）等距离划分成<strong>EXP_TABLE_SIZE</strong>等份，并将每个区间中的sigmoid值计算好存入到数组expTable中，需要使用时，直接从数组中查找。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 申请EXP_TABLE_SIZE+1个空间</span></div><div class="line">expTable = (real *)<span class="built_in">malloc</span>((EXP_TABLE_SIZE + <span class="number">1</span>) * <span class="keyword">sizeof</span>(real));</div><div class="line">  <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; EXP_TABLE_SIZE; i++) &#123;</div><div class="line">    expTable[i] = <span class="built_in">exp</span>((i / (real)EXP_TABLE_SIZE * <span class="number">2</span> - <span class="number">1</span>) * MAX_EXP); </div><div class="line">    <span class="comment">// 1/(1+e^6) ~ 1/(1+e^-6)即 0.01 ~ 1 的样子  </span></div><div class="line">    expTable[i] = expTable[i] / (expTable[i] + <span class="number">1</span>);                   <span class="comment">// Precompute f(x) = x / (x + 1)</span></div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<blockquote>
<p>注意：在上述代码中，作者使用的是小于EXP_TABLE_SIZE，实际的区间是[−6,6)。</p>
</blockquote>
<h1 id="构建词库"><a href="#构建词库" class="headerlink" title="构建词库"></a>构建词库</h1><p>在word2vec源码中，提供了两种构建词库的方法，分别为：</p>
<ul>
<li>指定词库：ReadVocab()方法</li>
<li>从词的文本构建词库：LearnVocabFromTrainFile()方法</li>
</ul>
<h2 id="构建词库的过程"><a href="#构建词库的过程" class="headerlink" title="构建词库的过程"></a>构建词库的过程</h2><p>在这里，我们以从词的文本构建词库为例。构建词库的过程如下所示：<img src="/2018/03/17/机器学习/NLP/Word2vec-C代码/构建词库.png" alt="构建词库"></p>
<p>在这部分中，最主要的工作是对文本进行处理，包括低频词的处理，hash表的处理等等。首先，会在词库中增加一个“&lt;/s&gt;”的词，同时，在读取文本的过程中，将换行符“\n”也表示成该该词</p>
<h2 id="对词的哈希处理"><a href="#对词的哈希处理" class="headerlink" title="对词的哈希处理"></a>对词的哈希处理</h2><p>在存储词的过程中，同时保留这两个数组：</p>
<ul>
<li>存储词的vocab</li>
<li>存储词的hash的vocab_hash</li>
</ul>
<p>其中，在vocab中，存储的是词对应的结构体：</p>
<p>在vocab_hash中存储的是词在词库中的Index，vocab_hash的下标是词计算出的hash值。</p>
<p>在对词的处理过程中，主要包括：</p>
<ul>
<li>计算词的hash值：</li>
</ul>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 取词的hash值</span></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">GetWordHash</span><span class="params">(<span class="keyword">char</span> *word)</span> </span>&#123;</div><div class="line">        <span class="keyword">unsigned</span> <span class="keyword">long</span> <span class="keyword">long</span> a, hash = <span class="number">0</span>;</div><div class="line">        <span class="keyword">for</span> (a = <span class="number">0</span>; a &lt; <span class="built_in">strlen</span>(word); a++) hash = hash * <span class="number">257</span> + word[a];</div><div class="line">        hash = hash % vocab_hash_size;</div><div class="line">        <span class="keyword">return</span> hash;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<ul>
<li>SearchVocab检索词是否存在。如不存在则返回-1，否则，返回该词在词库中的索引：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">while (1) &#123;</div><div class="line">    if (vocab_hash[hash] == -1) return -1;// 不存在该词</div><div class="line">    //strcmp两个词相等，则返回0，所以要加上!</div><div class="line">    if (!strcmp(word, vocab[vocab_hash[hash]].word)) return vocab_hash[hash];// 返回索引值</div><div class="line">    hash = (hash + 1) % vocab_hash_size;// 处理冲突</div><div class="line">&#125;</div><div class="line">return -1;// 不存在该词</div></pre></td></tr></table></figure>
<p>在这个过程中，使用到了线性探测的开放定址法处理冲突，开放定址法就是一旦发生冲突，就去寻找下一个空的散列地址。</p>
<ul>
<li>不存在，则插入新词。</li>
</ul>
<h2 id="对低频词的处理"><a href="#对低频词的处理" class="headerlink" title="对低频词的处理"></a>对低频词的处理</h2><p>在循环读取每一个词的过程中，当出现“vocab_size &gt; vocab_hash_size * 0.7”时，需要对低频词进行处理。其中，vocab_size表示的是目前词库中词的个数，vocab_hash_size表示的是初始设定的hash表的大小。</p>
<p><code>ReduceVocab()</code></p>
<p>在处理低频词的过程中，通过参数“min_reduce”来控制，若词出现的次数小于等于该值时，则从词库中删除该词。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">for (a = 0; a &lt; vocab_size; a++) if (vocab[a].cn &gt; min_reduce) &#123;</div><div class="line">		vocab[b].cn = vocab[a].cn;</div><div class="line">		vocab[b].word = vocab[a].word;</div><div class="line">		b++;</div><div class="line">	&#125; else free(vocab[a].word);</div><div class="line">	vocab_size = b;// 删减后词的个数</div></pre></td></tr></table></figure>
<p>在删除了低频词后，需要重新对词库中的词进行hash值的计算。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">for (a = 0; a &lt; vocab_hash_size; a++) vocab_hash[a] = -1;</div><div class="line">for (a = 0; a &lt; vocab_size; a++) &#123;</div><div class="line">  // Hash will be re-computed, as it is not actual</div><div class="line">  hash = GetWordHash(vocab[a].word);</div><div class="line">  while (vocab_hash[hash] != -1) hash = (hash + 1) % vocab_hash_size;</div><div class="line">  vocab_hash[hash] = a;</div><div class="line">&#125;</div><div class="line">fflush(stdout);</div><div class="line">min_reduce++;</div></pre></td></tr></table></figure>
<h2 id="根据词频对词库中的词排序"><a href="#根据词频对词库中的词排序" class="headerlink" title="根据词频对词库中的词排序"></a>根据词频对词库中的词排序</h2><p>基于以上的过程，程序已经将词从文件中提取出来，并存入到指定的词库中（vocab数组），接下来，需要根据每一个词的词频对词库中的词按照词频从大到小排序，其基本过程在函数SortVocab中，排序过程为</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">qsort(&amp;vocab[1], vocab_size - 1, sizeof(struct vocab_word), VocabCompare);</div></pre></td></tr></table></figure>
<blockquote>
<p><strong>原</strong> <strong>型</strong>:void qsort(void <em>base, int nelem, int width, int (</em>fcmp)(const void <em>,const void </em>));</p>
<p><strong>功</strong> <strong>能</strong>: 使用快速排序例程进行排序</p>
<p>参 数：</p>
<p>1 待排序数组首地址</p>
<p>2 数组中待排序元素数量</p>
<p>3 各元素的占用空间大小</p>
<p>4 指向函数的指针，用于确定排序的顺序</p>
<p><strong>说</strong> <strong>明：</strong>qsort函数是ANSI C标准中提供的，其声明在stdlib.h文件中，是根据二分法写的，其时间复杂度为n*log(n)。</p>
<p>qsort要求提供的函数是需要自己定义的一个比较函数，比较函数使得qsort通用性更好。有了比较函数qsort可以实现对数组、字符串、结构体等结构进行升序或降序排序。<br>如int cmp(const void <em>a, const void </em>b)中有两个元素作为参数（参数的格式不能变的。）返回一个int值，如果比较函数返回大于0，qsort就认为a &gt; b，返回小于0,qsort就认为a &lt; b。qsort知道元素的大小了，就可以把大的放前面去。如果你的比较函数返回本来应该是1的（即a &gt; b），而却返回-1（小于0的数），那么qsort认为a &lt; b，就把b放在前面去，但实际上是a &gt; b的，所以就造成了降序排序的差别了。简单来说，比较函数的作用就是给qsort指明元素的大小事怎么比较的。</p>
</blockquote>
<p>保持字符“&lt; \s&gt;”在最开始的位置。排序后，根据“min_count”对低频词进行处理，与上述一样，再对剩下的词重新计算hash值。</p>
<p>至此，整个对词的处理过程就已经结束了。接下来，将是对网络结构的处理和词向量的训练。</p>
<h1 id="初始化网络结构"><a href="#初始化网络结构" class="headerlink" title="初始化网络结构"></a>初始化网络结构</h1><p>有了以上的对词的处理，就已经处理好了所有的训练样本，此时，便可以开始网络结构的初始化和接下来的网络训练。网络的初始化的过程在<code>InitNet()</code>函数中完成。</p>
<h2 id="初始化网络参数"><a href="#初始化网络参数" class="headerlink" title="初始化网络参数"></a>初始化网络参数</h2><p>在初始化的过程中，主要的参数包括<strong>词向量的初始化</strong>和<strong>映射层到输出层的权重的初始化</strong>，如下图所示：</p>
<p><img src="http://img.blog.csdn.net/20170227183342463?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ29vZ2xlMTk4OTAxMDI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></p>
<p>词向量的初始化：为每个词分配空间，大小是vocab_size*layer1_size。</p>
<h6 id="初始化的时候要分配所有词-词向量长度的空间？为何要这么大？"><a href="#初始化的时候要分配所有词-词向量长度的空间？为何要这么大？" class="headerlink" title="初始化的时候要分配所有词*词向量长度的空间？为何要这么大？"></a>初始化的时候要分配所有词*词向量长度的空间？为何要这么大？</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">// layer1_size是词向量的长度</div><div class="line">a = posix_memalign((void **)&amp;syn0, 128, (long long)vocab_size * layer1_size * sizeof(real));</div></pre></td></tr></table></figure>
<blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt; int posix_memalign (void **memptr,</div><div class="line">&gt;                     size_t alignment,</div><div class="line">&gt;                     size_t size);</div><div class="line">&gt;</div></pre></td></tr></table></figure>
</blockquote>
<p>&gt;</p>
<blockquote>
<p>调用<em>posix_memalign( )</em>成功时会返回<em>size</em>字节的动态内存，并且这块内存的地址是<em>alignment</em>的倍数。参数<em>alignment</em>必须是2的幂，还是<em>void</em>指针的大小的倍数。返回的内存块的地址放在了<em>memptr</em>里面，函数返回值是<em>0</em>。</p>
</blockquote>
<p>CBOW网络有两种可选的算法：层次Softmax和Negative Sampling。在输入参数时选择任意一种。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 层次softmax的结构</span></div><div class="line">	<span class="keyword">if</span> (hs) &#123;</div><div class="line">		<span class="comment">// 映射层到输出层之间的权重，就是Huffman树的非叶子结点的向量θ</span></div><div class="line">		a = posix_memalign((<span class="keyword">void</span> **)&amp;syn1, <span class="number">128</span>, (<span class="keyword">long</span> <span class="keyword">long</span>)vocab_size * layer1_size * <span class="keyword">sizeof</span>(real));</div><div class="line">		<span class="keyword">if</span> (syn1 == <span class="literal">NULL</span>) &#123;<span class="built_in">printf</span>(<span class="string">"Memory allocation failed\n"</span>); <span class="built_in">exit</span>(<span class="number">1</span>);&#125;</div><div class="line">		<span class="keyword">for</span> (a = <span class="number">0</span>; a &lt; vocab_size; a++) <span class="keyword">for</span> (b = <span class="number">0</span>; b &lt; layer1_size; b++)</div><div class="line">			syn1[a * layer1_size + b] = <span class="number">0</span>;<span class="comment">// 权重初始化为0</span></div><div class="line">	&#125;</div><div class="line">	</div><div class="line">	<span class="comment">// 负采样的结构</span></div><div class="line">	<span class="keyword">if</span> (negative&gt;<span class="number">0</span>) &#123;</div><div class="line">		a = posix_memalign((<span class="keyword">void</span> **)&amp;syn1neg, <span class="number">128</span>, (<span class="keyword">long</span> <span class="keyword">long</span>)vocab_size * layer1_size * <span class="keyword">sizeof</span>(real));</div><div class="line">		<span class="keyword">if</span> (syn1neg == <span class="literal">NULL</span>) &#123;<span class="built_in">printf</span>(<span class="string">"Memory allocation failed\n"</span>); <span class="built_in">exit</span>(<span class="number">1</span>);&#125;</div><div class="line">		<span class="keyword">for</span> (a = <span class="number">0</span>; a &lt; vocab_size; a++) <span class="keyword">for</span> (b = <span class="number">0</span>; b &lt; layer1_size; b++)</div><div class="line">			syn1neg[a * layer1_size + b] = <span class="number">0</span>;</div><div class="line">	&#125;</div></pre></td></tr></table></figure>
<p>在初始化的过程中，<strong>映射层到输出层的权重都初始化为0</strong>，而对于每一个词向量的初始化，作者的初始化方法如下代码所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">// 随机初始化</div><div class="line">	for (a = 0; a &lt; vocab_size; a++) for (b = 0; b &lt; layer1_size; b++) &#123;</div><div class="line">		next_random = next_random * (unsigned long long)25214903917 + 11;</div><div class="line">		// 1、与：相当于将数控制在一定范围内</div><div class="line">		// 2、0xFFFF：65536</div><div class="line">		// 3、/65536：[0,1]之间</div><div class="line">		syn0[a * layer1_size + b] = (((next_random &amp; 0xFFFF) / (real)65536) - 0.5) / layer1_size;// 初始化词向量</div><div class="line">	&#125;</div></pre></td></tr></table></figure>
<p>首先，生成一个很大的next_random的数，通过与“0xFFFF”进行与运算截断，再除以65536得到[0,1]之间的数，最终，得到的初始化的向量的范围为：[−0.5/m,0.5/m]，其中，m为词向量的长度。</p>
<h6 id=""><a href="#" class="headerlink" title=" "></a> </h6><h2 id="3-2、Huffman树的构建"><a href="#3-2、Huffman树的构建" class="headerlink" title="3.2、Huffman树的构建"></a>3.2、Huffman树的构建</h2><p>在层次Softmax中需要使用到Huffman树以及Huffman编码，因此，在网络结构的初始化过程中，也需要初始化Huffman树。在生成Huffman树的过程中，首先定义了3个长度为vocab_size*2+1的数组：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">// 申请2倍的词的空间</div><div class="line">long long *count = (long long *)calloc(vocab_size * 2 + 1, sizeof(long long));</div><div class="line">long long *binary = (long long *)calloc(vocab_size * 2 + 1, sizeof(long long));</div><div class="line">long long *parent_node = (long long *)calloc(vocab_size * 2 + 1, sizeof(long long));</div></pre></td></tr></table></figure>
<p>其中，count数组中前vocab_size存储的是每一个词的对应的词频，词频是从高到低排序。后面的vocab_size先初始化为很大的数。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">// 分成两半进行初始化</div><div class="line">for (a = 0; a &lt; vocab_size; a++) count[a] = vocab[a].cn;// 前半部分初始化为每个词出现的次数</div><div class="line">for (a = vocab_size; a &lt; vocab_size * 2; a++) count[a] = 1e15;// 后半部分初始化为一个固定的常数</div></pre></td></tr></table></figure>
<p>构建Huffman树的过程如下所示</p>
<p><img src="http://img.blog.csdn.net/20170224145522192?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ29vZ2xlMTk4OTAxMDI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></p>
<p>首先，设置两个指针pos1和pos2，分别指向最后一个词和最后一个词的后一位</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">// 两个指针：</div><div class="line">// pos1指向前半截的尾部</div><div class="line">// pos2指向后半截的开始</div><div class="line">pos1 = vocab_size - 1;</div><div class="line">pos2 = vocab_size;</div></pre></td></tr></table></figure>
<p>从两个指针所指的数中选择出最小的值，记为min1i，</p>
<p>如pos1所指的值最小，此时，将pos1左移，再比较pos1和pos2所指的数，选择出最小的值，记为min2i，将他们的和存储到pos2所指的位置。并将此时pos2所指的位置设置为min1i和min2i的父节点，同时，记min2i所指的位置的编码为1（这里令右子树的编码为1）。</p>
<p>如pos2所指的值小，此时，将pos2右移，再比较pos1和pos2，选出最小的值，记为min2i，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line">// Following algorithm constructs the Huffman tree by adding one node at a time</div><div class="line">	// 每次增加一个节点，构建Huffman树</div><div class="line">	for (a = 0; a &lt; vocab_size - 1; a++) &#123;</div><div class="line">		// First, find two smallest nodes &apos;min1, min2&apos;</div><div class="line">		// 选择最小的节点min1</div><div class="line">		if (pos1 &gt;= 0) &#123;</div><div class="line">			if (count[pos1] &lt; count[pos2]) &#123;</div><div class="line">				min1i = pos1;</div><div class="line">				pos1--;</div><div class="line">			&#125; else &#123;</div><div class="line">				min1i = pos2;</div><div class="line">				pos2++;</div><div class="line">			&#125;</div><div class="line">		&#125; else &#123;</div><div class="line">			min1i = pos2;</div><div class="line">			pos2++;</div><div class="line">		&#125;</div><div class="line">		// 选择最小的节点min2</div><div class="line">		if (pos1 &gt;= 0) &#123;</div><div class="line">			if (count[pos1] &lt; count[pos2]) &#123;</div><div class="line">				min2i = pos1;</div><div class="line">				pos1--;</div><div class="line">			&#125; else &#123;</div><div class="line">				min2i = pos2;</div><div class="line">				pos2++;</div><div class="line">			&#125;</div><div class="line">		&#125; else &#123;</div><div class="line">			min2i = pos2;</div><div class="line">			pos2++;</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		count[vocab_size + a] = count[min1i] + count[min2i];</div><div class="line">		// 设置父节点</div><div class="line">		parent_node[min1i] = vocab_size + a;</div><div class="line">		parent_node[min2i] = vocab_size + a;</div><div class="line">		binary[min2i] = 1;// 设置一个子树的编码为1</div><div class="line">	&#125;</div></pre></td></tr></table></figure>
<p>构建好Huffman树后，此时，需要根据构建好的Huffman树生成对应节点的Huffman编码。假设，上述的数据生成的最终的Huffman树为：</p>
<p><img src="http://img.blog.csdn.net/20170228154343740?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ29vZ2xlMTk4OTAxMDI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></p>
<p>此时，count数组，binary数组和parent_node数组分别为：</p>
<p><img src="http://img.blog.csdn.net/20170228160752188?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ29vZ2xlMTk4OTAxMDI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></p>
<p>在生成Huffman编码的过程中，针对每一个词（词都在叶子节点上），从叶子节点开始，将编码存入到code数组中，如对于上图中的“R”节点来说，其code数组为{1,0}，再对其反转便是Huffman编码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">// Now assign binary code to each vocabulary word</div><div class="line">	// 为每一个词分配二进制编码，即Huffman编码</div><div class="line">	for (a = 0; a &lt; vocab_size; a++) &#123;// 针对每一个词</div><div class="line">		b = a;</div><div class="line">		i = 0;</div><div class="line">		while (1) &#123;</div><div class="line">			code[i] = binary[b];// 找到当前的节点的编码</div><div class="line">			point[i] = b;// 记录从叶子节点到根结点的序列</div><div class="line">			i++;</div><div class="line">			b = parent_node[b];// 找到当前节点的父节点</div><div class="line">			if (b == vocab_size * 2 - 2) break;// 已经找到了根结点，根节点是没有编码的</div><div class="line">		&#125;</div><div class="line">		vocab[a].codelen = i;// 词的编码长度</div><div class="line">		vocab[a].point[0] = vocab_size - 2;// 根结点</div><div class="line">		for (b = 0; b &lt; i; b++) &#123;</div><div class="line">			vocab[a].code[i - b - 1] = code[b];// 编码的反转</div><div class="line">			vocab[a].point[i - b] = point[b] - vocab_size;// 记录的是从根结点到叶子节点的路径</div><div class="line">		&#125;</div><div class="line">	&#125;</div></pre></td></tr></table></figure>
<h2 id="3-3、负样本选中表的初始化"><a href="#3-3、负样本选中表的初始化" class="headerlink" title="3.3、负样本选中表的初始化"></a>3.3、负样本选中表的初始化</h2><p>（自己没看）</p>
<p>如果是采用负采样的方法，此时还需要初始化每个词被选中的概率。在所有的词构成的词典中，每一个词出现的频率有高有低，我们希望，对于那些高频的词，被选中成为负样本的概率要大点，同时，对于那些出现频率比较低的词，我们希望其被选中成为负样本的频率低点。这个原理于“轮盘赌”的策略一致（详细可以参见“<a href="http://blog.csdn.net/google19890102/article/details/45622307" target="_blank" rel="noopener">优化算法——遗传算法</a>”）。在程序中，实现这部分功能的代码为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">// 生成负采样的概率表</div><div class="line">void InitUnigramTable() &#123;</div><div class="line">        int a, i;</div><div class="line">        double train_words_pow = 0;</div><div class="line">        double d1, power = 0.75;</div><div class="line">        table = (int *)malloc(table_size * sizeof(int));// int --&gt; int</div><div class="line">        for (a = 0; a &lt; vocab_size; a++) train_words_pow += pow(vocab[a].cn, power);</div><div class="line">        // 类似轮盘赌生成每个词的概率</div><div class="line">        i = 0;</div><div class="line">        d1 = pow(vocab[i].cn, power) / train_words_pow;</div><div class="line">        for (a = 0; a &lt; table_size; a++) &#123;</div><div class="line">                table[a] = i;</div><div class="line">                if (a / (double)table_size &gt; d1) &#123;</div><div class="line">                        i++;</div><div class="line">                        d1 += pow(vocab[i].cn, power) / train_words_pow;</div><div class="line">                &#125;</div><div class="line">                if (i &gt;= vocab_size) i = vocab_size - 1;</div><div class="line">        &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在实现的过程中，没有直接使用每一个词的频率，而是使用了词的0.75次方。</p>
<h1 id="4、多线程模型训练"><a href="#4、多线程模型训练" class="headerlink" title="4、多线程模型训练"></a>4、多线程模型训练</h1><p>以上的各个部分是为训练词向量做准备，即准备训练数据，构建训练模型。在上述的初始化完成后，接下来就是根据不同的方法对模型进行训练，在实现的过程中，作者使用了多线程的方法对其进行训练。</p>
<h2 id="4-1、多线程的处理"><a href="#4-1、多线程的处理" class="headerlink" title="4.1、多线程的处理"></a>4.1、多线程的处理</h2><p>为了能够对文本进行加速训练，在实现的过程中，作者使用了多线程的方法，TrainModelThread()</p>
<p>对每一个线程上分配指定大小的文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">// 利用多线程对训练文件划分，每个线程训练一部分的数据</div><div class="line">fseek(fi, file_size / (long long)num_threads * (long long)id, SEEK_SET);</div></pre></td></tr></table></figure>
<p>这个过程可以通过下图简单的描述：</p>
<p><img src="http://img.blog.csdn.net/20170301130559630?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ29vZ2xlMTk4OTAxMDI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></p>
<p>在实现多线程的过程中，作者并没有加锁的操作，而是对模型参数和词向量的修改可以任意执行，这一点类似于基于随机梯度的方法，训练的过程与训练样本的训练是没有关系的，这样可以大大加快对词向量的训练。抛开多线程的部分，在每一个线程内执行的是对模型和词向量的训练。</p>
<p>作者在实现的过程中，主要实现了两个模型，即CBOW模型和Skip-gram模型，在每个模型中，又分别使用到了两种不同的训练方法，即层次Softmax和Negative Sampling方法。</p>
<h2 id="4-2、CBOW模型"><a href="#4-2、CBOW模型" class="headerlink" title="4.2、CBOW模型"></a>4.2、CBOW模型</h2><h3 id="4-2-1、从输入层到映射层"><a href="#4-2-1、从输入层到映射层" class="headerlink" title="4.2.1、从输入层到映射层"></a>4.2.1、从输入层到映射层</h3><p>首先找到每个词对应的词向量，并将这些词的词向量相加，程序代码如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">// in -&gt; hidden</div><div class="line">cw = 0;</div><div class="line">//b是随机生成的0到window-1，相当于左右各看window-b/2个词</div><div class="line">for (a = b; a &lt; window * 2 + 1 - b; a++) if (a != window) &#123;</div><div class="line">  //sentence_position 单词在句子中的位置</div><div class="line">  c = sentence_position - window + a;</div><div class="line">  // 判断c是否越界</div><div class="line">  if (c &lt; 0) continue;</div><div class="line">  if (c &gt;= sentence_length) continue;</div><div class="line">  // 找到c对应的索引</div><div class="line">  last_word = sen[c];</div><div class="line">  if (last_word == -1) continue;</div><div class="line">  // neu1就是隐藏层向量，也就是上下文对应vector的和</div><div class="line">  for (c = 0; c &lt; layer1_size; c++) neu1[c] += syn0[c + last_word * layer1_size];</div><div class="line">  cw++;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>当累加完窗口内的所有的词向量的之后，存储在映射层neu1中，并取平均，程序代码如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">for (c = 0; c &lt; layer1_size; c++) neu1[c] /= cw;</div></pre></td></tr></table></figure>
<p>当取得了映射层的结果后，此时就需要使用Hierarchical Softmax或者Negative Sampling对模型进行训练。</p>
<h3 id="4-2-2、Hierarchical-Softmax"><a href="#4-2-2、Hierarchical-Softmax" class="headerlink" title="4.2.2、Hierarchical Softmax"></a>4.2.2、Hierarchical Softmax</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">if (hs) for (d = 0; d &lt; vocab[word].codelen; d++) &#123;</div><div class="line">          f = 0;</div><div class="line">          // point存储了从该词的叶子结点的编号到root的序号，这些序号可以对应到syn1的位置，也就是参数向量的位置</div><div class="line">          l2 = vocab[word].point[d] * layer1_size;</div><div class="line">          // Propagate hidden -&gt; output</div><div class="line">          // q=sigma(x*theta)</div><div class="line">          for (c = 0; c &lt; layer1_size; c++) f += neu1[c] * syn1[c + l2];</div><div class="line">          if (f &lt;= -MAX_EXP) continue;</div><div class="line">          else if (f &gt;= MAX_EXP) continue;</div><div class="line">          // 查表得知sigma的值，省去计算的时间</div><div class="line">          else f = expTable[(int)((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))];</div><div class="line">          // g = eta(1-d-q)</div><div class="line">          g = (1 - vocab[word].code[d] - f) * alpha;</div><div class="line">          // Propagate errors output -&gt; hidden</div><div class="line">          // e = e + g*theta</div><div class="line">          for (c = 0; c &lt; layer1_size; c++) neu1e[c] += g * syn1[c + l2];</div><div class="line">          // Learn weights hidden -&gt; output</div><div class="line">          // theta = theta + g*x</div><div class="line">          for (c = 0; c &lt; layer1_size; c++) syn1[c + l2] += g * neu1[c];</div><div class="line">        &#125;</div></pre></td></tr></table></figure>
<p>接下来更新Context(w)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">// hidden -&gt; in</div><div class="line">        for (a = b; a &lt; window * 2 + 1 - b; a++) if (a != window) &#123;</div><div class="line">          c = sentence_position - window + a;</div><div class="line">          if (c &lt; 0) continue;</div><div class="line">          if (c &gt;= sentence_length) continue;</div><div class="line">          last_word = sen[c];</div><div class="line">          if (last_word == -1) continue;</div><div class="line">          for (c = 0; c &lt; layer1_size; c++) syn0[c + last_word * layer1_size] += neu1e[c];</div><div class="line">        &#125;</div></pre></td></tr></table></figure>
<h1 id="Word2Vec为什么快"><a href="#Word2Vec为什么快" class="headerlink" title="Word2Vec为什么快"></a>Word2Vec为什么快</h1><ul>
<li>用查表代替计算sigmoid</li>
<li>相对于神经网络的结构，去掉了隐藏层</li>
</ul>
<h1 id="增量训练"><a href="#增量训练" class="headerlink" title="增量训练"></a>增量训练</h1><p>从搜索引擎爬包含新词的文本，加上一个小语料，训练一个w2v模型。</p>
<p>对于每个新词，找出小模型中最接近的10个词，以及每个词与新词的相似度打分score。</p>
<p>再从大模型中找出每个词的词向量，每个维度乘以小模型中的score，最多叠加5个。再对每个维度取加权平均。</p>
<p>最后转成单位向量。</p>
<p>参考</p>
<p>【1】<a href="http://blog.csdn.net/google19890102/article/details/51887344" target="_blank" rel="noopener">http://blog.csdn.net/google19890102/article/details/51887344</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2018/03/17/机器学习/NLP/Word2Vec原理/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/03/17/机器学习/NLP/Word2Vec原理/" class="post-title-link" itemprop="url">Word2Vec原理</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-03-17 20:50:31" itemprop="dateCreated datePublished" datetime="2018-03-17T20:50:31+08:00">2018-03-17</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2018-03-25 12:46:10" itemprop="dateModified" datetime="2018-03-25T12:46:10+08:00">2018-03-25</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>参考</p>
<p>《word2vec数学原理.pdf》</p>
<h1 id="词向量基础"><a href="#词向量基础" class="headerlink" title="词向量基础"></a>词向量基础</h1><p>​    用词向量来表示词并不是word2vec的首创，在很久之前就出现了。最早的词向量是很冗长的，它使用是词向量维度大小为整个词汇表的大小，对于每个具体的词汇表中的词，将对应的位置置为1。比如我们有下面的5个词组成的词汇表，词”Queen”的序号为2， 那么它的词向量就是(0,1,0,0,0)。同样的道理，词”Woman”的词向量就是(0,0,0,1,0)。这种词向量的编码方式我们一般叫做1-of-N representation或者one hot representation.</p>
<p><img src="/2018/03/17/机器学习/NLP/Word2Vec原理/Word2Vec-CBOW和Skip-Gram模型/one hot representation.png" alt="one hot representation"></p>
<p>​    One hot representation用来表示词向量非常简单，但是却有很多问题。最大的问题是我们的词汇表一般都非常大，比如达到百万级别，这样每个词都用百万维的向量来表示简直是内存的灾难。这样的向量其实除了一个位置是1，其余的位置全部都是0，表达的效率不高，能不能把词向量的维度变小呢？</p>
<p>​    Dristributed representation可以解决One hot representation的问题，它的思路是通过训练，将每个词都映射到一个较短的词向量上来。所有的这些词向量就构成了向量空间，进而可以用普通的统计学的方法来研究词与词之间的关系。这个较短的词向量维度是多大呢？这个一般需要我们在训练时自己来指定。</p>
<p>​    比如下图我们将词汇表里的词用”Royalty”,”Masculinity”, “Femininity”和”Age”4个维度来表示，King这个词对应的词向量可能是(0.99,0.99,0.05,0.7)(0.99,0.99,0.05,0.7)。当然在实际情况中，我们并不能对词向量的每个维度做一个很好的解释。</p>
<p><img src="/2018/03/17/机器学习/NLP/Word2Vec原理/Word2Vec-CBOW和Skip-Gram模型/Dristributed representation.png" alt="Dristributed representation"></p>
<p>​    有了用Dristributed representation表示的较短的词向量，我们就可以较容易的分析词之间的关系了，比如我们将词的维度降维到2维，有一个有趣的研究表明，用下图的词向量表示我们的词时，我们可以发现：</p>
<script type="math/tex; mode=display">
\vec {King} - \vec {Man} + \vec {Woman} = \vec {Queen}</script><p><img src="/2018/03/17/机器学习/NLP/Word2Vec原理/Word2Vec-CBOW和Skip-Gram模型/vector relation.png" alt="vector relation"></p>
<p>可见我们只要得到了词汇表里所有词对应的词向量，那么我们就可以做很多有趣的事情了。不过，怎么训练得到合适的词向量呢？一个很常见的方法是使用神经网络语言模型。</p>
<h1 id="2-CBOW与Skip-Gram用于神经网络语言模型"><a href="#2-CBOW与Skip-Gram用于神经网络语言模型" class="headerlink" title="2. CBOW与Skip-Gram用于神经网络语言模型"></a>2. CBOW与Skip-Gram用于神经网络语言模型</h1><p>​    在word2vec出现之前，已经有用神经网络DNN来用训练词向量进而处理词与词之间的关系了。采用的方法一般是一个三层的神经网络结构（当然也可以多层），分为输入层，隐藏层和输出层(softmax层)。</p>
<p>​    这个模型是如何定义数据的输入和输出呢？一般分为CBOW（Continuous Bag-of-Words） 与Skip-Gram两种模型。</p>
<h2 id="2-1-CBOW"><a href="#2-1-CBOW" class="headerlink" title="2.1 CBOW"></a>2.1 CBOW</h2><p>上下文相关的词对应的词向量，而输出就是这特定的一个词的词向量。比如下面这段话，我们的上下文大小取值为4，特定的这个词是”Learning”，也就是我们需要的输出词向量,上下文对应的词有8个，前后各4个，这8个词是我们模型的输入。由于CBOW使用的是词袋模型，因此这8个词都是平等的，也就是不考虑他们和我们关注的词之间的距离大小，只要在我们上下文之内即可。</p>
<p><img src="/2018/03/17/机器学习/NLP/Word2Vec原理/Word2Vec-CBOW和Skip-Gram模型/cbow.png" alt="cbow"></p>
<p>在这个CBOW神经网络模型中，输入层有8个神经元（8个词向量），输出层有词汇表D大小的神经元。</p>
<p>目标函数通常为</p>
<script type="math/tex; mode=display">
L=\sum_{w \in C} \log p(w|Context(w))</script><blockquote>
<p> 为什么是这个形式？</p>
<p> 根据n-gram模型和对数最大似然，得到这个目标函数。</p>
</blockquote>
<p>包括四个层：输入、投影、隐藏、输出。</p>
<p><img src="/2018/03/17/机器学习/NLP/Word2Vec原理/Word2Vec-CBOW和Skip-Gram模型/神经网络层.png" alt="神经网络层"></p>
<p>对于语料C中的任意一个词w，将Context(w)设为取前面的n-1个词，这样二元对(Context(w),w)就是一个训练样本。</p>
<p>投影层向量$x_w$的构造是，将输入层的n-1个词向量按顺序首尾相接的拼起来，长度就是m(n-1)了（每个词向量的长度是m）。</p>
<p>从而</p>
<script type="math/tex; mode=display">
z_w = tanh(W{x_w}+p) \\
y_w=Uz_w+q</script><p>其中，tanh是双曲正切函数，用来做隐藏层的激活函数。</p>
<p>经过上面两步计算得到的$y<em>w=(y</em>{w,1},y<em>{w,2},…,y</em>{w,N})^T$是一个长度为N的向量，其分量不能代表概率。如果想要$y_{w,i}$表示当上下文为Context(w)时下一次为词典D中第i个词的概率，则还需要做一个<strong>softmax归一化</strong>，之后得到</p>
<script type="math/tex; mode=display">
p(w|Context(w))=\frac {e^{y_{w,i_w}}} {\sum_{i=1}^N e^{y_{w,i_w}}}</script><p>其中$i_w$表示词w在词典D中的索引。</p>
<p>与n-gram相比，神经概率语言模型的<strong>优势</strong>是：</p>
<p>1、词语之间的相似性可以通过词向量来体现。</p>
<p>1）神经网络模型通过上下文来预测，那么相似的上下文的词的词向量也是相似的；</p>
<p>2）概率函数关于词向量是光滑的，即词向量的一个小变化对概率的影响也是一个小变化。</p>
<p>2、词向量自带平滑功能（因为$p(w|Context(w)) \in (0,1)$不会为零）。</p>
<h2 id="2-2-Skip-Gram"><a href="#2-2-Skip-Gram" class="headerlink" title="2.2 Skip-Gram"></a>2.2 Skip-Gram</h2><p>与CBOW相反，输入是一个特定向量，输出是特定词对应的上下文词向量。</p>
<h1 id="3、基于Hierarchical-Softmax的模型"><a href="#3、基于Hierarchical-Softmax的模型" class="headerlink" title="3、基于Hierarchical Softmax的模型"></a>3、基于Hierarchical Softmax的模型</h1><h2 id="3-1-CBOW模型"><a href="#3-1-CBOW模型" class="headerlink" title="3.1 CBOW模型"></a>3.1 CBOW模型</h2><p>网络的构建</p>
<p><img src="/2018/03/17/机器学习/NLP/Word2Vec原理/Word2Vec-CBOW和Skip-Gram模型/cbow_network.png" alt="cbow_network"></p>
<p>输入层是上下文的若干个词的词向量</p>
<p>投影层就是将这些词向量直接相加。</p>
<p>层次Softmax的基本思想就是：</p>
<p><strong>对于词典D中的任意词w，Huffman树中必存在一条从根结点到词w对应结点的路径$p^w$，且这条路径是唯一的。路径$p^w$上存在$l^w-1$个分支，将每个分支看做是一个二分类，每一次分类就产生一个概率，将这些概率连乘起来，就是所需的$p(w|Context(w))$ 。</strong></p>
<p>条件概率连乘的公示可以写为</p>
<script type="math/tex; mode=display">
p(w|Context(w))=\prod_{j=2}^{l^w}p(d_j^w|x_w,\theta_{j-1}^w)</script><p>其中，</p>
<script type="math/tex; mode=display">
p(d_j^w|x_w,\theta_{j-1}^w)=[\sigma(x_w^T\theta_{j-1}^w)]^{1-d_j^w} \cdot [1-\sigma(x_w^T\theta_{j-1}^w)]^{d_j^w} \tag{3-1}</script><p>这里$\sigma(x<em>w^T\theta</em>{j-1}^w)$表示分到正类的概率。</p>
<p>将3-1代入对数似然函数，得到</p>
<script type="math/tex; mode=display">
L=\sum_{w \in C} \sum_{j=2}^{l^w} \{ (1-d_j^w)\log [\sigma(x_w^T\theta_{j-1}^w)] + d_j^w \log[1-\sigma(x_w^T\theta_{j-1}^w)] \}</script><p>其中，令</p>
<script type="math/tex; mode=display">
L(w,j)=(1-d_j^w)\log [\sigma(x_w^T\theta_{j-1}^w)] + d_j^w \log[1-\sigma(x_w^T\theta_{j-1}^w)]</script><p>至此，已经推导出对数似然函数，这就是CBOW模型的目标函数。</p>
<blockquote>
<p>参数$\theta$是怎样的矩阵？</p>
<p>我理解，任何用树的多个二分类问题，目标函数都可以表示成这种形式。</p>
</blockquote>
<p>为了使该目标函数最大化，word2vec采用的是<strong>随机梯度上升法</strong>。每取一个样本，计算梯度再刷新所有的参数。推导出更新公式为</p>
<script type="math/tex; mode=display">
\theta_{j-1}^w = \theta_{j-1}^w + \eta [1-d_j^w-\sigma(x_w^T\theta_{j-1}^w)]x_w^T</script><p>同样的</p>
<script type="math/tex; mode=display">
\frac {\partial L(w,j)} {\partial x_w}=[1-d_j^w-\sigma(x_w^T\theta_{j-1}^w)]\theta_{j-1}^w</script><p>我们的最终目的是要求词典D中每个词的词向量，而这里的$x_w$表示的是Context(w)所有词向量的累加和，那么如何利用偏导对$v(\tilde w), w \in Context(\tilde w)$进行更新呢，word2vec算法中直接取</p>
<script type="math/tex; mode=display">
v(\tilde w) = v(\tilde w)+\eta \sum_{j=2}^{l^w} \frac {\partial L(w,j)} {\partial x_w}</script><p>即把后面增量的值贡献到Context(w)的每一个词的词向量上。</p>
<p>伪代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">for j=huffman code:</div><div class="line">&#123;</div><div class="line">  1)q=sigma(x*theta)</div><div class="line">  2)g=eta(1-d-q)</div><div class="line">  3)e=e+g*theta</div><div class="line">  4)theta=theta+g*x</div><div class="line">&#125;</div><div class="line">for w in Context(w):</div><div class="line">&#123;</div><div class="line">  v(w) = v(w) + e</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>注意3、 4两步不能颠倒位置，要先计算出e，再更新θ。</p>
<h1 id="4、基于Negative-Sampling的模型"><a href="#4、基于Negative-Sampling的模型" class="headerlink" title="4、基于Negative Sampling的模型"></a>4、基于Negative Sampling的模型</h1><p>NEG不使用Huffman树，而是利用随机负采样，能大幅度提高性能。</p>
<p>前面的原理差不多，也是用梯度下降，关键在于，h-softmax是通过Huffman树的路径长度来进行迭代和更新参数；NEG是通过找出负采样来迭代。</p>
<h2 id="4-1-负采样算法"><a href="#4-1-负采样算法" class="headerlink" title="4.1 负采样算法"></a>4.1 负采样算法</h2><p>词典D中的词在语料C中出现的次数有高有低，对于那些高频词，被选为负采样的概率就应该比较大，反之较小。这是我们对采样过程的一个大致要求，本质上就是一个<strong>带权采样问题</strong>。</p>
<blockquote>
<p>设词典D中的每个词w对应一条线段l(w)，则线段的长度可以表示为</p>
<script type="math/tex; mode=display">
len(w)=\frac {count(w)} {\sum_{u \in D} count(u)}</script><p>也就是计算词频再归一化。现在将这些线段首尾相连拼接在一起，形成一个长度为1的单位线段。如果随机往这个单位线段上打点，则其中越长的线段命中概率越大。</p>
</blockquote>
<p>通过这些线段得到一个非等距剖分（假设分成N个区间），再定义一个等距剖分（假设分成M个区间），$M &gt;&gt; N$ 。</p>
<p><img src="/2018/03/17/机器学习/NLP/Word2Vec原理/Word2Vec-CBOW和Skip-Gram模型/table映射.png" alt="table映射"></p>
<p>建立如下的映射关系</p>
<script type="math/tex; mode=display">
Table(i) = w_k, \ where\  m_i \in I_k,\ i=1,2,...,M-1</script><p>那么，采样就是每次生成一个[1,M-1]之间的整数r，Table(r)就是一个样本。如果负采样的时候选到自己，就跳过再选。</p>
<h1 id="Word2Vec与神经网络模型的区别"><a href="#Word2Vec与神经网络模型的区别" class="headerlink" title="Word2Vec与神经网络模型的区别"></a>Word2Vec与神经网络模型的区别</h1><ul>
<li>NN的输入是类似n-gram，取前N-1个词，w2v取前后各n-1个词</li>
<li>NN多了隐层，输入到隐层用双曲正切当激活函数。</li>
<li>NN的输入词向量是首尾拼接，W2V是加总。这样当窗口中向量不足时，也不需要补。</li>
<li>NN的输出是一个长度为N的向量，就是整个词汇表的长度，然后再做一个softmax归一化，得到给定上下文时下一个词恰好为词汇表的某个词的概率。</li>
</ul>
<h1 id="疑问"><a href="#疑问" class="headerlink" title="疑问"></a>疑问</h1><p>1）在这里非叶子结点对应的那些向量就可以扮演参数的角色？</p>
<p>就是说可以把$\theta$当做待求的参数，这样就可以用sigmoid来求分类的概率。</p>
<p>参考</p>
<p>【1】<a href="http://www.cnblogs.com/pinard/p/7160330.html" target="_blank" rel="noopener">word2vec原理(一) CBOW与Skip-Gram模型基础</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2018/03/17/Elastic Search/iau的ES使用场景/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/03/17/Elastic Search/iau的ES使用场景/" class="post-title-link" itemprop="url">iau的ES使用场景</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-03-17 20:50:31" itemprop="dateCreated datePublished" datetime="2018-03-17T20:50:31+08:00">2018-03-17</time>
            </span>
          

          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Elastic-Search/" itemprop="url" rel="index"><span itemprop="name">Elastic Search</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>比较brand和competitor的交叉人群</p>
<p>分别算出brand和competitor的人群，然后再计算包含brand和competitor的人群，就得出交叉人群。</p>
<p>比较brand和competitor的距离</p>
<p>就是比较brand和competitor的每个产品和对方关键词的相关度。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/24/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/24/">24</a><span class="page-number current">25</span><a class="page-number" href="/page/26/">26</a><span class="space">&hellip;</span><a class="page-number" href="/page/32/">32</a><a class="extend next" rel="next" href="/page/26/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Schwimmer</p>
              <div class="site-description motion-element" itemprop="description">Record and Think!</div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">315</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">17</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">55</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          

          

          
          

          
            
          
          

        </div>
      </div>

      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Schwimmer</span>

  

  
</div>


  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.5.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v7.2.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/utils.js?v=7.2.0"></script>

  <script src="/js/motion.js?v=7.2.0"></script>



  
  


  <script src="/js/schemes/muse.js?v=7.2.0"></script>



  

  


  <script src="/js/next-boot.js?v=7.2.0"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


  

  

  

  



  




  

  

  

  

  

  

  

  

  

  

  

  

  

  

</body>
</html>
