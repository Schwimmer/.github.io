<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">






















<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">

<link rel="stylesheet" href="/css/main.css?v=7.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.2.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.2.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '7.2.0',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="Record and Think!">
<meta property="og:type" content="website">
<meta property="og:title" content="Schwimmer&#39;s Blog">
<meta property="og:url" content="https://schwimmer.github.io/page/28/index.html">
<meta property="og:site_name" content="Schwimmer&#39;s Blog">
<meta property="og:description" content="Record and Think!">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Schwimmer&#39;s Blog">
<meta name="twitter:description" content="Record and Think!">





  
  
  <link rel="canonical" href="https://schwimmer.github.io/page/28/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Schwimmer's Blog</title>
  




  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-143240576-1"></script>
  <script>
    var host = window.location.hostname;
    if (host !== "localhost" || !true) {
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-143240576-1');
    }
  </script>









  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Schwimmer's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
        </li>
      
    </ul>
  

  
    

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2017/09/11/机器学习/决策树/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/09/11/机器学习/决策树/" class="post-title-link" itemprop="url">决策树</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2017-09-11 11:49:53" itemprop="dateCreated datePublished" datetime="2017-09-11T11:49:53+08:00">2017-09-11</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-03-17 20:50:31" itemprop="dateModified" datetime="2018-03-17T20:50:31+08:00">2018-03-17</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2017/09/11/机器学习/决策树/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/09/11/机器学习/决策树/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="1-决策树"><a href="#1-决策树" class="headerlink" title="1 决策树"></a>1 决策树</h1><p>公式符号</p>
<script type="math/tex; mode=display">
\begin{align}
&Ent(X)\ \ \ 熵 \\
&Gain(X)\ \ \ 信息增益\\
&Gini(X)\ \ \ 基尼指数\\
&D\ \ \ 训练集\\
&A\ \ \ 训练集的某个特征\\
&N\ \ \ 特征A的类别总数\\
&K\ \ \ 标签分类的数量\\
\end{align}</script><h2 id="1-1-关键步骤-python实现"><a href="#1-1-关键步骤-python实现" class="headerlink" title="1.1 关键步骤-python实现"></a>1.1 关键步骤-python实现</h2><p>创建决策树分支的createBranch()伪代码函数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">检查数据集中每个子项是否属于同一个分类：</span><br><span class="line">	IF YES return 类标签；</span><br><span class="line">	ELSE </span><br><span class="line">		寻找划分数据集的最好特征；</span><br><span class="line">		划分数据集；</span><br><span class="line">		创建分支节点；</span><br><span class="line">			for 每个划分的子集</span><br><span class="line">				递归调用createBranch()并增加返回结果到分支节点中</span><br><span class="line">        return 分支节点</span><br></pre></td></tr></table></figure>
<p>对label的分类计算熵</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcEnt</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">	labelNum = len(dataSet)</span><br><span class="line">    ent = <span class="number">0.0</span></span><br><span class="line">	<span class="comment">#定义字典存放每个类别的count统计</span></span><br><span class="line">	labelCounts = &#123;&#125;</span><br><span class="line">    <span class="comment">#统计每个label的个数</span></span><br><span class="line">	<span class="keyword">for</span> featureVec <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="comment">#最后一列是label</span></span><br><span class="line">		label = featureVec[<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">if</span> label <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys(): labelCounts[label] = <span class="number">0</span></span><br><span class="line">        labelCounts[label] += <span class="number">1</span></span><br><span class="line">    <span class="comment">#计算概率以及熵</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> labelCounts:</span><br><span class="line">        prob = float(labelCounts[key]) / labelNum</span><br><span class="line">        ent -= prob * log(<span class="number">2</span>, prob)</span><br><span class="line">    <span class="keyword">return</span> ent</span><br></pre></td></tr></table></figure>
<p>对数据集进行划分</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">splitDataSet</span><span class="params">(dataSet, axis, value)</span>:</span></span><br><span class="line">    subDataSet = []</span><br><span class="line">    <span class="keyword">for</span> featureVec <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="keyword">if</span> featureVec[axis] == value:</span><br><span class="line">            reducedFeatVec = featureVec[:axis]</span><br><span class="line">            reducedFeatVec.extend(featureVec[axis+<span class="number">1</span>:])</span><br><span class="line">    		resDataSet.append(reducedFeatVec)</span><br><span class="line">    <span class="keyword">return</span> subDataSet</span><br></pre></td></tr></table></figure>
<p>选出最好的数据集划分方式</p>
<p><strong>信息增益</strong></p>
<p><strong>熵</strong>的定义是</p>
<script type="math/tex; mode=display">
Ent(X) = -\sum_{i=1}^{n}p(x_i)log_2p(x_i)</script><p>n是类别总数。</p>
<p><strong>条件熵</strong>$Ent(Y|X)$表示在已知X的条件下Y的不确定性，定义为给定X时Y的条件概率分布的熵对X的期望</p>
<script type="math/tex; mode=display">
Ent(Y|X)=\sum_{i=1}^np_iEnt(Y|X=x_i)</script><p>对于训练集D以及其中的特征A，熵就是</p>
<script type="math/tex; mode=display">
Ent(D) = -\sum_{k=1}^K \frac {|C_k|}{|D|} log_2\frac{|C_k|}{|D|}</script><p>其中，K是标签分类的数量，$C_k$是每个分类的样本数</p>
<p>条件熵就是</p>
<script type="math/tex; mode=display">
\begin{aligned} 
Ent(D|A) &=\sum_{i=1}^N\frac{|D_i|}{|D|}Ent(D_i) \\
&=\sum_{i=1}^N\frac{|D_i|}{|D|}(-\sum_{k=1}^K \frac {|D_{ik}|}{|D_i|} log_2\frac{|D_{ik}|}{|D_i|})
\end{aligned}</script><p>其中，N是特征A的类别总数，$D_i$是特征A的每种类别的数量。</p>
<p>信息增益就是两者之差</p>
<script type="math/tex; mode=display">
Gain(D,A)=Ent(D)-Ent(D|A)</script><p>信息增益也称为<strong>互信息</strong>。</p>
<p>找出信息增益最大的来划分数据集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseBestFeature</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">	<span class="comment">#feature数量，最后一列是label</span></span><br><span class="line">	numFeature = len(dataSet[<span class="number">0</span>]<span class="number">-1</span>)</span><br><span class="line">    bestInfoGain = <span class="number">0.0</span></span><br><span class="line">    bestFeature = <span class="number">-1</span></span><br><span class="line">	<span class="comment">#先计算熵</span></span><br><span class="line">	baseEntropy = calcEnt(dataSet)</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(numFeature):</span><br><span class="line">		<span class="comment">#首先需要知道该特征有几个值</span></span><br><span class="line">		uniqueValue = set([sample[i] <span class="keyword">for</span> sample <span class="keyword">in</span> dataSet]) <span class="comment">#用set去重是最快方法</span></span><br><span class="line">		newEntropy = <span class="number">0.0</span></span><br><span class="line">        <span class="comment">#对于每个特征，计算条件熵</span></span><br><span class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> uniqueValue:</span><br><span class="line">            <span class="comment">#用这个特征划分数据集</span></span><br><span class="line">            subDataSet = splitDataSet(dataSet, i, value)</span><br><span class="line">            newEntropy += calcEnt(subDataSet)</span><br><span class="line">        <span class="comment">#计算信息增益</span></span><br><span class="line">        infoGain = baseEntropy-newEntropy</span><br><span class="line">        <span class="keyword">if</span> infoGain &gt; bestInfoGain:</span><br><span class="line">            bestInfoGain = infoGain</span><br><span class="line">            bestFeature = i</span><br><span class="line"><span class="keyword">return</span> bestFeature</span><br></pre></td></tr></table></figure>
<p>如果所有特征都处理过了，但是类标签依然不是唯一的，用投票决定</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">majorityCnt</span><span class="params">(classList)</span>:</span></span><br><span class="line">	classCount=&#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> vote <span class="keyword">in</span> classList:</span><br><span class="line">        <span class="keyword">if</span> vote <span class="keyword">not</span> <span class="keyword">in</span> classCount.keys() classCount[vote] = <span class="number">0</span></span><br><span class="line">        classCount[vote] += <span class="number">1</span></span><br><span class="line">    sortedClassCount = sorted(classCount, key = operator.itemgetter(<span class="number">1</span>), reverse = <span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<h2 id="1-2-ID3算法"><a href="#1-2-ID3算法" class="headerlink" title="1.2 ID3算法"></a>1.2 ID3算法</h2><p>与上面的步骤类似。但是ID3只有树的生成，容易过拟合。</p>
<h2 id="1-3-C4-5算法"><a href="#1-3-C4-5算法" class="headerlink" title="1.3 C4.5算法"></a>1.3 C4.5算法</h2><p>与ID3相比，C4.5用信息增益比来选择特征。</p>
<p><strong>信息增益比</strong></p>
<p>在面对类别比较少的离散数据时，两者差不多。但如果面对连续的数据（如体重、身高、年龄、距离等），或者每列数据没有明显的类别之分（最极端的例子的该列所有数据都独一无二）。</p>
<p>那么根据信息增益公式，$Ent(D)$不变，当数据独一无二时，</p>
<script type="math/tex; mode=display">
Ent(D|A)=\sum_{i=1}^n \frac {1}{n}Ent(D_i)</script><p>这样$Ent(D|A)$最小，程序会倾向于这种划分，导致划分效果差。</p>
<p>信息增益比的公式为</p>
<script type="math/tex; mode=display">
Gain_R(D,A)=\frac {Gain(D,A)}{Ent(D)}</script><p>可以理解成<strong>对分支数目的惩罚项</strong>。</p>
<h2 id="1-5-CART算法"><a href="#1-5-CART算法" class="headerlink" title="1.5 CART算法"></a>1.5 CART算法</h2><p>CART是分类与回归树，由特征选择、树的生成和剪枝组成。</p>
<p>CART是在给定输入变量X条件下输出随机变量Y的条件概率分布的方法。CART假设决策树是<strong>二叉树</strong>，内部结点特征的取值为是和否，约定左是右否。</p>
<p>决策树等价于递归的二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定的条件下输出的条件概率分布。</p>
<p>1.5.1 CART的生成</p>
<p>递归构建二叉树的过程。回归树用<strong>最小二乘</strong>，分类树用<strong>基尼指数</strong>。</p>
<p>1）回归树</p>
<p>2）分类树</p>
<p>假设有K个类，样本点属于第k类的概率是$p_k$，则基尼指数定义为</p>
<script type="math/tex; mode=display">
Gini(p) = \sum_{k=1}^K p_k(1-p_k) = 1-\sum_{k=1}^K p_k^2=1-\sum_{k=1}^K(\frac{|C_k|}{|D|})^2</script><p>如果是两分类问题，则概率分布的基尼指数为</p>
<script type="math/tex; mode=display">
Gini(P)=2p(1-p)</script><p>若样本集合D根据特征A是否取某一值a被划分为$D_1$和$D_2$两部分，即</p>
<script type="math/tex; mode=display">
D_1=\{(x,y)\in D | A(x)=a\}, D_2=D-D_1</script><p>则在特征A的条件下，集合D的基尼指数为</p>
<script type="math/tex; mode=display">
Gini(D,A)=\frac {|D_1|}{|D|}Gini(D_1)+\frac {|D_2|}{|D|}Gini(D_2)</script><p>Gini越大，样本集合的不确定性越大，与熵相似。</p>
<p><strong>算法过程</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">输入：训练集D，停止条件</span><br><span class="line">输出：CART决策树</span><br><span class="line"></span><br><span class="line">从根结点递归对每个结点进行以下操作，构建二叉树：</span><br><span class="line">1）对每个特征和可能的取值a，根据A=a的为是或否，将D分割成D1和D2，计算基尼指数</span><br><span class="line">2）选出基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。</span><br><span class="line">3）对两个子结点递归调用#1、#2，直至满足停止条件</span><br><span class="line">4）生成CART决策树</span><br></pre></td></tr></table></figure>
<p>3）CART剪枝</p>
<h2 id="1-6-决策树的剪枝"><a href="#1-6-决策树的剪枝" class="headerlink" title="1.6 决策树的剪枝"></a>1.6 决策树的剪枝</h2><blockquote>
<p>如何判断剪枝后泛华性能提升？</p>
<p>用<strong>留出法</strong>。将一部分训练集作为验证集。</p>
</blockquote>
<p>剪枝是为了解决过拟合。通过极小化决策树整体的损失函数来实现。设树T的叶结点个数为$|T|$，t是T的叶结点，该叶结点有$N<em>t$个样本点，其中k类的样本点有$N</em>{tk}$个，则损失函数定义为</p>
<script type="math/tex; mode=display">
C_{\alpha}(T) = \sum_{t=1}^T N_tEnt_t(T) + \alpha|T|</script><p>由于</p>
<script type="math/tex; mode=display">
Ent_t(T) =  - \sum_{k=1}^K \frac {N_{tk}}{N_t} log_2\frac {N_{tk}}{N_t}</script><p>则令</p>
<script type="math/tex; mode=display">
C(T) = - \sum_{t=1}^T\sum_{k=1}^KN_{tk}log_2\frac {N_{tk}}{N_t}</script><p>于是</p>
<script type="math/tex; mode=display">
C_\alpha(T) = C(T) +\alpha|T|</script><p>这里，$C(T)$表示训练数据的预测误差，$|T|$表示模型复杂度，$\alpha$控制两者影响，较大时选择较简单的树，反之亦然，等于0时就不考虑模型复杂度。</p>
<p>两种剪枝思路</p>
<p><strong>预剪枝（Pre-Pruning）</strong></p>
<p>构造的同时剪枝。比如设一个阈值，熵减小的数量小于这个阈值，即使还可以继续降低熵，也停止继续创建分支。</p>
<blockquote>
<p>有些分支虽然当前划分时性能下降，但后续划分有可能又会提高，仅根据当前验证集来判断是否要继续划分，往往会导致欠拟合。</p>
</blockquote>
<p><strong>后剪枝（Post-Pruning）</strong></p>
<p>三种主要方法</p>
<p><strong>1）REP错误率降低剪枝</strong></p>
<p>简单粗暴，对每个非叶结点的子树，用其替换一个叶结点，类别用子树覆盖训练样本中类最多的代替。这样产生的简化树再跟原树比较在测试数据集中的效果。若错误更少就替换。算法以Bottom-up的方式遍历所有的子树，直到没有任何改进时，终止。</p>
<p><strong>2）PEP悲观剪枝</strong></p>
<h2 id="1-7-连续和缺失值处理"><a href="#1-7-连续和缺失值处理" class="headerlink" title="1.7 连续和缺失值处理"></a>1.7 连续和缺失值处理</h2><p>1）连续值离散化</p>
<p>jueceshu最简单的策略是<strong>二分法</strong>，也是C4.5采用的机制。</p>
<p>对于连续属性a，可以考察包含n-1个元素的候选划分点集合</p>
<script type="math/tex; mode=display">
T_a=\left \{  \frac {a^i+a^{i+1}} {2} | 1 \leqslant i \leqslant n-1    \right \}</script><p>即把区间的中位点作为候选划分点，然后像离散值那样考察划分点，再选出最优的划分点。</p>
<p>2）缺失值</p>
<p>考虑：①如何在属性值缺失的情况下进行划分属性选择？②给定划分属性，若样本在该属性的值缺失，如何划分？</p>
<p>靠<strong>权重</strong>。在判定划分时，权重相等，用已知的样本来划分属性。对于每个划分属性，若属性缺失，将缺失的记录根据属性的每个划分所占比例作为权重，分到属性的每个子结点中。</p>
<h2 id="1-8-多变量决策树"><a href="#1-8-多变量决策树" class="headerlink" title="1.8 多变量决策树"></a>1.8 多变量决策树</h2><p>非子结点不再针对某个属性，而是多个属性的线性组合。即，每个非子结点都是一个线性分类器。</p>
<h1 id="2、随机森林"><a href="#2、随机森林" class="headerlink" title="2、随机森林"></a>2、随机森林</h1><p>随机森林如何随机的，特征也随机，样本也随机</p>
<h1 id="3、GBDT"><a href="#3、GBDT" class="headerlink" title="3、GBDT"></a>3、GBDT</h1><p><a href="https://www.zhihu.com/question/266195966" target="_blank" rel="noopener">决策树是否应该用one-hot编码</a></p>
<p>参考</p>
<p>统计学习方法</p>
<p><a href="http://www.jianshu.com/p/794d08199e5e" target="_blank" rel="noopener">决策树的剪枝问题</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2017/08/13/机器学习/NLP/Gensim-LDA/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/08/13/机器学习/NLP/Gensim-LDA/" class="post-title-link" itemprop="url">（转）LDA</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2017-08-13 11:49:53" itemprop="dateCreated datePublished" datetime="2017-08-13T11:49:53+08:00">2017-08-13</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-02-01 23:21:31" itemprop="dateModified" datetime="2018-02-01T23:21:31+08:00">2018-02-01</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2017/08/13/机器学习/NLP/Gensim-LDA/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/08/13/机器学习/NLP/Gensim-LDA/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><h1 id="gensim-LDA"><a href="#gensim-LDA" class="headerlink" title="gensim-LDA"></a>gensim-LDA</h1><p><a href="http://blog.csdn.net/whzhcahzxh/article/details/17528261" target="_blank" rel="noopener">http://blog.csdn.net/whzhcahzxh/article/details/17528261</a></p>
<p>引用gensim包，gensim包中引用corpora,models, similarities，分别做语料库建立，模型库和相似度比较库</p>
<p>from gensim import corpora, models, similarities</p>
<p>import jieba</p>
<h3 id="1、分词"><a href="#1、分词" class="headerlink" title="1、分词"></a>1、分词</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sentences = [&quot;我喜欢吃土豆&quot;,&quot;土豆是个百搭的东西&quot;,&quot;我不喜欢今天雾霾的北京&quot;]</span><br><span class="line">words=[]</span><br><span class="line">for doc in sentences:</span><br><span class="line"># 结巴分词返回的是一个generator，要用list()转成list</span><br><span class="line">    words.append(list(jieba.cut(doc)))</span><br><span class="line">print words</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<p>[[u’\u6211’, u’\u559c\u6b22’, u’\u5403’, u’\u571f\u8c46’], [u’\u571f\u8c46’, u’\u662f’, u’\u4e2a’, u’\u767e’, u’\u642d’, u’\u7684’, u’\u4e1c\u897f’], [u’\u6211’, u’\u4e0d’, u’\u559c\u6b22’, u’\u4eca\u5929’, u’\u96fe’, u’\u973e’, u’\u7684’, u’\u5317\u4eac’]]</p>
<p>此时输出的格式为unicode，不影响后期运算，因此我保留不变，如果想看分词结果可以用循环输出jieba分词结果</p>
<h3 id="2、分词结果构造词典"><a href="#2、分词结果构造词典" class="headerlink" title="2、分词结果构造词典"></a>2、分词结果构造词典</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">dic = corpora.Dictionary(words)</span><br><span class="line"># 词袋中的所有词</span><br><span class="line">print dic</span><br><span class="line"># 每个词和编号</span><br><span class="line">print dic.token2id</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<p>Dictionary(15 unique tokens: [u’\u973e’, u’\u5403’, u’\u5317\u4eac’, u’\u7684’, u’\u4e1c\u897f’]…)</p>
<p>{u’\u973e’: 14, u’\u5403’: 0, u’\u5317\u4eac’: 12, u’\u7684’: 9, u’\u4e1c\u897f’: 4, u’\u4e2a’: 5, u’\u642d’: 6, u’\u662f’: 7, u’\u6211’: 3, u’\u559c\u6b22’: 1, u’\u4eca\u5929’: 11, u’\u571f\u8c46’: 2, u’\u4e0d’: 10, u’\u96fe’: 13, u’\u767e’: 8}</p>
<p>为方便看数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">for word,index in dic.token2id.iteritems():</span><br><span class="line">    print word +&quot; 编号为:&quot;+ str(index)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<p>北京 编号为:12<br>搭 编号为:6<br>的 编号为:9<br>喜欢 编号为:1<br>不 编号为:10<br>东西 编号为:4<br>土豆 编号为:2<br>霾 编号为:14<br>是 编号为:7<br>个 编号为:5<br>雾 编号为:13<br>百 编号为:8<br>今天 编号为:11<br>我 编号为:3<br>吃 编号为:0</p>
<h3 id="3、生成语料库"><a href="#3、生成语料库" class="headerlink" title="3、生成语料库"></a>3、生成语料库</h3><p>词袋模型</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">corpus = [dic.doc2bow(text) for text in words]</span><br><span class="line">print corpus</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<p>[[(0, 1), (1, 1), (2, 1), (3, 1)], [(2, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1)], [(1, 1), (3, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1)]]</p>
<h3 id="4、TFIDF变换"><a href="#4、TFIDF变换" class="headerlink" title="4、TFIDF变换"></a>4、TFIDF变换</h3><p>通过语料库得到tfidf的值，由tfidf来描述句子</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#通过语料库得到tfidf模型</span></span><br><span class="line">tfidf = models.TfidfModel(corpus)</span><br><span class="line"></span><br><span class="line">vec = [(<span class="number">0</span>, <span class="number">1</span>), (<span class="number">4</span>, <span class="number">1</span>)]</span><br><span class="line"><span class="keyword">print</span> tfidf[vec]</span><br><span class="line"><span class="comment">#对corpus的每个文档中的每个词计算tfidf，得到的结果是，每个文档的每个词都是一个元组，包括id和tfidf值</span></span><br><span class="line">corpus_tfidf = tfidf[corpus]</span><br><span class="line"><span class="keyword">for</span> doc <span class="keyword">in</span> corpus_tfidf:</span><br><span class="line">    <span class="keyword">print</span> doc</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<p>[(0, 0.7071067811865475), (4, 0.7071067811865475)]<br>[(0, 0.8425587958192721), (1, 0.3109633824035548), (2, 0.3109633824035548), (3, 0.3109633824035548)]<br>[(2, 0.16073253746956623), (4, 0.4355066251613605), (5, 0.4355066251613605), (6, 0.4355066251613605), (7, 0.4355066251613605), (8, 0.4355066251613605), (9, 0.16073253746956623)]<br>[(1, 0.1586956620869655), (3, 0.1586956620869655), (9, 0.1586956620869655), (10, 0.42998768831312806), (11, 0.42998768831312806), (12, 0.42998768831312806), (13, 0.42998768831312806), (14, 0.42998768831312806)]</p>
<p>vec是查询文本向量，比较vec和训练中的三句话相似度</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features=14)</span><br><span class="line">sims = index[tfidf[vec]]</span><br><span class="line">print list(enumerate(sims))</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<p>[(0, 0.59577906), (1, 0.30794966), (2, 0.0)]</p>
<p>表示和第1句话相似度为59.578%，和第二句话的相似度位30.79%，第三句没有相似度，</p>
<p>我们看看vec这句话是什么：0为吃，4为东西，所以vec这句话可以是[“吃东西”]或者[“东西吃”]</p>
<p>而第一句话”我喜欢吃土豆”,”土豆是个百搭的东西”明显有相似度，而第三句话”我不喜欢今天雾霾的北京”，相似度几乎为0，至于为什么第一句比第二句更相似，就需要考虑TfIdf document representation和cosine similarity measure了</p>
<p>回到tfidf转换，接着训练LSI模型，假定三句话属于2个主题，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lsi = models.LsiModel(corpus_tfidf, id2word=dic, num_topics=<span class="number">2</span>)</span><br><span class="line">lsiout=lsi.print_topics(<span class="number">2</span>)</span><br><span class="line"><span class="keyword">print</span> lsiout[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">print</span> lsiout[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<p>0.532<em>“吃” + 0.290</em>“喜欢” + 0.290<em>“我” + 0.258</em>“土豆” + 0.253<em>“霾” + 0.253</em>“雾” + 0.253<em>“北京” + 0.253</em>“今天” + 0.253<em>“不” + 0.166</em>“东西”<br>0.393<em>“百” + 0.393</em>“搭” + 0.393<em>“东西” + 0.393</em>“是” + 0.393<em>“个” + -0.184</em>“霾” + -0.184<em>“雾” + -0.184</em>“北京” + -0.184<em>“今天” + -0.184</em>“不”</p>
<p>这就是基于SVD建立的两个主题模型内容</p>
<p>将文章投影到主题空间中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">corpus_lsi = lsi[corpus_tfidf]</span><br><span class="line"><span class="keyword">for</span> doc <span class="keyword">in</span> corpus_lsi:</span><br><span class="line">    <span class="keyword">print</span> doc</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<p>[(0, -0.70861576320682107), (1, 0.1431958007198823)]<br>[(0, -0.42764142348481798), (1, -0.88527674470703799)]<br>[(0, -0.66124862582594512), (1, 0.4190711252114323)]</p>
<p>因此第一三两句和主题一相似，第二句和主题二相似</p>
<p>同理做个LDA</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">lda = models.LdaModel(corpus_tfidf, id2word=dic, num_topics=<span class="number">2</span>)</span><br><span class="line">ldaOut=lda.print_topics(<span class="number">2</span>)</span><br><span class="line"><span class="keyword">print</span> ldaOut[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">print</span> ldaOut[<span class="number">1</span>]</span><br><span class="line">corpus_lda = lda[corpus_tfidf]</span><br><span class="line"><span class="keyword">for</span> doc <span class="keyword">in</span> corpus_lda:</span><br><span class="line">    <span class="keyword">print</span> doc</span><br></pre></td></tr></table></figure>
<p>得到的结果每次都变，给一次的输出：</p>
<p>0.077<em>吃 + 0.075</em>北京 + 0.075<em>雾 + 0.074</em>今天 + 0.073<em>不 + 0.072</em>霾 + 0.070<em>喜欢 + 0.068</em>我 + 0.062<em>的 + 0.061</em>土豆<br>0.091<em>吃 + 0.073</em>搭 + 0.073<em>土豆 + 0.073</em>个 + 0.073<em>是 + 0.072</em>百 + 0.071<em>东西 + 0.066</em>我 + 0.065<em>喜欢 + 0.059</em>霾<br>[(0, 0.31271095988105352), (1, 0.68728904011894654)]<br>[(0, 0.19957991735916861), (1, 0.80042008264083142)]<br>[(0, 0.80940337254233863), (1, 0.19059662745766134)]</p>
<p>第一二句和主题二相似，第三句和主题一相似</p>
<p>结论和LSI不一样，我估计这和样本数目太少，区别度不高有关，毕竟让我来区分把第一句和哪一句分在一个主题，我也不确定</p>
<p>输入一句话，查询属于LSI得到的哪个主题类型，先变成词袋模型，然后查询LSI：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">query = <span class="string">"雾霾"</span></span><br><span class="line">query_bow = dic.doc2bow(list(jieba.cut(query)))</span><br><span class="line"><span class="keyword">print</span> query_bow</span><br><span class="line">query_lsi = lsi[query_bow]</span><br><span class="line"><span class="keyword">print</span> query_lsi</span><br></pre></td></tr></table></figure>
<p>输出:</p>
<p>[(13, 1), (14, 1)]<br>[(0, 0.50670602027401368), (1, -0.3678056037187441)]</p>
<p>与第一个主题相似</p>
<p>比较和第几句话相似，用LSI得到的索引接着做，并排序输出</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">index = similarities.MatrixSimilarity(lsi[corpus])</span><br><span class="line">sims = index[query_lsi]</span><br><span class="line"><span class="keyword">print</span> list(enumerate(sims))</span><br><span class="line">sort_sims = sorted(enumerate(sims), key=<span class="keyword">lambda</span> item: -item[<span class="number">1</span>])</span><br><span class="line"><span class="keyword">print</span> sort_sims</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<p>[(0, 0.90161765), (1, -0.10271341), (2, 0.99058259)]<br>[(2, 0.99058259), (0, 0.90161765), (1, -0.10271341)]</p>
<p>可见和第二句话相似度很高，因为只有第二句话出现了雾霾两个词，可是惊讶的是和第一句话的相似度也很高，这得益于LSI模型的算法：<strong>在A和C共现，B和C共现的同时，可以找到A和B的相似度</strong></p>
<p>代码位于<code>blogcodes/gensim_lda.py</code></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2017/08/12/机器学习/NLP/Gensim-Tutorials/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/08/12/机器学习/NLP/Gensim-Tutorials/" class="post-title-link" itemprop="url">Gensim-Tutorials</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2017-08-12 11:49:53" itemprop="dateCreated datePublished" datetime="2017-08-12T11:49:53+08:00">2017-08-12</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-02-01 23:21:31" itemprop="dateModified" datetime="2018-02-01T23:21:31+08:00">2018-02-01</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2017/08/12/机器学习/NLP/Gensim-Tutorials/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/08/12/机器学习/NLP/Gensim-Tutorials/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="http://radimrehurek.com/gensim/tutorial.html" target="_blank" rel="noopener">http://radimrehurek.com/gensim/tutorial.html</a></p>
<p>Gensim 使用Python标准logging模块来记录log，使用方法是</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import logging</span><br><span class="line">logging.basicConfig(format=&apos;%(asctime)s : %(levelname)s : %(message)s&apos;, level=logging.INFO)</span><br></pre></td></tr></table></figure>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2017/08/12/机器学习/NLP/NLP代码片段/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/08/12/机器学习/NLP/NLP代码片段/" class="post-title-link" itemprop="url">NLP代码片段</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2017-08-12 11:49:53" itemprop="dateCreated datePublished" datetime="2017-08-12T11:49:53+08:00">2017-08-12</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-03-31 10:28:31" itemprop="dateModified" datetime="2018-03-31T10:28:31+08:00">2018-03-31</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2017/08/12/机器学习/NLP/NLP代码片段/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/08/12/机器学习/NLP/NLP代码片段/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="根据标点拆分句子"><a href="#根据标点拆分句子" class="headerlink" title="根据标点拆分句子"></a>根据标点拆分句子</h1><p><code>[AtomSplit.java](../../../../gitlab/user-gene/nlp/src/main/java/com/buzzinate/nlp/segment/AtomSplit.java)</code></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> List&lt;String&gt; <span class="title">splitSentences</span><span class="params">(String text)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">		List&lt;String&gt; result = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line">		<span class="keyword">int</span> last = <span class="number">0</span>;</span><br><span class="line">		<span class="keyword">for</span> (Term term: Segment.split(text, ToAnalysis.USE_USER_DEFINE)) &#123;</span><br><span class="line">			<span class="keyword">if</span> (sentenceNatures.contains(term.getNatrue().natureStr) &amp;&amp; !isWhiteSpace(term.getName())) &#123;</span><br><span class="line">				<span class="keyword">if</span> (term.getOffe() &gt; last) &#123;</span><br><span class="line">					String snippet = text.substring(last, term.getOffe()).trim();  </span><br><span class="line">					<span class="keyword">if</span> (snippet.length() &gt; <span class="number">0</span>) result.add(snippet);</span><br><span class="line">				&#125;</span><br><span class="line">				last = term.getOffe() + term.getName().length();</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">if</span> (text.length() &gt; last) &#123;</span><br><span class="line">			String snippet = text.substring(last, text.length()).trim();  </span><br><span class="line">			<span class="keyword">if</span> (snippet.length() &gt; <span class="number">0</span>) result.add(snippet);</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">return</span> result;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">isWhiteSpace</span><span class="params">(String term)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> term.length() == <span class="number">1</span> &amp;&amp; (Character.isWhitespace(term.charAt(<span class="number">0</span>)) || term.charAt(<span class="number">0</span>) == <span class="string">'-'</span>);</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>
<h1 id="给每个字符标记类型"><a href="#给每个字符标记类型" class="headerlink" title="给每个字符标记类型"></a>给每个字符标记类型</h1><p><code>[AtomSplit.java](../../../../gitlab/user-gene/nlp/src/main/java/com/buzzinate/nlp/segment/AtomSplit.java)</code></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 给每个字符标记类型，如</span></span><br><span class="line"><span class="comment"> * [2011(AT_NUM), -(AT_PUNC), 34(AT_NUM), -(AT_PUNC), 43(AT_NUM),  (AT_PUNC), 为(AT_CHINESE), 中(AT_CHINESE), 国(AT_CHINESE)]</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> text</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> List&lt;Atom&gt; <span class="title">split</span><span class="params">(String text)</span> </span>&#123;</span><br><span class="line">	List&lt;Atom&gt; result = <span class="keyword">new</span> ArrayList&lt;Atom&gt;();</span><br><span class="line">	<span class="keyword">int</span> last = <span class="number">0</span>;</span><br><span class="line">	AtomType t = AtomType.AT_LETTER;</span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; text.length(); i++) &#123;</span><br><span class="line">		<span class="keyword">char</span> ch = text.charAt(i);</span><br><span class="line">		<span class="keyword">if</span> (TextUtil.isAlphaOrDigit(ch) || ch == <span class="string">'\''</span> || ch == <span class="string">'.'</span>) &#123;</span><br><span class="line">			<span class="keyword">if</span> (i == last) &#123;</span><br><span class="line">				t = AtomType.AT_LETTER;</span><br><span class="line">				<span class="keyword">if</span> (Character.isDigit(ch)) t = AtomType.AT_NUM;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (Character.isLetter(ch)) &#123;</span><br><span class="line">			<span class="keyword">if</span> (i &gt; last) result.add(<span class="keyword">new</span> Atom(text.substring(last, i), t));</span><br><span class="line">			result.add(<span class="keyword">new</span> Atom(text.substring(i, i+<span class="number">1</span>), AtomType.AT_CHINESE));</span><br><span class="line">			last = i + <span class="number">1</span>;</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			<span class="keyword">if</span> (i &gt; last) result.add(<span class="keyword">new</span> Atom(text.substring(last, i), t));</span><br><span class="line">			<span class="keyword">if</span> (t != AtomType.AT_LETTER || !isConnectChar(ch)) result.add(<span class="keyword">new</span> Atom(text.substring(i, i+<span class="number">1</span>), AtomType.AT_PUNC));</span><br><span class="line">			last = i + <span class="number">1</span>;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">if</span> (text.length() &gt; last) result.add(<span class="keyword">new</span> Atom(text.substring(last, text.length()), t));</span><br><span class="line">	<span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="判断字符串的语言"><a href="#判断字符串的语言" class="headerlink" title="判断字符串的语言"></a>判断字符串的语言</h1><p>有两个开源的项目可以使用。一个是Apache Tika，一个是language-detection。language-detection是google Code上开源的一个语言检测软件包，不折不扣的日货，但使用起来非常方便，其project链接如下：<a href="http://code.google.com/p/language-detection" target="_blank" rel="noopener">http://code.google.com/p/language-detection</a>。基本上，你只需要引用langdetect.jar和其依赖的jsonic-1.3.0.jar（也是日货）即可</p>
<p><code>/rocket-iaudience-api/src/main/java/com/iclick/rocket/iaudience/api/common/LanguageDetectUtil.java</code></p>
<h2 id="判断中文字符"><a href="#判断中文字符" class="headerlink" title="判断中文字符"></a>判断中文字符</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Character.isLetter()</span><br></pre></td></tr></table></figure>
<h2 id="判断是否为英文或数字"><a href="#判断是否为英文或数字" class="headerlink" title="判断是否为英文或数字"></a>判断是否为英文或数字</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">isAlphaOrDigit</span><span class="params">(<span class="keyword">char</span> ch)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">if</span> (ch &gt;= <span class="string">'a'</span> &amp;&amp; ch &lt;= <span class="string">'z'</span>)</span><br><span class="line">		<span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">	<span class="keyword">if</span> (ch &gt;= <span class="string">'A'</span> &amp;&amp; ch &lt;= <span class="string">'Z'</span>)</span><br><span class="line">		<span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">	<span class="keyword">if</span> (ch &gt;= <span class="string">'0'</span> &amp;&amp; ch &lt;= <span class="string">'9'</span>)</span><br><span class="line">		<span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">	<span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="去除停用词"><a href="#去除停用词" class="headerlink" title="去除停用词"></a>去除停用词</h1><p>java</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ArrayList&lt;String&gt; wordSet = new ArrayList&lt;String&gt;();</span><br><span class="line">// 自动去除停用词</span><br><span class="line">		for (Term term : NotionalTokenizer.segment(simplePhrase)) &#123;</span><br><span class="line">			wordSet.add(term.word);</span><br><span class="line">		&#125;</span><br></pre></td></tr></table></figure>
<p>python参考gensim</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">documents = [&quot;Human machine interface for lab abc computer applications&quot;,</span><br><span class="line">             &quot;A survey of user opinion of computer system response time&quot;,</span><br><span class="line">             &quot;The EPS user interface management system&quot;,</span><br><span class="line">             &quot;System and human system engineering testing of EPS&quot;,              </span><br><span class="line">             &quot;Relation of user perceived response time to error measurement&quot;,</span><br><span class="line">             &quot;The generation of random binary unordered trees&quot;,</span><br><span class="line">             &quot;The intersection graph of paths in trees&quot;,</span><br><span class="line">             &quot;Graph minors IV Widths of trees and well quasi ordering&quot;,</span><br><span class="line">             &quot;Graph minors A survey&quot;]</span><br><span class="line">#停用词</span><br><span class="line">stoplist = set(&apos;for a of the and to in&apos;.split())</span><br><span class="line">texts = [ [word for word in document.lower().split() if word not in stoplist ]</span><br><span class="line">         for document in documents ]</span><br><span class="line">         </span><br><span class="line">#删除仅出现一次的词</span><br><span class="line">from collections import defaultdict</span><br><span class="line">frequency = defaultdict(int)</span><br><span class="line">for text in texts:</span><br><span class="line">    for token in text:</span><br><span class="line">        frequency[token] += 1</span><br><span class="line">texts = [[token for token in text if frequency[token] &gt; 1 ] for text in texts]</span><br></pre></td></tr></table></figure>
<h1 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h1><p>python用jieba</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line">sentences = [<span class="string">"我喜欢吃土豆"</span>,<span class="string">"土豆是个百搭的东西"</span>,<span class="string">"我不喜欢今天雾霾的北京"</span>]</span><br><span class="line">words=[]</span><br><span class="line"><span class="keyword">for</span> doc <span class="keyword">in</span> sentences:</span><br><span class="line"><span class="comment"># 结巴分词返回的是一个generator，要用list()转成list</span></span><br><span class="line">    words.append(list(jieba.cut(doc)))</span><br><span class="line"><span class="keyword">print</span> words</span><br></pre></td></tr></table></figure>
<p>java用hanlp</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ArrayList&lt;String&gt; wordSet = new ArrayList&lt;String&gt;();</span><br><span class="line">// 自动去除停用词</span><br><span class="line">		for (Term term : NotionalTokenizer.segment(simplePhrase)) &#123;</span><br><span class="line">			wordSet.add(term.word);</span><br><span class="line">		&#125;</span><br></pre></td></tr></table></figure>
<h1 id="英文词干化"><a href="#英文词干化" class="headerlink" title="英文词干化"></a>英文词干化</h1>
          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2017/07/12/机器学习/NLP/网页关键词提取/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/07/12/机器学习/NLP/网页关键词提取/" class="post-title-link" itemprop="url">网页关键词提取</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2017-07-12 11:49:53" itemprop="dateCreated datePublished" datetime="2017-07-12T11:49:53+08:00">2017-07-12</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-03-31 10:28:31" itemprop="dateModified" datetime="2018-03-31T10:28:31+08:00">2018-03-31</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2017/07/12/机器学习/NLP/网页关键词提取/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/07/12/机器学习/NLP/网页关键词提取/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>代码位于<code>nlp</code> 项目</p>
<h1 id="jsoup解析html的DOM"><a href="#jsoup解析html的DOM" class="headerlink" title="jsoup解析html的DOM"></a>jsoup解析html的DOM</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Document doc =Jsoup.connect(url).userAgent(&quot;Mozilla&quot;).get();</span><br></pre></td></tr></table></figure>
<h1 id="提取网页"><a href="#提取网页" class="headerlink" title="提取网页"></a>提取网页</h1><p><code>getPageDetail</code>获取网页提取的结果，返回<code>WebPageInfo</code>类，该类包括</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> String domain;</span><br><span class="line"><span class="keyword">public</span> String url;</span><br><span class="line"><span class="keyword">public</span> String rawTitle;</span><br><span class="line"><span class="keyword">public</span> String title;</span><br><span class="line"><span class="keyword">public</span> String content;</span><br><span class="line"><span class="keyword">public</span> String summary;</span><br><span class="line"><span class="keyword">public</span> HashMap&lt;String, String&gt; meta;</span><br><span class="line"><span class="keyword">public</span> HashMap&lt;String, List&lt;String&gt;&gt; calculation;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">long</span> freq = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">public</span> String createTime;</span><br></pre></td></tr></table></figure>
<p><code>meta_desc</code>，来自网页meta的<code>description</code>元素</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">meta.put(&quot;description&quot;, meta_desc);</span><br></pre></td></tr></table></figure>
<p><code>meta_keywords</code>，来自网页meta的<code>keywords</code> </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">meta.put(&quot;keywords&quot;, meta_keywords);</span><br></pre></td></tr></table></figure>
<h2 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h2><p><code>rawTitle=doc.title()</code></p>
<p><code>title</code>，通过<code>ExtractUtil.extractTitle(doc.body(), rawTitle)</code>进一步抽取。目的是去掉标题中的无关信息，如网站信息等</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extractTitle</span></span>(root: <span class="type">Element</span>, rawTitle:<span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span>(<span class="type">StringUtils</span>.isBlank(rawTitle)) </span><br><span class="line">      <span class="keyword">return</span> <span class="string">""</span>;</span><br><span class="line">    <span class="keyword">val</span> titleCnt = <span class="keyword">new</span> <span class="type">HashMap</span>[<span class="type">String</span>, <span class="type">Int</span>] <span class="keyword">with</span> <span class="type">HashMapUtil</span>.<span class="type">IntHashMap</span>[<span class="type">String</span>]</span><br><span class="line">    titleCnt.adjustOrPut(te.extract(rawTitle).trim, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    titleCnt.adjustOrPut(te.extractFirst(rawTitle).trim, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">val</span> tq = <span class="keyword">new</span> <span class="type">PriorityQueue</span>[<span class="type">String</span>](<span class="number">2</span>)</span><br><span class="line">    extractTitle0(root, rawTitle, <span class="number">1</span>, tq)</span><br><span class="line">    <span class="keyword">for</span> (candidate &lt;- tq.values) titleCnt.adjustOrPut(candidate.trim, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">var</span> maxCnt = <span class="number">0</span></span><br><span class="line">    <span class="keyword">var</span> title = rawTitle</span><br><span class="line">    titleCnt.foreach &#123; <span class="keyword">case</span> (candidate, cnt) =&gt;</span><br><span class="line">      <span class="keyword">if</span> (maxCnt &lt; cnt) &#123;</span><br><span class="line">        maxCnt = cnt</span><br><span class="line">        title = candidate</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (maxCnt == cnt &amp;&amp; candidate.length &gt; title.length) title = candidate</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    title</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p><code>te.extract</code>，</p>
<p>首先要<code>split</code>。通过判断unicode字符的类别（<a href="http://blog.csdn.net/weixin_36082485/article/details/53154065" target="_blank" rel="noopener">Unicode字符类</a>）来分割标题 </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; title.length(); i++) &#123;</span><br><span class="line">			<span class="keyword">char</span> ch = title.charAt(i);</span><br><span class="line">			<span class="keyword">int</span> type = Character.getType(ch);</span><br><span class="line">  			<span class="comment">// 标点，前引号</span></span><br><span class="line">			<span class="keyword">if</span> (type == Character.INITIAL_QUOTE_PUNCTUATION) quoteCnt++;</span><br><span class="line">  			<span class="comment">// 标点，开始</span></span><br><span class="line">			<span class="keyword">if</span> (type == Character.START_PUNCTUATION) quoteCnt++;</span><br><span class="line">			<span class="keyword">if</span> (quoteCnt == <span class="number">0</span> &amp;&amp; !lastLetter &amp;&amp; !lastDigit &amp;&amp; splitChars.contains(ch)) &#123;</span><br><span class="line">				parts.add(title.substring(last, i));</span><br><span class="line">				last = i + <span class="number">1</span>;</span><br><span class="line">			&#125;</span><br><span class="line">  			<span class="comment">// 标点，后引号</span></span><br><span class="line">			<span class="keyword">if</span> (type == Character.FINAL_QUOTE_PUNCTUATION) quoteCnt--;</span><br><span class="line">  			<span class="comment">// 标点，结束</span></span><br><span class="line">			<span class="keyword">if</span> (type == Character.END_PUNCTUATION) quoteCnt--;</span><br><span class="line">			<span class="keyword">if</span> (ch &gt;= <span class="string">'A'</span> &amp;&amp; ch &lt;= <span class="string">'Z'</span> || ch &gt;= <span class="string">'a'</span> &amp;&amp; ch &lt;= <span class="string">'z'</span>) lastLetter = <span class="keyword">true</span>;</span><br><span class="line">			<span class="keyword">else</span> lastLetter = <span class="keyword">false</span>;</span><br><span class="line">			<span class="keyword">if</span> (Character.isDigit(ch)) lastDigit = <span class="keyword">true</span>;</span><br><span class="line">			<span class="keyword">else</span> <span class="keyword">if</span> (!splitChars.contains(ch)) lastDigit = <span class="keyword">false</span>;</span><br><span class="line">		&#125;</span><br></pre></td></tr></table></figure>
<p>然后提取最长的part。提取的原则是，</p>
<p>1、如果出现不重要的字符前缀后缀<code>ignoreSuffixes</code>、<code>ignorePrefixes</code>，降低part的长度</p>
<p>2、第一个part的长度翻倍，可能是考虑到真的标题往往出现在第一块，如</p>
<p><code>清润饮食“熄灭”冬季之火 - 素食 - 大渡网-佛教资讯，生活，人文，心灵感悟，佛艺时尚杂志，佛教音乐，佛教常识，佛教视频</code></p>
<p><code>从草根到精英——大陆网络民族主义流变-观点评论-时事评论-四月网-青年思想门户-M4.CN</code></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> List&lt;String&gt; ignoreSuffixes = Arrays.asList(<span class="string">"频道"</span>, <span class="string">"站"</span>, <span class="string">"网"</span>, <span class="string">"报"</span>, <span class="string">"集"</span>, <span class="string">"公司"</span>, <span class="string">".com"</span>, <span class="string">".cn"</span>, <span class="string">"平台"</span>, <span class="string">"门户"</span>, <span class="string">"博客"</span>, <span class="string">"精选"</span>, <span class="string">"博客精选"</span>);</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">static</span> List&lt;String&gt; ignorePrefixes = Arrays.asList(<span class="string">"Powered by"</span>);</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">static</span> HashSet&lt;Character&gt; splitChars = <span class="keyword">new</span> HashSet&lt;Character&gt;(Arrays.asList(<span class="string">'|'</span>, <span class="string">'_'</span>, <span class="string">'-'</span>, <span class="string">'—'</span>, <span class="string">'－'</span>, <span class="string">'&lt;'</span>, <span class="string">'&gt;'</span>, <span class="string">'«'</span>, <span class="string">'»'</span>));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> String <span class="title">getLongestPart</span><span class="params">(List&lt;String&gt; parts)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">double</span> longestNumWords = <span class="number">0</span>;</span><br><span class="line">		String longestPart = <span class="string">""</span>;</span><br><span class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; parts.size(); i++) &#123;</span><br><span class="line">			String p = parts.get(i).trim();</span><br><span class="line">			</span><br><span class="line">			<span class="keyword">int</span> ignoreCount = <span class="number">0</span>;</span><br><span class="line">			<span class="keyword">for</span> (String is: ignoreSuffixes) <span class="keyword">if</span> (p.toLowerCase().endsWith(is)) ignoreCount++;</span><br><span class="line">			<span class="keyword">for</span> (String ip: ignorePrefixes) <span class="keyword">if</span> (p.toLowerCase().startsWith(ip)) ignoreCount++;</span><br><span class="line">			<span class="keyword">int</span> colonCnt = StringUtils.countMatches(p, <span class="string">","</span>);</span><br><span class="line">			<span class="keyword">if</span> (colonCnt &gt; <span class="number">0</span>) ignoreCount += colonCnt - <span class="number">1</span>;</span><br><span class="line">			colonCnt = StringUtils.countMatches(p, <span class="string">"，"</span>);</span><br><span class="line">			<span class="keyword">if</span> (colonCnt &gt; <span class="number">0</span>) ignoreCount += colonCnt - <span class="number">1</span>;</span><br><span class="line">			<span class="keyword">double</span> numWords = TextUtil.countNumWords(p);</span><br><span class="line">			numWords = numWords / (<span class="number">1</span> + <span class="number">2</span> * ignoreCount);</span><br><span class="line">			<span class="keyword">if</span> (i == <span class="number">0</span>) numWords = numWords * <span class="number">2</span>;</span><br><span class="line">            <span class="keyword">if</span> (numWords &gt; longestNumWords) &#123;</span><br><span class="line">            	longestNumWords = numWords;</span><br><span class="line">            	longestPart = p;</span><br><span class="line">            &#125;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">if</span> (longestPart.length() == <span class="number">0</span>) <span class="keyword">return</span> <span class="string">""</span>;</span><br><span class="line">		<span class="keyword">else</span> <span class="keyword">return</span> longestPart.trim();</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>
<p><code>extractTitle0(root, rawTitle, 1, tq)</code> </p>
<p>传入root和刚才提取的rawTitle，递归遍历root的各个head元素，<code>h</code>，<code>title</code>，每种赋值不同权重。再寻找与rawTitle的最长公共子串。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTitle0</span></span>(node: <span class="type">Node</span>, title: <span class="type">String</span>, weight: <span class="type">Double</span>, tq: <span class="type">PriorityQueue</span>[<span class="type">String</span>], depth: <span class="type">Int</span> = <span class="number">0</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    node <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> textNode: <span class="type">TextNode</span> =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> text = textNode.text.trim</span><br><span class="line">        <span class="keyword">if</span> (text.length &gt; <span class="number">0</span>) &#123;</span><br><span class="line">          <span class="keyword">val</span> lcs = <span class="type">TextUtil</span>.findLcs(title, text)</span><br><span class="line">          <span class="keyword">val</span> nwords = <span class="type">TextUtil</span>.countNumWords(lcs)</span><br><span class="line">          <span class="keyword">val</span> pos = title.indexOf(lcs)</span><br><span class="line">          <span class="keyword">if</span> (pos != <span class="number">-1</span> &amp;&amp; nwords &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            tq.add(nwords * weight / (<span class="number">1</span> + math.log(<span class="number">2</span> + pos)), lcs)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125; </span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">Element</span> =&gt; &#123;</span><br><span class="line">        <span class="keyword">var</span> w = weight</span><br><span class="line">        <span class="keyword">if</span> (e.tagName.startsWith(<span class="string">"h"</span>)) w = w * <span class="number">1.2</span></span><br><span class="line">        <span class="keyword">if</span> (e.tagName == <span class="string">"a"</span>) w = w / <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> (e.className.contains(<span class="string">"title"</span>)) w = w * <span class="number">1.5</span></span><br><span class="line">        <span class="keyword">if</span> (e.tagName != <span class="string">"title"</span> &amp;&amp; !isNegativeBlock(e.className + <span class="string">" "</span> + e.id) &amp;&amp; depth &lt; <span class="type">Extract_STOP_DEPTH</span>) &#123;</span><br><span class="line">          <span class="keyword">for</span> (n &lt;- e.childNodes.asScala) extractTitle0(n, title, w, tq, depth + <span class="number">1</span>)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">case</span> _ =&gt; &#123;&#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>通过以上方法提取出各种title后，选出出现频率最高的作为最终的title。</p>
<h2 id="content"><a href="#content" class="headerlink" title="content"></a>content</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extractContent</span></span>(url: <span class="type">String</span>, doc: <span class="type">Document</span>): <span class="type">List</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> rawTitle = doc.title</span><br><span class="line">    <span class="keyword">if</span>(doc.body == <span class="literal">null</span>)</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">null</span></span><br><span class="line">    <span class="type">ExtractUtil</span>.cleanup(doc.body)</span><br><span class="line">    <span class="keyword">val</span> title = <span class="type">ExtractUtil</span>.extractTitle(doc.body, rawTitle)</span><br><span class="line">    <span class="keyword">val</span> metaKeywords = <span class="type">ExtractUtil</span>.extractMeta(doc, <span class="string">"keywords"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> blocks = <span class="type">ExtractUtil</span>.extractBlocks(doc, title) map &#123; block =&gt;</span><br><span class="line">      <span class="type">SnippetBlock</span>(block.snippets map &#123; snippet =&gt; <span class="type">TextProcess</span>.normalize(urlReg.matcher(snippet).replaceAll(<span class="string">""</span>)) &#125;, block.score, block.isArticle, block.imgs)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> normalTitle = <span class="type">TextUtil</span>.fillText(title)</span><br><span class="line">    <span class="keyword">val</span> normalRawTitle = <span class="type">TextUtil</span>.fillText(doc.title)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> allsnippets = blocks.filter(_.isArticle).flatMap &#123; b =&gt; b.snippets &#125;</span><br><span class="line">    <span class="keyword">return</span> allsnippets;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h3 id="清洗doc"><a href="#清洗doc" class="headerlink" title="清洗doc"></a>清洗doc</h3><p><code>ExtractUtil.cleanup(doc.body)</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cleanup</span></span>(root: <span class="type">Element</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> cleanNodes = <span class="keyword">for</span> &#123;</span><br><span class="line">      e &lt;- root.getAllElements.asScala</span><br><span class="line">      <span class="keyword">if</span> <span class="type">INVALID_TAGS</span>.contains(e.tagName) || e.attr(<span class="string">"style"</span>).contains(<span class="string">"display:none"</span>)</span><br><span class="line">    &#125; <span class="keyword">yield</span> e</span><br><span class="line">    <span class="keyword">for</span> (cn &lt;- cleanNodes) cn.remove</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h3 id="提取title、metakeywords"><a href="#提取title、metakeywords" class="headerlink" title="提取title、metakeywords"></a>提取title、metakeywords</h3><h3 id="提取blocks"><a href="#提取blocks" class="headerlink" title="提取blocks"></a>提取blocks</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extractBlocks</span></span>(doc: <span class="type">Document</span>, title: <span class="type">String</span>): <span class="type">List</span>[<span class="type">SnippetBlock</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> blocks = <span class="keyword">new</span> <span class="type">ListBuffer</span>[<span class="type">BlockDetail</span>]</span><br><span class="line">    <span class="keyword">val</span> bd = <span class="keyword">new</span> <span class="type">BlockDetailBuffer</span></span><br><span class="line">    extractBlocks(doc.body, blocks, bd)</span><br><span class="line">    <span class="keyword">if</span> (bd.isDefined) blocks += bd.result</span><br><span class="line">    calcScore(title, blocks.result filterNot(b =&gt; hasICP(b))) ++ <span class="type">List</span>(<span class="type">SnippetBlock</span>(<span class="type">List</span>(extractMeta(doc, <span class="string">"keywords"</span>)), <span class="number">1</span>d, <span class="literal">true</span>, <span class="type">List</span>()), <span class="type">SnippetBlock</span>(<span class="type">List</span>(extractMeta(doc, <span class="string">"description"</span>)), <span class="number">0</span>d, <span class="literal">false</span>, <span class="type">List</span>()))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">extractBlocks</span></span>(root: <span class="type">Node</span>, blocks: <span class="type">ListBuffer</span>[<span class="type">BlockDetail</span>], bd: <span class="type">BlockDetailBuffer</span>, inLink: <span class="type">Boolean</span> = <span class="literal">false</span>, depth: <span class="type">Int</span> = <span class="number">0</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  root <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> tn: <span class="type">TextNode</span> =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> text = <span class="type">StringUtils</span>.replace(tn.text, <span class="string">"\u00a0"</span>, <span class="string">" "</span>).trim</span><br><span class="line">      <span class="keyword">if</span> (text.length &gt; <span class="number">0</span>) bd.add(text, inLink)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">Element</span> =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> isLink = inLink || (e.tagName == <span class="string">"a"</span>) </span><br><span class="line">      <span class="keyword">if</span>(depth &lt; <span class="type">Extract_STOP_DEPTH</span>)&#123;</span><br><span class="line">      	e.childNodes.asScala foreach &#123; c =&gt; extractBlocks(c, blocks, bd, isLink, depth + <span class="number">1</span>) &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      </span><br><span class="line">      <span class="keyword">if</span> (e.tagName == <span class="string">"img"</span> || e.tagName == <span class="string">"embed"</span>) &#123;</span><br><span class="line">        bd.addImg(e)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (e.isBlock) &#123;</span><br><span class="line">        <span class="keyword">if</span> (bd.isDefined) blocks += bd.result</span><br><span class="line">        bd.clear</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">case</span> _ =&gt; &#123;&#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>提取每个TextNode的文本，放到<code>BlockDetailBuffer</code>中。将每个<code>BlockDetailBuffer</code>的内容放到<code>BlockDetail</code>的list <code>blocks</code>中。</p>
<p>过滤掉包含<code>icp备</code>或<code>icp证</code>的文本，再对所有的blocks计算打分<code>calcScore</code></p>
<p>最后提取所有是文本的snippet，作为content</p>
<h3 id="提取keywords"><a href="#提取keywords" class="headerlink" title="提取keywords"></a>提取keywords</h3><p>同样是先clean，提取title、metaKeyword，</p>
<p>再提取blocks</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> blocks:<span class="type">List</span>[<span class="type">SnippetBlock</span>] =  <span class="type">ExtractUtil</span>.extractBlocks(doc, title).map &#123; block =&gt;</span><br><span class="line">      &#123;</span><br><span class="line">              <span class="type">SnippetBlock</span>(block.snippets map &#123; snippet =&gt; <span class="type">TextProcess</span>.normalize(urlReg.matcher(snippet).replaceAll(<span class="string">""</span>)) &#125;, block.score, block.isArticle, block.imgs)</span><br><span class="line"></span><br><span class="line">	      <span class="keyword">val</span> temp  = block.snippets map &#123; snippet =&gt; <span class="type">TextProcess</span>.normalize(urlReg.matcher(snippet).replaceAll(<span class="string">""</span>)) &#125;</span><br><span class="line">	      maxLen += temp.map(<span class="type">AtomSplit</span>.count(_)).sum	</span><br><span class="line">	      <span class="keyword">val</span> retVal:<span class="type">SnippetBlock</span> = <span class="keyword">if</span>(maxLen &lt; <span class="type">MAX_CONTENT_LENGTH</span> || maxflag)&#123;<span class="type">SnippetBlock</span>(temp, block.score, block.isArticle, block.imgs)&#125; <span class="keyword">else</span> <span class="literal">null</span></span><br><span class="line">	      <span class="keyword">if</span>(maxLen &gt; <span class="type">MAX_CONTENT_LENGTH</span>)&#123;</span><br><span class="line">	        maxflag = <span class="literal">false</span></span><br><span class="line">	      &#125;</span><br><span class="line">	      retVal</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;.filter( _ != <span class="literal">null</span>)</span><br></pre></td></tr></table></figure>
<h4 id="dlg"><a href="#dlg" class="headerlink" title="dlg"></a>dlg</h4><p>再提取dlg</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val dlg = DlgExtractor.extract(normalTitle, TextUtil.fillText(doc.title), blocks, 6)/*.filter(_._2 &gt; 1.0)*/</span><br></pre></td></tr></table></figure>
<p>分词后，计算每个词的权重，</p>
<p>First of all, for any web page, we can use jsoup to  obtain the Document Object Model (DOM) , which can access all the HTML elements of it.</p>
<p>After that，we clean up  HTML elements by drop some invalid or useless tags, such as the tags with “display:none” property. </p>
<p>At last, We extract the Content and Keywords of HTML. </p>
<p>For extracting Content, we iterate through the DOM tree to find all TextNode elements, extract the text and take them as the Snippets. Then we calculate the scores of all Snippets, and get the available Snippets as Content.</p>
<p>For extracing Keywords, besides the Snippets from TextNode elements, we also collect the title, keywords and description from <meta> tag, store them as Blocks.   For every Block, we segment words to generate the corpus by ansj_seg, and calculate the weight of every word using TFIDF. Finally, we get the TOP 10 words as Keywords of web page.</p>
<p>我们解析了10万左右的网页，根据解析的网页content打上safe和unsafe的label，后期我们会对safe和unsafe进一步细分。</p>
<p>训练过程：我们载入所有含标签的训练样本，由于fasttext提供了适用于各种语言的Word2Vec预向量集，将网页内容转为词向量，通过fasttext训练出模型并保存到本地。</p>
<p>预测过程：载入模型到内存，当输入一个网页的content后，转为词向量，根据模型给出safe或unsafe的分类结果。</p>
<p>We have analyzed some 100 thousand web pages, classified text in categories, such as safe and unsafe by content of these web pages, and we will extent more categories in future.</p>
<p>In order to train the text classifier model, we load all samples containing a training sentence per line along with the labels, and transfer all words to vectors using  pre-trained word vectors model published by fastText.  Then we use the code from Github to run the training program. Once the model was trained, we save it on disk as a file.</p>
<p>When input a content of web page, we transfer it to word vectors and run the prediction program, as a result we get the category of this web page.</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2017/07/12/hadoop-spark/spark/spark-sql笔记/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/07/12/hadoop-spark/spark/spark-sql笔记/" class="post-title-link" itemprop="url">spark-sql笔记</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2017-07-12 11:49:53" itemprop="dateCreated datePublished" datetime="2017-07-12T11:49:53+08:00">2017-07-12</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-01-30 15:49:02" itemprop="dateModified" datetime="2018-01-30T15:49:02+08:00">2018-01-30</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/hadoop-spark/" itemprop="url" rel="index"><span itemprop="name">hadoop-spark</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2017/07/12/hadoop-spark/spark/spark-sql笔记/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/07/12/hadoop-spark/spark/spark-sql笔记/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>spark sql可以用基本的SQL语法，或者hiveQL。Spark SQL也能从hive中读数据。</p>
<p>Data Frame的来源可以是：结构化数据、hive表、外部数据库或者RDD</p>
<h2 id="Starting-Point-SQLContext"><a href="#Starting-Point-SQLContext" class="headerlink" title="Starting Point: SQLContext"></a>Starting Point: SQLContext</h2><p>The entry point into all functionality in Spark SQL is the <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SQLContext" target="_blank" rel="noopener"><code>SQLContext</code></a> class, or one of its descendants. To create a basic <code>SQLContext</code>, all you need is a SparkContext.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sc: <span class="type">SparkContext</span> <span class="comment">// An existing SparkContext.</span></span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> org.apache.spark.sql.<span class="type">SQLContext</span>(sc)</span><br><span class="line"></span><br><span class="line"><span class="comment">// this is used to implicitly convert an RDD to a DataFrame.</span></span><br><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br></pre></td></tr></table></figure>
<p>在SQLContext的基础上，也要创建一个HiveContext，可以用Hive的udf，也能读hive表。<br>HiveContext是一个独立的包，不需要安装hive</p>
<h2 id="Creating-DataFrames"><a href="#Creating-DataFrames" class="headerlink" title="Creating DataFrames"></a>Creating DataFrames</h2><p>With a <code>SQLContext</code>, applications can create <code>DataFrame</code>s from an <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#interoperating-with-rdds" target="_blank" rel="noopener">existing <code>RDD</code></a>, from a Hive table, or from <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#data-sources" target="_blank" rel="noopener">data sources</a>.</p>
<p>As an example, the following creates a <code>DataFrame</code> based on the content of a JSON file:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sc: <span class="type">SparkContext</span> <span class="comment">// An existing SparkContext.</span></span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> org.apache.spark.sql.<span class="type">SQLContext</span>(sc)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> df = sqlContext.read.json(<span class="string">"examples/src/main/resources/people.json"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Displays the content of the DataFrame to stdout</span></span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure>
<h2 id="Creating-Datasets"><a href="#Creating-Datasets" class="headerlink" title="Creating Datasets"></a>Creating Datasets</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SQLContext</span></span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// Encoders for most common types are automatically provided by importing sqlContext.implicits._</span></span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="type">SQLContext</span>.getOrCreate(sc)</span><br><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line"><span class="keyword">val</span> ds = <span class="type">Seq</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>).toDS()</span><br><span class="line">ds.map(_ + <span class="number">1</span>).collect() <span class="comment">// Returns: Array(2, 3, 4)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Encoders are also created for case classes.</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">ds</span> </span>= <span class="type">Seq</span>(<span class="type">Person</span>(<span class="string">"Andy"</span>, <span class="number">32</span>)).toDS()</span><br><span class="line"></span><br><span class="line"><span class="comment">// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name.</span></span><br><span class="line"><span class="keyword">val</span> path = <span class="string">"examples/src/main/resources/people.json"</span></span><br><span class="line"><span class="keyword">val</span> people = sqlContext.read.json(path).as[<span class="type">Person</span>]</span><br></pre></td></tr></table></figure>
<h2 id="在DataFrame中创建表并查询"><a href="#在DataFrame中创建表并查询" class="headerlink" title="在DataFrame中创建表并查询"></a>在DataFrame中创建表并查询</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sqlContext = <span class="type">SQLContext</span>.getOrCreate(sc)</span><br><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line"></span><br><span class="line"><span class="comment">//把andy和32匹配到name和age列。</span></span><br><span class="line"><span class="comment">//先创建一个DataFrame，再注册为table</span></span><br><span class="line"><span class="keyword">val</span> ds = sc.parallelize(<span class="type">Seq</span>((<span class="string">"Andy"</span>, <span class="number">32</span>))).toDF(<span class="string">"name"</span>,<span class="string">"age"</span>)    </span><br><span class="line">ds.registerTempTable(<span class="string">"ds1"</span>)</span><br><span class="line">sqlContext.sql(<span class="string">"select * from ds1"</span>)</span><br></pre></td></tr></table></figure>
<h2 id="sparkSQL链接GP"><a href="#sparkSQL链接GP" class="headerlink" title="sparkSQL链接GP"></a>sparkSQL链接GP</h2><p>1、maven中增加包<br>一开始试过8.2的包就不行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;postgresql&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;postgresql&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;8.3-606.jdbc3&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure></p>
<p>2、连接jdbc<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> jdbcDF = sqlContext.read.format(<span class="string">"jdbc"</span>).options(</span><br><span class="line">      <span class="type">Map</span>(<span class="string">"url"</span> -&gt; <span class="string">"jdbc:postgresql://10.1.1.230:5432/xmo_dw"</span>,</span><br><span class="line">          <span class="string">"user"</span>-&gt;<span class="string">"david_xu"</span>,  </span><br><span class="line">          <span class="string">"password"</span>-&gt;<span class="string">"w7dtfxHD"</span>,</span><br><span class="line">        <span class="string">"dbtable"</span> -&gt; <span class="string">"(select * from xmo_dw.bshare_blacklist_tagid) as aa"</span>)).load().show()</span><br></pre></td></tr></table></figure></p>
<h2 id="在spark-sql命令行中测试连接"><a href="#在spark-sql命令行中测试连接" class="headerlink" title="在spark sql命令行中测试连接"></a>在spark sql命令行中测试连接</h2><p>/usr/lib/spark/bin/spark-sql —jars /home/david/jars/postgresql-8.3-606.jdbc4.jar</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">CREATE</span> <span class="type">TEMPORARY</span> <span class="type">TABLE</span> temp_imageviews</span><br><span class="line"><span class="type">USING</span> org.apache.spark.sql.jdbc</span><br><span class="line"><span class="type">OPTIONS</span> (</span><br><span class="line">  driver <span class="string">"org.postgresql.Driver"</span>,</span><br><span class="line">  url <span class="string">"jdbc:postgresql://10.1.1.230:5432/xmo_dw"</span>,</span><br><span class="line">  user <span class="string">"david_xu"</span>,</span><br><span class="line">  password <span class="string">"w7dtfxHD"</span>,</span><br><span class="line">  dbtable <span class="string">"(select date_i,opxpid::text from xmo_dw.imageviews where date_i=20160512 limit 20000000) as aa"</span>,</span><br><span class="line">  numPartitions <span class="string">"6"</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/lib/spark/bin/</span><br><span class="line">/usr/lib/spark/bin/spark-sql <span class="comment">--executor-memory 100g</span></span><br><span class="line"></span><br><span class="line">add jar /home/wilson/sparksql/postgresql-8.3-606.jdbc3.jar;</span><br><span class="line"><span class="keyword">set</span>  spark.sql.shuffle.partitions=<span class="number">20</span>;</span><br><span class="line"> <span class="keyword">CREATE</span> <span class="keyword">TEMPORARY</span> <span class="keyword">TABLE</span> temp_imageviews</span><br><span class="line"><span class="keyword">USING</span> org.apache.spark.sql.jdbc</span><br><span class="line">OPTIONS (</span><br><span class="line">  driver <span class="string">"org.postgresql.Driver"</span>,</span><br><span class="line">  <span class="keyword">url</span> <span class="string">"jdbc:postgresql://10.1.1.230:5432/xmo_dw"</span>,</span><br><span class="line">  <span class="keyword">user</span> <span class="string">"david_xu"</span>,</span><br><span class="line">  <span class="keyword">password</span> <span class="string">"w7dtfxHD"</span>,</span><br><span class="line">  dbtable <span class="string">"(select  timeslot,record_server,referring_site ,opxsid  from  xmo_dw.imageviews   where  date_i=20160515 limit  5000000) as aa"</span>,</span><br><span class="line">  numPartitions <span class="string">"6"</span>);</span><br></pre></td></tr></table></figure>
<h3 id="spark-sql读取HDFS建表"><a href="#spark-sql读取HDFS建表" class="headerlink" title="spark sql读取HDFS建表"></a>spark sql读取HDFS建表</h3><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">/usr/lib/spark/bin/spark-sql -e "<span class="keyword">CREATE</span> <span class="keyword">TEMPORARY</span> <span class="keyword">TABLE</span> rtbreq_hour</span><br><span class="line"><span class="keyword">USING</span> org.apache.spark.sql.bytes</span><br><span class="line">OPTIONS (</span><br><span class="line">  paths <span class="string">'$&#123;rtbreq_path&#125;'</span></span><br><span class="line">);</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">TEMPORARY</span> <span class="keyword">TABLE</span> rtbreq_tanx_hour</span><br><span class="line"><span class="keyword">USING</span> org.apache.spark.sql.bytes</span><br><span class="line">OPTIONS(</span><br><span class="line"> paths <span class="string">'$&#123;rtbreq_tanx_path&#125;'</span></span><br><span class="line">);</span><br><span class="line"><span class="keyword">select</span> ip,<span class="keyword">count</span>(<span class="number">1</span>) cnt <span class="keyword">from</span> (<span class="keyword">select</span> ip <span class="keyword">from</span> rtbreq_hour <span class="keyword">union</span> all <span class="keyword">select</span> ip <span class="keyword">from</span> rtbreq_tanx_hour) a <span class="keyword">group</span> <span class="keyword">by</span> ip <span class="keyword">having</span> cnt &gt; <span class="number">30</span> <span class="keyword">order</span> <span class="keyword">by</span> cnt <span class="keyword">desc</span>;</span><br><span class="line"><span class="keyword">select</span> bxid,<span class="keyword">count</span>(<span class="number">1</span>) cnt <span class="keyword">from</span> (<span class="keyword">select</span> bxid <span class="keyword">from</span> rtbreq_hour <span class="keyword">union</span> all <span class="keyword">select</span> bxid <span class="keyword">from</span> rtbreq_tanx_hour) a <span class="keyword">group</span> <span class="keyword">by</span> bxid <span class="keyword">having</span> cnt &gt; <span class="number">1000</span> <span class="keyword">order</span> <span class="keyword">by</span> cnt <span class="keyword">desc</span>;</span><br><span class="line">" &gt; rtbreq/$day/$hour.txt</span><br></pre></td></tr></table></figure>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2017/07/12/机器学习/面试刷题/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/07/12/机器学习/面试刷题/" class="post-title-link" itemprop="url">面试刷题</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2017-07-12 11:49:53" itemprop="dateCreated datePublished" datetime="2017-07-12T11:49:53+08:00">2017-07-12</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-03-17 20:50:31" itemprop="dateModified" datetime="2018-03-17T20:50:31+08:00">2018-03-17</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2017/07/12/机器学习/面试刷题/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/07/12/机器学习/面试刷题/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <ol>
<li><a href="https://zhuanlan.zhihu.com/p/29965072" target="_blank" rel="noopener">那些深度学习《面试》你可能需要知道的</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/29969587" target="_blank" rel="noopener">如何准备机器学习工程师的面试 ？</a></li>
<li><a href="https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/mp/homepage%3F__biz%3DMzI4MTQ2NjU5NA%3D%3D%26hid%3D1%26sn%3D93c4875b14fa5ad6f174829b2b8b4463%26scene%3D18%26devicetype%3DiPhone%2BOS9.3.5%26version%3D16051327%26lang%3Dzh_CN%26nettype%3D3G%2B%26ascene%3D7%26session_us%3Dgh_58f9504ddd59%26fontScale%3D100%26pass_ticket%3Dg8f%252FuIJqlsyfsGEdnZPm0SWWYRiZWOQHMp6bSSJ39kpkzb%252BgyByne%252BKNjMf%252Fo4pp%26wx_header%3D1%26scene%3D1" target="_blank" rel="noopener">七月在线实验室—-BAT机器学习面试题</a></li>
<li><a href="https://www.zhihu.com/question/23259302" target="_blank" rel="noopener">如何准备机器学习工程师的面试 ？</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/27238249" target="_blank" rel="noopener">读完这21个机器学习面试问题和答案，入职率提升99%</a></li>
<li><a href="https://www.zhihu.com/question/25565713#answer-8243688" target="_blank" rel="noopener">国内互联网公司机器学习数据挖掘类的职位面试主要考察什么方面的东西？</a></li>
<li>….等等</li>
</ol>
<p>如何判断一个而链表中是否有环？<br>给定一棵二叉查找树中的两个元素，求它们的最近公共祖先。<br>给一个栈排序<br>基于比较的排序算法的时间复杂度是什么？证明？<br>如何求一个带权图中两个结点直接按的最短路径？如果有些权值是负的怎么办？<br>求一个字符串中所有的回文子串。<br>对这些问题你都要能够推导你的解法的时间和空间复杂度（大 O 表示法），并且尽量用最低的复杂度解决。<br><strong>只有通过大量的练习才能将这些不同类型的问题烂熟于胸，从而在面试中迅速地给出一个高效的解法。常用的算法面试准备平台有 InterviewBit、LeetCode、Interview Cake、Pramp、<a href="http://interviewing.io/" target="_blank" rel="noopener">http://interviewing.io</a> 等。</strong></p>
<p>概率论和统计典型问题</p>
<p>给出一个群体中男性和女性各自的平均身高，求整个群体的平均身高。<br>一次调查表明意大利三分之一的汽车都是法拉利，并且在那之中一半的车都是红色的。如果你在意大利的街头看到一辆红色的汽车驶来，请问它是法拉利的可能性有多大？<br>你试图找出在自己的网站上放置版头的最佳方案。变量包括版头的尺寸（大、中、小）以及放置的位置（顶部、中间、底部）。假定需要 95% 的置信水平，请问你至少需要多少次访问和点击来确定某个方案比其他的组合都要好？<br><strong>很多机器学习算法都以概率论和统计作为理论基础。对于这些基础知识有清晰的概念是极为重要的。当然同时你也要能够将这些抽象的概念与现实联系起来。</strong><br>数据建模和评估典型问题</p>
<p>一位农民想搞明白是什么因素影响了他的牛奶产量。他记录了每天的气温（30 - 40 度）、湿度（60 - 90%）、饲料消耗（2000 - 2500 千克）以及牛奶产量（500 - 1000 升）。<br>假设问题是要预测每天的牛奶产量，你会如何处理数据并建立模型？<br>这是一个什么类型的机器学习问题？<br>你的公司在开发一个面部表情识别系统。这个系统接受 1920 x 1080 的图片作为输入，并告诉用户图片中的人脸处于以下哪种情绪状态：平常、高兴、悲伤、愤怒和恐惧。当图片中没有人脸时系统要能够分辨这种情况。<br>这是一个什么类型的机器学习问题？<br>如果每个像素点由 3 个值来表示（RGB），那么输入数据的原始维度有多大？有办法降维吗？<br>如何对系统的输出进行编码？为什么？<br>过去几个世纪的气象数据展现出一种循环的气温模式：一会升高一会下降。对于这样的数据（一个年平均气温的序列），你会如何建模并预测未来 5 年的平均气温？<br>你在一家在线新闻网站工作，需要从各处收集文本，并将不同来源的内容聚集成一篇报道。你会如何设计这样一个系统？会用到哪些机器学习技术？<br><strong>应用机器学习算法和库</strong></p>
<p>你用一个给定的数据集训练一个单隐层的神经网络，发现网络的权值在训练中强烈地震荡（有时在负值和正值之间变化）。为了解决这个问题你需要调整哪个参数？<br>支持向量机的训练在本质上是在最优化哪个值？<br>LASSO 回归用 L1-norm 作为惩罚项，而岭回归（Ridge Regression）则使用 L2-norm 作为惩罚项。这两者哪个更有可能得到一个稀疏（某些项的系数为 0）的模型？<br>在用反向传播法训练一个 10 层的神经网络时，你发现前 3 层的权值完全没有变化，而 4 ~ 6 层的权值则变化得非常慢。这是为什么？如何解决？<br>你手上有一个关于小麦产出的数据集，包括年降雨量 R、平均海拔 A 以及小麦产量 O。你经过初步分析认为产量跟年降雨量的平方以及平均海报的对数之间存在关系，即：O = β_0 + β_1 x R^2 + β_2 x log(A)。能用线性回归求出系数 β 吗？<br>你可以通过像 Kaggle 比赛那样的数据科学和机器学习挑战来了解各种各样的问题和它们之间的细微差别。多多参加这些比赛，并尝试应用不同的机器学习模型。<br>软件工程和系统设计典型问题</p>
<p>你有一个电商网站，当用户点击一个商品打开详情页面时，你想基于商品特征和用户的购买历史为用户推荐 5 个其他的商品显示在页面的底部。你需要哪些服务和数据表来实现这个功能？请写一个查询语句或一段过程式代码来返回所要推荐的 5 个商品。<br>对于 YouTube 那样的在线视频网站，你会收集哪些数据来衡量用户的参与度和视频的人气度？<br>一个简单的垃圾邮件检测系统是这样的：它每次处理一封邮件，统计不同单词的出现频率（Term frequency），并将这些频率与之前已经被标注为垃圾 / 正常邮件的那些频率进行比较。现在需要对这系统进行拓展来处理海量的邮件流量，请设计一个 Map-Reduce 方案在一个集群上部署这个系统。<br>你要生成一个实时的热力图，来展示用户正在浏览和点击一个网页的哪些部分。在客户端和服务端分别需要哪些组件 / 服务 / API 来实现这个功能？</p>
<p><a href="http://blog.csdn.net/u010496169/article/details/73743973" target="_blank" rel="noopener">机器学习岗位面试问题汇总 之 集成学习</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2017/07/12/机器学习/计算广告/广告反作弊/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/07/12/机器学习/计算广告/广告反作弊/" class="post-title-link" itemprop="url">广告反作弊</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2017-07-12 11:49:53" itemprop="dateCreated datePublished" datetime="2017-07-12T11:49:53+08:00">2017-07-12</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-03-17 20:50:31" itemprop="dateModified" datetime="2018-03-17T20:50:31+08:00">2018-03-17</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/计算广告/" itemprop="url" rel="index"><span itemprop="name">计算广告</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2017/07/12/机器学习/计算广告/广告反作弊/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/07/12/机器学习/计算广告/广告反作弊/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>秒针发布过《互联网广告反作弊技术白皮书》</p>
<p>腾讯灯塔联手秒针、AdMaster发布</p>
<h1 id="腾讯《2017广告反欺诈白皮书》"><a href="#腾讯《2017广告反欺诈白皮书》" class="headerlink" title="腾讯《2017广告反欺诈白皮书》"></a>腾讯《2017广告反欺诈白皮书》</h1><p>日均校验40亿广告请求，识别的作弊比率在15%左右。部分行业和campaign中，作弊率有60%。</p>
<h2 id="欺诈手段"><a href="#欺诈手段" class="headerlink" title="欺诈手段"></a>欺诈手段</h2><ul>
<li>虚假流量。</li>
</ul>
<p>某APP在地推时，新增用户暴涨。95%的新增用户的共同特征：</p>
<p><strong>ROM编译机名称一致；指令逃逸差异数与正常用户不一致；CPU结构为X86，为PC机模拟器；文件系统类型的差异度与正常用户不一致</strong>。</p>
<p>某APP的新增用户中，工作室批量刷量的特征有：</p>
<p><strong>安装时间具有明显的规律；手机APP安装数量一致；明显的地域集中性</strong>。</p>
<p>2.2 黑产技术</p>
<p>1、广告作弊类型分类</p>
<p>1）模拟器刷量：电脑模拟器刷量、手机软件模拟刷量、脚本刷量（录制行为跑循环任务）。</p>
<p>2）真假机用户：储备大量手机或者sim卡，利用数据线push命令到手机，手机执行命令。</p>
<p>3）静默安装（真机真用户假行为）。人工方式或网络传播方式将木马/具有再分发能力的应用植入到用户手机，形成僵尸网络，在用户无感知的情况下，完成App的下载、激活和删除等一系列操作。</p>
<p>4）羊毛党（真机真用户真行为假动机）：登录一次就删除应用、使用时长极短、留存率极低。在大部份的情况下，这种用户对业务的健康发展并无太大价值。</p>
<p>5）广告素材、篇幅偷换（不可见）。“1 像素广告”指在用户的手机屏幕上只展示1个像素大小的广告。这种广告，用户看不见，但统计工具可以统计到，仍然会作为曝光广告与广告主结算，给广告主带来经济损失。</p>
<p>6）以次充好（不匹配）。部分媒体会将广告主原本定向的一线城市用户偷偷换成二三线城市用户，达到以次充好的目的。</p>
<h2 id="应对方法"><a href="#应对方法" class="headerlink" title="应对方法"></a>应对方法</h2><p>2.3 反作弊技术</p>
<p>1）用户群体数据检测。低阶技术，常见方式有：</p>
<p><strong>a.看留存率</strong>。真实的用户的留存曲线是一条平滑的指数衰减曲线，如果发现留存曲线存在陡升陡降的异常波动，则判断刷量者干预了数据。</p>
<p><strong>b.看用户终端、网络信息</strong>。如根据经验分析渠道新增用户或者启动用户的设备排名;2G、3G、4G的使用比例分布是否正常等。</p>
<p><strong>c.看用户注册信息</strong>。比如说注册昵称的分布和规律等。</p>
<p>以上操作均效率低下。</p>
<p>2）用户行为特征分析</p>
<p><strong>a.单个指标</strong>。与黑IP库进行比对，是否为黑名单IP、是否为代理IP;与IMEI库进行对比，是否为为黑IMEI;</p>
<p><strong>b.群体指标</strong>。用户的IP、IMEI、机型、OS、位置信息、运营商、接入方式的<strong>分布</strong>是否符合先验数据的分布</p>
<p><strong>c.设备一致性验证</strong>。CPU、制造商、MAC地址、IMEI、机型、操作系统的一致性验证。</p>
<p>简单粗暴，没有黑白转换机制，误判率高。</p>
<p><strong>这些方法容易被刷量者利用</strong>。在某电商专业Android app游戏激活、注册、留存、付费、应用市场好评平台上，买主只需要很小的代价，即可刷出完全符合正常用户规律的留存率、IP分布机型分布使用时长等。</p>
<p>3）终端特征分析+云端交叉验证</p>
<p>“查”模型负责找寻黑产界的新型作弊方式，提升整体模型的覆盖率</p>
<p>“杀”模型负责准确识别恶意份子</p>
<p>“验”模型通过多业务交叉验证，负责保证“查”、“杀”模型的准确率</p>
<p><strong>终端识别模块</strong>（灯塔SDK稽核模块）:该模块主要是采用机器学习算法选取系统中所有可用的信息作为特征，然后对这些特征进行运算得到该<strong>设备的指纹，</strong>可以有效识别手机模拟器、修改系统参数等行为。</p>
<p><strong>基于规则的识别模块</strong>（业务自有模块）:该模块一般是通过业务经验及对历史可疑渠道的总结形成的<strong>反作弊规则</strong>，可以理解为多维组合规则，一般需根据业务成本、对渠道的容忍度<strong>设置关键变量的阈值</strong>。</p>
<p><strong>基于数据挖掘的识别模块</strong>（灯塔云端模块）:该模块主要从硬件信息、用户活跃、用户行为进行多维度、多业务交叉验证，分别计算每个维度下面的不同特征值，结合决策树、LR、贝叶斯网络等多种算法进行精准的定位。<strong>分类</strong></p>
<p>为了增强识别的准确性和稳定性，模块之间、模块内部均采用集成学习方法的思想，其核心思想是在模块内对同一个训练集训练不同的分类器，然后把这些分类器结合起来构成一个最终的分类器，而每一个模块可以针对不同的作弊手段进行识别，再把模块与模块结合，才能识别所有的作弊手段。    </p>
<h1 id="admaster《广告反欺诈研究报告》2016"><a href="#admaster《广告反欺诈研究报告》2016" class="headerlink" title="admaster《广告反欺诈研究报告》2016"></a>admaster《广告反欺诈研究报告》2016</h1><p>今年 1 月 29 日和 3 月 2 日,宝洁公司首席品牌官 Marc Pritchard 分别在美国互动广告局(Interactive<br>Advertising Bureau, IAB)和美国广告主协会(Association of National Advertisers, ANA)两个年度<br>重要峰会上进行主题发言,针对数字广告透明度和可见性标准的言论引发了全球营销圈和数字行业的热议。<br>宝洁呼吁业界在四个方面采取行动:</p>
<ol>
<li>数字广告采纳一套统一的可见性测量标准;</li>
<li>贯彻第三方测评机构</li>
</ol>
<p>的验证审核;</p>
<ol>
<li>提倡全面透明的代理公司合同机制;</li>
<li>预防广告欺诈。</li>
</ol>
<p>AdMaster先后推出了 BlueAir、定投识别 (VOA)、监播实录 (SNAP) 等技术产品,逐步建立了“全方位、深层次、多角度”的广告反欺诈解决方案。尤其在程序化购买中,AdMaster 在投放前预判 (Pre-bid),即事前广告反欺诈技术。</p>
<p><strong>BlueAir</strong>可以对智能电视IP、地域、频次以及User Agent等维度的异常流量进行甄别，将行为逻辑上不正常的设备加入黑名单，从而保障广告投放的安全。同时，与海信、康佳、创维、欢网等硬件厂家共同建立智能电视设备白名单，以便从设备维度进一步甄别流量真实性。</p>
<p><strong>定投识别</strong></p>
<p><strong>监播实录</strong></p>
<p><strong>投放前预判</strong>。在流量请求、广告未展现时,根据历史流量质量进行排查,从而提前避免广告在无效流量上的投放。</p>
<h2 id="无效流量类型"><a href="#无效流量类型" class="headerlink" title="无效流量类型"></a>无效流量类型</h2><p>1、广告可见性问题引发的低质量流量</p>
<p>目前媒体的环境导致广告不易可见。AdMaster 在 AdServing 广告投放管理技术上能够实现广告可见性的预估判断。在多种广告形式的后测方面,通过独创的模型评估广告可见性表现。</p>
<p>2、机器人无效流量（Non-Human Traffic）</p>
<p>从最初的 cookie 和 IP不变的前提下,反复刷新页面和点击广告,造成广告曝光和点击的增加,到通过木马或者恶意程序控制海量人肉刷机、伪造大量 IP 与设备信息进行模拟访问、或将 IP 和 cookie、User Agent 一起进行轮替的流量造假方式,都属于机器人无效流量。</p>
<p><strong>BlueAir</strong> 广告反欺诈技术能够结合历史异常数据,能够在流量请求、广告未展现时,即根据历史流量质量判断此流量的质量,在投放前通过 <strong>Pre-bid</strong> 判断出流量的异常,杜绝流量造假现象发生。</p>
<p>3、视频类无效流量</p>
<p>(1)针对剧目投偏现象,AdMaster <strong>定投识别</strong> (VOA) 功能通过 Referrer/ 剧目 ID 等方式解析 广告曝光时所<br><strong>播放剧目名称</strong>,并与广告主定投的剧目内容进行匹配。在移动端定投评估中,高诚信度的视频媒体也提供高度<br>配合。剧目投偏比例,一目了然。</p>
<p>(2)针对时有发生的曝光代码调用,但是素材未正确展示的广告欺诈现象,AdMaster 利用<strong>监播实录</strong> (SNAP)<br>功能,采用类似于“神秘访客”概念的方法从海量抽样监测,将视频内容播放前的所有贴片内容录制下来,并<br>通过图像识别与适当的素材进行对比,判断素材是否被正确展示,以及是否按照要求展示。</p>
<p>4、智能电视无效流量</p>
<p>支持智能电视广告的监测模式一般有 3 种 : 分别是第三方 SDK 监测、C2S 和 S2S 两种 API。前两种相比 S2S 更为安全,也更容易监测流量异常情况,可以说 C2S API 是智能电视广告监测安全的起点和基础(S2S API 传输方式目前很难识别无效流量)。</p>
<h1 id="inmobi《移动广告反作弊白皮书》"><a href="#inmobi《移动广告反作弊白皮书》" class="headerlink" title="inmobi《移动广告反作弊白皮书》"></a>inmobi《移动广告反作弊白皮书》</h1><p>没技术</p>
<p>反作弊措施</p>
<p>1）剔除自动流量。</p>
<p>识别机器人脚本，分析展示和点击的质量。</p>
<p>2）数据信号双重检测。</p>
<p>将媒体共享的人群信息和从SDK收集的信息比对，对所有无效信息定位和删除。</p>
<p>[部分有关 广告联盟作弊 与反作弊资料收集]（<a href="http://cwiki.apachecn.org/pages/viewpage.action?pageId=4882639）" target="_blank" rel="noopener">http://cwiki.apachecn.org/pages/viewpage.action?pageId=4882639）</a></p>
<p>roadmap中，AD fraud的业界措施</p>
<p>监控广告投放的效果；</p>
<p>保存曝光设备的ip</p>
<p>记录用户在landing页的行为</p>
<p>网页分析</p>
<p><strong>基于数据挖掘的识别模块</strong>（灯塔云端模块）:该模块主要从硬件信息、用户活跃、用户行为进行多维度、多业务交叉验证，分别计算每个维度下面的不同特征值，结合决策树、LR、贝叶斯网络等多种算法进行精准的定位。为了增强识别的准确性和稳定性，模块之间、模块内部均采用集成学习方法的思想，其核心思想是在模块内对同一个训练集训练不同的分类器，然后把这些分类器结合起来构成一个最终的分类器，而每一个模块可以针对不同的作弊手段进行识别，再把模块与模块结合，才能识别所有的作弊手段。</p>
<p>这个方法未来肯定要实现的，大致是通过集成学习的方式，综合硬件信息，网页特征，用户行为</p>
<p>最初我们只有url信息，提取url的特征判断投放的网页是否安全；</p>
<p>后面对每条</p>
<p>前期，通过分析url内容，判断网页是否安全，是否为虚假网页，网页内容与广告品牌是否有冲突。</p>
<p>后面，分析曝光的详细信息，包括设备信息，用户特征，判断是否为作弊流量；分析网页元素和广告位置，判断广告可见性。</p>
<p>再后面，在对url和黑名单有一定积累的基础上，在投放或竞价前检测投放环境，主动识别和屏蔽非安全流量</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2017/07/12/算法背后的数学原理/利率计算/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/07/12/算法背后的数学原理/利率计算/" class="post-title-link" itemprop="url">利率计算</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2017-07-12 11:49:53" itemprop="dateCreated datePublished" datetime="2017-07-12T11:49:53+08:00">2017-07-12</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-01-29 21:42:24" itemprop="dateModified" datetime="2018-01-29T21:42:24+08:00">2018-01-29</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/数学/" itemprop="url" rel="index"><span itemprop="name">数学</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2017/07/12/算法背后的数学原理/利率计算/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/07/12/算法背后的数学原理/利率计算/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>等额本息</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">每月月供额=〔贷款本金×月利率×(1＋月利率)＾还款月数〕÷〔(1＋月利率)＾还款月数-1〕</span><br><span class="line">每月应还利息=贷款本金×月利率×〔(1+月利率)^还款月数-(1+月利率)^(还款月序号-1)〕÷〔(1+月利率)^还款月数-1〕</span><br><span class="line">每月应还本金=贷款本金×月利率×(1+月利率)^(还款月序号-1)÷〔(1+月利率)^还款月数-1〕</span><br><span class="line">总利息=还款月数×每月月供额-贷款本金</span><br></pre></td></tr></table></figure>
<p>等额本金</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">每月月供额=(贷款本金÷还款月数)+(贷款本金-已归还本金累计额)×月利率</span><br><span class="line">每月应还本金=贷款本金÷还款月数</span><br><span class="line">每月应还利息=剩余本金×月利率=(贷款本金-已归还本金累计额)×月利率</span><br><span class="line">每月月供递减额=每月应还本金×月利率=贷款本金÷还款月数×月利率</span><br><span class="line">总利息=还款月数×(总贷款额×月利率-月利率×(总贷款额÷还款月数)*(还款月数-1)÷2+总贷款额÷还款月数)</span><br></pre></td></tr></table></figure>
<p><a href="https://zhuanlan.zhihu.com/p/22920169" target="_blank" rel="noopener">信用卡账单分期真实年化利率</a></p>
<p><img src="https://pic4.zhimg.com/v2-177bf28088e8ef5462e3b5cc3d1ab2eb_b.png" alt=""></p>
<h1 id="年利率"><a href="#年利率" class="headerlink" title="年利率"></a>年利率</h1><p>利息率=利息量÷本金÷时间×100%</p>
<h1 id="IRR"><a href="#IRR" class="headerlink" title="IRR"></a>IRR</h1><p><strong>内部收益率 (IRR) 的定义是：净现值 (NPV) 为零时的折现率。</strong></p>
<p>综合考虑了每期的流入流出现金的量和时间，加权出来的结果。<br>IRR实质上是一个折现率，用IRR折现时会达到该项目的净现值NPV为0的状态。也可以理解为一个项目的预期收益率。<br>举例来说，如IRR为8%，可以简单解释为以8%的利率借钱投资于此项目，刚好可以不赚不赔。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2017/07/12/机器学习/集成学习/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/07/12/机器学习/集成学习/" class="post-title-link" itemprop="url">集成学习</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2017-07-12 11:49:53" itemprop="dateCreated datePublished" datetime="2017-07-12T11:49:53+08:00">2017-07-12</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-08-16 16:41:49" itemprop="dateModified" datetime="2018-08-16T16:41:49+08:00">2018-08-16</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2017/07/12/机器学习/集成学习/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/07/12/机器学习/集成学习/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>个体学习器一般是弱学习器。</p>
<blockquote>
<p> 弱学习器是指泛华性能略优于随机猜测的学习器，例如二分上略高于50%的学习器。</p>
</blockquote>
<p>要获得好的集成，个体学习器应“好而不同”，即个体学习器要有一定的<strong>准确性和多样性</strong>（学习器之间有差异）。</p>
<p>理论上，假设个体学习器的误差是相互独立，那么随着学习器数量增大，集成的错误率将指数下降，最终趋向于零。</p>
<p>但实际上不可能相互独立。且<strong>准确性和多样性本身就是矛盾的</strong>，追求准确性就要牺牲多样性。所以<strong>如何产生并结合“好而不同”的学习器，是集成学习研究的核心</strong>。</p>
<p>根据集成的方式不同，</p>
<p>1）个体学习器存在强依赖性，必须串行生成，如Boosting；</p>
<p>2）个体学习器间不存在强依赖关系，可同时并行生成，如Bagging和随机森林。</p>
<h1 id="mic或stacking方法"><a href="#mic或stacking方法" class="headerlink" title="mic或stacking方法"></a>mic或stacking方法</h1><p><a href="https://blog.csdn.net/sb19931201/article/details/56315689?locationNum=1&amp;fps=1" target="_blank" rel="noopener">https://blog.csdn.net/sb19931201/article/details/56315689?locationNum=1&amp;fps=1</a> 从这篇帖子来</p>
<p><a href="https://blog.csdn.net/a358463121/article/details/53054686#t18" target="_blank" rel="noopener">https://blog.csdn.net/a358463121/article/details/53054686#t18</a></p>
<p><a href="https://zhuanlan.zhihu.com/jlbookworm" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/jlbookworm</a></p>
<p><a href="https://blog.csdn.net/wstcjf/article/details/77989963" target="_blank" rel="noopener">https://blog.csdn.net/wstcjf/article/details/77989963</a> 文章的思路有点问题？</p>
<p><a href="https://blog.csdn.net/xiaoliuzz/article/details/79298841" target="_blank" rel="noopener">https://blog.csdn.net/xiaoliuzz/article/details/79298841</a></p>
<p><a href="https://blog.csdn.net/yc1203968305/article/details/73526615" target="_blank" rel="noopener">https://blog.csdn.net/yc1203968305/article/details/73526615</a></p>
<p><a href="https://www.cnblogs.com/zhizhan/p/5051881.html" target="_blank" rel="noopener">https://www.cnblogs.com/zhizhan/p/5051881.html</a></p>
<p><a href="https://mlwave.com/kaggle-ensembling-guide/" target="_blank" rel="noopener">https://mlwave.com/kaggle-ensembling-guide/</a></p>
<h1 id="Stacking"><a href="#Stacking" class="headerlink" title="Stacking"></a>Stacking</h1><p><a href="http://tech.ifeng.com/a/20170929/44704115_0.shtml" target="_blank" rel="noopener">Kaggle机器学习之模型融合（stacking）心得</a></p>
<p><a href="https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python" target="_blank" rel="noopener">Introduction to Ensembling/Stacking in Python</a> </p>
<p><a href="https://www.kdnuggets.com/2017/02/stacking-models-imropved-predictions.html" target="_blank" rel="noopener">Stacking Models for Improved Predictions</a></p>
<h2 id="使用sklearn进行集成学习——理论"><a href="#使用sklearn进行集成学习——理论" class="headerlink" title="使用sklearn进行集成学习——理论"></a><a href="https://www.cnblogs.com/jasonfreak/p/5657196.html" target="_blank" rel="noopener">使用sklearn进行集成学习——理论</a></h2><p>1 前言<br>2 集成学习是什么？<br>3 偏差和方差<br>　　3.1 模型的偏差和方差是什么？<br>　　3.2 bagging的偏差和方差<br>　　3.3 boosting的偏差和方差<br>　　3.4 模型的独立性<br>　　3.5 小结<br>4 Gradient Boosting<br>　　4.1 拟合残差<br>　　4.2 拟合反向梯度<br>　　　　4.2.1 契机：引入损失函数<br>　　　　4.2.2 难题一：任意损失函数的最优化<br>　　　　4.2.3 难题二：无法对测试样本计算反向梯度<br>　　4.3 常见的损失函数<br>　　4.4 步子太大容易扯着蛋：缩减<br>　　4.5 初始模型<br>　　4.5 Gradient Tree Boosting<br>　　4.6 小结<br>5 总结<br>6 参考资料</p>
<hr>
<h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1 前言"></a>1 前言</h1><p>　　很多人在竞赛（Kaggle，天池等）或工程实践中使用了集成学习（例如，RF、GTB等），确实也取得了不错的效果，在保证准确度的同时也提升了模型防止过拟合的能力。但是，我们真的用对了集成学习吗？</p>
<p>　　sklearn提供了sklearn.ensemble库，支持众多集成学习算法和模型。恐怕大多数人使用这些工具时，要么使用默认参数，要么根据模型在测试集上的性能试探性地进行调参（当然，完全不懂的参数还是不动算了），要么将调参的工作丢给调参算法（网格搜索等）。这样并不能真正地称为“会”用sklearn进行集成学习。</p>
<p>　　我认为，学会调参是进行集成学习工作的前提。然而，第一次遇到这些算法和模型时，肯定会被其丰富的参数所吓到，要知道，教材上教的伪代码可没这么多参数啊！！！没关系，暂时，我们只要记住一句话：参数可分为两种，一种是影响模型在训练集上的准确度或影响防止过拟合能力的参数；另一种不影响这两者的其他参数。模型在样本总体上的准确度（后简称准确度）由其在训练集上的准确度及其防止过拟合的能力所共同决定，所以在调参时，我们主要对第一种参数进行调整，最终达到的效果是：模型在训练集上的准确度和防止过拟合能力的大和谐！</p>
<p>　　本篇博文将详细阐述模型参数背后的理论知识，在下篇博文中，我们将对最热门的两个模型Random Forrest和Gradient Tree Boosting（含分类和回归，所以共4个模型）进行具体的参数讲解。如果你实在无法静下心来学习理论，你也可以在下篇博文中找到最直接的调参指导，虽然我不赞同这么做。</p>
<hr>
<h1 id="2-集成学习是什么？"><a href="#2-集成学习是什么？" class="headerlink" title="2 集成学习是什么？"></a>2 集成学习是什么？</h1><p>　　我们还是花一点时间来说明一下集成学习是什么，如果对此有一定基础的同学可以跳过本节。简单来说，集成学习是一种技术框架，其按照不同的思路来组合基础模型，从而达到其利断金的目的。</p>
<p>　　目前，有三种常见的集成学习框架：bagging，boosting和stacking。国内，南京大学的周志华教授对集成学习有很深入的研究，其在09年发表的一篇概述性论文<a href="http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/springerEBR09.pdf" target="_blank" rel="noopener">《</a><a href="http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/springerEBR09.pdf" target="_blank" rel="noopener">Ensemble Learning》</a>对这三种集成学习框架有了明确的定义，概括如下：</p>
<p> 　　bagging：从训练集从进行子抽样组成每个基模型所需要的子训练集，对所有基模型预测的结果进行综合产生最终的预测结果：</p>
<p><img src="https://images2015.cnblogs.com/blog/927391/201607/927391-20160717135005498-1140287801.jpg" alt="img"></p>
<p>　　boosting：训练过程为阶梯状，基模型按次序一一进行训练（实现上可以做到并行），基模型的训练集按照某种策略每次都进行一定的转化。对所有基模型预测的结果进行线性综合产生最终的预测结果：</p>
<p><img src="https://images2015.cnblogs.com/blog/927391/201607/927391-20160717135023373-1810846145.jpg" alt="img"></p>
<p>　　stacking：将训练好的所有基模型对训练基进行预测，第j个基模型对第i个训练样本的预测值将作为新的训练集中第i个样本的第j个特征值，最后基于新的训练集进行训练。同理，预测的过程也要先经过所有基模型的预测形成新的测试集，最后再对测试集进行预测：</p>
<p><img src="https://images2015.cnblogs.com/blog/927391/201607/927391-20160716073722420-208134951.jpg" alt="img"></p>
<p>　　有了这些基本概念之后，直觉将告诉我们，由于不再是单一的模型进行预测，所以模型有了“集思广益”的能力，也就不容易产生过拟合现象。但是，直觉是不可靠的，接下来我们将从模型的偏差和方差入手，彻底搞清楚这一问题。</p>
<hr>
<h1 id="3-偏差和方差"><a href="#3-偏差和方差" class="headerlink" title="3 偏差和方差"></a>3 偏差和方差</h1><p>　　广义的偏差（bias）描述的是预测值和真实值之间的差异，方差（variance）描述距的是预测值作为随机变量的离散程度。<a href="http://scott.fortmann-roe.com/docs/BiasVariance.html" target="_blank" rel="noopener">《Understanding the Bias-Variance Tradeoff》</a>当中有一副图形象地向我们展示了偏差和方差的关系：</p>
<p><img src="https://images2015.cnblogs.com/blog/927391/201607/927391-20160716124330623-527064401.jpg" alt="img"></p>
<h2 id="3-1-模型的偏差和方差是什么？"><a href="#3-1-模型的偏差和方差是什么？" class="headerlink" title="3.1 模型的偏差和方差是什么？"></a>3.1 模型的偏差和方差是什么？</h2><p>　　模型的偏差是一个相对来说简单的概念：训练出来的模型在训练集上的准确度。</p>
<p>　　要解释模型的方差，首先需要重新审视模型：模型是随机变量。设样本容量为n的训练集为随机变量的集合(X1, X2, …, Xn)，那么模型是以这些随机变量为输入的随机变量函数（其本身仍然是随机变量）：F(X1, X2, …, Xn)。抽样的随机性带来了模型的随机性。</p>
<p>　　定义随机变量的值的差异是计算方差的前提条件，通常来说，我们遇到的都是数值型的随机变量，数值之间的差异再明显不过（减法运算）。但是，模型的差异性呢？我们可以理解模型的差异性为模型的结构差异，例如：线性模型中权值向量的差异，树模型中树的结构差异等。在研究模型方差的问题上，我们并不需要对方差进行定量计算，只需要知道其概念即可。</p>
<p>　　研究模型的方差有什么现实的意义呢？我们认为方差越大的模型越容易过拟合：假设有两个训练集A和B，经过A训练的模型Fa与经过B训练的模型Fb差异很大，这意味着Fa在类A的样本集合上有更好的性能，而Fb反之，这便是我们所说的过拟合现象。</p>
<p>　　我们常说集成学习框架中的基模型是弱模型，通常来说弱模型是偏差高（在训练集上准确度低）方差小（防止过拟合能力强）的模型。但是，并不是所有集成学习框架中的基模型都是弱模型。bagging和stacking中的基模型为强模型（偏差低方差高），boosting中的基模型为弱模型。</p>
<p>　　在bagging和boosting框架中，通过计算基模型的期望和方差，我们可以得到模型整体的期望和方差。为了简化模型，我们假设基模型的权重、方差及两两间的相关系数相等。由于bagging和boosting的基模型都是线性组成的，那么有：</p>
<p> <img src="https://images2015.cnblogs.com/blog/927391/201607/927391-20160716145131217-650617034.png" alt="img"></p>
<h2 id="3-2-bagging的偏差和方差"><a href="#3-2-bagging的偏差和方差" class="headerlink" title="3.2 bagging的偏差和方差"></a>3.2 bagging的偏差和方差</h2><p>　　对于bagging来说，每个基模型的权重等于1/m且期望近似相等（子训练集都是从原训练集中进行子抽样），故我们可以进一步化简得到：</p>
<p><img src="https://images2015.cnblogs.com/blog/927391/201607/927391-20160716145206701-383430284.png" alt="img"></p>
<p>　　根据上式我们可以看到，整体模型的期望近似于基模型的期望，这也就意味着整体模型的偏差和基模型的偏差近似。同时，整体模型的方差小于等于基模型的方差（当相关性为1时取等号），随着基模型数（m）的增多，整体模型的方差减少，从而防止过拟合的能力增强，模型的准确度得到提高。但是，模型的准确度一定会无限逼近于1吗？并不一定，当基模型数增加到一定程度时，方差公式第二项的改变对整体方差的作用很小，防止过拟合的能力达到极限，这便是准确度的极限了。另外，在此我们还知道了为什么bagging中的基模型一定要为强模型，否则就会导致整体模型的偏差度低，即准确度低。</p>
<p>　　Random Forest是典型的基于bagging框架的模型，其在bagging的基础上，进一步降低了模型的方差。Random Fores中基模型是树模型，在树的内部节点分裂过程中，不再是将所有特征，而是随机抽样一部分特征纳入分裂的候选项。这样一来，基模型之间的相关性降低，从而在方差公式中，第一项显著减少，第二项稍微增加，整体方差仍是减少。</p>
<h2 id="3-3-boosting的偏差和方差"><a href="#3-3-boosting的偏差和方差" class="headerlink" title="3.3 boosting的偏差和方差"></a>3.3 boosting的偏差和方差</h2><p>　　对于boosting来说，基模型的训练集抽样是强相关的，那么模型的相关系数近似等于1，故我们也可以针对boosting化简公式为：</p>
<p><img src="https://images2015.cnblogs.com/blog/927391/201607/927391-20160717142500264-1717908455.png" alt="img"></p>
<p>　　通过观察整体方差的表达式，我们容易发现，若基模型不是弱模型，其方差相对较大，这将导致整体模型的方差很大，即无法达到防止过拟合的效果。因此，boosting框架中的基模型必须为弱模型。</p>
<p>　　因为基模型为弱模型，导致了每个基模型的准确度都不是很高（因为其在训练集上的准确度不高）。随着基模型数的增多，整体模型的期望值增加，更接近真实值，因此，整体模型的准确度提高。但是准确度一定会无限逼近于1吗？仍然并不一定，因为训练过程中准确度的提高的主要功臣是整体模型在训练集上的准确度提高，而随着训练的进行，整体模型的方差变大，导致防止过拟合的能力变弱，最终导致了准确度反而有所下降。</p>
<p>　　基于boosting框架的Gradient Tree Boosting模型中基模型也为树模型，同Random Forrest，我们也可以对特征进行随机抽样来使基模型间的相关性降低，从而达到减少方差的效果。</p>
<h2 id="3-4-模型的独立性"><a href="#3-4-模型的独立性" class="headerlink" title="3.4 模型的独立性"></a>3.4 模型的独立性</h2><p>　　聪明的读者这时肯定要问了，如何衡量基模型的独立性？我们说过，抽样的随机性决定了模型的随机性，如果两个模型的训练集抽样过程不独立，则两个模型则不独立。这时便有一个天大的陷阱在等着我们：bagging中基模型的训练样本都是独立的随机抽样，但是基模型却不独立呢？</p>
<p>　　我们讨论模型的随机性时，抽样是针对于样本的整体。而bagging中的抽样是针对于训练集（整体的子集），所以并不能称其为对整体的独立随机抽样。那么到底bagging中基模型的相关性体现在哪呢？在知乎问答<a href="https://www.zhihu.com/question/26760839" target="_blank" rel="noopener">《为什么说bagging是减少variance，而boosting是减少bias?》</a>中请教用户<a href="https://www.zhihu.com/people/guo-ni-he" target="_blank" rel="noopener">“过拟合”</a>后，我总结bagging的抽样为两个过程：</p>
<ol>
<li>样本抽样：整体模型F(X1, X2, …, Xn)中各输入随机变量（X1, X2, …, Xn）对样本的抽样</li>
<li>子抽样：从整体模型F(X1, X2, …, Xn)中随机抽取若干输入随机变量成为基模型的输入随机变量</li>
</ol>
<p>　　假若在子抽样的过程中，两个基模型抽取的输入随机变量有一定的重合，那么这两个基模型对整体样本的抽样将不再独立，这时基模型之间便具有了相关性。</p>
<h2 id="3-5-小结"><a href="#3-5-小结" class="headerlink" title="3.5 小结"></a>3.5 小结</h2><p>　　还记得调参的目标吗：模型在训练集上的准确度和防止过拟合能力的大和谐！为此，我们目前做了一些什么工作呢？</p>
<ol>
<li>使用模型的偏差和方差来描述其在训练集上的准确度和防止过拟合的能力</li>
<li>对于bagging来说，整体模型的偏差和基模型近似，随着训练的进行，整体模型的方差降低</li>
<li>对于boosting来说，整体模型的初始偏差较高，方差较低，随着训练的进行，整体模型的偏差降低（虽然也不幸地伴随着方差增高），当训练过度时，因方差增高，整体模型的准确度反而降低</li>
<li>整体模型的偏差和方差与基模型的偏差和方差息息相关</li>
</ol>
<p>　　这下总算有点开朗了，那些让我们抓狂的参数，现在可以粗略地分为两类了：控制整体训练过程的参数和基模型的参数，这两类参数都在影响着模型在训练集上的准确度以及防止过拟合的能力。</p>
<hr>
<h1 id="4-Gradient-Boosting"><a href="#4-Gradient-Boosting" class="headerlink" title="4 Gradient Boosting"></a>4 Gradient Boosting</h1><p>　　对基于Gradient Boosting框架的模型的进行调试时，我们会遇到一个重要的概念：损失函数。在本节中，我们将把损失函数的“今生来世”讲个清楚！</p>
<p>　　基于boosting框架的整体模型可以用线性组成式来描述，其中h<a href="x">i</a>为基模型与其权值的乘积：</p>
<p><img src="https://images2015.cnblogs.com/blog/927391/201607/927391-20160717144731264-398372888.png" alt="img"></p>
<p>　　根据上式，整体模型的训练目标是使预测值F(x)逼近真实值y，也就是说要让每一个基模型的预测值逼近各自要预测的部分真实值。由于要同时考虑所有基模型，导致了整体模型的训练变成了一个非常复杂的问题。所以，研究者们想到了一个贪心的解决手段：每次只训练一个基模型。那么，现在改写整体模型为迭代式：</p>
<p><img src="https://images2015.cnblogs.com/blog/927391/201607/927391-20160717144844795-1514741556.png" alt="img"></p>
<p>　　这样一来，每一轮迭代中，只要集中解决一个基模型的训练问题：使F<a href="x">i</a>逼近真实值y。</p>
<h2 id="4-1-拟合残差"><a href="#4-1-拟合残差" class="headerlink" title="4.1 拟合残差"></a>4.1 拟合残差</h2><p>　　使F<a href="x">i</a>逼近真实值，其实就是使h<a href="x">i</a>逼近真实值和上一轮迭代的预测值F<a href="x">i-1</a>之差，即残差（y-F<a href="x">i-1</a>）。最直接的做法是构建基模型来拟合残差，在博文<a href="http://blog.csdn.net/w28971023/article/details/8240756" target="_blank" rel="noopener">《GBDT（MART） 迭代决策树入门教程 | 简介》</a>中，作者举了一个生动的例子来说明通过基模型拟合残差，最终达到整体模型F(x)逼近真实值。</p>
<p>　　研究者发现，残差其实是最小均方损失函数的关于预测值的反向梯度：</p>
<p><img src="https://images2015.cnblogs.com/blog/927391/201607/927391-20160717152356576-837065946.png" alt="img"></p>
<p>　　也就是说，若F<a href="x">i-1</a>加上拟合了反向梯度的h<a href="x">i</a>得到F<a href="x">i</a>，该值可能将导致平方差损失函数降低，预测的准确度提高！这显然不是巧合，但是研究者们野心更大，希望能够创造出一种对任意损失函数都可行的训练方法，那么仅仅拟合残差是不恰当的了。</p>
<h2 id="4-2-拟合反向梯度"><a href="#4-2-拟合反向梯度" class="headerlink" title="4.2 拟合反向梯度"></a>4.2 拟合反向梯度</h2><h3 id="4-2-1-契机：引入任意损失函数"><a href="#4-2-1-契机：引入任意损失函数" class="headerlink" title="4.2.1 契机：引入任意损失函数"></a>4.2.1 契机：引入任意损失函数</h3><p>　　引入任意损失函数后，我们可以定义整体模型的迭代式如下：</p>
<p><img src="https://images2015.cnblogs.com/blog/927391/201607/927391-20160717155418592-692164582.png" alt="img"></p>
<p>　　在这里，损失函数被定义为<a href="https://zh.wikipedia.org/wiki/%E6%B3%9B%E5%87%BD" target="_blank" rel="noopener">泛函</a>。</p>
<h3 id="4-2-2-难题一：任意损失函数的最优化"><a href="#4-2-2-难题一：任意损失函数的最优化" class="headerlink" title="4.2.2 难题一：任意损失函数的最优化"></a>4.2.2 难题一：任意损失函数的最优化</h3><p>　　对任意损失函数（且是泛函）的最优化是困难的。我们需要打破思维的枷锁，将整体损失函数L’定义为n元普通函数（n为样本容量），损失函数L定义为2元普通函数（记住！！！这里的损失函数不再是泛函！！！）：</p>
<p><img src="https://images2015.cnblogs.com/blog/927391/201607/927391-20160717161734873-1080465986.png" alt="img"></p>
<p>　　我们不妨使用<a href="https://en.wikipedia.org/wiki/Method_of_steepest_descent" target="_blank" rel="noopener">梯度最速下降法</a>来解决整体损失函数L’最小化的问题，先求整体损失函数的反向梯度：</p>
<p><img src="https://images2015.cnblogs.com/blog/927391/201607/927391-20160717161743857-1762234391.png" alt="img"></p>
<p>　　假设已知样本x的当前预测值为F<a href="x">i-1</a>，下一步将预测值按照反向梯度，依照步长为r[i]，进行更新：</p>
<p><img src="https://images2015.cnblogs.com/blog/927391/201607/927391-20160717162353389-352979333.png" alt="img"></p>
<p>　　步长r[i]不是固定值，而是设计为：</p>
<p><img src="https://images2015.cnblogs.com/blog/927391/201607/927391-20160717162221748-1981866230.png" alt="img"></p>
<h3 id="4-2-3-难题二：无法对测试样本计算反向梯度"><a href="#4-2-3-难题二：无法对测试样本计算反向梯度" class="headerlink" title="4.2.3 难题二：无法对测试样本计算反向梯度"></a>4.2.3 难题二：无法对测试样本计算反向梯度</h3><p>　　问题又来了，由于测试样本中y是未知的，所以无法求反向梯度。这正是Gradient Boosting框架中的基模型闪亮登场的时刻！在第i轮迭代中，我们创建训练集如下：</p>
<p><img src="https://images2015.cnblogs.com/blog/927391/201607/927391-20160717162908686-1639692645.png" alt="img"></p>
<p>　　也就是说，让基模型拟合反向梯度函数，这样我们就可以做到只输入x这一个参数，就可求出其对应的反向梯度了（当然，通过基模型预测出来的反向梯度并不是准确的，这也提供了泛化整体模型的机会）。</p>
<p>　　综上，假设第i轮迭代中，根据新训练集训练出来的基模型为f<a href="x">i</a>，那么最终的迭代公式为：</p>
<p><img src="https://images2015.cnblogs.com/blog/927391/201607/927391-20160717171931342-64219972.png" alt="img"></p>
<h2 id="4-3-常见的损失函数"><a href="#4-3-常见的损失函数" class="headerlink" title="4.3 常见的损失函数"></a>4.3 常见的损失函数</h2><p>　　ls：最小均方回归中用到的损失函数。在之前我们已经谈到，从拟合残差的角度来说，残差即是该损失函数的反向梯度值（所以又称反向梯度为伪残差）。不同的是，从拟合残差的角度来说，步长是无意义的。该损失函数是sklearn中Gradient Tree Boosting回归模型默认的损失函数。</p>
<p>　　deviance：<a href="http://www.duzelong.com/wordpress/201507/archives1326/" target="_blank" rel="noopener">逻辑回归</a>中用到的损失函数。熟悉逻辑回归的读者肯定还记得，逻辑回归本质是求极大似然解，其认为样本服从几何分布，样本属于某类别的概率可以logistic函数表达。所以，如果该损失函数可用在多类别的分类问题上，故其是sklearn中Gradient Tree Boosting分类模型默认的损失函数。</p>
<p>　　exponential：指数损失函数，表达式为：</p>
<p><img src="https://images2015.cnblogs.com/blog/927391/201607/927391-20160717164817170-1319916901.png" alt="img"></p>
<p>　　对该损失函数求反向梯度得：</p>
<p><img src="https://images2015.cnblogs.com/blog/927391/201607/927391-20160717165216123-16910201.png" alt="img"></p>
<p>　　这时，在第i轮迭代中，新训练集如下：</p>
<p> <img src="https://images2015.cnblogs.com/blog/927391/201607/927391-20160717165246186-1781792701.png" alt="img"></p>
<p>　　脑袋里有什么东西浮出水面了吧？让我们看看<a href="http://breezedeus.github.io/2015/07/12/breezedeus-adaboost-exponential-loss.html" target="_blank" rel="noopener">Adaboost算法</a>中，第i轮迭代中第j个样本权值的更新公式：</p>
<p><img src="https://images2015.cnblogs.com/blog/927391/201607/927391-20160717170109811-1251363012.png" alt="img"></p>
<p>　　样本的权值什么时候会用到呢？计算第i轮损失函数的时候会用到：</p>
<p><img src="https://images2015.cnblogs.com/blog/927391/201607/927391-20160717170902529-315971230.png" alt="img"></p>
<p>　　让我们再回过头来，看看使用指数损失函数的Gradient Boosting计算第i轮损失函数：</p>
<p><img src="https://images2015.cnblogs.com/blog/927391/201607/927391-20160717171050389-2042221750.png" alt="img"></p>
<p>　　天呐，两个公式就差了一个对权值的归一项。这并不是巧合，当损失函数是指数损失时，Gradient Boosting相当于二分类的Adaboost算法。是的，指数损失仅能用于二分类的情况。</p>
<h2 id="4-4-步子太大容易扯着蛋：缩减"><a href="#4-4-步子太大容易扯着蛋：缩减" class="headerlink" title="4.4 步子太大容易扯着蛋：缩减"></a>4.4 步子太大容易扯着蛋：缩减</h2><p>　　缩减也是一个相对显见的概念，也就是说使用Gradient Boosting时，每次学习的步长缩减一点。这有什么好处呢？缩减思想认为每次走一小步，多走几次，更容易逼近真实值。如果步子迈大了，使用最速下降法时，容易迈过最优点。将缩减代入迭代公式：</p>
<p><img src="https://images2015.cnblogs.com/blog/927391/201607/927391-20160717172203889-897514111.png" alt="img"></p>
<p> 　　缩减需要配合基模型数一起使用，当缩减率v降低时，基模型数要配合增大，这样才能提高模型的准确度。</p>
<h2 id="4-5-初始模型"><a href="#4-5-初始模型" class="headerlink" title="4.5 初始模型"></a>4.5 初始模型</h2><p>　　还有一个不那么起眼的问题，初始模型F<a href="x">0</a>是什么呢？如果没有定义初始模型，整体模型的迭代式一刻都无法进行！所以，我们定义初始模型为：</p>
<p><img src="https://images2015.cnblogs.com/blog/927391/201607/927391-20160717172644920-1113326686.png" alt="img"></p>
<p>　　根据上式可知，对于不同的损失函数来说，初始模型也是不一样的。对所有的样本来说，根据初始模型预测出来的值都一样。</p>
<h2 id="4-5-Gradient-Tree-Boosting"><a href="#4-5-Gradient-Tree-Boosting" class="headerlink" title="4.5 Gradient Tree Boosting"></a>4.5 Gradient Tree Boosting</h2><p>　　终于到了备受欢迎的Gradient Tree Boosting模型了！但是，可讲的却已经不多了。我们已经知道了该模型的基模型是树模型，并且可以通过对特征的随机抽样进一步减少整体模型的方差。我们可以在维基百科的<a href="https://en.wikipedia.org/wiki/Gradient_boosting" target="_blank" rel="noopener">Gradient Boosting</a>词条中找到其伪代码实现。</p>
<h2 id="4-6-小结"><a href="#4-6-小结" class="headerlink" title="4.6 小结"></a>4.6 小结</h2><p>　　到此，读者应当很清楚Gradient Boosting中的损失函数有什么意义了。要说偏差描述了模型在训练集准确度，则损失函数则是描述该准确度的间接量纲。也就是说，模型采用不同的损失函数，其训练过程会朝着不同的方向进行！</p>
<hr>
<h1 id="5-总结"><a href="#5-总结" class="headerlink" title="5 总结"></a>5 总结</h1><p>　　磨刀不误砍柴功，我们花了这么多时间来学习必要的理论，我强调一次：必要的理论！集成学习模型的调参工作的核心就是找到合适的参数，能够使整体模型在训练集上的准确度和防止过拟合的能力达到协调，从而达到在样本总体上的最佳准确度。有了本文的理论知识铺垫，在下篇中，我们将对Random Forest和Gradient Tree Boosting中的每个参数进行详细阐述，同时也有一些小试验证明我们的结论。</p>
<hr>
<h1 id="6-参考资料"><a href="#6-参考资料" class="headerlink" title="6 参考资料"></a>6 参考资料</h1><ol>
<li><a href="http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/springerEBR09.pdf" target="_blank" rel="noopener">《</a><a href="http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/springerEBR09.pdf" target="_blank" rel="noopener">Ensemble Learning》</a></li>
<li><a href="http://scott.fortmann-roe.com/docs/BiasVariance.html" target="_blank" rel="noopener">《Understanding the Bias-Variance Tradeoff》</a></li>
<li><a href="https://www.zhihu.com/question/26760839" target="_blank" rel="noopener">《为什么说bagging是减少variance，而boosting是减少bias?》</a></li>
<li><a href="http://blog.csdn.net/w28971023/article/details/8240756" target="_blank" rel="noopener">《GBDT（MART） 迭代决策树入门教程 | 简介》</a></li>
<li><a href="https://zh.wikipedia.org/wiki/%E6%B3%9B%E5%87%BD" target="_blank" rel="noopener">泛函</a></li>
<li><a href="https://en.wikipedia.org/wiki/Method_of_steepest_descent" target="_blank" rel="noopener">梯度最速下降法</a></li>
<li><a href="http://www.duzelong.com/wordpress/201507/archives1326/" target="_blank" rel="noopener">《logistic regression(二分类、多分类)》</a></li>
<li><a href="http://breezedeus.github.io/2015/07/12/breezedeus-adaboost-exponential-loss.html" target="_blank" rel="noopener">《Adaboost与指数损失》</a></li>
</ol>
<h2 id="使用sklearn进行集成学习——实践"><a href="#使用sklearn进行集成学习——实践" class="headerlink" title="使用sklearn进行集成学习——实践"></a><a href="https://www.cnblogs.com/jasonfreak/p/5720137.html" target="_blank" rel="noopener">使用sklearn进行集成学习——实践</a></h2><h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><p>1 Random Forest和Gradient Tree Boosting参数详解<br>2 如何调参？<br>　　2.1 调参的目标：偏差和方差的协调<br>　　2.2 参数对整体模型性能的影响<br>　　2.3 一个朴实的方案：贪心的坐标下降法<br>　　　　2.3.1 Random Forest调参案例：Digit Recognizer<br>　　　　　　2.3.1.1 调整过程影响类参数<br>　　　　　　2.3.1.2 调整子模型影响类参数<br>　　　　2.3.2 Gradient Tree Boosting调参案例：Hackathon3.x<br>　　　　　　2.3.2.1 调整过程影响类参数<br>　　　　　　2.3.2.2 调整子模型影响类参数<br>　　　　　　2.3.2.3 杀一记回马枪<br>　　2.4 “局部最优解”（温馨提示：看到这里有彩蛋！）<br>　　2.5 类别不均衡的陷阱<br>3 总结<br>4 参考资料</p>
<hr>
<h1 id="1-Random-Forest和Gradient-Tree-Boosting参数详解"><a href="#1-Random-Forest和Gradient-Tree-Boosting参数详解" class="headerlink" title="1 Random Forest和Gradient Tree Boosting参数详解"></a>1 Random Forest和Gradient Tree Boosting参数详解</h1><p>　　在sklearn.ensemble库中，我们可以找到Random Forest分类和回归的实现：RandomForestClassifier和RandomForestRegression，Gradient Tree Boosting分类和回归的实现：GradientBoostingClassifier和GradientBoostingRegression。有了这些模型后，立马上手操练起来？少侠请留步！且听我说一说，使用这些模型时常遇到的问题：</p>
<ul>
<li>明明模型调教得很好了，可是效果离我的想象总有些偏差？——模型训练的第一步就是要定好目标，往错误的方向走太多也是后退。</li>
<li>凭直觉调了某个参数，可是居然没有任何作用，有时甚至起到反作用？——定好目标后，接下来就是要确定哪些参数是影响目标的，其对目标是正影响还是负影响，影响的大小。</li>
<li>感觉训练结束遥遥无期，sklearn只是个在小数据上的玩具？——虽然sklearn并不是基于分布式计算环境而设计的，但我们还是可以通过某些策略提高训练的效率。</li>
<li>模型开始训练了，但是训练到哪一步了呢？——饱暖思淫欲啊，目标，性能和效率都得了满足后，我们有时还需要有别的追求，例如训练过程的输出，袋外得分计算等等。</li>
</ul>
<p>　　通过总结这些常见的问题，我们可以把模型的参数分为4类：目标类、性能类、效率类和附加类。下表详细地展示了4个模型参数的意义：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><strong>参数</strong></th>
<th><strong>类型</strong></th>
<th><strong>RandomForestClassifier</strong></th>
<th><strong>RandomForestRegressor</strong></th>
<th><strong>GradientBoostingClassifier</strong></th>
<th><strong>GradientBoostingRegressor</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>loss</td>
<td>目标</td>
<td></td>
<td></td>
<td>损失函数● exponential：模型等同AdaBoost★ deviance：和Logistic Regression的损失函数一致</td>
<td>损失函数● exponential：模型等同AdaBoost★ deviance：和Logistic Regression的损失函数一致</td>
</tr>
<tr>
<td>alpha</td>
<td>目标</td>
<td></td>
<td></td>
<td>损失函数为huber或quantile的时，alpha为损失函数中的参数</td>
<td>损失函数为huber或quantile的时，alpha为损失函数中的参数</td>
</tr>
<tr>
<td>class_weight</td>
<td>目标</td>
<td>类别的权值</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>n_estimators</td>
<td>性能</td>
<td>子模型的数量● int：个数★ 10：默认值</td>
<td>子模型的数量● int：个数★ 10：默认值</td>
<td>子模型的数量● int：个数★ 100：默认值</td>
<td>子模型的数量● int：个数★ 100：默认值</td>
</tr>
<tr>
<td>learning_rate</td>
<td>性能</td>
<td></td>
<td></td>
<td>学习率（缩减）</td>
<td>学习率（缩减）</td>
</tr>
<tr>
<td>criterion</td>
<td>性能</td>
<td>判断节点是否继续分裂采用的计算方法● entropy★ gini</td>
<td>判断节点是否继续分裂采用的计算方法★ mse</td>
<td></td>
<td></td>
</tr>
<tr>
<td>max_features</td>
<td>性能</td>
<td>节点分裂时参与判断的最大特征数● int：个数● float：占所有特征的百分比★ auto：所有特征数的开方● sqrt：所有特征数的开方● log2：所有特征数的log2值● None：等于所有特征数</td>
<td>节点分裂时参与判断的最大特征数● int：个数● float：占所有特征的百分比★ auto：所有特征数的开方● sqrt：所有特征数的开方● log2：所有特征数的log2值● None：等于所有特征数</td>
<td>节点分裂时参与判断的最大特征数● int：个数● float：占所有特征的百分比● auto：所有特征数的开方● sqrt：所有特征数的开方● log2：所有特征数的log2值★ None：等于所有特征数</td>
<td>节点分裂时参与判断的最大特征数● int：个数● float：占所有特征的百分比● auto：所有特征数的开方● sqrt：所有特征数的开方● log2：所有特征数的log2值★ None：等于所有特征数</td>
</tr>
<tr>
<td>max_depth</td>
<td>性能</td>
<td>最大深度，如果max_leaf_nodes参数指定，则忽略● int：深度★ None：树会生长到所有叶子都分到一个类，或者某节点所代表的样本数已小于min_samples_split</td>
<td>最大深度，如果max_leaf_nodes参数指定，则忽略● int：深度★ None：树会生长到所有叶子都分到一个类，或者某节点所代表的样本数已小于min_samples_split</td>
<td>最大深度，如果max_leaf_nodes参数指定，则忽略● int：深度★ 3：默认值</td>
<td>最大深度，如果max_leaf_nodes参数指定，则忽略● int：深度★ 3：默认值</td>
</tr>
<tr>
<td>min_samples_split</td>
<td>性能</td>
<td>分裂所需的最小样本数● int：样本数★ 2：默认值</td>
<td>分裂所需的最小样本数● int：样本数★ 2：默认值</td>
<td>分裂所需的最小样本数● int：样本数★ 2：默认值</td>
<td>分裂所需的最小样本数● int：样本数★ 2：默认值</td>
</tr>
<tr>
<td>min_samples_leaf</td>
<td>性能</td>
<td>叶节点最小样本数● int：样本数★ 1：默认值</td>
<td>叶节点最小样本数● int：样本数★ 1：默认值</td>
<td>叶节点最小样本数● int：样本数★ 1：默认值</td>
<td>叶节点最小样本数● int：样本数★ 1：默认值</td>
</tr>
<tr>
<td>min_weight_fraction_leaf</td>
<td>性能</td>
<td>叶节点最小样本权重总值● float：权重总值★ 0：默认值</td>
<td>叶节点最小样本权重总值● float：权重总值★ 0：默认值</td>
<td>叶节点最小样本权重总值● float：权重总值★ 0：默认值</td>
<td>叶节点最小样本权重总值● float：权重总值★ 0：默认值</td>
</tr>
<tr>
<td>max_leaf_nodes</td>
<td>性能</td>
<td>最大叶节点数● int：个数★ None：不限制叶节点数</td>
<td>最大叶节点数● int：个数★ None：不限制叶节点数</td>
<td>最大叶节点数● int：个数★ None：不限制叶节点数</td>
<td>最大叶节点数● int：个数★ None：不限制叶节点数</td>
</tr>
<tr>
<td>bootstrap</td>
<td>性能</td>
<td>是否bootstrap对样本抽样● False：子模型的样本一致，子模型间强相关★ True：默认值</td>
<td>是否bootstrap对样本抽样● False：子模型的样本一致，子模型间强相关★ True：默认值</td>
<td></td>
<td></td>
</tr>
<tr>
<td>subsample</td>
<td>性能</td>
<td></td>
<td></td>
<td>子采样率● float：采样率★ 1.0：默认值</td>
<td>子采样率● float：采样率★ 1.0：默认值</td>
</tr>
<tr>
<td>init</td>
<td>性能</td>
<td></td>
<td></td>
<td>初始子模型</td>
<td>初始子模型</td>
</tr>
<tr>
<td>n_jobs</td>
<td>效率</td>
<td>并行数● int：个数● -1：跟CPU核数一致★ 1:默认值</td>
<td>并行数● int：个数● -1：跟CPU核数一致★ 1:默认值</td>
<td></td>
<td></td>
</tr>
<tr>
<td>warm_start</td>
<td>效率</td>
<td>是否热启动，如果是，则下一次训练是以追加树的形式进行● bool：热启动★ False：默认值</td>
<td>是否热启动，如果是，则下一次训练是以追加树的形式进行● bool：热启动★ False：默认值</td>
<td>是否热启动，如果是，则下一次训练是以追加树的形式进行● bool：热启动★ False：默认值</td>
<td>是否热启动，如果是，则下一次训练是以追加树的形式进行● bool：热启动★ False：默认值</td>
</tr>
<tr>
<td>presort</td>
<td>效率</td>
<td></td>
<td></td>
<td>是否预排序,预排序可以加速查找最佳分裂点，对于稀疏数据不管用● Bool★ auto：非稀疏数据则预排序，若稀疏数据则不预排序</td>
<td>是否预排序,预排序可以加速查找最佳分裂点，对于稀疏数据不管用● Bool★ auto：非稀疏数据则预排序，若稀疏数据则不预排序</td>
</tr>
<tr>
<td>oob_score</td>
<td>附加</td>
<td>是否计算<a href="http://scikit-learn.org/stable/auto_examples/ensemble/plot_ensemble_oob.html" target="_blank" rel="noopener">袋外得分</a>★ False：默认值</td>
<td>是否计算<a href="http://scikit-learn.org/stable/auto_examples/ensemble/plot_ensemble_oob.html" target="_blank" rel="noopener">袋外得分</a>★ False：默认值</td>
<td></td>
<td></td>
</tr>
<tr>
<td>random_state</td>
<td>附加</td>
<td>随机器对象</td>
<td>随机器对象</td>
<td>随机器对象</td>
<td>随机器对象</td>
</tr>
<tr>
<td>verbose</td>
<td>附加</td>
<td>日志冗长度● int：冗长度★ 0：不输出训练过程● 1：偶尔输出● &gt;1：对每个子模型都输出</td>
<td>日志冗长度● int：冗长度★ 0：不输出训练过程● 1：偶尔输出● &gt;1：对每个子模型都输出</td>
<td>日志冗长度● int：冗长度★ 0：不输出训练过程● 1：偶尔输出● &gt;1：对每个子模型都输出</td>
<td>日志冗长度● int：冗长度★ 0：不输出训练过程● 1：偶尔输出● &gt;1：对每个子模型都输出</td>
</tr>
</tbody>
</table>
</div>
<p><em># ★：默认值</em></p>
<p>　　不难发现，基于bagging的Random Forest模型和基于boosting的Gradient Tree Boosting模型有不少共同的参数，然而某些参数的默认值又相差甚远。在<a href="http://www.cnblogs.com/jasonfreak/p/5657196.html" target="_blank" rel="noopener">《使用sklearn进行集成学习——理论》</a>一文中，我们对bagging和boosting两种集成学习技术有了初步的了解。Random Forest的子模型都拥有较低的偏差，整体模型的训练过程旨在降低方差，故其需要较少的子模型（n_estimators默认值为10）且子模型不为弱模型（max_depth的默认值为None），同时，降低子模型间的相关度可以起到减少整体模型的方差的效果（max_features的默认值为auto）。另一方面，Gradient Tree Boosting的子模型都拥有较低的方差，整体模型的训练过程旨在降低偏差，故其需要较多的子模型（n_estimators默认值为100）且子模型为弱模型（max_depth的默认值为3），但是降低子模型间的相关度不能显著减少整体模型的方差（max_features的默认值为None）。</p>
<hr>
<h1 id="2-如何调参？"><a href="#2-如何调参？" class="headerlink" title="2 如何调参？"></a>2 如何调参？</h1><p>　　聪明的读者应当要发问了：”博主，就算你列出来每个参数的意义，然并卵啊！我还是不知道无从下手啊！”</p>
<p>　　参数分类的目的在于缩小调参的范围，首先我们要明确训练的目标，把目标类的参数定下来。接下来，我们需要根据数据集的大小，考虑是否采用一些提高训练效率的策略，否则一次训练就三天三夜，法国人孩子都生出来了。然后，我们终于进入到了重中之重的环节：调整那些影响整体模型性能的参数。</p>
<h2 id="2-1-调参的目标：偏差和方差的协调"><a href="#2-1-调参的目标：偏差和方差的协调" class="headerlink" title="2.1 调参的目标：偏差和方差的协调"></a>2.1 调参的目标：偏差和方差的协调</h2><p>　　同样在<a href="http://www.cnblogs.com/jasonfreak/p/5657196.html" target="_blank" rel="noopener">《使用sklearn进行集成学习——理论》</a>中，我们已讨论过偏差和方差是怎样影响着模型的性能——准确度。调参的目标就是为了达到整体模型的偏差和方差的大和谐！进一步，这些参数又可分为两类：过程影响类及子模型影响类。在子模型不变的前提下，某些参数可以通过改变训练的过程，从而影响模型的性能，诸如：“子模型数”（n_estimators）、“学习率”（learning_rate）等。另外，我们还可以通过改变子模型性能来影响整体模型的性能，诸如：“最大树深度”（max_depth）、“分裂条件”（criterion）等。正由于bagging的训练过程旨在降低方差，而boosting的训练过程旨在降低偏差，过程影响类的参数能够引起整体模型性能的大幅度变化。一般来说，在此前提下，我们继续微调子模型影响类的参数，从而进一步提高模型的性能。</p>
<h2 id="2-2-参数对整体模型性能的影响"><a href="#2-2-参数对整体模型性能的影响" class="headerlink" title="2.2 参数对整体模型性能的影响"></a>2.2 参数对整体模型性能的影响</h2><p>　　假设模型是一个多元函数F，其输出值为模型的准确度。我们可以固定其他参数，从而对某个参数对整体模型性能的影响进行分析：是正影响还是负影响，影响的单调性？</p>
<p>　　对Random Forest来说，增加“子模型数”（n_estimators）可以明显降低整体模型的方差，且不会对子模型的偏差和方差有任何影响。模型的准确度会随着“子模型数”的增加而提高。由于减少的是整体模型方差公式的第二项，故准确度的提高有一个上限。在不同的场景下，“分裂条件”（criterion）对模型的准确度的影响也不一样，该参数需要在实际运用时灵活调整。调整“最大叶节点数”（max_leaf_nodes）以及“最大树深度”（max_depth）之一，可以粗粒度地调整树的结构：叶节点越多或者树越深，意味着子模型的偏差越低，方差越高；同时，调整“分裂所需最小样本数”（min_samples_split）、“叶节点最小样本数”（min_samples_leaf）及“叶节点最小权重总值”（min_weight_fraction_leaf），可以更细粒度地调整树的结构：分裂所需样本数越少或者叶节点所需样本越少，也意味着子模型越复杂。一般来说，我们总采用bootstrap对样本进行子采样来降低子模型之间的关联度，从而降低整体模型的方差。适当地减少“分裂时考虑的最大特征数”（max_features），给子模型注入了另外的随机性，同样也达到了降低子模型之间关联度的效果。但是一味地降低该参数也是不行的，因为分裂时可选特征变少，模型的偏差会越来越大。在下图中，我们可以看到这些参数对Random Forest整体模型性能的影响：</p>
<p><img src="http://images2015.cnblogs.com/blog/927391/201607/927391-20160731184710919-487730249.jpg" alt="img"></p>
<p>　　对Gradient Tree Boosting来说，“子模型数”（n_estimators）和“学习率”（learning_rate）需要联合调整才能尽可能地提高模型的准确度：想象一下，A方案是走4步，每步走3米，B方案是走5步，每步走2米，哪个方案可以更接近10米远的终点？同理，子模型越复杂，对应整体模型偏差低，方差高，故“最大叶节点数”（max_leaf_nodes）、“最大树深度”（max_depth）等控制子模型结构的参数是与Random Forest一致的。类似“分裂时考虑的最大特征数”（max_features），降低“子采样率”（subsample），也会造成子模型间的关联度降低，整体模型的方差减小，但是当子采样率低到一定程度时，子模型的偏差增大，将引起整体模型的准确度降低。还记得“初始模型”（init）是什么吗？不同的损失函数有不一样的初始模型定义，通常，初始模型是一个更加弱的模型（以“平均”情况来预测），虽说支持自定义，大多数情况下保持默认即可。在下图中，我们可以看到这些参数对Gradient Tree Boosting整体模型性能的影响：</p>
<p><img src="http://images2015.cnblogs.com/blog/927391/201607/927391-20160731184748841-69767136.jpg" alt="img"></p>
<h2 id="2-3-一个朴实的方案：贪心的坐标下降法"><a href="#2-3-一个朴实的方案：贪心的坐标下降法" class="headerlink" title="2.3 一个朴实的方案：贪心的坐标下降法"></a>2.3 一个朴实的方案：贪心的坐标下降法</h2><p>　　到此为止，我们终于知道需要调整哪些参数，对于单个参数，我们也知道怎么调整才能提升性能。然而，表示模型的函数F并不是一元函数，这些参数需要共同调整才能得到全局最优解。也就是说，把这些参数丢给调参算法（诸如Grid Search）咯？对于小数据集，我们还能这么任性，但是参数组合爆炸，在大数据集上，或许我的子子孙孙能够看到训练结果吧。实际上网格搜索也不一定能得到全局最优解，而另一些研究者从解优化问题的角度尝试解决调参问题。</p>
<p>　　<a href="https://zh.wikipedia.org/wiki/%E5%9D%90%E6%A0%87%E4%B8%8B%E9%99%8D%E6%B3%95" target="_blank" rel="noopener">坐标下降法</a>是一类优化算法，其最大的优势在于不用计算待优化的目标函数的梯度。我们最容易想到一种特别朴实的类似于坐标下降法的方法，与坐标下降法不同的是，其不是循环使用各个参数进行调整，而是贪心地选取了对整体模型性能影响最大的参数。参数对整体模型性能的影响力是动态变化的，故每一轮坐标选取的过程中，这种方法在对每个坐标的下降方向进行一次直线搜索（line search）。首先，找到那些能够提升整体模型性能的参数，其次确保提升是单调或近似单调的。这意味着，我们筛选出来的参数是对整体模型性能有正影响的，且这种影响不是偶然性的，要知道，训练过程的随机性也会导致整体模型性能的细微区别，而这种区别是不具有单调性的。最后，在这些筛选出来的参数中，选取影响最大的参数进行调整即可。</p>
<p>　　无法对整体模型性能进行量化，也就谈不上去比较参数影响整体模型性能的程度。是的，我们还没有一个准确的方法来量化整体模型性能，只能通过交叉验证来近似计算整体模型性能。然而交叉验证也存在随机性，假设我们以验证集上的平均准确度作为整体模型的准确度，我们还得关心在各个验证集上准确度的变异系数，如果变异系数过大，则平均值作为整体模型的准确度也是不合适的。在接下来的案例分析中，我们所谈及的整体模型性能均是指平均准确度，请各位留心。</p>
<h3 id="2-3-1-Random-Forest调参案例：Digit-Recognizer"><a href="#2-3-1-Random-Forest调参案例：Digit-Recognizer" class="headerlink" title="2.3.1 Random Forest调参案例：Digit Recognizer"></a>2.3.1 Random Forest调参案例：Digit Recognizer</h3><p>　　在这里，我们选取Kaggle上101教学赛中的<a href="https://www.kaggle.com/c/digit-recognizer" target="_blank" rel="noopener">Digit Recognizer</a>作为案例来演示对RandomForestClassifier调参的过程。当然，我们也不要傻乎乎地手工去设定不同的参数，然后训练模型。借助sklearn.grid_search库中的GridSearchCV类，不仅可以自动化调参，同时还可以对每一种参数组合进行交叉验证计算平均准确度。</p>
<h4 id="2-3-1-1-调整过程影响类参数"><a href="#2-3-1-1-调整过程影响类参数" class="headerlink" title="2.3.1.1 调整过程影响类参数"></a>2.3.1.1 调整过程影响类参数</h4><p>　　首先，我们需要对过程影响类参数进行调整，而Random Forest的过程影响类参数只有“子模型数”（n_estimators）。“子模型数”的默认值为10，在此基础上，我们以10为单位，考察取值范围在1至201的调参情况：</p>
<p><img src="http://images2015.cnblogs.com/blog/927391/201607/927391-20160730162932106-838038825.png" alt="img"></p>
<p><em># 左图为模型在验证集上的平均准确度，右图为准确度的变异系数。横轴为参数的取值。</em></p>
<p>　　通过上图我们可以看到，随着“子模型数”的增加，整体模型的方差减少，其防止过拟合的能力增强，故整体模型的准确度提高。当“子模型数”增加到40以上时，准确度的提升逐渐不明显。考虑到训练的效率，最终我们选择“子模型数”为200。此时，在Kaggle上提交结果，得分为：0.96500，很凑合。</p>
<h4 id="2-3-1-2-调整子模型影响类参数"><a href="#2-3-1-2-调整子模型影响类参数" class="headerlink" title="2.3.1.2 调整子模型影响类参数"></a>2.3.1.2 调整子模型影响类参数</h4><p>　　在设定“子模型数”（n_estimators）为200的前提下，我们依次对子模型影响类的参数对整体模型性能的影响力进行分析。</p>
<p>　　对“分裂条件”（criterion）分别取值gini和entropy，得到调参结果如下：</p>
<p><img src="http://images2015.cnblogs.com/blog/927391/201607/927391-20160730165700763-1666232416.png" alt="img"></p>
<p>　　显见，在此问题中，“分裂条件”保持默认值gini更加合适。</p>
<p>　　对“分裂时参与判断的最大特征数”（max_feature）以1为单位，设定取值范围为28至47，得到调参结果如下：</p>
<p><img src="http://images2015.cnblogs.com/blog/927391/201607/927391-20160730170223903-942174073.png" alt="img"></p>
<p>　　</p>
<p>　　“分裂时参与判断的最大特征数”的默认值auto，即总特征数（sqrt(784)=28）的开方。通过提升该参数，整体模型的准确度得到了提升。可见，该参数的默认值过小，导致了子模型的偏差过大，从而整体模型的偏差过大。同时，我们还注意到，该参数对整体模型性能的影响是近似单调的：从28到38，模型的准确度逐步抖动提升。所以，我们可考虑将该参数纳入下一步的调参工作。</p>
<p>　　对“最大深度”（max_depth）以10为单位，设定取值范围为10到100，得到调参结果如下：</p>
<p><img src="http://images2015.cnblogs.com/blog/927391/201607/927391-20160730171248153-89195782.png" alt="img"></p>
<p>　　随着树的深度加深，子模型的偏差减少，整体模型的准确度得到提升。从理论上来说，子模型训练的后期，随着方差增大，子模型的准确度稍微降低，从而影响整体模型的准确度降低。看图中，似乎取值范围从40到60的情况可以印证这一观点。不妨以1为单位，设定取值范围为40到59，更加细致地分析：</p>
<p><img src="http://images2015.cnblogs.com/blog/927391/201607/927391-20160730171849434-652571651.png" alt="img"></p>
<p>　　有点傻眼了，怎么跟预想的不太一样？为什么模型准确度的变化在40到59之间没有鲜明的“规律”了？要分析这个问题，我们得先思考一下，少一层子节点对子模型意味着什么？若少的那一层给原子模型带来的是方差增大，则新子模型会准确度提高；若少的那一层给原子模型带来的是偏差减小，则新子模型会准确度降低。所以，细粒度的层次变化既可能使整体模型的准确度提升，也可能使整体模型的准确度降低。从而也说明了，该参数更适合进行粗粒度的调整。在训练的现阶段，“抖动”现象的发生说明，此时对该参数的调整已不太合适了。</p>
<p>　　对“分裂所需的最小样本数”（min_samples_split）以1为单位，设定取值范围为2到11，得到调参的结果：</p>
<p><img src="http://images2015.cnblogs.com/blog/927391/201607/927391-20160730173505919-181940029.png" alt="img"></p>
<p>　　我们看到，随着分裂所需的最小样本数的增加，子模型的结构变得越来越简单，理论上来说，首先应当因方差减小导致整体模型的准确度提升。但是，在训练的现阶段，子模型的偏差增大的幅度比方差减小的幅度更大，所以整体模型的准确度持续下降。该参数的默认值为2，调参后，最优解保持2不变。</p>
<p>　　对“叶节点最小样本数”（min_samples_leaf）以1为单位，设定取值范围为1到10，得到调参结果如下：</p>
<p><img src="http://images2015.cnblogs.com/blog/927391/201607/927391-20160730174140309-373143237.png" alt="img"></p>
<p>　　同“分裂所需的最小样本数”，该参数也在调参后，保持最优解1不变。</p>
<p>　　对“最大叶节点数”（max_leaf_nodes）以100为单位，设定取值范围为2500到3400，得到调参结果如下：</p>
<p><img src="http://images2015.cnblogs.com/blog/927391/201607/927391-20160730174432372-770058569.png" alt="img"></p>
<p>　　类似于“最大深度”，该参数的增大会带来模型准确的提升，可是由于后期“不规律”的抖动，我们暂时不进行处理。</p>
<p>　　通过对以上参数的调参情况，我们可以总结如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>默认值准确度</th>
<th>调整后最佳准确度</th>
<th>提升幅度</th>
</tr>
</thead>
<tbody>
<tr>
<td>分裂条件（criterion）</td>
<td>0.964023809524</td>
<td>0.964023809524</td>
<td>0</td>
</tr>
<tr>
<td>分裂时参与判断的最大特征数（max_feature）</td>
<td>0.963380952381</td>
<td>0.964428571429</td>
<td>0.00104762</td>
</tr>
<tr>
<td>最大深度（max_depth）</td>
<td></td>
<td></td>
<td>抖动</td>
</tr>
<tr>
<td>分裂所需的最小样本数（min_samples_split）</td>
<td>0.963976190476</td>
<td>0.963976190476</td>
<td>0</td>
</tr>
<tr>
<td>叶节点最小样本数（min_samples_leaf）</td>
<td>0.963595238095</td>
<td>0.963595238095</td>
<td>0</td>
</tr>
<tr>
<td>最大叶节点数（max_leaf_nodes）</td>
<td></td>
<td></td>
<td>抖动</td>
</tr>
</tbody>
</table>
</div>
<p>　　接下来，我们固定分裂时参与判断的最大特征（max_features）为38，在Kaggle上提交一次结果：0.96671，比上一次调参好了0.00171，基本与我们预期的提升效果一致。</p>
<p>　　还需要继续下一轮坐标下降式调参吗？一般来说没有太大的必要，在本轮中出现了两个发生抖动现象的参数，而其他参数的调整均没有提升整体模型的性能。还是得老调重弹：数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。在DR竞赛中，与其期待通过对RandomForestClassifier调参来进一步提升整体模型的性能，不如挖掘出更有价值的特征，或者使用自带特征挖掘技能的模型（正如此题，图分类的问题更适合用神经网络来学习）。但是，在这里，我们还是可以自信地说，通过贪心的坐标下降法，比那些用网格搜索法穷举所有参数组合，自以为得到最优解的朋友们更进了一步。</p>
<h3 id="2-3-2-Gradient-Tree-Boosting调参案例：Hackathon3-x"><a href="#2-3-2-Gradient-Tree-Boosting调参案例：Hackathon3-x" class="headerlink" title="2.3.2 Gradient Tree Boosting调参案例：Hackathon3.x"></a>2.3.2 Gradient Tree Boosting调参案例：Hackathon3.x</h3><p>　　在这里，我们选取Analytics Vidhya上的<a href="https://datahack.analyticsvidhya.com/contest/data-hackathon-3x/" target="_blank" rel="noopener">Hackathon3.x</a>作为案例来演示对GradientBoostingClassifier调参的过程。</p>
<h4 id="2-3-2-1-调整过程影响类参数"><a href="#2-3-2-1-调整过程影响类参数" class="headerlink" title="2.3.2.1 调整过程影响类参数"></a>2.3.2.1 调整过程影响类参数</h4><p>　　GradientBoostingClassifier的过程影响类参数有“子模型数”（n_estimators）和“学习率”（learning_rate），我们可以使用GridSearchCV找到关于这两个参数的最优解。慢着！这里留了一个很大的陷阱：“子模型数”和“学习率”带来的性能提升是不均衡的，在前期会比较高，在后期会比较低，如果一开始我们将这两个参数调成最优，这样很容易陷入一个“局部最优解”。在目标函数都不确定的情况下（如是否凸？），谈局部最优解就是耍流氓，本文中“局部最优解”指的是调整各参数都无明显性能提升的一种状态，所以打了引号。下图中展示了这个两个参数的调参结果：</p>
<p><img src="http://images2015.cnblogs.com/blog/927391/201607/927391-20160731161516950-670327363.png" alt="img"></p>
<p><em># 图中颜色越深表示整体模型的性能越高</em></p>
<p>　　在此，我们先直觉地选择“子模型数”为60，“学习率”为0.1，此时的整体模型性能（平均准确度为0.8253）不是最好，但是也不差，良好水准。</p>
<h4 id="2-3-2-2-调整子模型影响类参数"><a href="#2-3-2-2-调整子模型影响类参数" class="headerlink" title="2.3.2.2 调整子模型影响类参数"></a>2.3.2.2 调整子模型影响类参数</h4><p>　　对子模型影响类参数的调整与Random Forest类似。最终我们对参数的调整如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>子模型数n_estimators</th>
<th>学习率learning_rate</th>
<th>叶节点最小样本数min_samples_leaf</th>
<th>最大深度max_depth</th>
<th>子采样率subsample</th>
<th>分裂时参与判断的最大特征数max_feature</th>
</tr>
</thead>
<tbody>
<tr>
<td>60</td>
<td>0.1</td>
<td>12</td>
<td>4</td>
<td>0.77</td>
<td>10</td>
</tr>
</tbody>
</table>
</div>
<p>　　到此，整体模型性能为0.8313，与workbench（0.8253）相比，提升了约0.006。</p>
<h4 id="2-3-2-3-杀一记回马枪"><a href="#2-3-2-3-杀一记回马枪" class="headerlink" title="2.3.2.3 杀一记回马枪"></a>2.3.2.3 杀一记回马枪</h4><p>　　还记得一开始我们对“子模型数”（n_estimators）和“学习率”（learning_rate）手下留情了吗？现在我们可以回过头来，调整这两个参数，调整的方法为成倍地放大“子模型数”，对应成倍地缩小“学习率”（learning_rate）。通过该方法，本例中整体模型性能又提升了约0.002。</p>
<h2 id="2-4-“局部最优解”"><a href="#2-4-“局部最优解”" class="headerlink" title="2.4 “局部最优解”"></a>2.4 “局部最优解”</h2><p>　　目前来说，在调参工作中，广泛使用的仍是一些经验法则。<a href="https://www.analyticsvidhya.com/blog/author/aarshay/" target="_blank" rel="noopener">Aarshay Jain</a>对Gradient Tree Boosting总结了一套<a href="https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/" target="_blank" rel="noopener">调参方法</a>，其核心思想在于：对过程影响类参数进行调整，毕竟它们对整体模型性能的影响最大，然后依据经验，在其他参数中选择对整体模型性能影响最大的参数，进行下一步调参。这种方法的关键是依照对整体模型性能的影响力给参数排序，然后按照该顺序对的参数进行调整。如何衡量参数对整体模型性能的影响力呢？基于经验，Aarshay提出他的见解：“最大叶节点数”（max_leaf_nodes）和“最大树深度”（max_depth）对整体模型性能的影响大于“分裂所需最小样本数”（min_samples_split）、“叶节点最小样本数”（min_samples_leaf）及“叶节点最小权重总值”（min_weight_fraction_leaf），而“分裂时考虑的最大特征数”（max_features）的影响力最小。</p>
<p>　　Aarshay提出的方法和贪心的坐标下降法最大的区别在于前者在调参之前就依照对整体模型性能的影响力给参数排序，而后者是一种“很自然”的贪心过程。还记得2.3.2.1小节中我们讨论过“子模型数”（n_estimators）和“学习率”（learning_rate）的调参问题吗？同理，贪心的坐标下降法容易陷入“局部最优解”。对Random Forest调参时会稍微好一点，因为当“子模型数”调到最佳状态时，有时就只剩下诸如““分裂时参与判断的最大特征数”等Aarshay认为影响力最小的参数可调了。但是，对Gradient Tree Boosting调参时，遇到“局部最优解”的可能性就大得多。</p>
<p>　　Aarshay同样对Hackathon3.x进行了调参试验，由于特征提取方式的差异，参数赋值相同的情况下，本文的整体模型性能仍与其相差0.007左右（唉，不得不再说一次，特征工程真的很重要）。首先，在过程影响类参数的选择上，Aarshay的方法与贪心的坐标下降法均选择了“子模型数”为60，“学习率”为0.1。接下来，Aarshay按照其定义的参数对整体模型性能的影响力，按序依次对参数进行调整。当子模型影响类参数确定完成后，Aarshay的方法提升了约0.008的整体模型性能，略胜于贪心的坐标下降法的0.006。但是，回过头来继续调试“子模型数”和“学习率”之后，Aarshay的方法又提升了约0.01的整体模型性能，远胜于贪心的坐标下降法的0.002。</p>
<p>　　诶！诶！诶！少侠请住手！你说我为什么要在这篇博文中介绍这种“无用”的贪心的坐标下降法？首先，这种方法很容易凭直觉就想到。人们往往花了很多的时间去搞懂模型的参数是什么含义，对整体模型性能有什么影响，搞懂这些已经不易了，所以接下来很多人选择了最直观的贪心的坐标下降法。通过一个实例，我们更容易记住这种方法的局限性。除了作为反面教材，贪心的坐标下降法就没有意义了吗？不难看到，Aarshay的方法仍有改进的地方，在依次对参数进行调整时，还是需要像贪心的坐标下降法中一样对参数的“动态”影响力进行分析一下，如果这种影响力是“抖动”的，可有可无的，那么我们就不需要对该参数进行调整。</p>
<h2 id="2-5-类别不均衡的陷阱"><a href="#2-5-类别不均衡的陷阱" class="headerlink" title="2.5 类别不均衡的陷阱"></a>2.5 类别不均衡的陷阱</h2><p>　　哈哈哈，这篇博文再次留了个陷阱，此段文字并不是跟全文一起发布！有人要说了，按照我的描述，Aarshay的调参试验不可再现啊！其实，我故意没说Aarshay的另一个关键处理：调参前的参数初始值。因为Hackathon3.x是一个类别不均衡的问题，所以如果直接先调试“最大深度”（max_depth），会发现其会保持默认值3作为最优解，而后面的调参中，“分裂所需最小样本数”（min_samples_split）、“叶节点最小样本数”（min_samples_leaf）再怎么调都没有很大作用。这是因为，正例样本远远小于反例，所以在低深度时，子模型就可能已经对正例过拟合了。所以，在类别不均衡时，只有先确定“叶节点最小样本数”（min_samples_leaf），再确定“分裂所需最小样本数”（min_samples_split），才能确定“最大深度”。而Aarshay设定的初始值，则以经验和直觉避开了这个险恶的陷阱。</p>
<p>　　如果实在觉得经验和直觉不靠谱，我还尝试了一种策略：首先，我们需要初步地调一次“子采样率”（subsample）和“分裂时考虑的最大特征数”（max_features），在此基础上依次调好“叶节点最小样本数”（min_samples_leaf）、“分裂所需最小样本数”（min_samples_split）以及“最大深度”（max_depth）。然后，按照Aarshay的方法，按影响力从大到小再调一次。通过这种方法，整体模型性能在未等比缩放过程影响类参数前，已达到约0.8352左右，比workbench相比，提升了约0.1，与Aarshay的调参试验差不多，甚至更好一点点。</p>
<p>　　回过头来，我们再次看看贪心的坐标下降法是怎么掉入这个陷阱的。在确定过程影响类参数后，贪心的坐标下降法按照“动态”的对整体模型性能的影响力大小，选择了“叶节点最小样本数”进行调参。这一步看似和上一段的描述是一致的，但是，一般来说，含随机性（“子采样率”和“分裂时考虑的最大特征数”先初步调过）的“叶节点最小样本数”要大于无随机性。举个例来说，因为增加了随机性，导致了子采样后，某子样本中只有一个正例，且其可以通过唯一的特征将其分类，但是这个特征并不是所有正例的共性，所以此时就要求“叶节点最小样本数”需要比无随机性时大。对贪心的坐标下降来说，“子采样率”和“分裂时考虑的最大特征数”在当下，对整体模型性能的影响比不上“叶节点最小样本数”，所以栽了个大跟头。</p>
<hr>
<h2 id="3-总结"><a href="#3-总结" class="headerlink" title="3 总结"></a>3 总结</h2><p>　　在这篇博文中，我一反常态，花了大部分时间去试验和说明一个有瑕疵的方案。数据挖掘的工作中的方法和技巧，有很大一部分暂时还未被严谨地证明，所以有很大部分人，特别是刚入门的小青年们（也包括曾经的我），误以为其是一门玄学。实际上，尽管没有被严谨地证明，我们还是可以通过试验、分析，特别是与现有方法进行对比，得到一个近似的合理性论证。</p>
<p>　　另外，小伙伴们你们有什么独到的调参方法吗？请不要有丝毫吝啬，狠狠地将你们的独门绝技全释放在我身上吧，请大胆留言，残酷批评！</p>
<hr>
<h2 id="4-参考资料"><a href="#4-参考资料" class="headerlink" title="4 参考资料"></a>4 参考资料</h2><ol>
<li><a href="http://www.cnblogs.com/jasonfreak/p/5657196.html" target="_blank" rel="noopener">《使用sklearn进行集成学习——理论》</a></li>
<li><a href="https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/" target="_blank" rel="noopener">Complete Guide to Parameter Tuning in Gradient Boosting (GBM) in Python</a></li>
<li><a href="https://zh.wikipedia.org/wiki/%E5%9D%90%E6%A0%87%E4%B8%8B%E9%99%8D%E6%B3%95" target="_blank" rel="noopener">坐标下降法</a></li>
<li><a href="https://www.kaggle.com/c/digit-recognizer" target="_blank" rel="noopener">Digit Recognizer</a></li>
<li><a href="https://datahack.analyticsvidhya.com/contest/data-hackathon-3x/" target="_blank" rel="noopener">Hackathon3.x</a></li>
</ol>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/27/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/27/">27</a><span class="page-number current">28</span><a class="page-number" href="/page/29/">29</a><span class="space">&hellip;</span><a class="page-number" href="/page/31/">31</a><a class="extend next" rel="next" href="/page/29/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Schwimmer</p>
              <div class="site-description motion-element" itemprop="description">Record and Think!</div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">308</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">25</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">61</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          

          

          
          

          
            
          
          

        </div>
      </div>

      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Schwimmer</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.9.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> v7.2.0</div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span>
    <span class="site-uv" title="总访客量">
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="post-meta-divider">|</span>
  

  
    <span class="post-meta-item-icon">
      <i class="fa fa-eye"></i>
    </span>
    <span class="site-pv" title="总访问量">
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    
      <div>
        
  <script>
    window._bd_share_config = {
      "common": {
        "bdText": "",
        "bdMini": "1",
        "bdMiniList": false,
        "bdPic": ""
      },
      "image": {
        "viewList": ["tsina", "douban", "sqq", "qzone", "weixin", "twi", "fbook"],
        "viewText": "分享到：",
        "viewSize": "16"
      },
      "slide": {
        "bdImg": "5",
        "bdPos": "left",
        "bdTop": "100"
      }
    }
  </script>

<script>
  with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='//bdimg.share.baidu.com/static/api/js/share.js?cdnversion='+~(-new Date()/36e5)];
</script>

      </div>
    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/utils.js?v=7.2.0"></script>

  <script src="/js/motion.js?v=7.2.0"></script>



  
  


  <script src="/js/schemes/muse.js?v=7.2.0"></script>




  

  


  <script src="/js/next-boot.js?v=7.2.0"></script>


  

  

  

  
  
<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://https-schwimmer-github-io.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>







  
  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
  

  
  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      document.getElementById(all[i].inputID + '-Frame').parentNode.className += ' has-jax';
    }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

    
  


  

  

  

  

  

  

  

  

  

  

  

</body>
</html>
