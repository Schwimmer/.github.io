<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">






















<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">

<link rel="stylesheet" href="/css/main.css?v=7.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.2.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.2.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '7.2.0',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="Record and Think!">
<meta property="og:type" content="website">
<meta property="og:title" content="Schwimmer&#39;s Blog">
<meta property="og:url" content="https://schwimmer.github.io/page/26/index.html">
<meta property="og:site_name" content="Schwimmer&#39;s Blog">
<meta property="og:description" content="Record and Think!">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Schwimmer&#39;s Blog">
<meta name="twitter:description" content="Record and Think!">





  
  
  <link rel="canonical" href="https://schwimmer.github.io/page/26/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Schwimmer's Blog</title>
  




  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-143240576-1"></script>
  <script>
    var host = window.location.hostname;
    if (host !== "localhost" || !true) {
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-143240576-1');
    }
  </script>









  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Schwimmer's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
        </li>
      
    </ul>
  

  
    

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2018/03/17/机器学习/NLP/Word2vec-C代码/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/03/17/机器学习/NLP/Word2vec-C代码/" class="post-title-link" itemprop="url">Word2Vec源码解读</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-03-17 20:50:31" itemprop="dateCreated datePublished" datetime="2018-03-17T20:50:31+08:00">2018-03-17</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-03-25 17:23:53" itemprop="dateModified" datetime="2018-03-25T17:23:53+08:00">2018-03-25</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2018/03/17/机器学习/NLP/Word2vec-C代码/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/03/17/机器学习/NLP/Word2vec-C代码/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="http://blog.csdn.net/google19890102/article/details/51887344" target="_blank" rel="noopener">机器学习算法实现解析——word2vec源码解析</a></p>
<p>源码在<code>/home/david/code/nlp/word2vec/word2vecC/word2vec.c</code></p>
<p>代码的主要工作包括：</p>
<ul>
<li>预处理。变量声明，全局变量遍历；</li>
<li>构建词库。包括文本处理，以及是否需要有指定词库。</li>
<li>初始化网络结构。参数初始化，Huffman编码的生成。</li>
<li>多线程模型训练。</li>
<li>最终结果的处理。</li>
</ul>
<p>以上的过程，可以用下图表示：</p>
<p><img src="/.io//算法流程.png" alt="算法流程"></p>
<h1 id="输入参数"><a href="#输入参数" class="headerlink" title="输入参数"></a>输入参数</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">-train text8 表示的是输入文件是text8</span><br><span class="line"></span><br><span class="line">-output vectors.bin 输出文件是vectors.bin</span><br><span class="line"></span><br><span class="line">-cbow 1 表示使用cbow模型，默认为Skip-Gram模型</span><br><span class="line"></span><br><span class="line">-size 200 每个单词的向量维度是200</span><br><span class="line"></span><br><span class="line">-window 8 训练的窗口大小为5就是考虑一个词前八个和后八个词语（实际代码中还有一个随机选窗口的过程，窗口大小小于等于8）</span><br><span class="line"></span><br><span class="line">-negative 0 使用ns的时候采样的样本数，默认0，通常是5-10</span><br><span class="line"></span><br><span class="line">-save-vocab 词汇表存储文件</span><br><span class="line"></span><br><span class="line">-read-vocab 词汇表加载文件</span><br><span class="line"></span><br><span class="line">-classes 输出单词类别数，默认为0，即不输出单词</span><br><span class="line"></span><br><span class="line">-hs 1不使用NEG方法，使用HS方法。-</span><br><span class="line"></span><br><span class="line">-sample 亚采样拒绝概率的参数</span><br><span class="line">指的是采样的阈值，如果一个词语在训练样本中出现的频率越大，那么就越会被采样。</span><br><span class="line"></span><br><span class="line">-binary为1指的是结果二进制存储，为0是普通存储（普通存储的时候是可以打开看到词语和对应的向量的）</span><br><span class="line"></span><br><span class="line">-iter 15 迭代次数</span><br></pre></td></tr></table></figure>
<h1 id="全局变量"><a href="#全局变量" class="headerlink" title="全局变量"></a>全局变量</h1><p><code>int *vocab_hash</code> </p>
<p>词在词库中的index，在构建词库时先初始化为-1</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// 词的结构体</span><br><span class="line">struct vocab_word &#123;</span><br><span class="line">        long long cn; // 出现的次数</span><br><span class="line">        int *point; // 从根结点到叶子节点的路径</span><br><span class="line">        char *word, *code, codelen;// 分别对应着词，Huffman编码，编码长度</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>vocab_word是词的结构体</p>
<p><code>vocab = (struct vocab_word *)calloc(vocab_max_size, sizeof(struct vocab_word));</code></p>
<p>vocab存储词</p>
<p><code>vocab_size</code> ：词汇表的总量</p>
<p><code>syn0</code> ：上下文词</p>
<p><code>syn1</code> ：$\theta_{j-1}^w$</p>
<p><code>neu1</code> ：映射层的向量，就是输入层的向量之和</p>
<p><code>neu1e</code> ：对应伪代码中的e</p>
<h1 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h1><p><img src="/.io//sigmoid1.png" alt=""></p>
<p>在预处理部分，对word2vec需要使用的参数进行初始化，在word2vec中是利用传入的方式对参数进行初始化的。</p>
<p>在预处理部分，实现了sigmoid函数值的近似计算。</p>
<p>如果每一次都请求计算sigmoid值，对性能将会有一定的影响，当sigmoid的值对精度的要求并不是非常严格时，可以采用近似计算。在word2vec中，将区间[−6,6]（设置的参数<strong>MAX_EXP</strong>为6）等距离划分成<strong>EXP_TABLE_SIZE</strong>等份，并将每个区间中的sigmoid值计算好存入到数组expTable中，需要使用时，直接从数组中查找。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 申请EXP_TABLE_SIZE+1个空间</span></span><br><span class="line">expTable = (real *)<span class="built_in">malloc</span>((EXP_TABLE_SIZE + <span class="number">1</span>) * <span class="keyword">sizeof</span>(real));</span><br><span class="line">  <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; EXP_TABLE_SIZE; i++) &#123;</span><br><span class="line">    expTable[i] = <span class="built_in">exp</span>((i / (real)EXP_TABLE_SIZE * <span class="number">2</span> - <span class="number">1</span>) * MAX_EXP); </span><br><span class="line">    <span class="comment">// 1/(1+e^6) ~ 1/(1+e^-6)即 0.01 ~ 1 的样子  </span></span><br><span class="line">    expTable[i] = expTable[i] / (expTable[i] + <span class="number">1</span>);                   <span class="comment">// Precompute f(x) = x / (x + 1)</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：在上述代码中，作者使用的是小于EXP_TABLE_SIZE，实际的区间是[−6,6)。</p>
</blockquote>
<h1 id="构建词库"><a href="#构建词库" class="headerlink" title="构建词库"></a>构建词库</h1><p>在word2vec源码中，提供了两种构建词库的方法，分别为：</p>
<ul>
<li>指定词库：ReadVocab()方法</li>
<li>从词的文本构建词库：LearnVocabFromTrainFile()方法</li>
</ul>
<h2 id="构建词库的过程"><a href="#构建词库的过程" class="headerlink" title="构建词库的过程"></a>构建词库的过程</h2><p>在这里，我们以从词的文本构建词库为例。构建词库的过程如下所示：<img src="/.io//构建词库.png" alt="构建词库"></p>
<p>在这部分中，最主要的工作是对文本进行处理，包括低频词的处理，hash表的处理等等。首先，会在词库中增加一个“&lt;/s&gt;”的词，同时，在读取文本的过程中，将换行符“\n”也表示成该该词</p>
<h2 id="对词的哈希处理"><a href="#对词的哈希处理" class="headerlink" title="对词的哈希处理"></a>对词的哈希处理</h2><p>在存储词的过程中，同时保留这两个数组：</p>
<ul>
<li>存储词的vocab</li>
<li>存储词的hash的vocab_hash</li>
</ul>
<p>其中，在vocab中，存储的是词对应的结构体：</p>
<p>在vocab_hash中存储的是词在词库中的Index，vocab_hash的下标是词计算出的hash值。</p>
<p>在对词的处理过程中，主要包括：</p>
<ul>
<li>计算词的hash值：</li>
</ul>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 取词的hash值</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">GetWordHash</span><span class="params">(<span class="keyword">char</span> *word)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">unsigned</span> <span class="keyword">long</span> <span class="keyword">long</span> a, hash = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (a = <span class="number">0</span>; a &lt; <span class="built_in">strlen</span>(word); a++) hash = hash * <span class="number">257</span> + word[a];</span><br><span class="line">        hash = hash % vocab_hash_size;</span><br><span class="line">        <span class="keyword">return</span> hash;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>SearchVocab检索词是否存在。如不存在则返回-1，否则，返回该词在词库中的索引：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">while (1) &#123;</span><br><span class="line">    if (vocab_hash[hash] == -1) return -1;// 不存在该词</span><br><span class="line">    //strcmp两个词相等，则返回0，所以要加上!</span><br><span class="line">    if (!strcmp(word, vocab[vocab_hash[hash]].word)) return vocab_hash[hash];// 返回索引值</span><br><span class="line">    hash = (hash + 1) % vocab_hash_size;// 处理冲突</span><br><span class="line">&#125;</span><br><span class="line">return -1;// 不存在该词</span><br></pre></td></tr></table></figure>
<p>在这个过程中，使用到了线性探测的开放定址法处理冲突，开放定址法就是一旦发生冲突，就去寻找下一个空的散列地址。</p>
<ul>
<li>不存在，则插入新词。</li>
</ul>
<h2 id="对低频词的处理"><a href="#对低频词的处理" class="headerlink" title="对低频词的处理"></a>对低频词的处理</h2><p>在循环读取每一个词的过程中，当出现“vocab_size &gt; vocab_hash_size * 0.7”时，需要对低频词进行处理。其中，vocab_size表示的是目前词库中词的个数，vocab_hash_size表示的是初始设定的hash表的大小。</p>
<p><code>ReduceVocab()</code></p>
<p>在处理低频词的过程中，通过参数“min_reduce”来控制，若词出现的次数小于等于该值时，则从词库中删除该词。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">for (a = 0; a &lt; vocab_size; a++) if (vocab[a].cn &gt; min_reduce) &#123;</span><br><span class="line">		vocab[b].cn = vocab[a].cn;</span><br><span class="line">		vocab[b].word = vocab[a].word;</span><br><span class="line">		b++;</span><br><span class="line">	&#125; else free(vocab[a].word);</span><br><span class="line">	vocab_size = b;// 删减后词的个数</span><br></pre></td></tr></table></figure>
<p>在删除了低频词后，需要重新对词库中的词进行hash值的计算。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">for (a = 0; a &lt; vocab_hash_size; a++) vocab_hash[a] = -1;</span><br><span class="line">for (a = 0; a &lt; vocab_size; a++) &#123;</span><br><span class="line">  // Hash will be re-computed, as it is not actual</span><br><span class="line">  hash = GetWordHash(vocab[a].word);</span><br><span class="line">  while (vocab_hash[hash] != -1) hash = (hash + 1) % vocab_hash_size;</span><br><span class="line">  vocab_hash[hash] = a;</span><br><span class="line">&#125;</span><br><span class="line">fflush(stdout);</span><br><span class="line">min_reduce++;</span><br></pre></td></tr></table></figure>
<h2 id="根据词频对词库中的词排序"><a href="#根据词频对词库中的词排序" class="headerlink" title="根据词频对词库中的词排序"></a>根据词频对词库中的词排序</h2><p>基于以上的过程，程序已经将词从文件中提取出来，并存入到指定的词库中（vocab数组），接下来，需要根据每一个词的词频对词库中的词按照词频从大到小排序，其基本过程在函数SortVocab中，排序过程为</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">qsort(&amp;vocab[1], vocab_size - 1, sizeof(struct vocab_word), VocabCompare);</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>原</strong> <strong>型</strong>:void qsort(void <em>base, int nelem, int width, int (</em>fcmp)(const void <em>,const void </em>));</p>
<p><strong>功</strong> <strong>能</strong>: 使用快速排序例程进行排序</p>
<p>参 数：</p>
<p>1 待排序数组首地址</p>
<p>2 数组中待排序元素数量</p>
<p>3 各元素的占用空间大小</p>
<p>4 指向函数的指针，用于确定排序的顺序</p>
<p><strong>说</strong> <strong>明：</strong>qsort函数是ANSI C标准中提供的，其声明在stdlib.h文件中，是根据二分法写的，其时间复杂度为n*log(n)。</p>
<p>qsort要求提供的函数是需要自己定义的一个比较函数，比较函数使得qsort通用性更好。有了比较函数qsort可以实现对数组、字符串、结构体等结构进行升序或降序排序。<br>如int cmp(const void <em>a, const void </em>b)中有两个元素作为参数（参数的格式不能变的。）返回一个int值，如果比较函数返回大于0，qsort就认为a &gt; b，返回小于0,qsort就认为a &lt; b。qsort知道元素的大小了，就可以把大的放前面去。如果你的比较函数返回本来应该是1的（即a &gt; b），而却返回-1（小于0的数），那么qsort认为a &lt; b，就把b放在前面去，但实际上是a &gt; b的，所以就造成了降序排序的差别了。简单来说，比较函数的作用就是给qsort指明元素的大小事怎么比较的。</p>
</blockquote>
<p>保持字符“&lt; \s&gt;”在最开始的位置。排序后，根据“min_count”对低频词进行处理，与上述一样，再对剩下的词重新计算hash值。</p>
<p>至此，整个对词的处理过程就已经结束了。接下来，将是对网络结构的处理和词向量的训练。</p>
<h1 id="初始化网络结构"><a href="#初始化网络结构" class="headerlink" title="初始化网络结构"></a>初始化网络结构</h1><p>有了以上的对词的处理，就已经处理好了所有的训练样本，此时，便可以开始网络结构的初始化和接下来的网络训练。网络的初始化的过程在<code>InitNet()</code>函数中完成。</p>
<h2 id="初始化网络参数"><a href="#初始化网络参数" class="headerlink" title="初始化网络参数"></a>初始化网络参数</h2><p>在初始化的过程中，主要的参数包括<strong>词向量的初始化</strong>和<strong>映射层到输出层的权重的初始化</strong>，如下图所示：</p>
<p><img src="http://img.blog.csdn.net/20170227183342463?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ29vZ2xlMTk4OTAxMDI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></p>
<p>词向量的初始化：为每个词分配空间，大小是vocab_size*layer1_size。</p>
<h6 id="初始化的时候要分配所有词-词向量长度的空间？为何要这么大？"><a href="#初始化的时候要分配所有词-词向量长度的空间？为何要这么大？" class="headerlink" title="初始化的时候要分配所有词*词向量长度的空间？为何要这么大？"></a>初始化的时候要分配所有词*词向量长度的空间？为何要这么大？</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// layer1_size是词向量的长度</span><br><span class="line">a = posix_memalign((void **)&amp;syn0, 128, (long long)vocab_size * layer1_size * sizeof(real));</span><br></pre></td></tr></table></figure>
<blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; int posix_memalign (void **memptr,</span><br><span class="line">&gt;                     size_t alignment,</span><br><span class="line">&gt;                     size_t size);</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p>&gt;</p>
<blockquote>
<p>调用<em>posix_memalign( )</em>成功时会返回<em>size</em>字节的动态内存，并且这块内存的地址是<em>alignment</em>的倍数。参数<em>alignment</em>必须是2的幂，还是<em>void</em>指针的大小的倍数。返回的内存块的地址放在了<em>memptr</em>里面，函数返回值是<em>0</em>。</p>
</blockquote>
<p>CBOW网络有两种可选的算法：层次Softmax和Negative Sampling。在输入参数时选择任意一种。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 层次softmax的结构</span></span><br><span class="line">	<span class="keyword">if</span> (hs) &#123;</span><br><span class="line">		<span class="comment">// 映射层到输出层之间的权重，就是Huffman树的非叶子结点的向量θ</span></span><br><span class="line">		a = posix_memalign((<span class="keyword">void</span> **)&amp;syn1, <span class="number">128</span>, (<span class="keyword">long</span> <span class="keyword">long</span>)vocab_size * layer1_size * <span class="keyword">sizeof</span>(real));</span><br><span class="line">		<span class="keyword">if</span> (syn1 == <span class="literal">NULL</span>) &#123;<span class="built_in">printf</span>(<span class="string">"Memory allocation failed\n"</span>); <span class="built_in">exit</span>(<span class="number">1</span>);&#125;</span><br><span class="line">		<span class="keyword">for</span> (a = <span class="number">0</span>; a &lt; vocab_size; a++) <span class="keyword">for</span> (b = <span class="number">0</span>; b &lt; layer1_size; b++)</span><br><span class="line">			syn1[a * layer1_size + b] = <span class="number">0</span>;<span class="comment">// 权重初始化为0</span></span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">// 负采样的结构</span></span><br><span class="line">	<span class="keyword">if</span> (negative&gt;<span class="number">0</span>) &#123;</span><br><span class="line">		a = posix_memalign((<span class="keyword">void</span> **)&amp;syn1neg, <span class="number">128</span>, (<span class="keyword">long</span> <span class="keyword">long</span>)vocab_size * layer1_size * <span class="keyword">sizeof</span>(real));</span><br><span class="line">		<span class="keyword">if</span> (syn1neg == <span class="literal">NULL</span>) &#123;<span class="built_in">printf</span>(<span class="string">"Memory allocation failed\n"</span>); <span class="built_in">exit</span>(<span class="number">1</span>);&#125;</span><br><span class="line">		<span class="keyword">for</span> (a = <span class="number">0</span>; a &lt; vocab_size; a++) <span class="keyword">for</span> (b = <span class="number">0</span>; b &lt; layer1_size; b++)</span><br><span class="line">			syn1neg[a * layer1_size + b] = <span class="number">0</span>;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>
<p>在初始化的过程中，<strong>映射层到输出层的权重都初始化为0</strong>，而对于每一个词向量的初始化，作者的初始化方法如下代码所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// 随机初始化</span><br><span class="line">	for (a = 0; a &lt; vocab_size; a++) for (b = 0; b &lt; layer1_size; b++) &#123;</span><br><span class="line">		next_random = next_random * (unsigned long long)25214903917 + 11;</span><br><span class="line">		// 1、与：相当于将数控制在一定范围内</span><br><span class="line">		// 2、0xFFFF：65536</span><br><span class="line">		// 3、/65536：[0,1]之间</span><br><span class="line">		syn0[a * layer1_size + b] = (((next_random &amp; 0xFFFF) / (real)65536) - 0.5) / layer1_size;// 初始化词向量</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>
<p>首先，生成一个很大的next_random的数，通过与“0xFFFF”进行与运算截断，再除以65536得到[0,1]之间的数，最终，得到的初始化的向量的范围为：[−0.5/m,0.5/m]，其中，m为词向量的长度。</p>
<h6 id=""><a href="#" class="headerlink" title=" "></a> </h6><h2 id="3-2、Huffman树的构建"><a href="#3-2、Huffman树的构建" class="headerlink" title="3.2、Huffman树的构建"></a>3.2、Huffman树的构建</h2><p>在层次Softmax中需要使用到Huffman树以及Huffman编码，因此，在网络结构的初始化过程中，也需要初始化Huffman树。在生成Huffman树的过程中，首先定义了3个长度为vocab_size*2+1的数组：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// 申请2倍的词的空间</span><br><span class="line">long long *count = (long long *)calloc(vocab_size * 2 + 1, sizeof(long long));</span><br><span class="line">long long *binary = (long long *)calloc(vocab_size * 2 + 1, sizeof(long long));</span><br><span class="line">long long *parent_node = (long long *)calloc(vocab_size * 2 + 1, sizeof(long long));</span><br></pre></td></tr></table></figure>
<p>其中，count数组中前vocab_size存储的是每一个词的对应的词频，词频是从高到低排序。后面的vocab_size先初始化为很大的数。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">// 分成两半进行初始化</span><br><span class="line">for (a = 0; a &lt; vocab_size; a++) count[a] = vocab[a].cn;// 前半部分初始化为每个词出现的次数</span><br><span class="line">for (a = vocab_size; a &lt; vocab_size * 2; a++) count[a] = 1e15;// 后半部分初始化为一个固定的常数</span><br></pre></td></tr></table></figure>
<p>构建Huffman树的过程如下所示</p>
<p><img src="http://img.blog.csdn.net/20170224145522192?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ29vZ2xlMTk4OTAxMDI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></p>
<p>首先，设置两个指针pos1和pos2，分别指向最后一个词和最后一个词的后一位</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// 两个指针：</span><br><span class="line">// pos1指向前半截的尾部</span><br><span class="line">// pos2指向后半截的开始</span><br><span class="line">pos1 = vocab_size - 1;</span><br><span class="line">pos2 = vocab_size;</span><br></pre></td></tr></table></figure>
<p>从两个指针所指的数中选择出最小的值，记为min1i，</p>
<p>如pos1所指的值最小，此时，将pos1左移，再比较pos1和pos2所指的数，选择出最小的值，记为min2i，将他们的和存储到pos2所指的位置。并将此时pos2所指的位置设置为min1i和min2i的父节点，同时，记min2i所指的位置的编码为1（这里令右子树的编码为1）。</p>
<p>如pos2所指的值小，此时，将pos2右移，再比较pos1和pos2，选出最小的值，记为min2i，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">// Following algorithm constructs the Huffman tree by adding one node at a time</span><br><span class="line">	// 每次增加一个节点，构建Huffman树</span><br><span class="line">	for (a = 0; a &lt; vocab_size - 1; a++) &#123;</span><br><span class="line">		// First, find two smallest nodes &apos;min1, min2&apos;</span><br><span class="line">		// 选择最小的节点min1</span><br><span class="line">		if (pos1 &gt;= 0) &#123;</span><br><span class="line">			if (count[pos1] &lt; count[pos2]) &#123;</span><br><span class="line">				min1i = pos1;</span><br><span class="line">				pos1--;</span><br><span class="line">			&#125; else &#123;</span><br><span class="line">				min1i = pos2;</span><br><span class="line">				pos2++;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125; else &#123;</span><br><span class="line">			min1i = pos2;</span><br><span class="line">			pos2++;</span><br><span class="line">		&#125;</span><br><span class="line">		// 选择最小的节点min2</span><br><span class="line">		if (pos1 &gt;= 0) &#123;</span><br><span class="line">			if (count[pos1] &lt; count[pos2]) &#123;</span><br><span class="line">				min2i = pos1;</span><br><span class="line">				pos1--;</span><br><span class="line">			&#125; else &#123;</span><br><span class="line">				min2i = pos2;</span><br><span class="line">				pos2++;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125; else &#123;</span><br><span class="line">			min2i = pos2;</span><br><span class="line">			pos2++;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		count[vocab_size + a] = count[min1i] + count[min2i];</span><br><span class="line">		// 设置父节点</span><br><span class="line">		parent_node[min1i] = vocab_size + a;</span><br><span class="line">		parent_node[min2i] = vocab_size + a;</span><br><span class="line">		binary[min2i] = 1;// 设置一个子树的编码为1</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>
<p>构建好Huffman树后，此时，需要根据构建好的Huffman树生成对应节点的Huffman编码。假设，上述的数据生成的最终的Huffman树为：</p>
<p><img src="http://img.blog.csdn.net/20170228154343740?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ29vZ2xlMTk4OTAxMDI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></p>
<p>此时，count数组，binary数组和parent_node数组分别为：</p>
<p><img src="http://img.blog.csdn.net/20170228160752188?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ29vZ2xlMTk4OTAxMDI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></p>
<p>在生成Huffman编码的过程中，针对每一个词（词都在叶子节点上），从叶子节点开始，将编码存入到code数组中，如对于上图中的“R”节点来说，其code数组为{1,0}，再对其反转便是Huffman编码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">// Now assign binary code to each vocabulary word</span><br><span class="line">	// 为每一个词分配二进制编码，即Huffman编码</span><br><span class="line">	for (a = 0; a &lt; vocab_size; a++) &#123;// 针对每一个词</span><br><span class="line">		b = a;</span><br><span class="line">		i = 0;</span><br><span class="line">		while (1) &#123;</span><br><span class="line">			code[i] = binary[b];// 找到当前的节点的编码</span><br><span class="line">			point[i] = b;// 记录从叶子节点到根结点的序列</span><br><span class="line">			i++;</span><br><span class="line">			b = parent_node[b];// 找到当前节点的父节点</span><br><span class="line">			if (b == vocab_size * 2 - 2) break;// 已经找到了根结点，根节点是没有编码的</span><br><span class="line">		&#125;</span><br><span class="line">		vocab[a].codelen = i;// 词的编码长度</span><br><span class="line">		vocab[a].point[0] = vocab_size - 2;// 根结点</span><br><span class="line">		for (b = 0; b &lt; i; b++) &#123;</span><br><span class="line">			vocab[a].code[i - b - 1] = code[b];// 编码的反转</span><br><span class="line">			vocab[a].point[i - b] = point[b] - vocab_size;// 记录的是从根结点到叶子节点的路径</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>
<h2 id="3-3、负样本选中表的初始化"><a href="#3-3、负样本选中表的初始化" class="headerlink" title="3.3、负样本选中表的初始化"></a>3.3、负样本选中表的初始化</h2><p>（自己没看）</p>
<p>如果是采用负采样的方法，此时还需要初始化每个词被选中的概率。在所有的词构成的词典中，每一个词出现的频率有高有低，我们希望，对于那些高频的词，被选中成为负样本的概率要大点，同时，对于那些出现频率比较低的词，我们希望其被选中成为负样本的频率低点。这个原理于“轮盘赌”的策略一致（详细可以参见“<a href="http://blog.csdn.net/google19890102/article/details/45622307" target="_blank" rel="noopener">优化算法——遗传算法</a>”）。在程序中，实现这部分功能的代码为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">// 生成负采样的概率表</span><br><span class="line">void InitUnigramTable() &#123;</span><br><span class="line">        int a, i;</span><br><span class="line">        double train_words_pow = 0;</span><br><span class="line">        double d1, power = 0.75;</span><br><span class="line">        table = (int *)malloc(table_size * sizeof(int));// int --&gt; int</span><br><span class="line">        for (a = 0; a &lt; vocab_size; a++) train_words_pow += pow(vocab[a].cn, power);</span><br><span class="line">        // 类似轮盘赌生成每个词的概率</span><br><span class="line">        i = 0;</span><br><span class="line">        d1 = pow(vocab[i].cn, power) / train_words_pow;</span><br><span class="line">        for (a = 0; a &lt; table_size; a++) &#123;</span><br><span class="line">                table[a] = i;</span><br><span class="line">                if (a / (double)table_size &gt; d1) &#123;</span><br><span class="line">                        i++;</span><br><span class="line">                        d1 += pow(vocab[i].cn, power) / train_words_pow;</span><br><span class="line">                &#125;</span><br><span class="line">                if (i &gt;= vocab_size) i = vocab_size - 1;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在实现的过程中，没有直接使用每一个词的频率，而是使用了词的0.75次方。</p>
<h1 id="4、多线程模型训练"><a href="#4、多线程模型训练" class="headerlink" title="4、多线程模型训练"></a>4、多线程模型训练</h1><p>以上的各个部分是为训练词向量做准备，即准备训练数据，构建训练模型。在上述的初始化完成后，接下来就是根据不同的方法对模型进行训练，在实现的过程中，作者使用了多线程的方法对其进行训练。</p>
<h2 id="4-1、多线程的处理"><a href="#4-1、多线程的处理" class="headerlink" title="4.1、多线程的处理"></a>4.1、多线程的处理</h2><p>为了能够对文本进行加速训练，在实现的过程中，作者使用了多线程的方法，TrainModelThread()</p>
<p>对每一个线程上分配指定大小的文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// 利用多线程对训练文件划分，每个线程训练一部分的数据</span><br><span class="line">fseek(fi, file_size / (long long)num_threads * (long long)id, SEEK_SET);</span><br></pre></td></tr></table></figure>
<p>这个过程可以通过下图简单的描述：</p>
<p><img src="http://img.blog.csdn.net/20170301130559630?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ29vZ2xlMTk4OTAxMDI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></p>
<p>在实现多线程的过程中，作者并没有加锁的操作，而是对模型参数和词向量的修改可以任意执行，这一点类似于基于随机梯度的方法，训练的过程与训练样本的训练是没有关系的，这样可以大大加快对词向量的训练。抛开多线程的部分，在每一个线程内执行的是对模型和词向量的训练。</p>
<p>作者在实现的过程中，主要实现了两个模型，即CBOW模型和Skip-gram模型，在每个模型中，又分别使用到了两种不同的训练方法，即层次Softmax和Negative Sampling方法。</p>
<h2 id="4-2、CBOW模型"><a href="#4-2、CBOW模型" class="headerlink" title="4.2、CBOW模型"></a>4.2、CBOW模型</h2><h3 id="4-2-1、从输入层到映射层"><a href="#4-2-1、从输入层到映射层" class="headerlink" title="4.2.1、从输入层到映射层"></a>4.2.1、从输入层到映射层</h3><p>首先找到每个词对应的词向量，并将这些词的词向量相加，程序代码如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">// in -&gt; hidden</span><br><span class="line">cw = 0;</span><br><span class="line">//b是随机生成的0到window-1，相当于左右各看window-b/2个词</span><br><span class="line">for (a = b; a &lt; window * 2 + 1 - b; a++) if (a != window) &#123;</span><br><span class="line">  //sentence_position 单词在句子中的位置</span><br><span class="line">  c = sentence_position - window + a;</span><br><span class="line">  // 判断c是否越界</span><br><span class="line">  if (c &lt; 0) continue;</span><br><span class="line">  if (c &gt;= sentence_length) continue;</span><br><span class="line">  // 找到c对应的索引</span><br><span class="line">  last_word = sen[c];</span><br><span class="line">  if (last_word == -1) continue;</span><br><span class="line">  // neu1就是隐藏层向量，也就是上下文对应vector的和</span><br><span class="line">  for (c = 0; c &lt; layer1_size; c++) neu1[c] += syn0[c + last_word * layer1_size];</span><br><span class="line">  cw++;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>当累加完窗口内的所有的词向量的之后，存储在映射层neu1中，并取平均，程序代码如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">for (c = 0; c &lt; layer1_size; c++) neu1[c] /= cw;</span><br></pre></td></tr></table></figure>
<p>当取得了映射层的结果后，此时就需要使用Hierarchical Softmax或者Negative Sampling对模型进行训练。</p>
<h3 id="4-2-2、Hierarchical-Softmax"><a href="#4-2-2、Hierarchical-Softmax" class="headerlink" title="4.2.2、Hierarchical Softmax"></a>4.2.2、Hierarchical Softmax</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">if (hs) for (d = 0; d &lt; vocab[word].codelen; d++) &#123;</span><br><span class="line">          f = 0;</span><br><span class="line">          // point存储了从该词的叶子结点的编号到root的序号，这些序号可以对应到syn1的位置，也就是参数向量的位置</span><br><span class="line">          l2 = vocab[word].point[d] * layer1_size;</span><br><span class="line">          // Propagate hidden -&gt; output</span><br><span class="line">          // q=sigma(x*theta)</span><br><span class="line">          for (c = 0; c &lt; layer1_size; c++) f += neu1[c] * syn1[c + l2];</span><br><span class="line">          if (f &lt;= -MAX_EXP) continue;</span><br><span class="line">          else if (f &gt;= MAX_EXP) continue;</span><br><span class="line">          // 查表得知sigma的值，省去计算的时间</span><br><span class="line">          else f = expTable[(int)((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))];</span><br><span class="line">          // g = eta(1-d-q)</span><br><span class="line">          g = (1 - vocab[word].code[d] - f) * alpha;</span><br><span class="line">          // Propagate errors output -&gt; hidden</span><br><span class="line">          // e = e + g*theta</span><br><span class="line">          for (c = 0; c &lt; layer1_size; c++) neu1e[c] += g * syn1[c + l2];</span><br><span class="line">          // Learn weights hidden -&gt; output</span><br><span class="line">          // theta = theta + g*x</span><br><span class="line">          for (c = 0; c &lt; layer1_size; c++) syn1[c + l2] += g * neu1[c];</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<p>接下来更新Context(w)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// hidden -&gt; in</span><br><span class="line">        for (a = b; a &lt; window * 2 + 1 - b; a++) if (a != window) &#123;</span><br><span class="line">          c = sentence_position - window + a;</span><br><span class="line">          if (c &lt; 0) continue;</span><br><span class="line">          if (c &gt;= sentence_length) continue;</span><br><span class="line">          last_word = sen[c];</span><br><span class="line">          if (last_word == -1) continue;</span><br><span class="line">          for (c = 0; c &lt; layer1_size; c++) syn0[c + last_word * layer1_size] += neu1e[c];</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<h1 id="Word2Vec为什么快"><a href="#Word2Vec为什么快" class="headerlink" title="Word2Vec为什么快"></a>Word2Vec为什么快</h1><ul>
<li>用查表代替计算sigmoid</li>
<li>相对于神经网络的结构，去掉了隐藏层</li>
</ul>
<h1 id="增量训练"><a href="#增量训练" class="headerlink" title="增量训练"></a>增量训练</h1><p>从搜索引擎爬包含新词的文本，加上一个小语料，训练一个w2v模型。</p>
<p>对于每个新词，找出小模型中最接近的10个词，以及每个词与新词的相似度打分score。</p>
<p>再从大模型中找出每个词的词向量，每个维度乘以小模型中的score，最多叠加5个。再对每个维度取加权平均。</p>
<p>最后转成单位向量。</p>
<p>参考</p>
<p>【1】<a href="http://blog.csdn.net/google19890102/article/details/51887344" target="_blank" rel="noopener">http://blog.csdn.net/google19890102/article/details/51887344</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2018/03/17/机器学习/NLP/Word2Vec原理/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/03/17/机器学习/NLP/Word2Vec原理/" class="post-title-link" itemprop="url">Word2Vec原理</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-03-17 20:50:31" itemprop="dateCreated datePublished" datetime="2018-03-17T20:50:31+08:00">2018-03-17</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-03-25 12:46:10" itemprop="dateModified" datetime="2018-03-25T12:46:10+08:00">2018-03-25</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2018/03/17/机器学习/NLP/Word2Vec原理/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/03/17/机器学习/NLP/Word2Vec原理/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>参考</p>
<p>《word2vec数学原理.pdf》</p>
<h1 id="词向量基础"><a href="#词向量基础" class="headerlink" title="词向量基础"></a>词向量基础</h1><p>​    用词向量来表示词并不是word2vec的首创，在很久之前就出现了。最早的词向量是很冗长的，它使用是词向量维度大小为整个词汇表的大小，对于每个具体的词汇表中的词，将对应的位置置为1。比如我们有下面的5个词组成的词汇表，词”Queen”的序号为2， 那么它的词向量就是(0,1,0,0,0)。同样的道理，词”Woman”的词向量就是(0,0,0,1,0)。这种词向量的编码方式我们一般叫做1-of-N representation或者one hot representation.</p>
<p><img src="/.io//one hot representation.png" alt="one hot representation"></p>
<p>​    One hot representation用来表示词向量非常简单，但是却有很多问题。最大的问题是我们的词汇表一般都非常大，比如达到百万级别，这样每个词都用百万维的向量来表示简直是内存的灾难。这样的向量其实除了一个位置是1，其余的位置全部都是0，表达的效率不高，能不能把词向量的维度变小呢？</p>
<p>​    Dristributed representation可以解决One hot representation的问题，它的思路是通过训练，将每个词都映射到一个较短的词向量上来。所有的这些词向量就构成了向量空间，进而可以用普通的统计学的方法来研究词与词之间的关系。这个较短的词向量维度是多大呢？这个一般需要我们在训练时自己来指定。</p>
<p>​    比如下图我们将词汇表里的词用”Royalty”,”Masculinity”, “Femininity”和”Age”4个维度来表示，King这个词对应的词向量可能是(0.99,0.99,0.05,0.7)(0.99,0.99,0.05,0.7)。当然在实际情况中，我们并不能对词向量的每个维度做一个很好的解释。</p>
<p><img src="/.io//Dristributed representation.png" alt="Dristributed representation"></p>
<p>​    有了用Dristributed representation表示的较短的词向量，我们就可以较容易的分析词之间的关系了，比如我们将词的维度降维到2维，有一个有趣的研究表明，用下图的词向量表示我们的词时，我们可以发现：</p>
<script type="math/tex; mode=display">
\vec {King} - \vec {Man} + \vec {Woman} = \vec {Queen}</script><p><img src="/.io//vector relation.png" alt="vector relation"></p>
<p>可见我们只要得到了词汇表里所有词对应的词向量，那么我们就可以做很多有趣的事情了。不过，怎么训练得到合适的词向量呢？一个很常见的方法是使用神经网络语言模型。</p>
<h1 id="2-CBOW与Skip-Gram用于神经网络语言模型"><a href="#2-CBOW与Skip-Gram用于神经网络语言模型" class="headerlink" title="2. CBOW与Skip-Gram用于神经网络语言模型"></a>2. CBOW与Skip-Gram用于神经网络语言模型</h1><p>​    在word2vec出现之前，已经有用神经网络DNN来用训练词向量进而处理词与词之间的关系了。采用的方法一般是一个三层的神经网络结构（当然也可以多层），分为输入层，隐藏层和输出层(softmax层)。</p>
<p>​    这个模型是如何定义数据的输入和输出呢？一般分为CBOW（Continuous Bag-of-Words） 与Skip-Gram两种模型。</p>
<h2 id="2-1-CBOW"><a href="#2-1-CBOW" class="headerlink" title="2.1 CBOW"></a>2.1 CBOW</h2><p>上下文相关的词对应的词向量，而输出就是这特定的一个词的词向量。比如下面这段话，我们的上下文大小取值为4，特定的这个词是”Learning”，也就是我们需要的输出词向量,上下文对应的词有8个，前后各4个，这8个词是我们模型的输入。由于CBOW使用的是词袋模型，因此这8个词都是平等的，也就是不考虑他们和我们关注的词之间的距离大小，只要在我们上下文之内即可。</p>
<p><img src="/.io//cbow.png" alt="cbow"></p>
<p>在这个CBOW神经网络模型中，输入层有8个神经元（8个词向量），输出层有词汇表D大小的神经元。</p>
<p>目标函数通常为</p>
<script type="math/tex; mode=display">
L=\sum_{w \in C} \log p(w|Context(w))</script><blockquote>
<p> 为什么是这个形式？</p>
<p> 根据n-gram模型和对数最大似然，得到这个目标函数。</p>
</blockquote>
<p>包括四个层：输入、投影、隐藏、输出。</p>
<p><img src="/.io//神经网络层.png" alt="神经网络层"></p>
<p>对于语料C中的任意一个词w，将Context(w)设为取前面的n-1个词，这样二元对(Context(w),w)就是一个训练样本。</p>
<p>投影层向量$x_w$的构造是，将输入层的n-1个词向量按顺序首尾相接的拼起来，长度就是m(n-1)了（每个词向量的长度是m）。</p>
<p>从而</p>
<script type="math/tex; mode=display">
z_w = tanh(W{x_w}+p) \\
y_w=Uz_w+q</script><p>其中，tanh是双曲正切函数，用来做隐藏层的激活函数。</p>
<p>经过上面两步计算得到的$y<em>w=(y</em>{w,1},y<em>{w,2},…,y</em>{w,N})^T$是一个长度为N的向量，其分量不能代表概率。如果想要$y_{w,i}$表示当上下文为Context(w)时下一次为词典D中第i个词的概率，则还需要做一个<strong>softmax归一化</strong>，之后得到</p>
<script type="math/tex; mode=display">
p(w|Context(w))=\frac {e^{y_{w,i_w}}} {\sum_{i=1}^N e^{y_{w,i_w}}}</script><p>其中$i_w$表示词w在词典D中的索引。</p>
<p>与n-gram相比，神经概率语言模型的<strong>优势</strong>是：</p>
<p>1、词语之间的相似性可以通过词向量来体现。</p>
<p>1）神经网络模型通过上下文来预测，那么相似的上下文的词的词向量也是相似的；</p>
<p>2）概率函数关于词向量是光滑的，即词向量的一个小变化对概率的影响也是一个小变化。</p>
<p>2、词向量自带平滑功能（因为$p(w|Context(w)) \in (0,1)$不会为零）。</p>
<h2 id="2-2-Skip-Gram"><a href="#2-2-Skip-Gram" class="headerlink" title="2.2 Skip-Gram"></a>2.2 Skip-Gram</h2><p>与CBOW相反，输入是一个特定向量，输出是特定词对应的上下文词向量。</p>
<h1 id="3、基于Hierarchical-Softmax的模型"><a href="#3、基于Hierarchical-Softmax的模型" class="headerlink" title="3、基于Hierarchical Softmax的模型"></a>3、基于Hierarchical Softmax的模型</h1><h2 id="3-1-CBOW模型"><a href="#3-1-CBOW模型" class="headerlink" title="3.1 CBOW模型"></a>3.1 CBOW模型</h2><p>网络的构建</p>
<p><img src="/.io//cbow_network.png" alt="cbow_network"></p>
<p>输入层是上下文的若干个词的词向量</p>
<p>投影层就是将这些词向量直接相加。</p>
<p>层次Softmax的基本思想就是：</p>
<p><strong>对于词典D中的任意词w，Huffman树中必存在一条从根结点到词w对应结点的路径$p^w$，且这条路径是唯一的。路径$p^w$上存在$l^w-1$个分支，将每个分支看做是一个二分类，每一次分类就产生一个概率，将这些概率连乘起来，就是所需的$p(w|Context(w))$ 。</strong></p>
<p>条件概率连乘的公示可以写为</p>
<script type="math/tex; mode=display">
p(w|Context(w))=\prod_{j=2}^{l^w}p(d_j^w|x_w,\theta_{j-1}^w)</script><p>其中，</p>
<script type="math/tex; mode=display">
p(d_j^w|x_w,\theta_{j-1}^w)=[\sigma(x_w^T\theta_{j-1}^w)]^{1-d_j^w} \cdot [1-\sigma(x_w^T\theta_{j-1}^w)]^{d_j^w} \tag{3-1}</script><p>这里$\sigma(x<em>w^T\theta</em>{j-1}^w)$表示分到正类的概率。</p>
<p>将3-1代入对数似然函数，得到</p>
<script type="math/tex; mode=display">
L=\sum_{w \in C} \sum_{j=2}^{l^w} \{ (1-d_j^w)\log [\sigma(x_w^T\theta_{j-1}^w)] + d_j^w \log[1-\sigma(x_w^T\theta_{j-1}^w)] \}</script><p>其中，令</p>
<script type="math/tex; mode=display">
L(w,j)=(1-d_j^w)\log [\sigma(x_w^T\theta_{j-1}^w)] + d_j^w \log[1-\sigma(x_w^T\theta_{j-1}^w)]</script><p>至此，已经推导出对数似然函数，这就是CBOW模型的目标函数。</p>
<blockquote>
<p>参数$\theta$是怎样的矩阵？</p>
<p>我理解，任何用树的多个二分类问题，目标函数都可以表示成这种形式。</p>
</blockquote>
<p>为了使该目标函数最大化，word2vec采用的是<strong>随机梯度上升法</strong>。每取一个样本，计算梯度再刷新所有的参数。推导出更新公式为</p>
<script type="math/tex; mode=display">
\theta_{j-1}^w = \theta_{j-1}^w + \eta [1-d_j^w-\sigma(x_w^T\theta_{j-1}^w)]x_w^T</script><p>同样的</p>
<script type="math/tex; mode=display">
\frac {\partial L(w,j)} {\partial x_w}=[1-d_j^w-\sigma(x_w^T\theta_{j-1}^w)]\theta_{j-1}^w</script><p>我们的最终目的是要求词典D中每个词的词向量，而这里的$x_w$表示的是Context(w)所有词向量的累加和，那么如何利用偏导对$v(\tilde w), w \in Context(\tilde w)$进行更新呢，word2vec算法中直接取</p>
<script type="math/tex; mode=display">
v(\tilde w) = v(\tilde w)+\eta \sum_{j=2}^{l^w} \frac {\partial L(w,j)} {\partial x_w}</script><p>即把后面增量的值贡献到Context(w)的每一个词的词向量上。</p>
<p>伪代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">for j=huffman code:</span><br><span class="line">&#123;</span><br><span class="line">  1)q=sigma(x*theta)</span><br><span class="line">  2)g=eta(1-d-q)</span><br><span class="line">  3)e=e+g*theta</span><br><span class="line">  4)theta=theta+g*x</span><br><span class="line">&#125;</span><br><span class="line">for w in Context(w):</span><br><span class="line">&#123;</span><br><span class="line">  v(w) = v(w) + e</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>注意3、 4两步不能颠倒位置，要先计算出e，再更新θ。</p>
<h1 id="4、基于Negative-Sampling的模型"><a href="#4、基于Negative-Sampling的模型" class="headerlink" title="4、基于Negative Sampling的模型"></a>4、基于Negative Sampling的模型</h1><p>NEG不使用Huffman树，而是利用随机负采样，能大幅度提高性能。</p>
<p>前面的原理差不多，也是用梯度下降，关键在于，h-softmax是通过Huffman树的路径长度来进行迭代和更新参数；NEG是通过找出负采样来迭代。</p>
<h2 id="4-1-负采样算法"><a href="#4-1-负采样算法" class="headerlink" title="4.1 负采样算法"></a>4.1 负采样算法</h2><p>词典D中的词在语料C中出现的次数有高有低，对于那些高频词，被选为负采样的概率就应该比较大，反之较小。这是我们对采样过程的一个大致要求，本质上就是一个<strong>带权采样问题</strong>。</p>
<blockquote>
<p>设词典D中的每个词w对应一条线段l(w)，则线段的长度可以表示为</p>
<script type="math/tex; mode=display">
len(w)=\frac {count(w)} {\sum_{u \in D} count(u)}</script><p>也就是计算词频再归一化。现在将这些线段首尾相连拼接在一起，形成一个长度为1的单位线段。如果随机往这个单位线段上打点，则其中越长的线段命中概率越大。</p>
</blockquote>
<p>通过这些线段得到一个非等距剖分（假设分成N个区间），再定义一个等距剖分（假设分成M个区间），$M &gt;&gt; N$ 。</p>
<p><img src="/.io//table映射.png" alt="table映射"></p>
<p>建立如下的映射关系</p>
<script type="math/tex; mode=display">
Table(i) = w_k, \ where\  m_i \in I_k,\ i=1,2,...,M-1</script><p>那么，采样就是每次生成一个[1,M-1]之间的整数r，Table(r)就是一个样本。如果负采样的时候选到自己，就跳过再选。</p>
<h1 id="Word2Vec与神经网络模型的区别"><a href="#Word2Vec与神经网络模型的区别" class="headerlink" title="Word2Vec与神经网络模型的区别"></a>Word2Vec与神经网络模型的区别</h1><ul>
<li>NN的输入是类似n-gram，取前N-1个词，w2v取前后各n-1个词</li>
<li>NN多了隐层，输入到隐层用双曲正切当激活函数。</li>
<li>NN的输入词向量是首尾拼接，W2V是加总。这样当窗口中向量不足时，也不需要补。</li>
<li>NN的输出是一个长度为N的向量，就是整个词汇表的长度，然后再做一个softmax归一化，得到给定上下文时下一个词恰好为词汇表的某个词的概率。</li>
</ul>
<h1 id="疑问"><a href="#疑问" class="headerlink" title="疑问"></a>疑问</h1><p>1）在这里非叶子结点对应的那些向量就可以扮演参数的角色？</p>
<p>就是说可以把$\theta$当做待求的参数，这样就可以用sigmoid来求分类的概率。</p>
<p>参考</p>
<p>【1】<a href="http://www.cnblogs.com/pinard/p/7160330.html" target="_blank" rel="noopener">word2vec原理(一) CBOW与Skip-Gram模型基础</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2018/03/17/Elastic Search/iau的ES使用场景/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/03/17/Elastic Search/iau的ES使用场景/" class="post-title-link" itemprop="url">iau的ES使用场景</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-03-17 20:50:31" itemprop="dateCreated datePublished" datetime="2018-03-17T20:50:31+08:00">2018-03-17</time>
            </span>
          

          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/Elastic-Search/" itemprop="url" rel="index"><span itemprop="name">Elastic Search</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2018/03/17/Elastic Search/iau的ES使用场景/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/03/17/Elastic Search/iau的ES使用场景/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>比较brand和competitor的交叉人群</p>
<p>分别算出brand和competitor的人群，然后再计算包含brand和competitor的人群，就得出交叉人群。</p>
<p>比较brand和competitor的距离</p>
<p>就是比较brand和competitor的每个产品和对方关键词的相关度。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2018/03/17/机器学习/NLP/NLP工具/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/03/17/机器学习/NLP/NLP工具/" class="post-title-link" itemprop="url">NL工具</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-03-17 20:50:31" itemprop="dateCreated datePublished" datetime="2018-03-17T20:50:31+08:00">2018-03-17</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-03-25 16:02:08" itemprop="dateModified" datetime="2018-03-25T16:02:08+08:00">2018-03-25</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2018/03/17/机器学习/NLP/NLP工具/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/03/17/机器学习/NLP/NLP工具/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="NLP工具"><a href="#NLP工具" class="headerlink" title="NLP工具"></a>NLP工具</h1><h1 id="Word2Vec-Java"><a href="#Word2Vec-Java" class="headerlink" title="Word2Vec-Java"></a>Word2Vec-Java</h1><h2 id="加载模型及词向量计算"><a href="#加载模型及词向量计算" class="headerlink" title="加载模型及词向量计算"></a>加载模型及词向量计算</h2><p>hdfs版 /pig-ext-lite/src/main/java/com/buzzinate/pig/util/WordVec.java</p>
<p>单机版 /persona/src/main/java/com/iclick/persona/nlp/word2vec/W2VContral.java</p>
<h2 id="词向量聚类"><a href="#词向量聚类" class="headerlink" title="词向量聚类"></a>词向量聚类</h2><p>/home/david/gitlab/user-gene/domain-classify-model/src/main/java/com/buzzinate/domain/classifier/DomainClassifier.java</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">	 * 通过keams得到中心点向量</span><br><span class="line">	 * </span><br><span class="line">	 * @param wordVec</span><br><span class="line">	 * @return</span><br><span class="line">	 */</span><br><span class="line">	public ArrayList&lt;float[]&gt; getCenterVecByKeams(ArrayList&lt;float[]&gt; wordVec) &#123;</span><br><span class="line">		ArrayList&lt;float[]&gt; centerVec = new ArrayList&lt;float[]&gt;();</span><br><span class="line">		List&lt;Classes&gt; cls = null;</span><br><span class="line">		logger.info(&quot;before keams： &quot; + wordVec.size());</span><br><span class="line">		System.out.println(&quot;before keams： &quot; + wordVec.size());</span><br><span class="line">		cls = KMeansClustering.getClusteringResult(wordVec,</span><br><span class="line">				wordVec.subList(0, Math.min(100, wordVec.size() - 1)));</span><br><span class="line">		for (Classes cl : cls) &#123;</span><br><span class="line">			if (!Float.isNaN(WVUtils.toFloat(cl.getCenter())[0]))</span><br><span class="line">				centerVec.add(WVUtils.toFloat(cl.getCenter()));</span><br><span class="line">		&#125;</span><br><span class="line">		logger.info(&quot;after keams： &quot; + centerVec.size());</span><br><span class="line">		System.out.println(&quot;after keams： &quot; + centerVec.size());</span><br><span class="line">		return centerVec;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="通过向量找最近的词"><a href="#通过向量找最近的词" class="headerlink" title="通过向量找最近的词"></a>通过向量找最近的词</h2><h1 id="Ansj-seg"><a href="#Ansj-seg" class="headerlink" title="Ansj-seg"></a>Ansj-seg</h1><h2 id="添加词到自定义词典"><a href="#添加词到自定义词典" class="headerlink" title="添加词到自定义词典"></a>添加词到自定义词典</h2><p>user-gene中用的是某一版ansj的基础上修改的，使用方法是</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Value value = new Value(newWord, new String[] &#123; &quot;userDefine&quot;, &quot;1000&quot; &#125;);</span><br><span class="line">		Library.insertWord(UserDefineRecognition.FOREST, value);</span><br></pre></td></tr></table></figure>
<p>知乎找到的方法是<a href="https://www.zhihu.com/question/32226656" target="_blank" rel="noopener">https://www.zhihu.com/question/32226656</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">作者：ansj</span><br><span class="line">链接：https://www.zhihu.com/question/32226656/answer/113724991</span><br><span class="line">来源：知乎</span><br><span class="line">著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</span><br><span class="line"></span><br><span class="line">package org.ansj;</span><br><span class="line"></span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.ansj.app.summary.SummaryComputer;</span><br><span class="line">import org.ansj.domain.Result;</span><br><span class="line">import org.ansj.domain.Term;</span><br><span class="line">import org.ansj.library.UserDefineLibrary;</span><br><span class="line">import org.ansj.splitWord.analysis.ToAnalysis;</span><br><span class="line">import org.nlpcn.commons.lang.tire.domain.Forest;</span><br><span class="line">import org.nlpcn.commons.lang.tire.domain.Value;</span><br><span class="line">import org.nlpcn.commons.lang.tire.library.Library;</span><br><span class="line"></span><br><span class="line">public class Test &#123;</span><br><span class="line">	public static void main(String[] args) throws Exception &#123;</span><br><span class="line">		</span><br><span class="line">		// 构造一个用户词典</span><br><span class="line">		Forest forest = Library.makeForest(&quot;library/default.dic&quot;);</span><br><span class="line">		forest = new Forest();</span><br><span class="line"></span><br><span class="line">		// 增加新词,中间按照&apos;\t&apos;隔开</span><br><span class="line">		UserDefineLibrary.insertWord(&quot;ansj中文分词&quot;, &quot;userDefine&quot;, 1000);</span><br><span class="line">		Result terms = ToAnalysis.parse(&quot;我觉得Ansj中文分词是一个不错的系统!我是王婆!&quot;);</span><br><span class="line">		System.out.println(&quot;增加新词例子:&quot; + terms);</span><br><span class="line"></span><br><span class="line">		// 删除词语,只能删除.用户自定义的词典.</span><br><span class="line">		UserDefineLibrary.removeWord(&quot;ansj中文分词&quot;);</span><br><span class="line">		terms = ToAnalysis.parse(&quot;我觉得ansj中文分词是一个不错的系统!我是王婆!&quot;);</span><br><span class="line">		System.out.println(&quot;删除用户自定义词典例子:&quot; + terms);</span><br><span class="line">		</span><br><span class="line">// 歧义词</span><br><span class="line">		Value value = new Value(&quot;济南下车&quot;, &quot;济南&quot;, &quot;n&quot;, &quot;下车&quot;, &quot;v&quot;);</span><br><span class="line">		System.out.println(ToAnalysis.parse(&quot;我经济南下车到广州.中国经济南下势头迅猛!&quot;));</span><br><span class="line">		Library.insertWord(UserDefineLibrary.ambiguityForest, value);</span><br><span class="line">		System.out.println(ToAnalysis.parse(&quot;我经济南下车到广州.中国经济南下势头迅猛!&quot;));</span><br><span class="line"></span><br><span class="line">		// 多用户词典</span><br><span class="line">		String str = &quot;神探夏洛克这部电影作者.是一个dota迷&quot;;</span><br><span class="line">		System.out.println(ToAnalysis.parse(str));</span><br><span class="line">		// 两个词汇 神探夏洛克 douta迷</span><br><span class="line">		Forest dic1 = new Forest();</span><br><span class="line">		Library.insertWord(dic1, new Value(&quot;神探夏洛克&quot;, &quot;define&quot;, &quot;1000&quot;));</span><br><span class="line">		Forest dic2 = new Forest();</span><br><span class="line">		Library.insertWord(dic2, new Value(&quot;dota迷&quot;, &quot;define&quot;, &quot;1000&quot;));</span><br><span class="line">		System.out.println(ToAnalysis.parse(str, dic1, dic2));</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果在spark下面用要修改</p>
<h1 id="Hanlp"><a href="#Hanlp" class="headerlink" title="Hanlp"></a>Hanlp</h1><p>官方文档<a href="http://hanlp.linrunsoft.com/doc/_build/html/index.html" target="_blank" rel="noopener">http://hanlp.linrunsoft.com/doc/_build/html/index.html</a></p>
<p><a href="https://github.com/hankcs/HanLP/releases" target="_blank" rel="noopener">https://github.com/hankcs/HanLP/releases</a></p>
<p><strong>HanLP</strong>中的数据分为<em>词典</em>和<em>模型</em>，其中<em>词典</em>是词法分析必需的，<em>模型</em>是句法分析必需的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data</span><br><span class="line">│</span><br><span class="line">├─dictionary</span><br><span class="line">└─model</span><br></pre></td></tr></table></figure>
<p>配置文件的作用是告诉HanLP数据包的位置，只需修改第一行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root=usr/home/HanLP/</span><br></pre></td></tr></table></figure>
<p>为data的<strong>父目录</strong>即可</p>
<blockquote>
<p>一开始用1.2.9版本，一直提示找不到HanLP.properties，后来换了1.3.2，默认是项目根目录下的data目录。</p>
</blockquote>
<p>4.1 分词</p>
<p><strong>标准分词</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">list&lt;Term&gt; termlist = StandardTokenizer.segment(&quot;XXXX&quot;);</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>HanLP</strong>中有一系列“开箱即用”的静态分词器，以<code>Tokenizer</code>结尾。</li>
<li><code>HanLP.segment</code>其实是对<code>StandardTokenizer.segment</code>的包装。</li>
<li>分词结果包含词性，每个词性的意思请查阅<a href="http://www.hankcs.com/nlp/part-of-speech-tagging.html#h2-8" target="_blank" rel="noopener">《HanLP词性标注集》</a>。</li>
<li>算法详解：<a href="http://www.hankcs.com/nlp/segment/the-word-graph-is-generated.html" target="_blank" rel="noopener">《词图的生成》</a></li>
</ul>
<blockquote>
<p>词性标注（Part-of-Speech tagging 或POS tagging)：又称词类标注或者简称标注，是指为分词结果中的每个单词标注一个正确的词性的程序，也即确定每个词是名词、动词、形容词或其他词性的过程。在汉语中，词性标注比较简单，因为汉语词汇词性多变的情况比较少见，大多词语只有一个词性，或者出现频次最高的词性远远高于第二位的词性。据说，只需选取最高频词性，即可实现80%准确率的中文词性标注程序。</p>
<p>利用HMM即可实现更高准确率的词性标注。</p>
</blockquote>
<p>去掉停用词、标点的分词</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">list&lt;Term&gt; termlist = NotionalTokenizer.segment()</span><br></pre></td></tr></table></figure>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2018/03/17/Elastic Search/ES的analyzer/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/03/17/Elastic Search/ES的analyzer/" class="post-title-link" itemprop="url">ES的analyzer</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-03-17 20:50:31" itemprop="dateCreated datePublished" datetime="2018-03-17T20:50:31+08:00">2018-03-17</time>
            </span>
          

          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/Elastic-Search/" itemprop="url" rel="index"><span itemprop="name">Elastic Search</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2018/03/17/Elastic Search/ES的analyzer/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/03/17/Elastic Search/ES的analyzer/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>转自<a href="http://blog.csdn.net/i6448038/article/details/51509439" target="_blank" rel="noopener">ElasticSearch 解析机制常见用法库 之 analyzer常用用法</a></p>
<p><img src="http://img.blog.csdn.net/20160526182911460?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></p>
<p> 单词或文档先经过Character Filters；Character Filters的作用就是对文本进行一个预处理，例如把文本中所有“&amp;”换成“and”，把“?”去掉等等操作。</p>
<p>​          之后就进入了十分重要的tokenizers模块了，Tokenizers的作用是进行分词，例如，“tom is a good doctor .”。经过Character Filters去掉句号“.”（假设）后，分词器Tokenizers会将这个文本分出很多词来：“tom”、“is”、“a”、“good”、“doctor”。</p>
<p>​          经过分词之后的集合，最后会进入Token Filter词单元模块进行处理，此模块的作用是对已经分词后的集合(tokens)单元再进行操作，例如把“tom”再次拆分成“t”、“o”、“m”等操作。最后得出来的结果集合，就是最终的集合。</p>
<p>​          所以整个流程是：单词 ====》Character Filter 预处理 =====》tokenizer分词 ====》 token filter对分词进行再处理。</p>
<p>​          到此为止，Analyzer是什么鬼？它干什么呢？</p>
<p>​          Analyzer是由一个tokenizer、零到多个Token Filter、还有零到多个CharFilters构成的，也就是说一个Analyzer就是一个完整的解析模块。</p>
<p>​          下面，着重介绍一下常用的Analyzer、Tokenizer、Token filter、Character Filter：</p>
<p><strong>Standard Analyzer</strong></p>
<p>一个“standard”标准类型的 analyzer 就是由 标准分词 “Standard Tokenizer”和标准分词过滤器“Standard Token Filter”、小写字母转换分词过滤“Lower case Token Filter”、还有“Stop Token Filter”过滤构成的</p>
<p>以下是一个standard类型</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>设置说明stopwords                 一个用于初始化stop filter的需要stop 单词的列表.默认为空 。</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>max_token_length             最大的token集合,即经过tokenizer过后得到的结果集的最大值。如果token的长度超过了设置的长度，将会继续分，默认255</td>
</tr>
<tr>
<td></td>
<td><strong>Stop Analyzer</strong></td>
</tr>
</tbody>
</table>
</div>
<p>一个stop类型的analyzer是由 Lower case Tokenizer 和 Stop Token Filter构成的。</p>
<p>以下是一个stop analyzer可以设置的属性:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>设置</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>stopwords</code></td>
<td>一个用于初始化stop filter的需要stop 单词的列表.默认单词是英语</td>
</tr>
<tr>
<td><code>stopwords_path</code></td>
<td>一个有stopwords 配置文件的路径(一个和config文件相关的路径或者URL)</td>
</tr>
</tbody>
</table>
</div>
<p><code>用“stopwords: _none_</code> ”来定义一个空的stopword列表</p>
<p><strong>Simple Analyzer</strong></p>
<p>一个simple类型的analyzer是由lower case Tokenizer构成的，具体信息可以查看此Tokenzier</p>
<p><strong>Whitespace Analyzer</strong></p>
<p>一个whitespace类型的analyzer是由Whitespace Tokenizer构成，请具体查看Whitespace Tokenizer</p>
<p>Keyword Analyzer</p>
<p>一个keyword类型的analyzer，它的Tokenizer将整块的数据作为一个小Token（即经过Tokenizer过滤后的数据），这对于像“邮政编码”、“id”等数据非常有用。注意：当使用并定义这种analyzer的时候，单纯的将fieled 设置为“not_analyzed”可能会更有意义。</p>
<p><strong>Pattern Analyzer</strong></p>
<p>一个pattern类型的analyzer可以通过正则表达式将文本分成”terms”(经过token Filter 后得到的东西 )。接受如下设置:</p>
<p>一个 <code>pattern</code> analyzer 可以做如下的属性设置:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><code>lowercase</code></th>
<th>terms是否是小写. 默认为 <code>true 小写</code>.</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>pattern</code></td>
<td>正则表达式的pattern, 默认是 <code>\W+</code>.</td>
</tr>
<tr>
<td><code>flags</code></td>
<td>正则表达式的flags.</td>
</tr>
<tr>
<td><code>stopwords</code></td>
<td>一个用于初始化stop filter的需要stop 单词的列表.默认单词是空的列表</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Snowball Analyzer</strong></p>
<p>一个snowball类型的analyzer是由standard tokenizer和standard filter、lowercase filter、stop filter、snowball filter这四个filter构成的。</p>
<p>snowball analyzer 在Lucene中通常是不推荐使用的。</p>
<p><strong>Language Analyzers</strong></p>
<p>一个用于解析特殊语言文本的analyzer集合。（ <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#arabic-analyzer" target="_blank" rel="noopener"><code>arabic</code></a>,<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#armenian-analyzer" target="_blank" rel="noopener"><code>armenian</code></a>, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#basque-analyzer" target="_blank" rel="noopener"><code>basque</code></a>, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#brazilian-analyzer" target="_blank" rel="noopener"><code>brazilian</code></a>, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#bulgarian-analyzer" target="_blank" rel="noopener"><code>bulgarian</code></a>, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#catalan-analyzer" target="_blank" rel="noopener"><code>catalan</code></a>, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#cjk-analyzer" target="_blank" rel="noopener"><code>cjk</code></a>, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#czech-analyzer" target="_blank" rel="noopener"><code>czech</code></a>, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#danish-analyzer" target="_blank" rel="noopener"><code>danish</code></a>, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#dutch-analyzer" target="_blank" rel="noopener"><code>dutch</code></a>, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#english-analyzer" target="_blank" rel="noopener"><code>english</code></a>, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#finnish-analyzer" target="_blank" rel="noopener"><code>finnish</code></a>, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#french-analyzer" target="_blank" rel="noopener"><code>french</code></a>,<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#galician-analyzer" target="_blank" rel="noopener"><code>galician</code></a>, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#german-analyzer" target="_blank" rel="noopener"><code>german</code></a>, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#greek-analyzer" target="_blank" rel="noopener"><code>greek</code></a>, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#hindi-analyzer" target="_blank" rel="noopener"><code>hindi</code></a>, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#hungarian-analyzer" target="_blank" rel="noopener"><code>hungarian</code></a>, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#indonesian-analyzer" target="_blank" rel="noopener"><code>indonesian</code></a>, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#irish-analyzer" target="_blank" rel="noopener"><code>irish</code></a>, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#italian-analyzer" target="_blank" rel="noopener"><code>italian</code></a>, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#latvian-analyzer" target="_blank" rel="noopener"><code>latvian</code></a>, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#lithuanian-analyzer" target="_blank" rel="noopener"><code>lithuanian</code></a>, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#norwegian-analyzer" target="_blank" rel="noopener"><code>norwegian</code></a>,<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#persian-analyzer" target="_blank" rel="noopener"><code>persian</code></a>, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#portuguese-analyzer" target="_blank" rel="noopener"><code>portuguese</code></a>, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#romanian-analyzer" target="_blank" rel="noopener"><code>romanian</code></a>, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#russian-analyzer" target="_blank" rel="noopener"><code>russian</code></a>, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#sorani-analyzer" target="_blank" rel="noopener"><code>sorani</code></a>, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#spanish-analyzer" target="_blank" rel="noopener"><code>spanish</code></a>, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#swedish-analyzer" target="_blank" rel="noopener"><code>swedish</code></a>, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#turkish-analyzer" target="_blank" rel="noopener"><code>turkish</code></a>, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#thai-analyzer" target="_blank" rel="noopener"><code>thai</code></a>.）可惜没有中文。不予考虑</p>
<p><strong>Custom Analyzer</strong></p>
<p>简而言之，是自定义的analyzer。允许多个零到多个tokenizer，零到多个 <code>Char Filters</code>. custom analyzer 的名字不能以 “_”开头.</p>
<p>The following are settings that can be set for a <code>custom</code> analyzer type:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Setting</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>tokenizer</code></td>
<td>通用的或者注册的tokenizer.</td>
</tr>
<tr>
<td><code>filter</code></td>
<td>通用的或者注册的 token filters.</td>
</tr>
<tr>
<td><code>char_filter</code></td>
<td>通用的或者注册的 character filters.</td>
</tr>
<tr>
<td><code>position_increment_gap</code></td>
<td>距离查询时，最大允许查询的距离，默认是100</td>
</tr>
</tbody>
</table>
</div>
<p>自定义的模板：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">index :</span><br><span class="line"></span><br><span class="line">    analysis :</span><br><span class="line"></span><br><span class="line">        analyzer :</span><br><span class="line"></span><br><span class="line">            myAnalyzer2 :</span><br><span class="line"></span><br><span class="line">                type : custom</span><br><span class="line"></span><br><span class="line">                tokenizer : myTokenizer1</span><br><span class="line"></span><br><span class="line">                filter : [myTokenFilter1, myTokenFilter2]</span><br><span class="line"></span><br><span class="line">                char_filter : [my_html]</span><br><span class="line"></span><br><span class="line">                position_increment_gap: 256</span><br><span class="line"></span><br><span class="line">        tokenizer :</span><br><span class="line"></span><br><span class="line">            myTokenizer1 :</span><br><span class="line"></span><br><span class="line">                type : standard</span><br><span class="line"></span><br><span class="line">                max_token_length : 900</span><br><span class="line"></span><br><span class="line">        filter :</span><br><span class="line"></span><br><span class="line">            myTokenFilter1 :</span><br><span class="line"></span><br><span class="line">                type : stop</span><br><span class="line"></span><br><span class="line">                stopwords : [stop1, stop2, stop3, stop4]</span><br><span class="line"></span><br><span class="line">            myTokenFilter2 :</span><br><span class="line"></span><br><span class="line">                type : length</span><br><span class="line"></span><br><span class="line">                min : 0</span><br><span class="line"></span><br><span class="line">                max : 2000</span><br><span class="line"></span><br><span class="line">        char_filter :</span><br><span class="line"></span><br><span class="line">              my_html :</span><br><span class="line"></span><br><span class="line">                type : html_strip</span><br><span class="line"></span><br><span class="line">                escaped_tags : [xxx, yyy]</span><br><span class="line"></span><br><span class="line">                read_ahead : 1024</span><br></pre></td></tr></table></figure>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2018/02/11/机器学习/推荐系统/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/02/11/机器学习/推荐系统/" class="post-title-link" itemprop="url">推荐系统</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-02-11 09:26:22" itemprop="dateCreated datePublished" datetime="2018-02-11T09:26:22+08:00">2018-02-11</time>
            </span>
          

          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2018/02/11/机器学习/推荐系统/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/02/11/机器学习/推荐系统/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="推荐算法概述"><a href="#推荐算法概述" class="headerlink" title="推荐算法概述"></a>推荐算法概述</h1><p>根据数据源的不同分为</p>
<p>1）基于人口的统计学推荐</p>
<p>2）基于内容的推荐Content-based</p>
<p>比如说电影推荐中，基于电影的内容，推荐相似的</p>
<p>3）基于协同过滤的推荐Collaborative Filtering-based</p>
<p>然后CF再分User-Based、Item-Based、Model-Based</p>
<p>User-Based、Item-Based都是将用户所有输入载入内存进行运算，又合称为Memory-Based</p>
<p>Model-Based包括Aspect Model，pLSA，LDA，聚类，SVD，Matrix Factorization等。</p>
<h2 id="SVD和矩阵分解算法"><a href="#SVD和矩阵分解算法" class="headerlink" title="SVD和矩阵分解算法"></a>SVD和矩阵分解算法</h2><p>SVD是矩阵分解的一种，不过两种方法在推荐系统中的用法不一样。</p>
<h3 id="基于SVD的推荐系统"><a href="#基于SVD的推荐系统" class="headerlink" title="基于SVD的推荐系统"></a>基于SVD的推荐系统</h3><p>对A做奇异值分解后，取奇异值矩阵的前N个对角值，相应的取U和V的行和列，再重新拼成A2。</p>
<p>此时的A2相当于A的有损压缩。</p>
<p>分析得知，U矩阵和V矩阵可以近似来代表A矩阵，换据话说就是<strong>将A矩阵压缩成U矩阵和V矩阵</strong>，至于压缩比例得看当时对S矩阵取前k个数的k值是多少。</p>
<p><a href="https://yanyiwu.com/work/2012/09/10/SVD-application-in-recsys.html" target="_blank" rel="noopener">SVD在推荐系统中的应用</a></p>
<p>代码见<code>svd_recommendation.py</code></p>
<p>假设行是user，列是item，SVD分解后，</p>
<ul>
<li>U的行和user的行数一致，代表user的主题分布</li>
<li>V的列和item的列数一致，代表item的主题分布</li>
<li>S是奇异值，从中选择k</li>
</ul>
<p><img src="https://yanyiwu.com/img/svd-recsys-p4.png" alt=""></p>
<p>该计算的含义不明白</p>
<h3 id="基于矩阵分解的推荐系统"><a href="#基于矩阵分解的推荐系统" class="headerlink" title="基于矩阵分解的推荐系统"></a>基于矩阵分解的推荐系统</h3><p>koren获得netflix grand prize时关于矩阵分解的的论文</p>
<p>Matrix Factorization Techniques For Recommender Systems   <a href="https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf" target="_blank" rel="noopener">https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf</a></p>
<p><a href="http://blog.csdn.net/winone361/article/details/50705739" target="_blank" rel="noopener">矩阵分解在推荐系统中的应用：NMF和经典SVD实战</a></p>
<h1 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h1><p>Factorization meets the neighborhood a multifaceted collaborative filtering model   <a href="http://vdisk.weibo.com/s/khQ1v" target="_blank" rel="noopener">http://vdisk.weibo.com/s/khQ1v</a></p>
<p>Collaborative Filtering with Temporal Dynamics     <a href="http://vdisk.weibo.com/s/khQ9o" target="_blank" rel="noopener">http://vdisk.weibo.com/s/khQ9o</a></p>
<p>其他推荐系统的论文</p>
<p>作者：严林链接：<a href="https://www.zhihu.com/question/25566638/answer/37455091来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。" target="_blank" rel="noopener">https://www.zhihu.com/question/25566638/answer/37455091来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</a></p>
<ol>
<li>The Wisdom of The Few 豆瓣阿稳在介绍豆瓣猜的时候极力推荐过这篇论文，豆瓣猜也充分应用了这篇论文中提出的算法；</li>
</ol>
<ol>
<li>Restricted Boltzmann Machines for Collaborative Filtering 目前Netflix使用的主要推荐算法之一；</li>
<li>Factorization Meets the Neighborhood: a Multifaceted Collaborative Filtering Model 这个无需强调重要性，LFM几乎应用到了每一个商业推荐系统中；</li>
<li>Collaborative Filtering with Temporal Dynamics 加入时间因素的SVD++模型，曾在Netflix Prize中大放溢彩的算法模型；</li>
<li>Context-Aware Recommender Systems 基于上下文的推荐模型，现在不论是工业界还是学术界都非常火的一个topic；</li>
<li>Toward the next generation of recommender systems 对下一代推荐系统的一个综述；</li>
<li>Item-Based Collaborative Filtering Recommendation Algorithms 基于物品的协同过滤，Amazon等电商网站的主力模型算法之一；</li>
<li>Information Seeking-Convergence of Search, Recommendations and Advertising 搜索、推荐和广告的大融合也是未来推荐系统的发展趋势之一；</li>
<li>Ad Click Prediction: a View from the Trenches 可以对推荐结果做CTR预测排序；</li>
<li>Performance of Recommender Algorithm on top-n Recommendation Task TopN预测的一个综合评测，TopN现在是推荐系统的主流话题，可以全部实现这篇文章中提到的算法大概对TopN有个体会；</li>
<li><a href="https://link.zhihu.com/?target=http%3A//dsec.pku.edu.cn/%7Ejinlong/publication/wjlthesis.pdf" target="_blank" rel="noopener">http://dsec.pku.edu.cn/~jinlong/publication/wjlthesis.pdf</a> 北大一博士对Netflix Prize算法的研究做的毕业论文，这篇论文本身对业界影响不大，但是Netflix Prize中运用到的算法极大地推动了推荐系统的发展</li>
</ol>
<h1 id="Hulu视频推荐算法"><a href="#Hulu视频推荐算法" class="headerlink" title="Hulu视频推荐算法"></a>Hulu视频推荐算法</h1><p>干货：从相关性到 RNN，一家线上“租碟店”的视频推荐算法演进</p>
<p><a href="http://blog.csdn.net/dzjx2eotaa24adr/article/details/79060659" target="_blank" rel="noopener">http://blog.csdn.net/dzjx2eotaa24adr/article/details/79060659</a></p>
<p>推荐系统的模型：</p>
<p>利用（Exploitation）：</p>
<blockquote>
<p>计算要给用户推荐什么，比较确定的兴趣。</p>
</blockquote>
<ul>
<li>货架：从协同滤波到深度神经网络</li>
<li>自动播放：RNN</li>
</ul>
<p>探索（Exploration）：</p>
<blockquote>
<p>探索用户的新兴趣，防止出现一样的推荐。</p>
</blockquote>
<ul>
<li>Adaptive：在线学习</li>
<li>Diversity：行列式点过程</li>
</ul>
<p>推荐系统的优化目标，如果从用户的偏好来讲，我们是希望提供给用户一个非常健康的组合，<strong>既符合他们的口味，又有足够的多样性</strong> 。</p>
<p>最后从算法或者数学抽象来讲，我们就把这个问题抽象成为<strong>“怎样最大化用户的观看时长”</strong>。包括</p>
<ul>
<li>点击进去看一个视频之后，你看了多长时间？这个跟视频本身的长度有关，跟你的完成度有关，我们叫playback_duration。</li>
<li>有效点击eCTR。（<strong>推荐系统优劣的评价</strong>）</li>
<li>整个内容的曝光量。（<strong>推荐系统的作用！</strong>）</li>
</ul>
<p><strong>把这三项乘在一起的话，就变成了每个用户的观看时长。</strong></p>
<p>我们最终想要最大化的是<strong>eCTR和曝光量的乘积</strong>。</p>
<p><img src="http://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/wc7YNPm3YxWCL7kfAbDibBgOrqyRRibiasdtSWibmAR6neaUZ4M5qWJGHChZNLa1mTDw47jcGICuUJV1G0jceYKt1A/0?wx_fmt=jpeg" alt=""></p>
<h2 id="视频推荐系统的框架"><a href="#视频推荐系统的框架" class="headerlink" title="视频推荐系统的框架"></a>视频推荐系统的框架</h2><p><img src="http://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/wc7YNPm3YxWCL7kfAbDibBgOrqyRRibiasdxyvlXlbCmxCXH188VCKcf4LSkXStuns7pGXyFTEB24FnMcy30cqpSA/0?wx_fmt=jpeg" alt=""></p>
<p>1）<strong>Data</strong>。最底下一层是基础数据。</p>
<p>2）<strong>Feature</strong>。第二层是特征，特征可以分为用户的特征和内容的特征。</p>
<p>3）<strong>Model</strong>，分为Exploitation、Exploration。</p>
<p><strong>Exploitation</strong>包括：</p>
<p>a、Relevant相关度，</p>
<p>b、Transparent透明度（对产品经理还是对最终用户是不是可解释）</p>
<p>c、Contextual上下文（时间、设备、地点这些上下文信息）</p>
<p><strong>Exploration</strong>包括</p>
<p>a、<strong>Coverage</strong>范围，对新内容我们有没有给足够的展现</p>
<p>b、<strong>Serendipity</strong>惊喜，有没有给用户惊喜、发现用户隐藏的一些兴趣。</p>
<p>c、<strong>Adapive</strong>适应性，当用户兴趣发生改变的时候，我们是不是非常快地适应了用户这种兴趣改变。</p>
<p>d、<strong>Diverse</strong>多样性，我们给出的这个套餐是不是组合了很多不同的类别，而不是非常单调、单一的。</p>
<p>4）<strong>Application</strong>。这些应用主要是服务用户在四个阶段的行为。</p>
<p>a、<strong>Onboarding</strong>：新用户，刚刚订阅我们的服务的时候，还在一个初始的、给我们一些信号的阶段。我们让用户选择他最喜欢的内容频道，然后用户会告诉我们他喜欢体育、喜剧或者动作片。基于这些大类，我们可以给他做一个冷启动，给他第一屏的推荐结果。</p>
<p>b、<strong>Convert-to-Pay</strong>：在一个新用户进来之后，我们大概有七天的时间把他转化为一个付费用户。在这个阶段，我们需要快速地探索用户的各种需求，让他体会到我们的服务非常有价值，那么他才愿意买单。</p>
<p>c、<strong>Retention</strong>留存：到了第三个阶段，就是用户已经是一个付费用户，那我们就需要留住他，所以就是不断的去给他更多的、他之前可能没有看过，但是和之前看过的很相关的内容。</p>
<p>d、<strong>Monetization</strong>变现：广告变现。一个已经付费的用户，已经留下来的用户，我们怎么样用广告把他的流量变现。</p>
<h2 id="视频推荐算法"><a href="#视频推荐算法" class="headerlink" title="视频推荐算法"></a>视频推荐算法</h2><h3 id="Exploitation"><a href="#Exploitation" class="headerlink" title="Exploitation"></a>Exploitation</h3><p><strong>基于用户行为和side information做货架场景的排序和自动播放的这种持续预测。</strong></p>
<h4 id="相似性算法"><a href="#相似性算法" class="headerlink" title="相似性算法"></a>相似性算法</h4><p>1）Relevant的角度，还是user-based+item-based</p>
<ul>
<li>基于存储的Memory-based：item-based CF</li>
<li>基于模型的Model-based：矩阵分解和神经网络。神经网络现在也有基于RBM的，还有Embedding-based Neural Network。</li>
</ul>
<p>Hulu经历了三代相关性算法的演进，第一代是item-based CF，第二代是基于矩阵分解，现在我们正在开发的第三代是基于Embedding-based Neural Network。从Netflix公开的文件来看，它主要使用的是SVD和RBM的方法。</p>
<p>a、第一代，协同滤波</p>
<p><img src="http://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/wc7YNPm3YxWCL7kfAbDibBgOrqyRRibiasdG05EQl0mwZxsSvO9LWLv4wRSpjZuia8V3QeNNib68Vn1p3KKFUVB1ddQ/0?wx_fmt=jpeg" alt=""></p>
<p>创建一个<strong>相关度矩阵</strong>。这个矩阵是很稀疏的，<strong>大约有百分之七八十的数据是实际上是零</strong>。</p>
<p>b、第二代，矩阵分解</p>
<p><img src="http://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/wc7YNPm3YxWCL7kfAbDibBgOrqyRRibiasd0Bcf9wn6dNsB1GvvadgaEDZqLu4L524RdicF7GBNEtFdrjnpg4aMhYA/0?wx_fmt=jpeg" alt=""></p>
<p>矩阵分解为了解决这个稀疏性的问题，使用了线性代数里面的一个特性，就是<strong>一个低秩矩阵，可以用两个相对低维度矩阵的乘积来表示</strong>。</p>
<p>相关度矩阵（评分矩阵）是低秩的，可以用一个P和Q的乘积来表示，P就是所有用户的特征。</p>
<blockquote>
<p>SVD待看</p>
</blockquote>
<p>c、第三代，深度学习</p>
<p><img src="http://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/wc7YNPm3YxWCL7kfAbDibBgOrqyRRibiasdZwA6QNMTAJicDKib9LOFp8acFV2fTEknWVtId5uqdRsshzicd68dUbib5Q/0?wx_fmt=jpeg" alt=""></p>
<p>把原来矩阵分解里面代数运算的步骤，用一个前向神经网络来替换。好处是，</p>
<ul>
<li>一方面非线性前向网络允许一些非线性映射，可以有更好的表达能力去model一个更复杂的分布。</li>
<li>另外可以直接把关于用户的处行为外的所有信息，用一个矢量feed到神经网络中去。</li>
</ul>
<p>对内容我们也可以做相应的处理，就是把元数据，比如说导演、演员信息用一个向量来表示，然后把它feed到神经网络里面去。</p>
<h4 id="相似性算法应用场景"><a href="#相似性算法应用场景" class="headerlink" title="相似性算法应用场景"></a>相似性算法应用场景</h4><p><img src="http://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/wc7YNPm3YxWCL7kfAbDibBgOrqyRRibiasdwib2nrpADovTp7nhgib9BWoK9ks0oxwn3ry0tvOpiaNHEWQGDVOwgzLNA/0?wx_fmt=jpeg" alt=""></p>
<p>对应这两种不同的场景，其实需要不同的相关运算。</p>
<ul>
<li>一个是所谓的货架场景，就是给一个网格里面按照相关性做了排序，然后<strong>希望用户点越靠上越靠左的这些内容</strong>。CF比较适合货架场景的召回和排序。</li>
<li>一个是自动播放的场景，就是播完一个内容之后，我们会<strong>自动地开始下一个我们觉得用户最可能看的内容</strong>。用RNN。</li>
</ul>
<p><img src="http://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/wc7YNPm3YxWCL7kfAbDibBgOrqyRRibiasd5kWZiaia9wmkE9vhDCYpS3tY6ChT7WShiciaGxYAztaTWtGK9fefg54Z3Q/0?wx_fmt=jpeg" alt=""></p>
<p><strong>用反向传播的方法去训练一个RNN模型</strong>，来预测用户在网络上的下一个行为。</p>
<blockquote>
<p><strong>如何评估</strong>：</p>
<p>仿真测试的方法就是当用户看完剧A/B/C之后，我们假装不知道这个用户接下来看了哪一个，<strong>然后基于时间序列的建模，来算出一个最可能看的剧，它可能是当前这一剧的下一集，或者是跳到另外一个剧D或者是另外一个剧E</strong>。根据RNN模型，我们找到最有可能的下一个剧，然后和用户实际看的下一个剧之间做比对，这是离线的一种评估方案。</p>
</blockquote>
<h3 id="Exploration"><a href="#Exploration" class="headerlink" title="Exploration"></a>Exploration</h3><h4 id="自适应"><a href="#自适应" class="headerlink" title="自适应"></a>自适应</h4><p>为了解决用户兴趣的时变以及新内容的冷启动，我们采用一种叫做<strong>多臂老虎机（MAB）</strong>的模型。</p>
<p><img src="http://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/wc7YNPm3YxWCL7kfAbDibBgOrqyRRibiasdVqpHCZCibfYIjb9e3icqiaKCEhNFOLhGcvglb3bJn2WhxRAgfWDiaRwcQg/0?wx_fmt=jpeg" alt=""></p>
<blockquote>
<p>原理</p>
<p><strong>一个赌徒可以在不同的时间选择不同的摇臂，每个摇臂会给这个赌徒不同的赢率。</strong>如果赌徒每次都选择摇臂1的话，有可能不是最优的，因为可能另外一个摇臂的反馈更好。</p>
</blockquote>
<p>在推荐系统中，每个摇臂就是推荐用户的一个剧，算法就是赌徒，根据一些策略选择推荐哪个剧。每个摇臂的奖励就是这个用户是否点击和观看了。</p>
<p><img src="http://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/wc7YNPm3YxWCL7kfAbDibBgOrqyRRibiasdElhBVwEdYY9uaZ8OQwlOqqSwOx6D7zHrDLZGsCkFLmJCBApKKQz48g/0?wx_fmt=jpeg" alt=""></p>
<p><strong>LinUCB算法</strong>。会根据当前推的结果，来实时更新对每个摇臂的点击率的预测。</p>
<p><img src="http://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/wc7YNPm3YxWCL7kfAbDibBgOrqyRRibiasd3ha2ia9opVTkYJJApg4zfNX5MiafHp1rWzYEyFxspzdVpibuuyxgk9DHQ/0?wx_fmt=jpeg" alt=""></p>
<p>在线上部署LinUCB的算法，有一个线上更新提取特征以及模型运算的过程，以及一个线下根据之前模型采集到的信号去更新模型参数的过程。</p>
<p>在我们这个实验里面，可能对大家比较有参考意义的就是我们发现的LinUCB的一些特征，其中包括用户当前看剧的完成度，就是他看到了第几集、是不是看到了高潮部分还是快要结束的部分。<strong>完成度是一个很重要的特征维度</strong>。然后就是上次给用户曝光这个剧的时间和现在之间的<strong>时间间隔</strong>，以及它历史上的<strong>点击率</strong>，还有这个剧的一些元数据信息，它在外面的流行程度，以及根据刚才讲的协同滤波的方法，得到的用户和这个剧之间的相关性。</p>
<p><img src="http://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/wc7YNPm3YxWCL7kfAbDibBgOrqyRRibiasd8QU2w6zpznjclZVh5pn46vEsibtUcX7srPQ8XF3n5lJsA7G5vuaOdcw/0?wx_fmt=jpeg" alt=""></p>
<h4 id="多样性"><a href="#多样性" class="headerlink" title="多样性"></a>多样性</h4><p><img src="http://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/wc7YNPm3YxWCL7kfAbDibBgOrqyRRibiasdSHII6DSUH6mKL05ngpzhapbd64zbLxYq2YEAIajqPAUmssUG69eY0g/0?wx_fmt=jpeg" alt=""></p>
<p><strong>为什么要关注多样性？</strong></p>
<p>因为用户的兴趣爱好可能<strong>不是单峰分布的</strong>。有可能用户有多个兴趣爱好，其中有非常突出的一个，就是这个比较高的右边的峰。但是还有一个比较低的，就是右边这张图里面的靠左的这个峰值。用多样性来考虑用户的多个爱好。不是只推荐他最喜欢的。</p>
<p>传统上使用<strong>启发式</strong>的方法，它会在多样性和相关性之间用一个加权平均的方法来获得一个总体的优化目标，然后两两之间比较当前推荐的差异性，然后试图最大化这个总的平衡了之后的优化目标，用穷举的方法。</p>
<p>我们在现有的启发式的搜索基础上，采用了一些不同的代数模型，就是把两两之间比较不相似性改变成为用一个多边形的体积来量化我们给出的不同物品之间的差异性。<strong>把每个物品看作一个多维空间里的向量，然后用这些向量总体张成的一个多边形的体积来度量这个集合的差异性。</strong></p>
<h2 id="推荐的应用场景"><a href="#推荐的应用场景" class="headerlink" title="推荐的应用场景"></a>推荐的应用场景</h2><h3 id="生成推荐的理由"><a href="#生成推荐的理由" class="headerlink" title="生成推荐的理由"></a>生成推荐的理由</h3><p>比如说，当我们推荐《终结者2》，我们说是由于你历史上看过《终结者1》，这时候就比完全没有任何原因的推荐显得更加顺理成章。如何构建一个推荐的理由？我们可以用刚才很简单的模板，就是因为你历史上看过和它相关的一个剧。</p>
<p><img src="http://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/wc7YNPm3YxWCL7kfAbDibBgOrqyRRibiasdtIXmKo2Cs0icjx0pgG3L0gXSh0evQEicH8QCdzhynoztackmh5f4lGhw/0?wx_fmt=jpeg" alt=""></p>
<p>但是如果我们想做得更加人性化、更加自然，我们要用一种<strong>知识图谱</strong>的方法。在知识图谱里面构建内容，用户的群组，相关性的信息，以及一些统计信息，包括这个剧的流行程度，它在外面的排名。我们用一种N元组的方法来记录这个知识图谱。</p>
<p><img src="http://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/wc7YNPm3YxWCL7kfAbDibBgOrqyRRibiasdSG2LoG2OQaLfDmlcrPNv1vJeQVStIctECXeQE1NIE2ErjV9amYibLIA/0?wx_fmt=jpeg" alt=""></p>
<h4 id="推理规则"><a href="#推理规则" class="headerlink" title="推理规则"></a>推理规则</h4><p>基于这个知识图谱，我们可以设定一些<strong>推理规则</strong>，手工建立规则。    每一条规则其实对应某一种经典算法，比如第一条规则，就是如果用户喜欢电视剧，一是由于他曾经看过电视剧，二是电视剧2和1非常相近，这就是item-based CF逻辑的一种表达方式。类似的话，我们还可以把user-based CF也用一条规则来表达。比如说这个地方列出的第二条规则，就是<strong>如果用户属于某一个群组，而这个群组里面60%的人都看过剧1，那就说明当前这个用户也可能会喜欢看剧1</strong>。</p>
<p><img src="http://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/wc7YNPm3YxWCL7kfAbDibBgOrqyRRibiasdBY4fNAxunpYzP6LEtRUsiaTibUGciafxfSab5Ab4Q7jscYzJ7nNIiaLPUw/0?wx_fmt=jpeg" alt=""></p>
<h3 id="语音对话推荐"><a href="#语音对话推荐" class="headerlink" title="语音对话推荐"></a>语音对话推荐</h3><p>然后当用户对当前的推荐不满的时候，他也可以用自然语言来告诉我们，为什么他不喜欢这个剧，以及他想要换另外一个什么样的剧。<strong>大家知道PC时代，鼠标和键盘是最流行的交互方式，到了移动时代，触屏变成了手机上的最流行的交互方式。随着物联网的发展，我们认为语音会成为下一代的交互方式。</strong></p>
<blockquote>
<p><strong>怎么识别用户的兴趣是否改变呢？</strong></p>
<p>其实对于多臂老虎机的问题模型来讲，认为用户的兴趣是一个可以实时更新的参数。</p>
<p><strong>知识图谱是怎么建立和生成的？</strong></p>
<p>一方面是从第三方采买的，有专门的构建知识图谱的厂商，他们会做数据清洗爬取。另外一部分是我们从内容提供商那里获得的一些元数据信息。</p>
</blockquote>
<p><strong>延伸</strong>：<a href="https://zhuanlan.zhihu.com/p/21388070?refer=resyschina" target="_blank" rel="noopener">bandit问题</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2018/02/11/工具和环境/Anaconda操作/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/02/11/工具和环境/Anaconda操作/" class="post-title-link" itemprop="url">Anaconda操作</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-02-11 09:26:22" itemprop="dateCreated datePublished" datetime="2018-02-11T09:26:22+08:00">2018-02-11</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-04-12 11:34:06" itemprop="dateModified" datetime="2018-04-12T11:34:06+08:00">2018-04-12</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/工具和环境/" itemprop="url" rel="index"><span itemprop="name">工具和环境</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2018/02/11/工具和环境/Anaconda操作/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/02/11/工具和环境/Anaconda操作/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="安装环境"><a href="#安装环境" class="headerlink" title="安装环境"></a>安装环境</h1><p>使用清华大学的镜像</p>
<p><a href="https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/" target="_blank" rel="noopener">https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/</a></p>
<h1 id="切换版本"><a href="#切换版本" class="headerlink" title="切换版本"></a>切换版本</h1><p>目前我装的是anaconda2的版本，若要使用py3，先安装python3</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n py3 python=3</span><br></pre></td></tr></table></figure>
<p>在py3下安装spyder</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install -n py3 spyder</span><br></pre></td></tr></table></figure>
<p>要启动py3的spyder，在切换到py3环境后，直接执行<code>spyder</code>即可</p>
<p>再安装jupyter</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install -n py3 jupyter</span><br></pre></td></tr></table></figure>
<p>当切换至py3的环境</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source activate py3</span><br></pre></td></tr></table></figure>
<h1 id="打开spyder报错"><a href="#打开spyder报错" class="headerlink" title="打开spyder报错"></a>打开spyder报错</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[warn] kq_init: detected broken kqueue; not using.: Undefined error: 0</span><br><span class="line">[warn] kq_init: detected broken kqueue; not using.: Undefined error: 0</span><br><span class="line">[warn] kq_init: detected broken kqueue; not using.: Undefined error: 0</span><br><span class="line">[warn] kq_init: detected broken kqueue; not using.: Undefined error: 0</span><br><span class="line">[warn] kq_init: detected broken kqueue; not using.: Undefined error: 0</span><br><span class="line">[warn] kq_init: detected broken kqueue; not using.: Undefined error: 0</span><br><span class="line">[warn] kq_init: detected broken kqueue; not using.: Undefined error: 0</span><br></pre></td></tr></table></figure>
<p><a href="https://stackoverflow.com/questions/40047607/problems-with-spyder-3-in-macos-sierra" target="_blank" rel="noopener">https://stackoverflow.com/questions/40047607/problems-with-spyder-3-in-macos-sierra</a></p>
<p>The problem is not with Spyder itself but with Qt, the graphical library on top of which Spyder is built on.</p>
<p>The current Qt version in Anaconda (<strong>5.6.0</strong>) does not support macOS Sierra. According to this <a href="https://github.com/Homebrew/homebrew-core/issues/5163#issuecomment-249350576" target="_blank" rel="noopener">Github comment</a>, the first versions that do it are <strong>5.6.2</strong> and <strong>5.7.1</strong>.</p>
<p>As soon as Continuum (the company behind Anaconda) updates Qt to one of these versions, I’m pretty sure those strange problems you’re seeing will be solved.</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2018/02/11/机器学习/EM算法/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/02/11/机器学习/EM算法/" class="post-title-link" itemprop="url">MCMC</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-02-11 09:26:22" itemprop="dateCreated datePublished" datetime="2018-02-11T09:26:22+08:00">2018-02-11</time>
            </span>
          

          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2018/02/11/机器学习/EM算法/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/02/11/机器学习/EM算法/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2018/02/11/机器学习/MCMC/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/02/11/机器学习/MCMC/" class="post-title-link" itemprop="url">MCMC</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-02-11 09:26:22" itemprop="dateCreated datePublished" datetime="2018-02-11T09:26:22+08:00">2018-02-11</time>
            </span>
          

          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2018/02/11/机器学习/MCMC/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/02/11/机器学习/MCMC/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="MCMC"><a href="#MCMC" class="headerlink" title="MCMC"></a>MCMC</h1><p><a href="http://garygu.xin/wordpress/index.php/2017/09/16/mcmc/" target="_blank" rel="noopener">马尔可夫链蒙特卡洛(MCMC)采样</a></p>
<p><a href="http://blog.csdn.net/wzu12345678/article/details/74002353" target="_blank" rel="noopener">Metropolis-Hasting 算法 &amp; 图上的Metropolis-Hasting Random Walk (MHRW)</a></p>
<p>Markov Chain Monte Carlo，用于模拟采样的算法。</p>
<p>像Word2Vec中的负采样算法（Negative Sampling），适合离散的概率分布，知道每个值的概率。</p>
<p>而连续分布（概率密度函数表示的）就不行了。</p>
<blockquote>
<p>知乎问题：<a href="https://www.zhihu.com/question/60437632" target="_blank" rel="noopener">为什么要使用MCMC方法？</a></p>
<p>为何不用等距采样替换MCMC</p>
<p>MCMC的应用是和”维数灾难”有关的。MCMC应用的概率模型，其参数维数往往巨大，但每个参数的支撑集非常小。比如一些NLP问题的参数只取{0,1}，但维数往往达到几千甚至上万左右</p>
</blockquote>
<p>参考</p>
<p><a href="http://garygu.xin/wordpress/index.php/2017/09/16/mcmc/" target="_blank" rel="noopener">马尔可夫链蒙特卡洛(MCMC)采样</a></p>
<p>延伸：word2vec里面也涉及到采样：。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2018/01/30/机器学习/机器学习随笔/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/01/30/机器学习/机器学习随笔/" class="post-title-link" itemprop="url">机器学习随笔</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-01-30 21:49:48" itemprop="dateCreated datePublished" datetime="2018-01-30T21:49:48+08:00">2018-01-30</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-03-17 20:50:31" itemprop="dateModified" datetime="2018-03-17T20:50:31+08:00">2018-03-17</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2018/01/30/机器学习/机器学习随笔/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/01/30/机器学习/机器学习随笔/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>根据数据采样来估计概率分布，往往可以用<strong>极大似然估计法</strong>。这种做法需要假定参数符合一个先验分布。贝叶斯分类用的就是这个思路。</p>
<p><a href="https://www.quora.com/What-are-the-most-important-practical-machine-learning-lessons-that-you-have-learned-in-your-career" target="_blank" rel="noopener">机器学习实践中学到的最重要的内容</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/25/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/25/">25</a><span class="page-number current">26</span><a class="page-number" href="/page/27/">27</a><span class="space">&hellip;</span><a class="page-number" href="/page/32/">32</a><a class="extend next" rel="next" href="/page/27/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Schwimmer</p>
              <div class="site-description motion-element" itemprop="description">Record and Think!</div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">314</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">22</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">59</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          

          

          
          

          
            
          
          

        </div>
      </div>

      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Schwimmer</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.9.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> v7.2.0</div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span>
    <span class="site-uv" title="总访客量">
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="post-meta-divider">|</span>
  

  
    <span class="post-meta-item-icon">
      <i class="fa fa-eye"></i>
    </span>
    <span class="site-pv" title="总访问量">
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    
      <div>
        
  <script>
    window._bd_share_config = {
      "common": {
        "bdText": "",
        "bdMini": "1",
        "bdMiniList": false,
        "bdPic": ""
      },
      "image": {
        "viewList": ["tsina", "douban", "sqq", "qzone", "weixin", "twi", "fbook"],
        "viewText": "分享到：",
        "viewSize": "16"
      },
      "slide": {
        "bdImg": "5",
        "bdPos": "left",
        "bdTop": "100"
      }
    }
  </script>

<script>
  with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='//bdimg.share.baidu.com/static/api/js/share.js?cdnversion='+~(-new Date()/36e5)];
</script>

      </div>
    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/utils.js?v=7.2.0"></script>

  <script src="/js/motion.js?v=7.2.0"></script>



  
  


  <script src="/js/schemes/muse.js?v=7.2.0"></script>




  

  


  <script src="/js/next-boot.js?v=7.2.0"></script>


  

  

  

  
  
<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://https-schwimmer-github-io.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>







  
  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
  

  
  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      document.getElementById(all[i].inputID + '-Frame').parentNode.className += ' has-jax';
    }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

    
  


  

  

  

  

  

  

  

  

  

  

  

</body>
</html>
