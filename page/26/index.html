<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">






















<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">

<link rel="stylesheet" href="/css/main.css?v=7.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.2.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.2.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '7.2.0',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="Record and Think!">
<meta property="og:type" content="website">
<meta property="og:title" content="Schwimmer&#39;s Blog">
<meta property="og:url" content="https://schwimmer.github.io/page/26/index.html">
<meta property="og:site_name" content="Schwimmer&#39;s Blog">
<meta property="og:description" content="Record and Think!">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Schwimmer&#39;s Blog">
<meta name="twitter:description" content="Record and Think!">





  
  
  <link rel="canonical" href="https://schwimmer.github.io/page/26/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Schwimmer's Blog</title>
  




  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-143240576-1"></script>
  <script>
    var host = window.location.hostname;
    if (host !== "localhost" || !true) {
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-143240576-1');
    }
  </script>









  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Schwimmer's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
        </li>
      
    </ul>
  

  
    

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2018/03/17/机器学习/人群画像/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/03/17/机器学习/人群画像/" class="post-title-link" itemprop="url">人群画像</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-03-17 20:50:31" itemprop="dateCreated datePublished" datetime="2018-03-17T20:50:31+08:00">2018-03-17</time>
            </span>
          

          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2018/03/17/机器学习/人群画像/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/03/17/机器学习/人群画像/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://zhuanlan.zhihu.com/p/27126412" target="_blank" rel="noopener">比你更了解你，浅谈用户画像</a></p>
<p>爱点击的性别预测模型</p>
<p>为什么用朴素贝叶斯？</p>
<p>如何选择特征？</p>
<blockquote>
<p>去除覆盖率低的，去除</p>
</blockquote>
<p>如何解决特征有依赖关系的问题？</p>
<blockquote>
<p>假设，对于同一个一级域名，下面的N级域名中男女分布比例在接近的合并为同一个特征。</p>
</blockquote>
<p>训练集和测试集？</p>
<p>线上效果</p>
<blockquote>
<p>鼎盛时期，平均每个cookie有5个url</p>
</blockquote>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2018/03/17/机器学习/BP神经网络/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/03/17/机器学习/BP神经网络/" class="post-title-link" itemprop="url">BP神经网络</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-03-17 20:50:31" itemprop="dateCreated datePublished" datetime="2018-03-17T20:50:31+08:00">2018-03-17</time>
            </span>
          

          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2018/03/17/机器学习/BP神经网络/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/03/17/机器学习/BP神经网络/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://www.cnblogs.com/wjy-lulu/p/6511616.html" target="_blank" rel="noopener">https://www.cnblogs.com/wjy-lulu/p/6511616.html</a></p>
<p><a href="https://www.cnblogs.com/biaoyu/archive/2015/06/20/4591304.html" target="_blank" rel="noopener">https://www.cnblogs.com/biaoyu/archive/2015/06/20/4591304.html</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2018/03/17/机器学习/SVM/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/03/17/机器学习/SVM/" class="post-title-link" itemprop="url">SVM</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-03-17 20:50:31" itemprop="dateCreated datePublished" datetime="2018-03-17T20:50:31+08:00">2018-03-17</time>
            </span>
          

          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2018/03/17/机器学习/SVM/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/03/17/机器学习/SVM/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>线性可分的推导<a href="http://blog.sina.com.cn/s/blog_4298002e010144k8.html" target="_blank" rel="noopener">http://blog.sina.com.cn/s/blog_4298002e010144k8.html</a></p>
<p>线性不可分的时候<a href="http://blog.csdn.net/american199062/article/details/51322852" target="_blank" rel="noopener">http://blog.csdn.net/american199062/article/details/51322852</a></p>
<p>松弛系数</p>
<p><img src="https://pic4.zhimg.com/80/ca45458396bf807868674316793205b7_hd.jpg" alt=""></p>
<p>允许错误的分类，但要付出代价。错分的苹果是大于1，在margin当中但分类正确的在0,1之间。</p>
<p>对于整体的惩罚力度，要另外使用一个参数C来衡量惩罚的程度。</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cmin+%5Cfrac%7B1%7D%7B2%7D%7C%7Cw%7C%7C%5E2+%2B+%5Cfrac%7B%5Cgamma%7D%7B2%7D%5Csum_%7Bn%3D1%7D%5E%7BN%7D%7B%5Cxi_n%5E2%7D+" alt=""></p>
<p>通过核函数可以以低的计算复杂度构造更复杂的分类器，而不用在低维映射到高维。</p>
<p>SMO优化 <a href="https://www.cnblogs.com/pinard/p/6111471.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6111471.html</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2018/03/17/机器学习/CTR预测专题/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/03/17/机器学习/CTR预测专题/" class="post-title-link" itemprop="url">CTR预测专题</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-03-17 20:50:31" itemprop="dateCreated datePublished" datetime="2018-03-17T20:50:31+08:00">2018-03-17</time>
            </span>
          

          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2018/03/17/机器学习/CTR预测专题/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/03/17/机器学习/CTR预测专题/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://www.zhihu.com/question/54009615" target="_blank" rel="noopener">如何评价CTR预估效果？</a></p>
<p><a href="https://www.zhihu.com/question/23652394" target="_blank" rel="noopener">为什么LR可以用来做CTR预估？</a></p>
<p><a href="http://blog.csdn.net/gexyz/article/details/78432192" target="_blank" rel="noopener">关于CTR预测的一个总结</a></p>
<p><a href="http://www.cnblogs.com/iloveyouforever/p/4353491.html" target="_blank" rel="noopener">为什么CTR预估使用AUC来评估模型？</a></p>
<p><a href="http://blog.csdn.net/cserchen/article/details/7535182" target="_blank" rel="noopener">广告计算中的AUC和ROC曲线</a></p>
<p><a href="http://www.cnblogs.com/qcloud1001/p/7513982.html" target="_blank" rel="noopener">常见计算广告点击率预估算法总结</a></p>
<p>关于贝叶斯平滑</p>
<p><a href="http://blog.csdn.net/z363115269/article/details/78637702" target="_blank" rel="noopener">http://blog.csdn.net/z363115269/article/details/78637702</a></p>
<p><a href="http://blog.csdn.net/google19890102/article/details/50492787" target="_blank" rel="noopener">http://blog.csdn.net/google19890102/article/details/50492787</a></p>
<p><a href="http://blog.csdn.net/jinping_shi/article/details/78334362" target="_blank" rel="noopener">http://blog.csdn.net/jinping_shi/article/details/78334362</a></p>
<p><a href="http://blog.csdn.net/wwqwkg6e/article/details/55000216" target="_blank" rel="noopener">http://blog.csdn.net/wwqwkg6e/article/details/55000216</a></p>
<p>贝叶斯平滑的思想是给CTR预设一个<strong>经验初始值</strong>，再通过当前的点击量和曝光量来修正这个初始值。如果某商品的点击量和曝光量都是0，那么该商品的CTR就是这个经验初始值；如果商品A和商品B的曝光量差别很大，那么可以通过这个经验初始值来修正，使得曝光量大的商品的权重增大。</p>
<p>贝叶斯平滑就是确定这个经验值的过程。贝叶斯平滑是基于贝叶斯统计推断的，因此经验值计算的过程依赖于数据的分布情况。</p>
<p>贝叶斯平滑的推导涉及贝叶斯参数估计</p>
<p>ctr能不能加入id类特征？</p>
<p>这里没看懂？为什么ctr越高的分段上权重越大？</p>
<blockquote>
<p>假设一个最简单的问题，预估广告的点击率CTR。为了便于讨论，假设你只有一个特征，就是每次展现广告在过去一个时间窗内的历史点击率ctr，现在目标是预测下一次点击的ctr。简单起见，不妨假设系统中只有两条候选广告。</p>
<p>显而易见，预测分数是和ctr正相关的。如果你使用的是离散LR，那么在分段之后，显然ctr越高的分段上权重越大。这个模型实际跑起来就是最简单的“热门广告”的效果。</p>
</blockquote>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2018/03/17/机器学习/SVD/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/03/17/机器学习/SVD/" class="post-title-link" itemprop="url">SVD</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-03-17 20:50:31" itemprop="dateCreated datePublished" datetime="2018-03-17T20:50:31+08:00">2018-03-17</time>
            </span>
          

          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2018/03/17/机器学习/SVD/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/03/17/机器学习/SVD/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>整理自：</p>
<p><a href="http://www.cnblogs.com/pinard/p/6251584.html" target="_blank" rel="noopener">奇异值分解(SVD)原理与在降维中的应用</a></p>
<p><a href="http://blog.sciencenet.cn/blog-696950-699380.html" target="_blank" rel="noopener">奇异值分解(SVD) —- 线性变换几何意义</a></p>
<p><a href="http://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html" target="_blank" rel="noopener">机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用</a></p>
<h1 id="特征值和特征分解"><a href="#特征值和特征分解" class="headerlink" title="特征值和特征分解"></a>特征值和特征分解</h1><script type="math/tex; mode=display">
Ax=\lambda x</script><p>其中A是一个n×n的矩阵，x是一个n维向量，则我们说<strong>λ</strong>是矩阵A的一个<strong>特征值</strong>，而<strong>x</strong>是矩阵A的特征值λ所对应的<strong>特征向量</strong>。</p>
<p>而从几何上看，A相当于对向量x进行了拉伸，λ是拉伸的尺度。</p>
<p>前提是A是一个对称矩阵。</p>
<blockquote>
<p><strong>对称矩阵</strong></p>
<p>转置后与原矩阵相等。任意矩阵乘以它的转置也是对称矩阵。</p>
</blockquote>
<p>特征分解时，A必须是方阵。</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170105115457425-1545975626.png" alt=""></p>
<p>SVD之后，对于奇异值,它跟我们特征分解中的特征值类似，在奇异值矩阵中也是按照从大到小排列，而且奇异值的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上的比例。也就是说，我们也可以用最大的k个的奇异值和对应的左右奇异向量来近似描述矩阵。</p>
<script type="math/tex; mode=display">
A_{m \times n} = U_{m \times m}\Sigma_{m \times n} V^T_{n \times n} \approx U_{m \times k}\Sigma_{k \times k} V^T_{k \times n}</script><p>这样，矩阵A就可以近似的表示为</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170105140822191-1774139119.png" alt=""></p>
<blockquote>
<p>（文本分析中）</p>
<p>三个矩阵有非常清楚的物理含义：</p>
<ul>
<li>第一个矩阵X中的每一行表示意思相关的一类词，其中的每个非零元素表示这类词中每个词的重要性（或者说相关性），数值越大越相关。</li>
<li>第三个矩阵Y中的每一列表示同一主题一类文章，其中每个元素表示这类文章中每篇文章的相关性。</li>
<li>第二个矩阵B则表示类词和文章之间的相关性。因此，我们只要对关联矩阵A进行一次奇异值分解，我们就可以同时完成了近义词分类和文章的分类。（同时得到每类文章和每类词的相关性）。</li>
</ul>
</blockquote>
<p>SVD的性质</p>
<p>降维</p>
<p>对于奇异值,它跟我们特征分解中的特征值类似，在奇异值矩阵中也是按照从大到小排列，而且奇异值的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上的比例。也就是说，我们也可以用最大的k个的奇异值和对应的左右奇异向量来近似描述矩阵。也就是说：</p>
<script type="math/tex; mode=display">
A_{m \times n} = U_{m \times m}\Sigma_{m \times n} V^T_{n \times n} \approx U_{m \times k}\Sigma_{k \times k} V^T_{k \times n}</script>
          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2018/03/17/总结与思考/从内容-用户画像到如何做算法研发/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/03/17/总结与思考/从内容-用户画像到如何做算法研发/" class="post-title-link" itemprop="url">从内容-用户画像到如何做算法研发</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-03-17 20:50:31" itemprop="dateCreated datePublished" datetime="2018-03-17T20:50:31+08:00">2018-03-17</time>
            </span>
          

          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/总结与思考/" itemprop="url" rel="index"><span itemprop="name">总结与思考</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2018/03/17/总结与思考/从内容-用户画像到如何做算法研发/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/03/17/总结与思考/从内容-用户画像到如何做算法研发/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>转自<a href="http://blog.csdn.net/bitcarmanlee/article/details/77574371" target="_blank" rel="noopener">http://blog.csdn.net/bitcarmanlee/article/details/77574371</a></p>
<p>要求以Spark的Mlib为载体，尽量所有人共用一个算法平台。这样做的好处是大家信息共享会更快，同一个平台也更好维护。比如，算法工程师写了一个巨牛逼的算法原型，然后他需要先给工程师讲懂这个算法，工程师看个人水平，先不说能否将算法实现，实现所花的时间，以及是否真的有时间和精力去帮着实现，实现的是不是有问题就是一个很大的问题了。来回一折腾，两个人都会比较累。</p>
<p><strong>怎么才算对算法有了真正的理解。</strong></p>
<p>首先我们看什么场景用什么算法，但实际用起来，效果并不是那么好。这个时候我们至少需要了解两方面：</p>
<p>算法的核心是什么，有什么潜在的需求？比如是不是对数据的分布做了什么假设么? </p>
<p>特征和数据集的情况是如何的？</p>
<p>而且很多算法做了很多很粗暴的假设，这种假设会导致算法存在一些固有的问题，如果你不了解其内部的这些假设，你会以为这些是他的一个特性，其实是一个缺点。比如Gini Importance，如果你不去了解的内部思想，你在理解数据时，就会造成误解，导致错误的认为先被选中的特征是很重要的，而其余的特征是不重要的，但实际上这些特征对响应变量的作用确实非常接近的。</p>
<p>我有时候觉得，引用算法工程师最流行的一个话，就是tricky。 中文我不知道怎么翻译更合适，很多时候是需要悟性和对事物本质的了解，才能了解一个算法的，绝对不是靠几个公式就能搞定的。</p>
<p>协同算法是我们应用的比较广泛的一个算法。 但是我觉得协同不应该算是一个算法，而是一种模式。 我们常见的很多模型，最后都是协同模式。举个例子来说，是不是个A1用户推荐文章B1,我们可能是这么做的：</p>
<p>把用户用向量做表征，文章也是<br>观察大量的用户A2,A3…AN 是不是有点击该B1<br>使用逻辑回归/SVM等分类算法训练模型<br>把A1,B1丢进模型，得到是否推荐。<br>但事实上这套算法，用的就是协同。为啥的？本质上还是相近的用户做的选择互相推荐。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2018/03/17/机器学习/NLP/Word2vec-C代码/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/03/17/机器学习/NLP/Word2vec-C代码/" class="post-title-link" itemprop="url">Word2Vec源码解读</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-03-17 20:50:31" itemprop="dateCreated datePublished" datetime="2018-03-17T20:50:31+08:00">2018-03-17</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-03-25 17:23:53" itemprop="dateModified" datetime="2018-03-25T17:23:53+08:00">2018-03-25</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2018/03/17/机器学习/NLP/Word2vec-C代码/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/03/17/机器学习/NLP/Word2vec-C代码/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="http://blog.csdn.net/google19890102/article/details/51887344" target="_blank" rel="noopener">机器学习算法实现解析——word2vec源码解析</a></p>
<p>源码在<code>/home/david/code/nlp/word2vec/word2vecC/word2vec.c</code></p>
<p>代码的主要工作包括：</p>
<ul>
<li>预处理。变量声明，全局变量遍历；</li>
<li>构建词库。包括文本处理，以及是否需要有指定词库。</li>
<li>初始化网络结构。参数初始化，Huffman编码的生成。</li>
<li>多线程模型训练。</li>
<li>最终结果的处理。</li>
</ul>
<p>以上的过程，可以用下图表示：</p>
<p><img src="//schwimmer.github.io/2018/03/17/机器学习/NLP/Word2vec-C代码/算法流程.png" alt="算法流程"></p>
<h1 id="输入参数"><a href="#输入参数" class="headerlink" title="输入参数"></a>输入参数</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">-train text8 表示的是输入文件是text8</span><br><span class="line"></span><br><span class="line">-output vectors.bin 输出文件是vectors.bin</span><br><span class="line"></span><br><span class="line">-cbow 1 表示使用cbow模型，默认为Skip-Gram模型</span><br><span class="line"></span><br><span class="line">-size 200 每个单词的向量维度是200</span><br><span class="line"></span><br><span class="line">-window 8 训练的窗口大小为5就是考虑一个词前八个和后八个词语（实际代码中还有一个随机选窗口的过程，窗口大小小于等于8）</span><br><span class="line"></span><br><span class="line">-negative 0 使用ns的时候采样的样本数，默认0，通常是5-10</span><br><span class="line"></span><br><span class="line">-save-vocab 词汇表存储文件</span><br><span class="line"></span><br><span class="line">-read-vocab 词汇表加载文件</span><br><span class="line"></span><br><span class="line">-classes 输出单词类别数，默认为0，即不输出单词</span><br><span class="line"></span><br><span class="line">-hs 1不使用NEG方法，使用HS方法。-</span><br><span class="line"></span><br><span class="line">-sample 亚采样拒绝概率的参数</span><br><span class="line">指的是采样的阈值，如果一个词语在训练样本中出现的频率越大，那么就越会被采样。</span><br><span class="line"></span><br><span class="line">-binary为1指的是结果二进制存储，为0是普通存储（普通存储的时候是可以打开看到词语和对应的向量的）</span><br><span class="line"></span><br><span class="line">-iter 15 迭代次数</span><br></pre></td></tr></table></figure>
<h1 id="全局变量"><a href="#全局变量" class="headerlink" title="全局变量"></a>全局变量</h1><p><code>int *vocab_hash</code> </p>
<p>词在词库中的index，在构建词库时先初始化为-1</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// 词的结构体</span><br><span class="line">struct vocab_word &#123;</span><br><span class="line">        long long cn; // 出现的次数</span><br><span class="line">        int *point; // 从根结点到叶子节点的路径</span><br><span class="line">        char *word, *code, codelen;// 分别对应着词，Huffman编码，编码长度</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>vocab_word是词的结构体</p>
<p><code>vocab = (struct vocab_word *)calloc(vocab_max_size, sizeof(struct vocab_word));</code></p>
<p>vocab存储词</p>
<p><code>vocab_size</code> ：词汇表的总量</p>
<p><code>syn0</code> ：上下文词</p>
<p><code>syn1</code> ：$\theta_{j-1}^w$</p>
<p><code>neu1</code> ：映射层的向量，就是输入层的向量之和</p>
<p><code>neu1e</code> ：对应伪代码中的e</p>
<h1 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h1><p><img src="//schwimmer.github.io/2018/03/17/机器学习/NLP/Word2vec-C代码/sigmoid1.png" alt=""></p>
<p>在预处理部分，对word2vec需要使用的参数进行初始化，在word2vec中是利用传入的方式对参数进行初始化的。</p>
<p>在预处理部分，实现了sigmoid函数值的近似计算。</p>
<p>如果每一次都请求计算sigmoid值，对性能将会有一定的影响，当sigmoid的值对精度的要求并不是非常严格时，可以采用近似计算。在word2vec中，将区间[−6,6]（设置的参数<strong>MAX_EXP</strong>为6）等距离划分成<strong>EXP_TABLE_SIZE</strong>等份，并将每个区间中的sigmoid值计算好存入到数组expTable中，需要使用时，直接从数组中查找。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 申请EXP_TABLE_SIZE+1个空间</span></span><br><span class="line">expTable = (real *)<span class="built_in">malloc</span>((EXP_TABLE_SIZE + <span class="number">1</span>) * <span class="keyword">sizeof</span>(real));</span><br><span class="line">  <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; EXP_TABLE_SIZE; i++) &#123;</span><br><span class="line">    expTable[i] = <span class="built_in">exp</span>((i / (real)EXP_TABLE_SIZE * <span class="number">2</span> - <span class="number">1</span>) * MAX_EXP); </span><br><span class="line">    <span class="comment">// 1/(1+e^6) ~ 1/(1+e^-6)即 0.01 ~ 1 的样子  </span></span><br><span class="line">    expTable[i] = expTable[i] / (expTable[i] + <span class="number">1</span>);                   <span class="comment">// Precompute f(x) = x / (x + 1)</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：在上述代码中，作者使用的是小于EXP_TABLE_SIZE，实际的区间是[−6,6)。</p>
</blockquote>
<h1 id="构建词库"><a href="#构建词库" class="headerlink" title="构建词库"></a>构建词库</h1><p>在word2vec源码中，提供了两种构建词库的方法，分别为：</p>
<ul>
<li>指定词库：ReadVocab()方法</li>
<li>从词的文本构建词库：LearnVocabFromTrainFile()方法</li>
</ul>
<h2 id="构建词库的过程"><a href="#构建词库的过程" class="headerlink" title="构建词库的过程"></a>构建词库的过程</h2><p>在这里，我们以从词的文本构建词库为例。构建词库的过程如下所示：<img src="//schwimmer.github.io/2018/03/17/机器学习/NLP/Word2vec-C代码/构建词库.png" alt="构建词库"></p>
<p>在这部分中，最主要的工作是对文本进行处理，包括低频词的处理，hash表的处理等等。首先，会在词库中增加一个“&lt;/s&gt;”的词，同时，在读取文本的过程中，将换行符“\n”也表示成该该词</p>
<h2 id="对词的哈希处理"><a href="#对词的哈希处理" class="headerlink" title="对词的哈希处理"></a>对词的哈希处理</h2><p>在存储词的过程中，同时保留这两个数组：</p>
<ul>
<li>存储词的vocab</li>
<li>存储词的hash的vocab_hash</li>
</ul>
<p>其中，在vocab中，存储的是词对应的结构体：</p>
<p>在vocab_hash中存储的是词在词库中的Index，vocab_hash的下标是词计算出的hash值。</p>
<p>在对词的处理过程中，主要包括：</p>
<ul>
<li>计算词的hash值：</li>
</ul>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 取词的hash值</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">GetWordHash</span><span class="params">(<span class="keyword">char</span> *word)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">unsigned</span> <span class="keyword">long</span> <span class="keyword">long</span> a, hash = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (a = <span class="number">0</span>; a &lt; <span class="built_in">strlen</span>(word); a++) hash = hash * <span class="number">257</span> + word[a];</span><br><span class="line">        hash = hash % vocab_hash_size;</span><br><span class="line">        <span class="keyword">return</span> hash;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>SearchVocab检索词是否存在。如不存在则返回-1，否则，返回该词在词库中的索引：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">while (1) &#123;</span><br><span class="line">    if (vocab_hash[hash] == -1) return -1;// 不存在该词</span><br><span class="line">    //strcmp两个词相等，则返回0，所以要加上!</span><br><span class="line">    if (!strcmp(word, vocab[vocab_hash[hash]].word)) return vocab_hash[hash];// 返回索引值</span><br><span class="line">    hash = (hash + 1) % vocab_hash_size;// 处理冲突</span><br><span class="line">&#125;</span><br><span class="line">return -1;// 不存在该词</span><br></pre></td></tr></table></figure>
<p>在这个过程中，使用到了线性探测的开放定址法处理冲突，开放定址法就是一旦发生冲突，就去寻找下一个空的散列地址。</p>
<ul>
<li>不存在，则插入新词。</li>
</ul>
<h2 id="对低频词的处理"><a href="#对低频词的处理" class="headerlink" title="对低频词的处理"></a>对低频词的处理</h2><p>在循环读取每一个词的过程中，当出现“vocab_size &gt; vocab_hash_size * 0.7”时，需要对低频词进行处理。其中，vocab_size表示的是目前词库中词的个数，vocab_hash_size表示的是初始设定的hash表的大小。</p>
<p><code>ReduceVocab()</code></p>
<p>在处理低频词的过程中，通过参数“min_reduce”来控制，若词出现的次数小于等于该值时，则从词库中删除该词。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">for (a = 0; a &lt; vocab_size; a++) if (vocab[a].cn &gt; min_reduce) &#123;</span><br><span class="line">		vocab[b].cn = vocab[a].cn;</span><br><span class="line">		vocab[b].word = vocab[a].word;</span><br><span class="line">		b++;</span><br><span class="line">	&#125; else free(vocab[a].word);</span><br><span class="line">	vocab_size = b;// 删减后词的个数</span><br></pre></td></tr></table></figure>
<p>在删除了低频词后，需要重新对词库中的词进行hash值的计算。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">for (a = 0; a &lt; vocab_hash_size; a++) vocab_hash[a] = -1;</span><br><span class="line">for (a = 0; a &lt; vocab_size; a++) &#123;</span><br><span class="line">  // Hash will be re-computed, as it is not actual</span><br><span class="line">  hash = GetWordHash(vocab[a].word);</span><br><span class="line">  while (vocab_hash[hash] != -1) hash = (hash + 1) % vocab_hash_size;</span><br><span class="line">  vocab_hash[hash] = a;</span><br><span class="line">&#125;</span><br><span class="line">fflush(stdout);</span><br><span class="line">min_reduce++;</span><br></pre></td></tr></table></figure>
<h2 id="根据词频对词库中的词排序"><a href="#根据词频对词库中的词排序" class="headerlink" title="根据词频对词库中的词排序"></a>根据词频对词库中的词排序</h2><p>基于以上的过程，程序已经将词从文件中提取出来，并存入到指定的词库中（vocab数组），接下来，需要根据每一个词的词频对词库中的词按照词频从大到小排序，其基本过程在函数SortVocab中，排序过程为</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">qsort(&amp;vocab[1], vocab_size - 1, sizeof(struct vocab_word), VocabCompare);</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>原</strong> <strong>型</strong>:void qsort(void <em>base, int nelem, int width, int (</em>fcmp)(const void <em>,const void </em>));</p>
<p><strong>功</strong> <strong>能</strong>: 使用快速排序例程进行排序</p>
<p>参 数：</p>
<p>1 待排序数组首地址</p>
<p>2 数组中待排序元素数量</p>
<p>3 各元素的占用空间大小</p>
<p>4 指向函数的指针，用于确定排序的顺序</p>
<p><strong>说</strong> <strong>明：</strong>qsort函数是ANSI C标准中提供的，其声明在stdlib.h文件中，是根据二分法写的，其时间复杂度为n*log(n)。</p>
<p>qsort要求提供的函数是需要自己定义的一个比较函数，比较函数使得qsort通用性更好。有了比较函数qsort可以实现对数组、字符串、结构体等结构进行升序或降序排序。<br>如int cmp(const void <em>a, const void </em>b)中有两个元素作为参数（参数的格式不能变的。）返回一个int值，如果比较函数返回大于0，qsort就认为a &gt; b，返回小于0,qsort就认为a &lt; b。qsort知道元素的大小了，就可以把大的放前面去。如果你的比较函数返回本来应该是1的（即a &gt; b），而却返回-1（小于0的数），那么qsort认为a &lt; b，就把b放在前面去，但实际上是a &gt; b的，所以就造成了降序排序的差别了。简单来说，比较函数的作用就是给qsort指明元素的大小事怎么比较的。</p>
</blockquote>
<p>保持字符“&lt; \s&gt;”在最开始的位置。排序后，根据“min_count”对低频词进行处理，与上述一样，再对剩下的词重新计算hash值。</p>
<p>至此，整个对词的处理过程就已经结束了。接下来，将是对网络结构的处理和词向量的训练。</p>
<h1 id="初始化网络结构"><a href="#初始化网络结构" class="headerlink" title="初始化网络结构"></a>初始化网络结构</h1><p>有了以上的对词的处理，就已经处理好了所有的训练样本，此时，便可以开始网络结构的初始化和接下来的网络训练。网络的初始化的过程在<code>InitNet()</code>函数中完成。</p>
<h2 id="初始化网络参数"><a href="#初始化网络参数" class="headerlink" title="初始化网络参数"></a>初始化网络参数</h2><p>在初始化的过程中，主要的参数包括<strong>词向量的初始化</strong>和<strong>映射层到输出层的权重的初始化</strong>，如下图所示：</p>
<p><img src="http://img.blog.csdn.net/20170227183342463?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ29vZ2xlMTk4OTAxMDI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></p>
<p>词向量的初始化：为每个词分配空间，大小是vocab_size*layer1_size。</p>
<h6 id="初始化的时候要分配所有词-词向量长度的空间？为何要这么大？"><a href="#初始化的时候要分配所有词-词向量长度的空间？为何要这么大？" class="headerlink" title="初始化的时候要分配所有词*词向量长度的空间？为何要这么大？"></a>初始化的时候要分配所有词*词向量长度的空间？为何要这么大？</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// layer1_size是词向量的长度</span><br><span class="line">a = posix_memalign((void **)&amp;syn0, 128, (long long)vocab_size * layer1_size * sizeof(real));</span><br></pre></td></tr></table></figure>
<blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; int posix_memalign (void **memptr,</span><br><span class="line">&gt;                     size_t alignment,</span><br><span class="line">&gt;                     size_t size);</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p>&gt;</p>
<blockquote>
<p>调用<em>posix_memalign( )</em>成功时会返回<em>size</em>字节的动态内存，并且这块内存的地址是<em>alignment</em>的倍数。参数<em>alignment</em>必须是2的幂，还是<em>void</em>指针的大小的倍数。返回的内存块的地址放在了<em>memptr</em>里面，函数返回值是<em>0</em>。</p>
</blockquote>
<p>CBOW网络有两种可选的算法：层次Softmax和Negative Sampling。在输入参数时选择任意一种。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 层次softmax的结构</span></span><br><span class="line">	<span class="keyword">if</span> (hs) &#123;</span><br><span class="line">		<span class="comment">// 映射层到输出层之间的权重，就是Huffman树的非叶子结点的向量θ</span></span><br><span class="line">		a = posix_memalign((<span class="keyword">void</span> **)&amp;syn1, <span class="number">128</span>, (<span class="keyword">long</span> <span class="keyword">long</span>)vocab_size * layer1_size * <span class="keyword">sizeof</span>(real));</span><br><span class="line">		<span class="keyword">if</span> (syn1 == <span class="literal">NULL</span>) &#123;<span class="built_in">printf</span>(<span class="string">"Memory allocation failed\n"</span>); <span class="built_in">exit</span>(<span class="number">1</span>);&#125;</span><br><span class="line">		<span class="keyword">for</span> (a = <span class="number">0</span>; a &lt; vocab_size; a++) <span class="keyword">for</span> (b = <span class="number">0</span>; b &lt; layer1_size; b++)</span><br><span class="line">			syn1[a * layer1_size + b] = <span class="number">0</span>;<span class="comment">// 权重初始化为0</span></span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">// 负采样的结构</span></span><br><span class="line">	<span class="keyword">if</span> (negative&gt;<span class="number">0</span>) &#123;</span><br><span class="line">		a = posix_memalign((<span class="keyword">void</span> **)&amp;syn1neg, <span class="number">128</span>, (<span class="keyword">long</span> <span class="keyword">long</span>)vocab_size * layer1_size * <span class="keyword">sizeof</span>(real));</span><br><span class="line">		<span class="keyword">if</span> (syn1neg == <span class="literal">NULL</span>) &#123;<span class="built_in">printf</span>(<span class="string">"Memory allocation failed\n"</span>); <span class="built_in">exit</span>(<span class="number">1</span>);&#125;</span><br><span class="line">		<span class="keyword">for</span> (a = <span class="number">0</span>; a &lt; vocab_size; a++) <span class="keyword">for</span> (b = <span class="number">0</span>; b &lt; layer1_size; b++)</span><br><span class="line">			syn1neg[a * layer1_size + b] = <span class="number">0</span>;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>
<p>在初始化的过程中，<strong>映射层到输出层的权重都初始化为0</strong>，而对于每一个词向量的初始化，作者的初始化方法如下代码所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// 随机初始化</span><br><span class="line">	for (a = 0; a &lt; vocab_size; a++) for (b = 0; b &lt; layer1_size; b++) &#123;</span><br><span class="line">		next_random = next_random * (unsigned long long)25214903917 + 11;</span><br><span class="line">		// 1、与：相当于将数控制在一定范围内</span><br><span class="line">		// 2、0xFFFF：65536</span><br><span class="line">		// 3、/65536：[0,1]之间</span><br><span class="line">		syn0[a * layer1_size + b] = (((next_random &amp; 0xFFFF) / (real)65536) - 0.5) / layer1_size;// 初始化词向量</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>
<p>首先，生成一个很大的next_random的数，通过与“0xFFFF”进行与运算截断，再除以65536得到[0,1]之间的数，最终，得到的初始化的向量的范围为：[−0.5/m,0.5/m]，其中，m为词向量的长度。</p>
<h6 id=""><a href="#" class="headerlink" title=" "></a> </h6><h2 id="3-2、Huffman树的构建"><a href="#3-2、Huffman树的构建" class="headerlink" title="3.2、Huffman树的构建"></a>3.2、Huffman树的构建</h2><p>在层次Softmax中需要使用到Huffman树以及Huffman编码，因此，在网络结构的初始化过程中，也需要初始化Huffman树。在生成Huffman树的过程中，首先定义了3个长度为vocab_size*2+1的数组：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// 申请2倍的词的空间</span><br><span class="line">long long *count = (long long *)calloc(vocab_size * 2 + 1, sizeof(long long));</span><br><span class="line">long long *binary = (long long *)calloc(vocab_size * 2 + 1, sizeof(long long));</span><br><span class="line">long long *parent_node = (long long *)calloc(vocab_size * 2 + 1, sizeof(long long));</span><br></pre></td></tr></table></figure>
<p>其中，count数组中前vocab_size存储的是每一个词的对应的词频，词频是从高到低排序。后面的vocab_size先初始化为很大的数。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">// 分成两半进行初始化</span><br><span class="line">for (a = 0; a &lt; vocab_size; a++) count[a] = vocab[a].cn;// 前半部分初始化为每个词出现的次数</span><br><span class="line">for (a = vocab_size; a &lt; vocab_size * 2; a++) count[a] = 1e15;// 后半部分初始化为一个固定的常数</span><br></pre></td></tr></table></figure>
<p>构建Huffman树的过程如下所示</p>
<p><img src="http://img.blog.csdn.net/20170224145522192?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ29vZ2xlMTk4OTAxMDI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></p>
<p>首先，设置两个指针pos1和pos2，分别指向最后一个词和最后一个词的后一位</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// 两个指针：</span><br><span class="line">// pos1指向前半截的尾部</span><br><span class="line">// pos2指向后半截的开始</span><br><span class="line">pos1 = vocab_size - 1;</span><br><span class="line">pos2 = vocab_size;</span><br></pre></td></tr></table></figure>
<p>从两个指针所指的数中选择出最小的值，记为min1i，</p>
<p>如pos1所指的值最小，此时，将pos1左移，再比较pos1和pos2所指的数，选择出最小的值，记为min2i，将他们的和存储到pos2所指的位置。并将此时pos2所指的位置设置为min1i和min2i的父节点，同时，记min2i所指的位置的编码为1（这里令右子树的编码为1）。</p>
<p>如pos2所指的值小，此时，将pos2右移，再比较pos1和pos2，选出最小的值，记为min2i，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">// Following algorithm constructs the Huffman tree by adding one node at a time</span><br><span class="line">	// 每次增加一个节点，构建Huffman树</span><br><span class="line">	for (a = 0; a &lt; vocab_size - 1; a++) &#123;</span><br><span class="line">		// First, find two smallest nodes &apos;min1, min2&apos;</span><br><span class="line">		// 选择最小的节点min1</span><br><span class="line">		if (pos1 &gt;= 0) &#123;</span><br><span class="line">			if (count[pos1] &lt; count[pos2]) &#123;</span><br><span class="line">				min1i = pos1;</span><br><span class="line">				pos1--;</span><br><span class="line">			&#125; else &#123;</span><br><span class="line">				min1i = pos2;</span><br><span class="line">				pos2++;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125; else &#123;</span><br><span class="line">			min1i = pos2;</span><br><span class="line">			pos2++;</span><br><span class="line">		&#125;</span><br><span class="line">		// 选择最小的节点min2</span><br><span class="line">		if (pos1 &gt;= 0) &#123;</span><br><span class="line">			if (count[pos1] &lt; count[pos2]) &#123;</span><br><span class="line">				min2i = pos1;</span><br><span class="line">				pos1--;</span><br><span class="line">			&#125; else &#123;</span><br><span class="line">				min2i = pos2;</span><br><span class="line">				pos2++;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125; else &#123;</span><br><span class="line">			min2i = pos2;</span><br><span class="line">			pos2++;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		count[vocab_size + a] = count[min1i] + count[min2i];</span><br><span class="line">		// 设置父节点</span><br><span class="line">		parent_node[min1i] = vocab_size + a;</span><br><span class="line">		parent_node[min2i] = vocab_size + a;</span><br><span class="line">		binary[min2i] = 1;// 设置一个子树的编码为1</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>
<p>构建好Huffman树后，此时，需要根据构建好的Huffman树生成对应节点的Huffman编码。假设，上述的数据生成的最终的Huffman树为：</p>
<p><img src="http://img.blog.csdn.net/20170228154343740?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ29vZ2xlMTk4OTAxMDI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></p>
<p>此时，count数组，binary数组和parent_node数组分别为：</p>
<p><img src="http://img.blog.csdn.net/20170228160752188?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ29vZ2xlMTk4OTAxMDI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></p>
<p>在生成Huffman编码的过程中，针对每一个词（词都在叶子节点上），从叶子节点开始，将编码存入到code数组中，如对于上图中的“R”节点来说，其code数组为{1,0}，再对其反转便是Huffman编码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">// Now assign binary code to each vocabulary word</span><br><span class="line">	// 为每一个词分配二进制编码，即Huffman编码</span><br><span class="line">	for (a = 0; a &lt; vocab_size; a++) &#123;// 针对每一个词</span><br><span class="line">		b = a;</span><br><span class="line">		i = 0;</span><br><span class="line">		while (1) &#123;</span><br><span class="line">			code[i] = binary[b];// 找到当前的节点的编码</span><br><span class="line">			point[i] = b;// 记录从叶子节点到根结点的序列</span><br><span class="line">			i++;</span><br><span class="line">			b = parent_node[b];// 找到当前节点的父节点</span><br><span class="line">			if (b == vocab_size * 2 - 2) break;// 已经找到了根结点，根节点是没有编码的</span><br><span class="line">		&#125;</span><br><span class="line">		vocab[a].codelen = i;// 词的编码长度</span><br><span class="line">		vocab[a].point[0] = vocab_size - 2;// 根结点</span><br><span class="line">		for (b = 0; b &lt; i; b++) &#123;</span><br><span class="line">			vocab[a].code[i - b - 1] = code[b];// 编码的反转</span><br><span class="line">			vocab[a].point[i - b] = point[b] - vocab_size;// 记录的是从根结点到叶子节点的路径</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>
<h2 id="3-3、负样本选中表的初始化"><a href="#3-3、负样本选中表的初始化" class="headerlink" title="3.3、负样本选中表的初始化"></a>3.3、负样本选中表的初始化</h2><p>（自己没看）</p>
<p>如果是采用负采样的方法，此时还需要初始化每个词被选中的概率。在所有的词构成的词典中，每一个词出现的频率有高有低，我们希望，对于那些高频的词，被选中成为负样本的概率要大点，同时，对于那些出现频率比较低的词，我们希望其被选中成为负样本的频率低点。这个原理于“轮盘赌”的策略一致（详细可以参见“<a href="http://blog.csdn.net/google19890102/article/details/45622307" target="_blank" rel="noopener">优化算法——遗传算法</a>”）。在程序中，实现这部分功能的代码为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">// 生成负采样的概率表</span><br><span class="line">void InitUnigramTable() &#123;</span><br><span class="line">        int a, i;</span><br><span class="line">        double train_words_pow = 0;</span><br><span class="line">        double d1, power = 0.75;</span><br><span class="line">        table = (int *)malloc(table_size * sizeof(int));// int --&gt; int</span><br><span class="line">        for (a = 0; a &lt; vocab_size; a++) train_words_pow += pow(vocab[a].cn, power);</span><br><span class="line">        // 类似轮盘赌生成每个词的概率</span><br><span class="line">        i = 0;</span><br><span class="line">        d1 = pow(vocab[i].cn, power) / train_words_pow;</span><br><span class="line">        for (a = 0; a &lt; table_size; a++) &#123;</span><br><span class="line">                table[a] = i;</span><br><span class="line">                if (a / (double)table_size &gt; d1) &#123;</span><br><span class="line">                        i++;</span><br><span class="line">                        d1 += pow(vocab[i].cn, power) / train_words_pow;</span><br><span class="line">                &#125;</span><br><span class="line">                if (i &gt;= vocab_size) i = vocab_size - 1;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在实现的过程中，没有直接使用每一个词的频率，而是使用了词的0.75次方。</p>
<h1 id="4、多线程模型训练"><a href="#4、多线程模型训练" class="headerlink" title="4、多线程模型训练"></a>4、多线程模型训练</h1><p>以上的各个部分是为训练词向量做准备，即准备训练数据，构建训练模型。在上述的初始化完成后，接下来就是根据不同的方法对模型进行训练，在实现的过程中，作者使用了多线程的方法对其进行训练。</p>
<h2 id="4-1、多线程的处理"><a href="#4-1、多线程的处理" class="headerlink" title="4.1、多线程的处理"></a>4.1、多线程的处理</h2><p>为了能够对文本进行加速训练，在实现的过程中，作者使用了多线程的方法，TrainModelThread()</p>
<p>对每一个线程上分配指定大小的文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// 利用多线程对训练文件划分，每个线程训练一部分的数据</span><br><span class="line">fseek(fi, file_size / (long long)num_threads * (long long)id, SEEK_SET);</span><br></pre></td></tr></table></figure>
<p>这个过程可以通过下图简单的描述：</p>
<p><img src="http://img.blog.csdn.net/20170301130559630?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ29vZ2xlMTk4OTAxMDI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></p>
<p>在实现多线程的过程中，作者并没有加锁的操作，而是对模型参数和词向量的修改可以任意执行，这一点类似于基于随机梯度的方法，训练的过程与训练样本的训练是没有关系的，这样可以大大加快对词向量的训练。抛开多线程的部分，在每一个线程内执行的是对模型和词向量的训练。</p>
<p>作者在实现的过程中，主要实现了两个模型，即CBOW模型和Skip-gram模型，在每个模型中，又分别使用到了两种不同的训练方法，即层次Softmax和Negative Sampling方法。</p>
<h2 id="4-2、CBOW模型"><a href="#4-2、CBOW模型" class="headerlink" title="4.2、CBOW模型"></a>4.2、CBOW模型</h2><h3 id="4-2-1、从输入层到映射层"><a href="#4-2-1、从输入层到映射层" class="headerlink" title="4.2.1、从输入层到映射层"></a>4.2.1、从输入层到映射层</h3><p>首先找到每个词对应的词向量，并将这些词的词向量相加，程序代码如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">// in -&gt; hidden</span><br><span class="line">cw = 0;</span><br><span class="line">//b是随机生成的0到window-1，相当于左右各看window-b/2个词</span><br><span class="line">for (a = b; a &lt; window * 2 + 1 - b; a++) if (a != window) &#123;</span><br><span class="line">  //sentence_position 单词在句子中的位置</span><br><span class="line">  c = sentence_position - window + a;</span><br><span class="line">  // 判断c是否越界</span><br><span class="line">  if (c &lt; 0) continue;</span><br><span class="line">  if (c &gt;= sentence_length) continue;</span><br><span class="line">  // 找到c对应的索引</span><br><span class="line">  last_word = sen[c];</span><br><span class="line">  if (last_word == -1) continue;</span><br><span class="line">  // neu1就是隐藏层向量，也就是上下文对应vector的和</span><br><span class="line">  for (c = 0; c &lt; layer1_size; c++) neu1[c] += syn0[c + last_word * layer1_size];</span><br><span class="line">  cw++;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>当累加完窗口内的所有的词向量的之后，存储在映射层neu1中，并取平均，程序代码如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">for (c = 0; c &lt; layer1_size; c++) neu1[c] /= cw;</span><br></pre></td></tr></table></figure>
<p>当取得了映射层的结果后，此时就需要使用Hierarchical Softmax或者Negative Sampling对模型进行训练。</p>
<h3 id="4-2-2、Hierarchical-Softmax"><a href="#4-2-2、Hierarchical-Softmax" class="headerlink" title="4.2.2、Hierarchical Softmax"></a>4.2.2、Hierarchical Softmax</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">if (hs) for (d = 0; d &lt; vocab[word].codelen; d++) &#123;</span><br><span class="line">          f = 0;</span><br><span class="line">          // point存储了从该词的叶子结点的编号到root的序号，这些序号可以对应到syn1的位置，也就是参数向量的位置</span><br><span class="line">          l2 = vocab[word].point[d] * layer1_size;</span><br><span class="line">          // Propagate hidden -&gt; output</span><br><span class="line">          // q=sigma(x*theta)</span><br><span class="line">          for (c = 0; c &lt; layer1_size; c++) f += neu1[c] * syn1[c + l2];</span><br><span class="line">          if (f &lt;= -MAX_EXP) continue;</span><br><span class="line">          else if (f &gt;= MAX_EXP) continue;</span><br><span class="line">          // 查表得知sigma的值，省去计算的时间</span><br><span class="line">          else f = expTable[(int)((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))];</span><br><span class="line">          // g = eta(1-d-q)</span><br><span class="line">          g = (1 - vocab[word].code[d] - f) * alpha;</span><br><span class="line">          // Propagate errors output -&gt; hidden</span><br><span class="line">          // e = e + g*theta</span><br><span class="line">          for (c = 0; c &lt; layer1_size; c++) neu1e[c] += g * syn1[c + l2];</span><br><span class="line">          // Learn weights hidden -&gt; output</span><br><span class="line">          // theta = theta + g*x</span><br><span class="line">          for (c = 0; c &lt; layer1_size; c++) syn1[c + l2] += g * neu1[c];</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<p>接下来更新Context(w)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// hidden -&gt; in</span><br><span class="line">        for (a = b; a &lt; window * 2 + 1 - b; a++) if (a != window) &#123;</span><br><span class="line">          c = sentence_position - window + a;</span><br><span class="line">          if (c &lt; 0) continue;</span><br><span class="line">          if (c &gt;= sentence_length) continue;</span><br><span class="line">          last_word = sen[c];</span><br><span class="line">          if (last_word == -1) continue;</span><br><span class="line">          for (c = 0; c &lt; layer1_size; c++) syn0[c + last_word * layer1_size] += neu1e[c];</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<h1 id="Word2Vec为什么快"><a href="#Word2Vec为什么快" class="headerlink" title="Word2Vec为什么快"></a>Word2Vec为什么快</h1><ul>
<li>用查表代替计算sigmoid</li>
<li>相对于神经网络的结构，去掉了隐藏层</li>
</ul>
<h1 id="增量训练"><a href="#增量训练" class="headerlink" title="增量训练"></a>增量训练</h1><p>从搜索引擎爬包含新词的文本，加上一个小语料，训练一个w2v模型。</p>
<p>对于每个新词，找出小模型中最接近的10个词，以及每个词与新词的相似度打分score。</p>
<p>再从大模型中找出每个词的词向量，每个维度乘以小模型中的score，最多叠加5个。再对每个维度取加权平均。</p>
<p>最后转成单位向量。</p>
<p>参考</p>
<p>【1】<a href="http://blog.csdn.net/google19890102/article/details/51887344" target="_blank" rel="noopener">http://blog.csdn.net/google19890102/article/details/51887344</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2018/03/17/机器学习/NLP/Word2Vec原理/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/03/17/机器学习/NLP/Word2Vec原理/" class="post-title-link" itemprop="url">Word2Vec原理</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-03-17 20:50:31" itemprop="dateCreated datePublished" datetime="2018-03-17T20:50:31+08:00">2018-03-17</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-03-25 12:46:10" itemprop="dateModified" datetime="2018-03-25T12:46:10+08:00">2018-03-25</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2018/03/17/机器学习/NLP/Word2Vec原理/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/03/17/机器学习/NLP/Word2Vec原理/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>参考</p>
<p>《word2vec数学原理.pdf》</p>
<h1 id="词向量基础"><a href="#词向量基础" class="headerlink" title="词向量基础"></a>词向量基础</h1><p>​    用词向量来表示词并不是word2vec的首创，在很久之前就出现了。最早的词向量是很冗长的，它使用是词向量维度大小为整个词汇表的大小，对于每个具体的词汇表中的词，将对应的位置置为1。比如我们有下面的5个词组成的词汇表，词”Queen”的序号为2， 那么它的词向量就是(0,1,0,0,0)。同样的道理，词”Woman”的词向量就是(0,0,0,1,0)。这种词向量的编码方式我们一般叫做1-of-N representation或者one hot representation.</p>
<p><img src="//schwimmer.github.io/2018/03/17/机器学习/NLP/Word2Vec原理/one hot representation.png" alt="one hot representation"></p>
<p>​    One hot representation用来表示词向量非常简单，但是却有很多问题。最大的问题是我们的词汇表一般都非常大，比如达到百万级别，这样每个词都用百万维的向量来表示简直是内存的灾难。这样的向量其实除了一个位置是1，其余的位置全部都是0，表达的效率不高，能不能把词向量的维度变小呢？</p>
<p>​    Dristributed representation可以解决One hot representation的问题，它的思路是通过训练，将每个词都映射到一个较短的词向量上来。所有的这些词向量就构成了向量空间，进而可以用普通的统计学的方法来研究词与词之间的关系。这个较短的词向量维度是多大呢？这个一般需要我们在训练时自己来指定。</p>
<p>​    比如下图我们将词汇表里的词用”Royalty”,”Masculinity”, “Femininity”和”Age”4个维度来表示，King这个词对应的词向量可能是(0.99,0.99,0.05,0.7)(0.99,0.99,0.05,0.7)。当然在实际情况中，我们并不能对词向量的每个维度做一个很好的解释。</p>
<p><img src="//schwimmer.github.io/2018/03/17/机器学习/NLP/Word2Vec原理/Dristributed representation.png" alt="Dristributed representation"></p>
<p>​    有了用Dristributed representation表示的较短的词向量，我们就可以较容易的分析词之间的关系了，比如我们将词的维度降维到2维，有一个有趣的研究表明，用下图的词向量表示我们的词时，我们可以发现：</p>
<script type="math/tex; mode=display">
\vec {King} - \vec {Man} + \vec {Woman} = \vec {Queen}</script><p><img src="//schwimmer.github.io/2018/03/17/机器学习/NLP/Word2Vec原理/vector relation.png" alt="vector relation"></p>
<p>可见我们只要得到了词汇表里所有词对应的词向量，那么我们就可以做很多有趣的事情了。不过，怎么训练得到合适的词向量呢？一个很常见的方法是使用神经网络语言模型。</p>
<h1 id="2-CBOW与Skip-Gram用于神经网络语言模型"><a href="#2-CBOW与Skip-Gram用于神经网络语言模型" class="headerlink" title="2. CBOW与Skip-Gram用于神经网络语言模型"></a>2. CBOW与Skip-Gram用于神经网络语言模型</h1><p>​    在word2vec出现之前，已经有用神经网络DNN来用训练词向量进而处理词与词之间的关系了。采用的方法一般是一个三层的神经网络结构（当然也可以多层），分为输入层，隐藏层和输出层(softmax层)。</p>
<p>​    这个模型是如何定义数据的输入和输出呢？一般分为CBOW（Continuous Bag-of-Words） 与Skip-Gram两种模型。</p>
<h2 id="2-1-CBOW"><a href="#2-1-CBOW" class="headerlink" title="2.1 CBOW"></a>2.1 CBOW</h2><p>上下文相关的词对应的词向量，而输出就是这特定的一个词的词向量。比如下面这段话，我们的上下文大小取值为4，特定的这个词是”Learning”，也就是我们需要的输出词向量,上下文对应的词有8个，前后各4个，这8个词是我们模型的输入。由于CBOW使用的是词袋模型，因此这8个词都是平等的，也就是不考虑他们和我们关注的词之间的距离大小，只要在我们上下文之内即可。</p>
<p><img src="//schwimmer.github.io/2018/03/17/机器学习/NLP/Word2Vec原理/cbow.png" alt="cbow"></p>
<p>在这个CBOW神经网络模型中，输入层有8个神经元（8个词向量），输出层有词汇表D大小的神经元。</p>
<p>目标函数通常为</p>
<script type="math/tex; mode=display">
L=\sum_{w \in C} \log p(w|Context(w))</script><blockquote>
<p> 为什么是这个形式？</p>
<p> 根据n-gram模型和对数最大似然，得到这个目标函数。</p>
</blockquote>
<p>包括四个层：输入、投影、隐藏、输出。</p>
<p><img src="//schwimmer.github.io/2018/03/17/机器学习/NLP/Word2Vec原理/神经网络层.png" alt="神经网络层"></p>
<p>对于语料C中的任意一个词w，将Context(w)设为取前面的n-1个词，这样二元对(Context(w),w)就是一个训练样本。</p>
<p>投影层向量$x_w$的构造是，将输入层的n-1个词向量按顺序首尾相接的拼起来，长度就是m(n-1)了（每个词向量的长度是m）。</p>
<p>从而</p>
<script type="math/tex; mode=display">
z_w = tanh(W{x_w}+p) \\
y_w=Uz_w+q</script><p>其中，tanh是双曲正切函数，用来做隐藏层的激活函数。</p>
<p>经过上面两步计算得到的$y<em>w=(y</em>{w,1},y<em>{w,2},…,y</em>{w,N})^T$是一个长度为N的向量，其分量不能代表概率。如果想要$y_{w,i}$表示当上下文为Context(w)时下一次为词典D中第i个词的概率，则还需要做一个<strong>softmax归一化</strong>，之后得到</p>
<script type="math/tex; mode=display">
p(w|Context(w))=\frac {e^{y_{w,i_w}}} {\sum_{i=1}^N e^{y_{w,i_w}}}</script><p>其中$i_w$表示词w在词典D中的索引。</p>
<p>与n-gram相比，神经概率语言模型的<strong>优势</strong>是：</p>
<p>1、词语之间的相似性可以通过词向量来体现。</p>
<p>1）神经网络模型通过上下文来预测，那么相似的上下文的词的词向量也是相似的；</p>
<p>2）概率函数关于词向量是光滑的，即词向量的一个小变化对概率的影响也是一个小变化。</p>
<p>2、词向量自带平滑功能（因为$p(w|Context(w)) \in (0,1)$不会为零）。</p>
<h2 id="2-2-Skip-Gram"><a href="#2-2-Skip-Gram" class="headerlink" title="2.2 Skip-Gram"></a>2.2 Skip-Gram</h2><p>与CBOW相反，输入是一个特定向量，输出是特定词对应的上下文词向量。</p>
<h1 id="3、基于Hierarchical-Softmax的模型"><a href="#3、基于Hierarchical-Softmax的模型" class="headerlink" title="3、基于Hierarchical Softmax的模型"></a>3、基于Hierarchical Softmax的模型</h1><h2 id="3-1-CBOW模型"><a href="#3-1-CBOW模型" class="headerlink" title="3.1 CBOW模型"></a>3.1 CBOW模型</h2><p>网络的构建</p>
<p><img src="//schwimmer.github.io/2018/03/17/机器学习/NLP/Word2Vec原理/cbow_network.png" alt="cbow_network"></p>
<p>输入层是上下文的若干个词的词向量</p>
<p>投影层就是将这些词向量直接相加。</p>
<p>层次Softmax的基本思想就是：</p>
<p><strong>对于词典D中的任意词w，Huffman树中必存在一条从根结点到词w对应结点的路径$p^w$，且这条路径是唯一的。路径$p^w$上存在$l^w-1$个分支，将每个分支看做是一个二分类，每一次分类就产生一个概率，将这些概率连乘起来，就是所需的$p(w|Context(w))$ 。</strong></p>
<p>条件概率连乘的公示可以写为</p>
<script type="math/tex; mode=display">
p(w|Context(w))=\prod_{j=2}^{l^w}p(d_j^w|x_w,\theta_{j-1}^w)</script><p>其中，</p>
<script type="math/tex; mode=display">
p(d_j^w|x_w,\theta_{j-1}^w)=[\sigma(x_w^T\theta_{j-1}^w)]^{1-d_j^w} \cdot [1-\sigma(x_w^T\theta_{j-1}^w)]^{d_j^w} \tag{3-1}</script><p>这里$\sigma(x<em>w^T\theta</em>{j-1}^w)$表示分到正类的概率。</p>
<p>将3-1代入对数似然函数，得到</p>
<script type="math/tex; mode=display">
L=\sum_{w \in C} \sum_{j=2}^{l^w} \{ (1-d_j^w)\log [\sigma(x_w^T\theta_{j-1}^w)] + d_j^w \log[1-\sigma(x_w^T\theta_{j-1}^w)] \}</script><p>其中，令</p>
<script type="math/tex; mode=display">
L(w,j)=(1-d_j^w)\log [\sigma(x_w^T\theta_{j-1}^w)] + d_j^w \log[1-\sigma(x_w^T\theta_{j-1}^w)]</script><p>至此，已经推导出对数似然函数，这就是CBOW模型的目标函数。</p>
<blockquote>
<p>参数$\theta$是怎样的矩阵？</p>
<p>我理解，任何用树的多个二分类问题，目标函数都可以表示成这种形式。</p>
</blockquote>
<p>为了使该目标函数最大化，word2vec采用的是<strong>随机梯度上升法</strong>。每取一个样本，计算梯度再刷新所有的参数。推导出更新公式为</p>
<script type="math/tex; mode=display">
\theta_{j-1}^w = \theta_{j-1}^w + \eta [1-d_j^w-\sigma(x_w^T\theta_{j-1}^w)]x_w^T</script><p>同样的</p>
<script type="math/tex; mode=display">
\frac {\partial L(w,j)} {\partial x_w}=[1-d_j^w-\sigma(x_w^T\theta_{j-1}^w)]\theta_{j-1}^w</script><p>我们的最终目的是要求词典D中每个词的词向量，而这里的$x_w$表示的是Context(w)所有词向量的累加和，那么如何利用偏导对$v(\tilde w), w \in Context(\tilde w)$进行更新呢，word2vec算法中直接取</p>
<script type="math/tex; mode=display">
v(\tilde w) = v(\tilde w)+\eta \sum_{j=2}^{l^w} \frac {\partial L(w,j)} {\partial x_w}</script><p>即把后面增量的值贡献到Context(w)的每一个词的词向量上。</p>
<p>伪代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">for j=huffman code:</span><br><span class="line">&#123;</span><br><span class="line">  1)q=sigma(x*theta)</span><br><span class="line">  2)g=eta(1-d-q)</span><br><span class="line">  3)e=e+g*theta</span><br><span class="line">  4)theta=theta+g*x</span><br><span class="line">&#125;</span><br><span class="line">for w in Context(w):</span><br><span class="line">&#123;</span><br><span class="line">  v(w) = v(w) + e</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>注意3、 4两步不能颠倒位置，要先计算出e，再更新θ。</p>
<h1 id="4、基于Negative-Sampling的模型"><a href="#4、基于Negative-Sampling的模型" class="headerlink" title="4、基于Negative Sampling的模型"></a>4、基于Negative Sampling的模型</h1><p>NEG不使用Huffman树，而是利用随机负采样，能大幅度提高性能。</p>
<p>前面的原理差不多，也是用梯度下降，关键在于，h-softmax是通过Huffman树的路径长度来进行迭代和更新参数；NEG是通过找出负采样来迭代。</p>
<h2 id="4-1-负采样算法"><a href="#4-1-负采样算法" class="headerlink" title="4.1 负采样算法"></a>4.1 负采样算法</h2><p>词典D中的词在语料C中出现的次数有高有低，对于那些高频词，被选为负采样的概率就应该比较大，反之较小。这是我们对采样过程的一个大致要求，本质上就是一个<strong>带权采样问题</strong>。</p>
<blockquote>
<p>设词典D中的每个词w对应一条线段l(w)，则线段的长度可以表示为</p>
<script type="math/tex; mode=display">
len(w)=\frac {count(w)} {\sum_{u \in D} count(u)}</script><p>也就是计算词频再归一化。现在将这些线段首尾相连拼接在一起，形成一个长度为1的单位线段。如果随机往这个单位线段上打点，则其中越长的线段命中概率越大。</p>
</blockquote>
<p>通过这些线段得到一个非等距剖分（假设分成N个区间），再定义一个等距剖分（假设分成M个区间），$M &gt;&gt; N$ 。</p>
<p><img src="//schwimmer.github.io/2018/03/17/机器学习/NLP/Word2Vec原理/table映射.png" alt="table映射"></p>
<p>建立如下的映射关系</p>
<script type="math/tex; mode=display">
Table(i) = w_k, \ where\  m_i \in I_k,\ i=1,2,...,M-1</script><p>那么，采样就是每次生成一个[1,M-1]之间的整数r，Table(r)就是一个样本。如果负采样的时候选到自己，就跳过再选。</p>
<h1 id="Word2Vec与神经网络模型的区别"><a href="#Word2Vec与神经网络模型的区别" class="headerlink" title="Word2Vec与神经网络模型的区别"></a>Word2Vec与神经网络模型的区别</h1><ul>
<li>NN的输入是类似n-gram，取前N-1个词，w2v取前后各n-1个词</li>
<li>NN多了隐层，输入到隐层用双曲正切当激活函数。</li>
<li>NN的输入词向量是首尾拼接，W2V是加总。这样当窗口中向量不足时，也不需要补。</li>
<li>NN的输出是一个长度为N的向量，就是整个词汇表的长度，然后再做一个softmax归一化，得到给定上下文时下一个词恰好为词汇表的某个词的概率。</li>
</ul>
<h1 id="疑问"><a href="#疑问" class="headerlink" title="疑问"></a>疑问</h1><p>1）在这里非叶子结点对应的那些向量就可以扮演参数的角色？</p>
<p>就是说可以把$\theta$当做待求的参数，这样就可以用sigmoid来求分类的概率。</p>
<p>参考</p>
<p>【1】<a href="http://www.cnblogs.com/pinard/p/7160330.html" target="_blank" rel="noopener">word2vec原理(一) CBOW与Skip-Gram模型基础</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2018/03/17/机器学习/NLP/NLP工具/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/03/17/机器学习/NLP/NLP工具/" class="post-title-link" itemprop="url">NL工具</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-03-17 20:50:31" itemprop="dateCreated datePublished" datetime="2018-03-17T20:50:31+08:00">2018-03-17</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-03-25 16:02:08" itemprop="dateModified" datetime="2018-03-25T16:02:08+08:00">2018-03-25</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2018/03/17/机器学习/NLP/NLP工具/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/03/17/机器学习/NLP/NLP工具/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="NLP工具"><a href="#NLP工具" class="headerlink" title="NLP工具"></a>NLP工具</h1><h1 id="Word2Vec-Java"><a href="#Word2Vec-Java" class="headerlink" title="Word2Vec-Java"></a>Word2Vec-Java</h1><h2 id="加载模型及词向量计算"><a href="#加载模型及词向量计算" class="headerlink" title="加载模型及词向量计算"></a>加载模型及词向量计算</h2><p>hdfs版 /pig-ext-lite/src/main/java/com/buzzinate/pig/util/WordVec.java</p>
<p>单机版 /persona/src/main/java/com/iclick/persona/nlp/word2vec/W2VContral.java</p>
<h2 id="词向量聚类"><a href="#词向量聚类" class="headerlink" title="词向量聚类"></a>词向量聚类</h2><p>/home/david/gitlab/user-gene/domain-classify-model/src/main/java/com/buzzinate/domain/classifier/DomainClassifier.java</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">	 * 通过keams得到中心点向量</span><br><span class="line">	 * </span><br><span class="line">	 * @param wordVec</span><br><span class="line">	 * @return</span><br><span class="line">	 */</span><br><span class="line">	public ArrayList&lt;float[]&gt; getCenterVecByKeams(ArrayList&lt;float[]&gt; wordVec) &#123;</span><br><span class="line">		ArrayList&lt;float[]&gt; centerVec = new ArrayList&lt;float[]&gt;();</span><br><span class="line">		List&lt;Classes&gt; cls = null;</span><br><span class="line">		logger.info(&quot;before keams： &quot; + wordVec.size());</span><br><span class="line">		System.out.println(&quot;before keams： &quot; + wordVec.size());</span><br><span class="line">		cls = KMeansClustering.getClusteringResult(wordVec,</span><br><span class="line">				wordVec.subList(0, Math.min(100, wordVec.size() - 1)));</span><br><span class="line">		for (Classes cl : cls) &#123;</span><br><span class="line">			if (!Float.isNaN(WVUtils.toFloat(cl.getCenter())[0]))</span><br><span class="line">				centerVec.add(WVUtils.toFloat(cl.getCenter()));</span><br><span class="line">		&#125;</span><br><span class="line">		logger.info(&quot;after keams： &quot; + centerVec.size());</span><br><span class="line">		System.out.println(&quot;after keams： &quot; + centerVec.size());</span><br><span class="line">		return centerVec;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="通过向量找最近的词"><a href="#通过向量找最近的词" class="headerlink" title="通过向量找最近的词"></a>通过向量找最近的词</h2><h1 id="Ansj-seg"><a href="#Ansj-seg" class="headerlink" title="Ansj-seg"></a>Ansj-seg</h1><h2 id="添加词到自定义词典"><a href="#添加词到自定义词典" class="headerlink" title="添加词到自定义词典"></a>添加词到自定义词典</h2><p>user-gene中用的是某一版ansj的基础上修改的，使用方法是</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Value value = new Value(newWord, new String[] &#123; &quot;userDefine&quot;, &quot;1000&quot; &#125;);</span><br><span class="line">		Library.insertWord(UserDefineRecognition.FOREST, value);</span><br></pre></td></tr></table></figure>
<p>知乎找到的方法是<a href="https://www.zhihu.com/question/32226656" target="_blank" rel="noopener">https://www.zhihu.com/question/32226656</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">作者：ansj</span><br><span class="line">链接：https://www.zhihu.com/question/32226656/answer/113724991</span><br><span class="line">来源：知乎</span><br><span class="line">著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</span><br><span class="line"></span><br><span class="line">package org.ansj;</span><br><span class="line"></span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import org.ansj.app.summary.SummaryComputer;</span><br><span class="line">import org.ansj.domain.Result;</span><br><span class="line">import org.ansj.domain.Term;</span><br><span class="line">import org.ansj.library.UserDefineLibrary;</span><br><span class="line">import org.ansj.splitWord.analysis.ToAnalysis;</span><br><span class="line">import org.nlpcn.commons.lang.tire.domain.Forest;</span><br><span class="line">import org.nlpcn.commons.lang.tire.domain.Value;</span><br><span class="line">import org.nlpcn.commons.lang.tire.library.Library;</span><br><span class="line"></span><br><span class="line">public class Test &#123;</span><br><span class="line">	public static void main(String[] args) throws Exception &#123;</span><br><span class="line">		</span><br><span class="line">		// 构造一个用户词典</span><br><span class="line">		Forest forest = Library.makeForest(&quot;library/default.dic&quot;);</span><br><span class="line">		forest = new Forest();</span><br><span class="line"></span><br><span class="line">		// 增加新词,中间按照&apos;\t&apos;隔开</span><br><span class="line">		UserDefineLibrary.insertWord(&quot;ansj中文分词&quot;, &quot;userDefine&quot;, 1000);</span><br><span class="line">		Result terms = ToAnalysis.parse(&quot;我觉得Ansj中文分词是一个不错的系统!我是王婆!&quot;);</span><br><span class="line">		System.out.println(&quot;增加新词例子:&quot; + terms);</span><br><span class="line"></span><br><span class="line">		// 删除词语,只能删除.用户自定义的词典.</span><br><span class="line">		UserDefineLibrary.removeWord(&quot;ansj中文分词&quot;);</span><br><span class="line">		terms = ToAnalysis.parse(&quot;我觉得ansj中文分词是一个不错的系统!我是王婆!&quot;);</span><br><span class="line">		System.out.println(&quot;删除用户自定义词典例子:&quot; + terms);</span><br><span class="line">		</span><br><span class="line">// 歧义词</span><br><span class="line">		Value value = new Value(&quot;济南下车&quot;, &quot;济南&quot;, &quot;n&quot;, &quot;下车&quot;, &quot;v&quot;);</span><br><span class="line">		System.out.println(ToAnalysis.parse(&quot;我经济南下车到广州.中国经济南下势头迅猛!&quot;));</span><br><span class="line">		Library.insertWord(UserDefineLibrary.ambiguityForest, value);</span><br><span class="line">		System.out.println(ToAnalysis.parse(&quot;我经济南下车到广州.中国经济南下势头迅猛!&quot;));</span><br><span class="line"></span><br><span class="line">		// 多用户词典</span><br><span class="line">		String str = &quot;神探夏洛克这部电影作者.是一个dota迷&quot;;</span><br><span class="line">		System.out.println(ToAnalysis.parse(str));</span><br><span class="line">		// 两个词汇 神探夏洛克 douta迷</span><br><span class="line">		Forest dic1 = new Forest();</span><br><span class="line">		Library.insertWord(dic1, new Value(&quot;神探夏洛克&quot;, &quot;define&quot;, &quot;1000&quot;));</span><br><span class="line">		Forest dic2 = new Forest();</span><br><span class="line">		Library.insertWord(dic2, new Value(&quot;dota迷&quot;, &quot;define&quot;, &quot;1000&quot;));</span><br><span class="line">		System.out.println(ToAnalysis.parse(str, dic1, dic2));</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果在spark下面用要修改</p>
<h1 id="Hanlp"><a href="#Hanlp" class="headerlink" title="Hanlp"></a>Hanlp</h1><p>官方文档<a href="http://hanlp.linrunsoft.com/doc/_build/html/index.html" target="_blank" rel="noopener">http://hanlp.linrunsoft.com/doc/_build/html/index.html</a></p>
<p><a href="https://github.com/hankcs/HanLP/releases" target="_blank" rel="noopener">https://github.com/hankcs/HanLP/releases</a></p>
<p><strong>HanLP</strong>中的数据分为<em>词典</em>和<em>模型</em>，其中<em>词典</em>是词法分析必需的，<em>模型</em>是句法分析必需的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data</span><br><span class="line">│</span><br><span class="line">├─dictionary</span><br><span class="line">└─model</span><br></pre></td></tr></table></figure>
<p>配置文件的作用是告诉HanLP数据包的位置，只需修改第一行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root=usr/home/HanLP/</span><br></pre></td></tr></table></figure>
<p>为data的<strong>父目录</strong>即可</p>
<blockquote>
<p>一开始用1.2.9版本，一直提示找不到HanLP.properties，后来换了1.3.2，默认是项目根目录下的data目录。</p>
</blockquote>
<p>4.1 分词</p>
<p><strong>标准分词</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">list&lt;Term&gt; termlist = StandardTokenizer.segment(&quot;XXXX&quot;);</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>HanLP</strong>中有一系列“开箱即用”的静态分词器，以<code>Tokenizer</code>结尾。</li>
<li><code>HanLP.segment</code>其实是对<code>StandardTokenizer.segment</code>的包装。</li>
<li>分词结果包含词性，每个词性的意思请查阅<a href="http://www.hankcs.com/nlp/part-of-speech-tagging.html#h2-8" target="_blank" rel="noopener">《HanLP词性标注集》</a>。</li>
<li>算法详解：<a href="http://www.hankcs.com/nlp/segment/the-word-graph-is-generated.html" target="_blank" rel="noopener">《词图的生成》</a></li>
</ul>
<blockquote>
<p>词性标注（Part-of-Speech tagging 或POS tagging)：又称词类标注或者简称标注，是指为分词结果中的每个单词标注一个正确的词性的程序，也即确定每个词是名词、动词、形容词或其他词性的过程。在汉语中，词性标注比较简单，因为汉语词汇词性多变的情况比较少见，大多词语只有一个词性，或者出现频次最高的词性远远高于第二位的词性。据说，只需选取最高频词性，即可实现80%准确率的中文词性标注程序。</p>
<p>利用HMM即可实现更高准确率的词性标注。</p>
</blockquote>
<p>去掉停用词、标点的分词</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">list&lt;Term&gt; termlist = NotionalTokenizer.segment()</span><br></pre></td></tr></table></figure>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2018/03/17/Elastic Search/iau的ES使用场景/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="Record and Think!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/03/17/Elastic Search/iau的ES使用场景/" class="post-title-link" itemprop="url">iau的ES使用场景</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-03-17 20:50:31" itemprop="dateCreated datePublished" datetime="2018-03-17T20:50:31+08:00">2018-03-17</time>
            </span>
          

          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/Elastic-Search/" itemprop="url" rel="index"><span itemprop="name">Elastic Search</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2018/03/17/Elastic Search/iau的ES使用场景/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/03/17/Elastic Search/iau的ES使用场景/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>比较brand和competitor的交叉人群</p>
<p>分别算出brand和competitor的人群，然后再计算包含brand和competitor的人群，就得出交叉人群。</p>
<p>比较brand和competitor的距离</p>
<p>就是比较brand和competitor的每个产品和对方关键词的相关度。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/25/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/25/">25</a><span class="page-number current">26</span><a class="page-number" href="/page/27/">27</a><span class="space">&hellip;</span><a class="page-number" href="/page/32/">32</a><a class="extend next" rel="next" href="/page/27/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Schwimmer</p>
              <div class="site-description motion-element" itemprop="description">Record and Think!</div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">319</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">25</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">63</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          

          

          
          

          
            
          
          

        </div>
      </div>

      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Schwimmer</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.9.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> v7.2.0</div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span>
    <span class="site-uv" title="总访客量">
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="post-meta-divider">|</span>
  

  
    <span class="post-meta-item-icon">
      <i class="fa fa-eye"></i>
    </span>
    <span class="site-pv" title="总访问量">
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    
      <div>
        
  <script>
    window._bd_share_config = {
      "common": {
        "bdText": "",
        "bdMini": "1",
        "bdMiniList": false,
        "bdPic": ""
      },
      "image": {
        "viewList": ["tsina", "douban", "sqq", "qzone", "weixin", "twi", "fbook"],
        "viewText": "分享到：",
        "viewSize": "16"
      },
      "slide": {
        "bdImg": "5",
        "bdPos": "left",
        "bdTop": "100"
      }
    }
  </script>

<script>
  with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='//bdimg.share.baidu.com/static/api/js/share.js?cdnversion='+~(-new Date()/36e5)];
</script>

      </div>
    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/utils.js?v=7.2.0"></script>

  <script src="/js/motion.js?v=7.2.0"></script>



  
  


  <script src="/js/schemes/muse.js?v=7.2.0"></script>




  

  


  <script src="/js/next-boot.js?v=7.2.0"></script>


  

  

  

  
  
<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://https-schwimmer-github-io.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>







  
  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
  

  
  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      document.getElementById(all[i].inputID + '-Frame').parentNode.className += ' has-jax';
    }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

    
  


  

  

  

  

  

  

  

  

  

  

  

</body>
</html>
