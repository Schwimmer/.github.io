<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222"/>






















<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0"/>

<link rel="stylesheet" href="/css/main.css?v=7.2.0"/>


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.2.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.2.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '7.2.0',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="Record and Think!">
<meta property="og:type" content="website">
<meta property="og:title" content="Schwimmer&#39;s Blog">
<meta property="og:url" content="https://schwimmer.github.io/page/3/index.html">
<meta property="og:site_name" content="Schwimmer&#39;s Blog">
<meta property="og:description" content="Record and Think!">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Schwimmer&#39;s Blog">
<meta name="twitter:description" content="Record and Think!">





  
  
  <link rel="canonical" href="https://schwimmer.github.io/page/3/"/>



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Schwimmer's Blog</title>
  




  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-143240576-1"></script>
  <script>
    var host = window.location.hostname;
    if (host !== "localhost" || !true) {
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-143240576-1');
    }
  </script>









  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Schwimmer's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br/>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br/>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br/>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br/>归档</a>

  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/19/地理信息挖掘/基于轨迹数据的团体挖掘/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/19/地理信息挖掘/基于轨迹数据的团体挖掘/" class="post-title-link" itemprop="url">未命名</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-19 09:52:44 / 修改时间：10:23:08" itemprop="dateCreated datePublished" datetime="2019-06-19T09:52:44+08:00">2019-06-19</time>
            </span>
          

          
            

            
          

          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/19/地理信息挖掘/基于轨迹数据的团体挖掘/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/19/地理信息挖掘/基于轨迹数据的团体挖掘/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://blog.csdn.net/weixin_42449669/article/details/87978320" target="_blank" rel="noopener">https://blog.csdn.net/weixin_42449669/article/details/87978320</a></p>
<p>轨迹数据挖掘介绍<br>轨迹挖掘分为伴随模式挖掘、频繁模式挖掘、周期模式挖掘等不同种类。</p>
<p>伴随模式挖掘是在轨迹数据中提取伴随的移动对象，通过分析移动对象群体的行为特征和规律，可以发现时空环境中的群体。典型的伴随模式有ｆｌｏｃｋ、ｃｏｎｖｏｙ、ｇａｔｈｅｒｉｎｇ等。<br>频繁模式挖掘是从大规模轨迹数据中发现频繁时序模式，例如挖掘公共性规律或公共性频繁路径等。一般来说轨迹数据中包含了位置、时间和语义信息，所以时空轨迹频繁模式挖掘与传统的频繁序列挖掘有一定的区别。频繁模式挖掘在生活模式挖掘、地点预测、用户相似性估计等方面有很多应用。<br>移动对象经常有一些周期性的活动行为，例如购物行为、动物的定期迁移行为等，通过挖掘此类轨迹可以预测移动对象未来的行为。一般将周期模式挖掘分为完全周期模式、部分周期模式、同步周期模式、异步周期模式等。<br>伴随关系分析将采用伴随模式挖掘。<br>伴随模式挖掘相关算法<br>轨迹相似度度量算法<br>比较经典的轨迹相似度计算方法有最长公共子序列（LCSS）、编辑距离（ED）、动态时间规整（DTW）等</p>
<p>DTW：<br>将时间序列进行延伸和缩短，来计算两个时间序列性之间的相似性，将两条轨迹上的所有点进行匹配，每个点都会匹配到另一条轨迹上距离最近的点，并且同一个点可以被另一条轨迹上的多个点所匹配。</p>
<p>LCSS：<br>无需匹配所有点计算相似度的情况下，可以用LCSS算法进行相似度计算，该算法可以使用动态规划的思想实现。但该算法在低精度轨迹数据集上，导致两条轨迹上只有很少的点能够匹配上，大部分的点无法匹配，导致相似度计算结果偏低。</p>
<p>ED：<br>是指两个字符串之间，由一个转成另一个所需的最少编辑次数。但是时间复杂度很高。</p>
<p>基于以上提到的轨迹相似度计算方法，还有一种基于轨迹形状的轨迹相似度计算方法，使用轨迹点的方向向量替代具体坐标计算相似度，该算法更倾向于关注轨迹的整体形状，而不是轨迹点之间的距离。<br>以上算法需要高精度的轨迹数据集、时间复杂度高，需要一种算法来针对低精度的稀疏的轨迹数据集，基于相似度计算的团体识别算法GMPMS (Group Movement Pattern Mining based on Similarity)。<br>轨迹数据降噪处理<br>由于设备原因或者信号问题造成的轨迹数据错误采样产生了噪声点。噪声点影响后续的轨迹挖掘与分析，因此需要首先过滤掉。一般来说有中值滤波、卡尔曼滤波与粒子滤波３种方式处理噪声点，其主要思想是通过前ｎ个点的位置预测当前点的位置，通过预测值对当前点的位置进行合理修正，达到轨迹平滑的效果。</p>
<p>计算的时间复杂度<br>为了减少两两计算相似度的复杂度，使用频繁项集挖掘, 发掘候选团体。候选团体发现的思想是从时间和空间两个纬度对用户轨迹进行切分,找出经常在同一时间出现在同一区域的候选团体,将这些团体的集合作为候选集。</p>
<p>基于相似度计算的团体识别算法GMPMS<br>数据预处理<br>数据降噪，去除噪声点（偏差较大的数据）<br>轨迹点聚合。将每个用户的轨迹分为停留点和路过点, 停留点: 时间相差15min, 距离相差200m的点的集合。聚合结果如图所示</p>
<p>p2和p3的聚合为一个点sp1，sp1的地理坐标用所有点的平均值，时间使用p2的时间作为sp1的起始时间，并且计算出sp1的停留时间，停留时间越长代表该点越重要。</p>
<p>时空轨迹快照<br>时空轨迹快照的目的是为了减少轨迹两两计算的时间复杂度</p>
<p>按时间切分<br>按照时间将轨迹数据切分为连续的时间快照，快照的时间间隔为T，轨迹稀疏时，T值要设置大些。如下图所示</p>
<pre><code>                                                                   图3-2

                                       S表示时间片，C表示时间片中的簇
</code></pre><p>按照空间切分<br>轨迹数据被切分成时间快照后，在每个快照中使用基于密度聚类的方法（ＤＢＳＣＡＮ）来进行聚类，得到每个快照中的轨迹簇，每个时间快照可能有一个或多个簇，为接下来候选团体的发现做准备。如图3-2中，C5,1和C5,2是时间片S5中的两个轨迹簇。</p>
<p>候选团体发现<br>参考“购物车分析”<br>“购物车分析”是数据挖掘领域中，关联规则分析的一个经典问题，其目的是从零售记录中分析出顾客经常同时购买商品的组合，挖掘出购物篮中有价值的信息。举例来说，人们经常会同时购买面包和黄油，因此这两种物品会频繁出现在同一购物清单上。通过机器学习中的频繁项集挖掘的方法可以将具有关联的物品组合找出来。</p>
<p>轨迹分析与“购物车分析”对比<br>以上提到的轨迹簇类似与电商中的购物车,轨迹簇中的人相当于有关联的物品。</p>
<p>采用频繁项集挖掘算法FP-growth并设置合适的阈值K和M，可以得到候选轨迹簇的集合。FP-growth算法可以在快照的轨迹簇中挖掘满足团体最小人数M和最少出现次数K的集合，并且对于集合出现次数的连续性没有限制。该算法只需要对数据集进行两次扫描即可得出频繁项集的结果，时间复杂度很低。FP-growth算法分为构建ＦＰ树和挖掘频繁项集两个步骤。<br>相似性度量<br>通过计算候选团体内，每个人之间的轨迹相似度Tsim。<br>采用基于轨迹质心距离的相似度计算方法，该算法更适合低经度下的轨迹相似度计算。<br>找出两条轨迹上的匹配点，将轨迹按照匹配点切分成子轨迹。之后，对于每段子轨迹，计算其轨迹上所有点的质心坐标，对比两个轨迹的质心距离，如果质心距离在阀值之内，则认为子轨迹相似。这个算法将子轨迹段上一系列的聚合为质心进行计算，减小了轨迹点不同数量和异常点造成的影响。<br>两条轨迹总相似度S=m/n, m是相似的子轨迹, n是子轨迹的数量<br>团体识别<br>通过人之间的相似度判断是否属于伴随关系，如果存在伴随关系，则认为这两个人之间存在着边相连，在一个候选集中，将存在边的人划分为同一个群体，因此一个候选集内可能存在一个或多个伴随团体。</p>
<p>基于相似度阀值的关系判别, 通过人工指定阀值，阀值需要在具体实践中调整, 适用于大团体识别<br>基于半监督学习的关系判别, 适用于小团体识别<br>参数设置</p>
<p>候选团体最小人数为2, 最少出现次数为4. 轨迹点匹配时间阀值5min, 轨迹点匹配距离阀值是1km, 子轨迹段相似的质心距离小于1km, 轨迹相似度阀值为0.5</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/18/深度学习笔记/深度学习入门/CH3 神经网络/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/18/深度学习笔记/深度学习入门/CH3 神经网络/" class="post-title-link" itemprop="url">未命名</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-18 08:33:06 / 修改时间：09:28:14" itemprop="dateCreated datePublished" datetime="2019-06-18T08:33:06+08:00">2019-06-18</time>
            </span>
          

          
            

            
          

          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/18/深度学习笔记/深度学习入门/CH3 神经网络/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/18/深度学习笔记/深度学习入门/CH3 神经网络/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>代码在</p>
<p><code>/Users/david/david/git/dl/deep-learning-from-scratch</code></p>
<h1 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h1><h2 id="sigmoid"><a href="#sigmoid" class="headerlink" title="sigmoid"></a>sigmoid</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">def sigmoid(x):</div><div class="line">	return 1 / (1 + np.exp(-x))</div></pre></td></tr></table></figure>
<p>x可以是标量，可以是np.array</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/18/深度学习笔记/深度学习入门/CH2 感知机/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/18/深度学习笔记/深度学习入门/CH2 感知机/" class="post-title-link" itemprop="url">未命名</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-18 08:19:36 / 修改时间：08:31:50" itemprop="dateCreated datePublished" datetime="2019-06-18T08:19:36+08:00">2019-06-18</time>
            </span>
          

          
            

            
          

          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/18/深度学习笔记/深度学习入门/CH2 感知机/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/18/深度学习笔记/深度学习入门/CH2 感知机/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>代码在</p>
<p><code>/Users/david/david/git/dl/deep-learning-from-scratch</code></p>
<p>实现与门、与非门和或门，在此基础上实现了感知机。</p>
<p>与门、与非门和或门 与感知机的思想的类似的，对任意的输入，最终变成0和1的输出，在输入到输出的时候还会带上权重和偏置。</p>
<h1 id="感知机和异或门"><a href="#感知机和异或门" class="headerlink" title="感知机和异或门"></a>感知机和异或门</h1><p>感知机的局限性，单层感知机是无法实现<strong>异或门</strong>的。</p>
<p>why？</p>
<p>比如与门、与非门和或门，在空间的划分都是直线，例如</p>
<p><img src="/2019/06/18/深度学习笔记/深度学习入门/CH2 感知机/pic/image-20190618082700331.png" alt="image-20190618082700331"></p>
<p>可以用单层感知机实现对0和1的分类，只要定义好公式和权重。</p>
<p>而异或门对于空间的划分</p>
<p><img src="/2019/06/18/深度学习笔记/深度学习入门/CH2 感知机/pic/image-20190618082714611.png" alt="image-20190618082714611"></p>
<p>是非线性的。</p>
<p><img src="/2019/06/18/深度学习笔记/深度学习入门/CH2 感知机/pic/image-20190618082736756.png" alt="image-20190618082736756"></p>
<p>若要实现异或门，必须上多层感知机。</p>
<p><img src="/2019/06/18/深度学习笔记/深度学习入门/CH2 感知机/pic/image-20190618082808824.png" alt="image-20190618082808824"></p>
<p><img src="/2019/06/18/深度学习笔记/深度学习入门/CH2 感知机/pic/image-20190618082816721.png" alt="image-20190618082816721"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">XOR</span><span class="params">(x1, x2)</span>:</span></div><div class="line">    s1 = NAND(x1, x2)</div><div class="line">    s2 = OR(x1, x2)</div><div class="line">    y = AND(s1, s2)</div><div class="line">    <span class="keyword">return</span> y</div></pre></td></tr></table></figure>
<p>P59</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/16/算法与数据结构/LeeCode/week4-1 验证二叉搜索树/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/16/算法与数据结构/LeeCode/week4-1 验证二叉搜索树/" class="post-title-link" itemprop="url">未命名</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-16 14:53:50 / 修改时间：15:30:35" itemprop="dateCreated datePublished" datetime="2019-06-16T14:53:50+08:00">2019-06-16</time>
            </span>
          

          
            

            
          

          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/16/算法与数据结构/LeeCode/week4-1 验证二叉搜索树/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/16/算法与数据结构/LeeCode/week4-1 验证二叉搜索树/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://blog.csdn.net/qq_30490125/article/details/53135274" target="_blank" rel="noopener">https://blog.csdn.net/qq_30490125/article/details/53135274</a></p>
<p>错误解答</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">isValidBST</span><span class="params">(self, root: TreeNode)</span> -&gt; bool:</span></div><div class="line">    <span class="keyword">if</span> root <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        <span class="keyword">return</span> <span class="keyword">True</span></div><div class="line">    <span class="keyword">if</span> root.left <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span> root.left.val &gt; root.val:</div><div class="line">        <span class="keyword">return</span> <span class="keyword">False</span></div><div class="line">    <span class="keyword">if</span> root.right <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span> root.right.val &lt; root.val:</div><div class="line">        <span class="keyword">return</span> <span class="keyword">False</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> self.isValidBST(root.left) <span class="keyword">or</span> <span class="keyword">not</span> self.isValidBST(root.right):</div><div class="line">        <span class="keyword">return</span> <span class="keyword">False</span></div><div class="line">    <span class="keyword">return</span> <span class="keyword">True</span></div></pre></td></tr></table></figure>
<p>反例</p>
<p><img src="/2019/06/16/算法与数据结构/LeeCode/week4-1 验证二叉搜索树/pic/image-20190616145426312.png" alt="image-20190616145426312"></p>
<p>方法1：</p>
<p>中序遍历，判断是否是升序</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/16/机器学习和深度学习算法理论/逻辑回归/损失函数-交叉熵/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/16/机器学习和深度学习算法理论/逻辑回归/损失函数-交叉熵/" class="post-title-link" itemprop="url">未命名</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-16 09:53:53 / 修改时间：10:00:56" itemprop="dateCreated datePublished" datetime="2019-06-16T09:53:53+08:00">2019-06-16</time>
            </span>
          

          
            

            
          

          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/16/机器学习和深度学习算法理论/逻辑回归/损失函数-交叉熵/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/16/机器学习和深度学习算法理论/逻辑回归/损失函数-交叉熵/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://blog.csdn.net/syyyy712/article/details/78252722" target="_blank" rel="noopener">逻辑回归算法之交叉熵函数理解</a></p>
<p>对于LR的函数</p>
<blockquote>
<p>怎么叫什么函数，输出函数？模型函数？</p>
</blockquote>
<p><img src="/2019/06/16/机器学习和深度学习算法理论/逻辑回归/损失函数-交叉熵/pic/SouthEast.png" alt="è¿éåå¾çæè¿°"></p>
<p>如果我们采用传统的平方误差函数，其画出的代价函数图<strong>就不会是一个凸函数</strong>，而是有很多局部最优点的歪歪扭扭的函数图像如下： </p>
<blockquote>
<p>为什么？</p>
</blockquote>
<p><img src="/2019/06/16/机器学习和深度学习算法理论/逻辑回归/损失函数-交叉熵/pic/SouthEast-20190616095710926.png" alt="è¿éåå¾çæè¿°"></p>
<p>这类函数图像不是我们所想要的，因为有太多局部最优点了，我们需要一个全局最优点并且曲线光滑，如下图是我们所想要得到的代价函数曲线图 </p>
<p><img src="/2019/06/16/机器学习和深度学习算法理论/逻辑回归/损失函数-交叉熵/pic/SouthEast-20190616095728402.png" alt="è¿éåå¾çæè¿°"></p>
<p>为了达到尽可能使得曲线光滑以及尽可能使得类似于像这种一元二次函数，因此我们给出代价函数表达式如下表示：</p>
<p><img src="/2019/06/16/机器学习和深度学习算法理论/逻辑回归/损失函数-交叉熵/pic/SouthEast-20190616095758497.png" alt="è¿éåå¾çæè¿°"></p>
<p><img src="/2019/06/16/机器学习和深度学习算法理论/逻辑回归/损失函数-交叉熵/pic/SouthEast-20190616095835034.png" alt="è¿éåå¾çæè¿°"></p>
<p><img src="/2019/06/16/机器学习和深度学习算法理论/逻辑回归/损失函数-交叉熵/pic/SouthEast-20190616095841664.png" alt="è¿éåå¾çæè¿°"></p>
<p>这个代价函数是一个分段函数，分别给出了y=0和y=1的代价函数图，从图中可以看出这个代价满足了凸函数的要求且没有局部最优点，只有全局最优点。 </p>
<p><img src="/2019/06/16/机器学习和深度学习算法理论/逻辑回归/损失函数-交叉熵/pic/SouthEast-20190616095853852.png" alt="è¿éåå¾çæè¿°"></p>
<p><a href="https://blog.csdn.net/jasonzzj/article/details/52017438" target="_blank" rel="noopener">交叉熵代价函数(损失函数)及其求导推导</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/16/深度学习笔记/优质git资源/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/16/深度学习笔记/优质git资源/" class="post-title-link" itemprop="url">未命名</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-16 09:28:33" itemprop="dateCreated datePublished" datetime="2019-06-16T09:28:33+08:00">2019-06-16</time>
            </span>
          

          

          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/16/深度学习笔记/优质git资源/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/16/深度学习笔记/优质git资源/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="深度学习500问"><a href="#深度学习500问" class="headerlink" title="深度学习500问"></a>深度学习500问</h1><p>以问答形式对常用的概率知识、线性代数、机器学习、深度学习、计算机视觉等热点问题进行阐述，以帮助自己及有需要的读者。 全书分为18个章节，50余万字。由于水平有限，书中不妥之处恳请广大读者批评指正。</p>
<p><a href="https://github.com/scutan90/DeepLearning-500-questions" target="_blank" rel="noopener">https://github.com/scutan90/DeepLearning-500-questions</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/16/深度学习笔记/Pytorch笔记/Pytorch常见问题/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/16/深度学习笔记/Pytorch笔记/Pytorch常见问题/" class="post-title-link" itemprop="url">未命名</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-16 09:09:35 / 修改时间：09:10:26" itemprop="dateCreated datePublished" datetime="2019-06-16T09:09:35+08:00">2019-06-16</time>
            </span>
          

          
            

            
          

          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/16/深度学习笔记/Pytorch笔记/Pytorch常见问题/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/16/深度学习笔记/Pytorch笔记/Pytorch常见问题/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="1、libiomp5-dylib-already-initialized"><a href="#1、libiomp5-dylib-already-initialized" class="headerlink" title="1、libiomp5.dylib already initialized"></a>1、libiomp5.dylib already initialized</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">OMP: Error #15: Initializing libiomp5.dylib, but found libomp.dylib already initialized.</div><div class="line">OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://www.intel.com/software/products/support/.</div></pre></td></tr></table></figure>
<p>解决方案</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">os.environ[&quot;KMP_DUPLICATE_LIB_OK&quot;]=&quot;TRUE&quot;</div></pre></td></tr></table></figure>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/15/深度学习笔记/Pytorch训练营/D5 构建网络模型/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/15/深度学习笔记/Pytorch训练营/D5 构建网络模型/" class="post-title-link" itemprop="url">未命名</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-15 11:23:22 / 修改时间：11:23:46" itemprop="dateCreated datePublished" datetime="2019-06-15T11:23:22+08:00">2019-06-15</time>
            </span>
          

          
            

            
          

          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/15/深度学习笔记/Pytorch训练营/D5 构建网络模型/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/15/深度学习笔记/Pytorch训练营/D5 构建网络模型/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong>第二周Day11-Day12</strong> </p>
<p><strong>任务名称</strong>：构建网络模型(包括torch.nn和torch.nn.functional操作[参考文档：(1)pytorch官方文档torch.nn和torch.nn.function(2)PyTorch<em>tutorial_0.0.5</em>余霆嵩文档17~29页)</p>
<p><strong>任务简介</strong>：构建网络模型是学习pytorch框架最重要的内容</p>
<p><strong>详细说明</strong>：</p>
<p>Day11~Day12的任务是构建网络模型，在学习构建网络模型之前需要我们了解官方文档9~10的资料，这里包括了基本的模块，卷积操作、BN操作、池化操作、激活函数等等在构建模型中最基层的方法。学习初期我们可以先学习经典的网络模型，Lenet,Alxnet,Vgg,Resnet等网络模型，后面随着学习的深入自己搭建网络模型或者在经典网络模型的基础上增加或修改其网络，使其达到更好的性能；</p>
<p>Torch.nn.function中的激活函数和池化是我们常用的，一般不使用torch.nn中的激活函数和池化操作，因为网络模型反向传播时不计算池化层的梯度，使用在torch.nn.function中重新封装了部分接口，当然也可以不使用torch.nn.function。</p>
<p>作业资料包下载链接：（学习资料里包含了老师对官方文档的重组文件，前面一周的文档内容也都有更新，对前面内容还有所疑惑的小伙伴可以回头看看文档资料）</p>
<p>链接：<a href="https://pan.baidu.com/s/17xW8rfG-14nu6vc9vjeBlQ" target="_blank" rel="noopener">https://pan.baidu.com/s/17xW8rfG-14nu6vc9vjeBlQ</a> </p>
<p>提取码：34k3 </p>
<p><strong>作业名称（详解）：</strong>（1）手敲官方文档9，10中的基本操作3遍；（2）自己尝试写一个5层的网络模型；</p>
<p><strong>作业提交形式</strong>：打卡提交文字或图片，总结内容不少于20字</p>
<p><strong>打卡截止时间：</strong>6/14</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/14/机器学习和深度学习算法理论/CNN/一些概念/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/14/机器学习和深度学习算法理论/CNN/一些概念/" class="post-title-link" itemprop="url">未命名</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-14 15:20:21" itemprop="dateCreated datePublished" datetime="2019-06-14T15:20:21+08:00">2019-06-14</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-16 09:42:11" itemprop="dateModified" datetime="2019-06-16T09:42:11+08:00">2019-06-16</time>
              </span>
            
          

          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/14/机器学习和深度学习算法理论/CNN/一些概念/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/14/机器学习和深度学习算法理论/CNN/一些概念/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="same-padding和valid-padding"><a href="#same-padding和valid-padding" class="headerlink" title="same padding和valid padding"></a>same padding和valid padding</h1><p>首先，padding的含义</p>
<p><a href="https://zhuanlan.zhihu.com/p/36278093" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/36278093</a></p>
<p>padding是增加各个边的pixels的数量，目的是保持feature map 不要太小（不要被卷积越卷越小），但也没必要超过原图的大小，所以不可以任意数量。</p>
<p>padding的上限是维持feature map 大小与原图大小一致，具体增加pixel的数量多少，由filter的尺寸和stride大小共同决定；</p>
<p><strong>padding 存在的意义在于</strong></p>
<blockquote>
<p>为了不丢弃原图信息<br>为了保持feature map 的大小与原图一致<br>为了让更深层的layer的input依旧保持有足够大的信息量<br>为了实现上述目的，且不做多余的事情，padding出来的pixel的值都是0，不存在噪音问题</p>
</blockquote>
<h1 id="计算输出尺寸"><a href="#计算输出尺寸" class="headerlink" title="计算输出尺寸"></a>计算输出尺寸</h1><ol>
<li><p>过滤器数量-输出卷的深度与过滤器的数量成正比。请记住该如何堆叠每个过滤器的输出以形成激活映射。激活图的深度等于过滤器的数量。</p>
</li>
<li><p>步幅（Stride）-如果步幅是 1，那么我们处理图片的精细度就进入单像素级别了。更高的步幅意味着同时处理更多的像素，从而产生较小的输出量。</p>
</li>
<li><p>零填充（zero padding）-这有助于我们保留输入图像的尺寸。如果添加了单零填充，则单步幅过滤器的运动会保持在原图尺寸。</p>
</li>
</ol>
<p>我们可以应用一个简单的公式来计算输出尺寸。输出图像的空间尺寸可以计算为（[W-F + 2P] / S）+1。在这里，W 是输入尺寸，F 是过滤器的尺寸，P 是填充数量，S 是步幅数字。假如我们有一张<code>32*32*3</code> 的输入图像，我们使用 10 个尺寸为<code>3*3*3</code>的过滤器，单步幅和零填充。</p>
<p>那么 W=32，F=3，P=0，S=1。输出深度等于应用的滤波器的数量，即 10，输出尺寸大小为<code>([32-3+0]/1)+1 = 30</code>。因此输出尺寸是 <code>30*30*10</code>。</p>
<h1 id="卷积核相关概念"><a href="#卷积核相关概念" class="headerlink" title="卷积核相关概念"></a>卷积核相关概念</h1><p>如Pytorch中，conv1.weight.shape() = [6,3,5,5]，输入通道数为 3，卷积核个数为 6，则 feature map 数为 6，卷积核大小为 5*5。</p>
<p>这些概念分别代表什么</p>
<h2 id="通道数"><a href="#通道数" class="headerlink" title="通道数"></a>通道数</h2><p><a href="https://blog.csdn.net/sscc_learning/article/details/79814146" target="_blank" rel="noopener">【CNN】理解卷积神经网络中的通道 channel</a></p>
<p> <a href="https://www.tensorflow.org/tutorials/layers" target="_blank" rel="noopener">tensorflow</a> 中给出的，对于输入样本中 <code>channels</code> 的含义。一般的RGB图片，<code>channels</code>数量是 3 （红、绿、蓝）；而monochrome图片，<code>channels</code> 数量是 1 。</p>
<h2 id="特征图feature-map"><a href="#特征图feature-map" class="headerlink" title="特征图feature map"></a>特征图feature map</h2>
          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2019/06/14/深度学习笔记/Pytorch笔记/完整的一次任务/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer"/>
      <meta itemprop="description" content="Record and Think!"/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/14/深度学习笔记/Pytorch笔记/完整的一次任务/" class="post-title-link" itemprop="url">未命名</a>
              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-14 10:59:11" itemprop="dateCreated datePublished" datetime="2019-06-14T10:59:11+08:00">2019-06-14</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-16 09:16:07" itemprop="dateModified" datetime="2019-06-16T09:16:07+08:00">2019-06-16</time>
              </span>
            
          

          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/06/14/深度学习笔记/Pytorch笔记/完整的一次任务/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/14/深度学习笔记/Pytorch笔记/完整的一次任务/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br/>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>《PyTorch模型训练实用教程》</p>
<p>代码在：<code>/Users/david/david/code/deep_learning/PyTorch_Tutorial_yuting</code></p>
<p>预先安装</p>
<p>requirement.txt</p>
<p>运行时报错</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">OMP: Error #15: Initializing libiomp5.dylib, but found libomp.dylib already initialized.</div><div class="line">OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://www.intel.com/software/products/support/.</div></pre></td></tr></table></figure>
<p>解决方案</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">os.environ[&quot;KMP_DUPLICATE_LIB_OK&quot;]=&quot;TRUE&quot;</div></pre></td></tr></table></figure>
<h1 id="第一章-数据"><a href="#第一章-数据" class="headerlink" title="第一章 数据"></a>第一章 数据</h1><h2 id="1-1-Cifar10-转-png"><a href="#1-1-Cifar10-转-png" class="headerlink" title="1.1 Cifar10 转 png"></a>1.1 Cifar10 转 png</h2><p>cifar-10 的测试集，10000张图片。</p>
<p>官网:<a href="http://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="noopener">http://www.cs.toronto.edu/~kriz/cifar.html</a></p>
<h3 id="读取并保存为图片"><a href="#读取并保存为图片" class="headerlink" title="读取并保存为图片"></a>读取并保存为图片</h3><p><strong>图片是序列化的，不能直接读取。</strong></p>
<p>运行代码:Code/1_data_prepare/1_1_cifar10_to_png.py</p>
<p>可在文件夹 Data/cifar-10-png/raw_test/下看到 0-9 个文件夹，对应 9 个类别。</p>
<p>将 测试集中的 10000 张图片解压出来，作为原始图片，将从这 10000 张图片中划分出训练集 (train)，验证集(valid)，测试集(test)。 </p>
<h2 id="1-2-训练集、验证集、测试集的划分"><a href="#1-2-训练集、验证集、测试集的划分" class="headerlink" title="1.2 训练集、验证集、测试集的划分"></a>1.2 训练集、验证集、测试集的划分</h2><p>把原始数据按 8:1:1 的比例划分为训练集(train set)、验证集(valid/dev set)和测试集(test set) </p>
<p>运行代码：Code/1_data_prepare/1_2_split_dataset.py</p>
<p>数据划分完毕，下一步是制作存放有图片路径及其标签的 txt。pytorch会根据txt的信息寻找图片，并读取图片数据和标签数据。</p>
<h2 id="1-3-Pytorch读图片数据集"><a href="#1-3-Pytorch读图片数据集" class="headerlink" title="1.3 Pytorch读图片数据集"></a>1.3 Pytorch读图片数据集</h2><h3 id="Dataset类"><a href="#Dataset类" class="headerlink" title="Dataset类"></a>Dataset类</h3><p>PyTorch 读取<strong>图片</strong>，主要是通过 Dataset 类，所以先简单了解一下 Dataset 类。抽象类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dataset</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="string">"""An abstract class representing a Dataset.</span></div><div class="line"></div><div class="line">    All other datasets should subclass it. All subclasses should override</div><div class="line">    ``__len__``, that provides the size of the dataset, and ``__getitem__``,</div><div class="line">    supporting integer indexing in range from 0 to len(self) exclusive.</div><div class="line">    """</div><div class="line">		</div><div class="line">    <span class="comment"># 接收一个 index，然后返回图片数据和标签</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></div><div class="line">        <span class="keyword">raise</span> NotImplementedError</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">raise</span> NotImplementedError</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__add__</span><span class="params">(self, other)</span>:</span></div><div class="line">        <span class="keyword">return</span> ConcatDataset([self, other])</div></pre></td></tr></table></figure>
<p>需要将list放到txt里面，读取txt，获取list。那么读取自己数据的基本流程就是:</p>
<ol>
<li>制作存储了图片的路径和标签信息的 txt</li>
<li>将这些信息转化为 list，该 list 每一个元素对应一个样本</li>
<li>通过 getitem 函数，读取数据和标签，并返回数据和标签</li>
</ol>
<p>在训练代码里是感觉不到这些操作的，只会看到通过 DataLoader 就可以获取一个batch 的数据，其实触发去读取图片这些操作的是 DataLoader 里的<strong>iter</strong>(self)，后面会详细讲解读取过程。在本小节，主要讲 Dataset 子类。</p>
<p>要让 PyTorch 能读取自己的数据集，只需要两步:</p>
<ol>
<li>制作图片数据的索引</li>
<li><strong>构建 Dataset 子类</strong></li>
</ol>
<h3 id="1、制作图片索引"><a href="#1、制作图片索引" class="headerlink" title="1、制作图片索引"></a>1、制作图片索引</h3><p>就是获取图片路径和标签，保存到txt。</p>
<p>运行代码 Code/1_data_prepare/1_3_generate_txt.py</p>
<h3 id="2、构建Dataset子类"><a href="#2、构建Dataset子类" class="headerlink" title="2、构建Dataset子类"></a>2、构建Dataset子类</h3><p>构建了MyDataset类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDataset</span><span class="params">(Dataset)</span>:</span></div><div class="line">  	<span class="comment"># 初始化中，从txt读到imgs对象</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, txt_path, transform = None, target_transform = None)</span>:</span></div><div class="line">        fh = open(txt_path, <span class="string">'r'</span>)</div><div class="line">        imgs = []</div><div class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> fh:</div><div class="line">            line = line.rstrip()</div><div class="line">            words = line.split()</div><div class="line">            imgs.append((words[<span class="number">0</span>], int(words[<span class="number">1</span>])))</div><div class="line"></div><div class="line">        <span class="comment"># 最主要就是要生成这个list， 然后DataLoader中给index，通过getitem读取图片数据</span></div><div class="line">        self.imgs = imgs        </div><div class="line">        <span class="comment"># Compose 类型，里边有一个 list，list定义了对图像的各种操作，如减均值，除标准差，随机裁剪，仿射变换等。</span></div><div class="line">        self.transform = transform</div><div class="line">        self.target_transform = target_transform</div><div class="line"></div><div class="line">    <span class="comment"># python内建的魔法方法，访问索引就会触发</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></div><div class="line">        fn, label = self.imgs[index]</div><div class="line">        img = Image.open(fn).convert(<span class="string">'RGB'</span>)     <span class="comment"># 像素值 0~255，在transfrom.totensor会除以255，使像素值变成 0~1</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> self.transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            img = self.transform(img)   <span class="comment"># 在这里做transform，转为tensor等等</span></div><div class="line"></div><div class="line">        <span class="keyword">return</span> img, label</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> len(self.imgs)</div></pre></td></tr></table></figure>
<p>Pytorch对图片的处理不能生成新图，而是覆盖原图（不是真正的覆盖，就是对象赋值）。当采用randomcrop之类的随机操作时，每个 epoch 输入进来的图片几乎不会是一模一样的，这达到了样本多样性的功能。</p>
<p>当 Mydataset 构建好，剩下的操作就交给 DataLoder，在 DataLoder 中，会触发Mydataset 中的 getiterm 函数读取一张图片的数据和标签，并拼接成一个 batch 返回，作为模型真正的输入。</p>
<h2 id="1-4-DataLoder加载图片"><a href="#1-4-DataLoder加载图片" class="headerlink" title="1.4 DataLoder加载图片"></a>1.4 DataLoder加载图片</h2><p>getitem是在DataLoader中触发的，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 构建MyDataset实例</span></div><div class="line">train_data = MyDataset(txt_path=train_txt_path, transform=trainTransform)</div><div class="line"></div><div class="line"><span class="comment"># 构建DataLoder</span></div><div class="line">train_loader = DataLoader(dataset=train_data, batch_size=train_bs, shuffle=<span class="keyword">True</span>)</div><div class="line"></div><div class="line"><span class="comment"># 这里的data就是__getitem__返回的img, label</span></div><div class="line"><span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(train_loader):</div><div class="line"></div><div class="line"><span class="string">"""</span></div><div class="line">-&gt;     def __iter__(self):</div><div class="line">        return _DataLoaderIter(self)</div><div class="line">        </div><div class="line">    class _DataLoaderIter(object):</div><div class="line">    	def __next__(self):</div><div class="line">    		# collate_fn (callable, optional): merges a list of samples to form a mini-batch.</div><div class="line">        # 这里调用__getitem__</div><div class="line">    		batch = self.collate_fn([self.dataset[i] for i in indices])</div><div class="line">"""</div><div class="line">  </div><div class="line">	<span class="comment"># 获取图片和标签</span></div><div class="line">  inputs, labels = data</div><div class="line">  inputs, labels = Variable(inputs), Variable(labels)</div></pre></td></tr></table></figure>
<blockquote>
<p>图片是通过 Image.open()函数读取进来的，当涉及如下问题：</p>
<p>图片的通道顺序(RGB ? BGR ?)</p>
<p>图片是<code>w*h*c ? c*w*h ?</code></p>
<p>像素值范围[0-1] or [0-255] ?</p>
<p>就要查看 MyDataset()类中 <strong>getitem</strong>()下读取图片用的是什么方法</p>
</blockquote>
<h2 id="1-5-数据增强和数据标准化"><a href="#1-5-数据增强和数据标准化" class="headerlink" title="1.5 数据增强和数据标准化"></a>1.5 数据增强和数据标准化</h2><p>在 PyTorch 中，数据增强方法放在了 transforms.py 文件中。</p>
<p>这一节主要介绍transforms的操作。</p>
<p>1.6 transforms 的二十二个方法</p>
<h1 id="第二章-模型"><a href="#第二章-模型" class="headerlink" title="第二章 模型"></a>第二章 模型</h1><h2 id="2-1-模型的搭建"><a href="#2-1-模型的搭建" class="headerlink" title="2.1 模型的搭建"></a>2.1 模型的搭建</h2><h3 id="2-1-1-模型定义的三要素"><a href="#2-1-1-模型定义的三要素" class="headerlink" title="2.1.1 模型定义的三要素"></a>2.1.1 模型定义的三要素</h3><p>1）首先，必须继承 nn.Module 这个类，要让 PyTorch 知道这个类是一个 Module。</p>
<p>2）其次，在<strong>init</strong>(self)中设置好需要的“组件”(如 conv、pooling、Linear、BatchNorm等）。</p>
<p>3）最后，在 forward(self, x)中用定义好的“组件”进行组装，就像搭积木，把网络结构搭建</p>
<p>在/Code/main<em>training/main.py 中可以看到定义了一个类<code>class Net(nn.Module)</code>，集成了nn.Module，先看<em>_init</em></em>(self)函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">		<span class="comment"># 初始化</span></div><div class="line">    super(Net, self).__init__()</div><div class="line">    <span class="comment"># 定义了一系列组件</span></div><div class="line">    self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</div><div class="line">    self.pool1 = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</div><div class="line">    self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">3</span>)</div><div class="line">    self.pool2 = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</div><div class="line">    self.conv3 = nn.Conv2d(<span class="number">16</span>, <span class="number">64</span>, <span class="number">3</span>)</div><div class="line">    self.pool3=nn.MaxPool2d(<span class="number">2</span>,<span class="number">1</span>)</div><div class="line">    self.fc1 = nn.Linear(<span class="number">64</span> * <span class="number">3</span> * <span class="number">3</span>, <span class="number">120</span>)</div><div class="line">    self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</div><div class="line">    self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</div></pre></td></tr></table></figure>
<p>当这些组件定义好之后，就可以定义 forward()函数，用来搭建网络结构</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">		<span class="comment"># x作为模型的输入，x 经过 conv1，然后经过激活函数 relu，再经过 pool1 操作</span></div><div class="line">    x = self.pool1(F.relu(self.conv1(x)))</div><div class="line">    <span class="comment"># 再做一轮</span></div><div class="line">    x = self.pool2(F.relu(self.conv2(x)))</div><div class="line">    <span class="comment"># 再做一轮</span></div><div class="line">    x=self.pool3(F.relu(self.conv3(x)))</div><div class="line">    <span class="comment"># 将 x 进行 reshape，为了后面做为全连接层的输入</span></div><div class="line">    x = x.view(<span class="number">-1</span>, <span class="number">64</span> * <span class="number">3</span>* <span class="number">3</span>)</div><div class="line">    <span class="comment"># 先经过全连接层 fc，然后经过 relu</span></div><div class="line">    x = F.relu(self.fc1(x))</div><div class="line">    x = F.relu(self.fc2(x))</div><div class="line">    x = self.fc3(x)</div><div class="line">    <span class="keyword">return</span> x</div></pre></td></tr></table></figure>
<h3 id="2-1-2-一个更复杂的模型"><a href="#2-1-2-一个更复杂的模型" class="headerlink" title="2.1.2 一个更复杂的模型"></a>2.1.2 一个更复杂的模型</h3><p>来看一个更复杂的模型，看Resnet网络的定义方法<a href="https://github.com/yuanlairuci110/pytorch-best-practice-master/blob/master/models/ResNet34.py" target="_blank" rel="noopener">https://github.com/yuanlairuci110/pytorch-best-practice-master/blob/master/models/ResNet34.py</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#coding:utf8</span></div><div class="line"><span class="keyword">from</span> .BasicModule <span class="keyword">import</span> BasicModule</div><div class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</div><div class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResidualBlock</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="string">'''</span></div><div class="line">    实现子module: Residual Block</div><div class="line">    '''</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, inchannel, outchannel, stride=<span class="number">1</span>, shortcut=None)</span>:</span></div><div class="line">        super(ResidualBlock, self).__init__()</div><div class="line">        self.left = nn.Sequential(</div><div class="line">                nn.Conv2d(inchannel, outchannel, <span class="number">3</span>, stride, <span class="number">1</span>, bias=<span class="keyword">False</span>),</div><div class="line">                nn.BatchNorm2d(outchannel),</div><div class="line">                nn.ReLU(inplace=<span class="keyword">True</span>),</div><div class="line">                nn.Conv2d(outchannel, outchannel, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, bias=<span class="keyword">False</span>),</div><div class="line">                nn.BatchNorm2d(outchannel) )</div><div class="line">        self.right = shortcut</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        out = self.left(x)</div><div class="line">        residual = x <span class="keyword">if</span> self.right <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">else</span> self.right(x)</div><div class="line">        out += residual</div><div class="line">        <span class="keyword">return</span> F.relu(out)</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet34</span><span class="params">(BasicModule)</span>:</span></div><div class="line">    <span class="string">'''</span></div><div class="line">    实现主module：ResNet34</div><div class="line">    ResNet34包含多个layer，每个layer又包含多个Residual block</div><div class="line">    用子module来实现Residual block，用_make_layer函数来实现layer</div><div class="line">    '''</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_classes=<span class="number">2</span>)</span>:</span></div><div class="line">        super(ResNet34, self).__init__()</div><div class="line">        self.model_name = <span class="string">'resnet34'</span></div><div class="line"></div><div class="line">        <span class="comment"># 前几层: 图像转换</span></div><div class="line">        self.pre = nn.Sequential(</div><div class="line">                nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">3</span>, bias=<span class="keyword">False</span>),</div><div class="line">                nn.BatchNorm2d(<span class="number">64</span>),</div><div class="line">                nn.ReLU(inplace=<span class="keyword">True</span>),</div><div class="line">                nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>))</div><div class="line">        </div><div class="line">        <span class="comment"># 重复的layer，分别有3，4，6，3个residual block</span></div><div class="line">        self.layer1 = self._make_layer( <span class="number">64</span>, <span class="number">128</span>, <span class="number">3</span>)</div><div class="line">        self.layer2 = self._make_layer( <span class="number">128</span>, <span class="number">256</span>, <span class="number">4</span>, stride=<span class="number">2</span>)</div><div class="line">        self.layer3 = self._make_layer( <span class="number">256</span>, <span class="number">512</span>, <span class="number">6</span>, stride=<span class="number">2</span>)</div><div class="line">        self.layer4 = self._make_layer( <span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, stride=<span class="number">2</span>)</div><div class="line"></div><div class="line">        <span class="comment">#分类用的全连接</span></div><div class="line">        self.fc = nn.Linear(<span class="number">512</span>, num_classes)</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_make_layer</span><span class="params">(self,  inchannel, outchannel, block_num, stride=<span class="number">1</span>)</span>:</span></div><div class="line">        <span class="string">'''</span></div><div class="line">        构建layer,包含多个residual block</div><div class="line">        '''</div><div class="line">        shortcut = nn.Sequential(</div><div class="line">                nn.Conv2d(inchannel,outchannel,<span class="number">1</span>,stride, bias=<span class="keyword">False</span>),</div><div class="line">                nn.BatchNorm2d(outchannel))</div><div class="line">        </div><div class="line">        layers = []</div><div class="line">        layers.append(ResidualBlock(inchannel, outchannel, stride, shortcut))</div><div class="line">        </div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, block_num):</div><div class="line">            layers.append(ResidualBlock(outchannel, outchannel))</div><div class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</div><div class="line">        </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        x = self.pre(x)</div><div class="line">        </div><div class="line">        x = self.layer1(x)</div><div class="line">        x = self.layer2(x)</div><div class="line">        x = self.layer3(x)</div><div class="line">        x = self.layer4(x)</div><div class="line"></div><div class="line">        x = F.avg_pool2d(x, <span class="number">7</span>)</div><div class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)</div><div class="line">        <span class="keyword">return</span> self.fc(x)</div></pre></td></tr></table></figure>
<p>这里用到了<code>torch.nn.Sequential</code></p>
<h3 id="2-1-3-nn-Sequential"><a href="#2-1-3-nn-Sequential" class="headerlink" title="2.1.3 nn.Sequential"></a>2.1.3 nn.Sequential</h3><p>这个是Sequential容器，将一系列操作包起来。例如Resnet有很多重复的block，就可以包起来。</p>
<p>官方文档中给了两个例子</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Example of using Sequential model = nn.Sequential(</span></div><div class="line">nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>), nn.ReLU(), nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>), nn.ReLU()</div><div class="line">)</div><div class="line"></div><div class="line"><span class="comment"># Example of using Sequential with OrderedDict</span></div><div class="line">model = nn.Sequential(OrderedDict([</div><div class="line">(<span class="string">'conv1'</span>, nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>)), (<span class="string">'relu1'</span>, nn.ReLU()),</div><div class="line">(<span class="string">'conv2'</span>, nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>)), (<span class="string">'relu2'</span>, nn.ReLU())</div><div class="line">]))</div></pre></td></tr></table></figure>
<p>总结：模型的定义就是先继承，再构建组件，最后组装(forward)。</p>
<h2 id="2-2-权值初始化的十种方法"><a href="#2-2-权值初始化的十种方法" class="headerlink" title="2.2 权值初始化的十种方法"></a>2.2 权值初始化的十种方法</h2><p>初始化方法会直接影响模型的收敛与否</p>
<h3 id="2-2-1-权重初始化流程"><a href="#2-2-1-权重初始化流程" class="headerlink" title="2.2.1 权重初始化流程"></a>2.2.1 权重初始化流程</h3><p>总共两步，</p>
<p>1）先设定什么层用什么初始化方法，初始化方法在torch.nn.init中给出；</p>
<p>2）实例化一个模型之后，执行该函数，即可完成初始化。</p>
<p>重点是第一步，看Main的方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义权值初始化</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize_weights</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</div><div class="line">            <span class="keyword">if</span> isinstance(m, nn.Conv2d):</div><div class="line">                torch.nn.init.xavier_normal_(m.weight.data)</div><div class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">                    <span class="comment"># 若有bias，初始化全为0</span></div><div class="line">                    m.bias.data.zero_()</div><div class="line">            <span class="keyword">elif</span> isinstance(m, nn.BatchNorm2d):</div><div class="line">                m.weight.data.fill_(<span class="number">1</span>)</div><div class="line">                m.bias.data.zero_()</div><div class="line">            <span class="keyword">elif</span> isinstance(m, nn.Linear):</div><div class="line">                torch.nn.init.normal_(m.weight.data, <span class="number">0</span>, <span class="number">0.01</span>)</div><div class="line">                m.bias.data.zero_()</div></pre></td></tr></table></figure>
<h3 id="2-2-2-常用初始化方法"><a href="#2-2-2-常用初始化方法" class="headerlink" title="2.2.2 常用初始化方法"></a>2.2.2 常用初始化方法</h3><p>1）Xavier，kaiming系列</p>
<p>2）其他方法分布</p>
<p>Xavier 初始化方法，论文在《Understanding the difficulty of training deep feedforward neural  networks》 </p>
<p>公式推导是从“方差一致性”出发，初始化的分布有均匀分布和正态分布两种。</p>
<h4 id="1、Xavier均匀分布"><a href="#1、Xavier均匀分布" class="headerlink" title="1、Xavier均匀分布"></a>1、Xavier均匀分布</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torch.nn.init.xavier_uniform_(tensor, gain=1)</div></pre></td></tr></table></figure>
<p>服从均匀分布U(-a, a)，分布的参数$a=gain * sqrt(6/fan_in+fan_out)$</p>
<p>这里有一个gain，增益的大小是依据激活函数类型来设定。如</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">nn.init.xavier_uniform_(w, gain=nn.init.calculate_gain(&apos;relu&apos;))</div></pre></td></tr></table></figure>
<p>上述方法也成为Glorot initialization</p>
<h4 id="2、Xavier正态分布"><a href="#2、Xavier正态分布" class="headerlink" title="2、Xavier正态分布"></a>2、Xavier正态分布</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torch.nn.init.xavier_normal_(tensor, gain=1)</div></pre></td></tr></table></figure>
<h4 id="3、kaiming均匀分布"><a href="#3、kaiming均匀分布" class="headerlink" title="3、kaiming均匀分布"></a>3、kaiming均匀分布</h4><p>论文在《 Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification》。公式推导同样从“方差一致性”出法，kaiming是针对 xavier 初始化方法在 relu 这一类激活函数表现不佳而提出的改进</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torch.nn.init.kaiming_uniform_(tensor, a=0, mode=&apos;fan_in&apos;, nonlinearity=&apos;leaky_relu&apos;)</div></pre></td></tr></table></figure>
<p>其中，a是激活函数的负半轴的斜率，relu是0</p>
<p>mode可选为fan_in或fan_out，前者使正向传播时方差一致，后者使反向传播时方差一致。</p>
<p>nonlinearity可选relu和leaky_relu，默认值为leaky_relu。</p>
<p>4、kaiming正态分布</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torch.nn.init.kaiming_normal_(tensor, a=0, mode=&apos;fan_in&apos;, nonlinearity=&apos;leaky_relu&apos;)</div></pre></td></tr></table></figure>
<p>5、其他方法</p>
<p>参见文档</p>
<p><strong>权值初始化杂谈</strong></p>
<p>1、从代码中发现，即使不进行初始化，模型的权重也不为空，而是有值的，这些值是什么时候赋给的呢？</p>
<blockquote>
<p>其实，在创建网络实例的过程中，一旦调用nn.Conv2d的时候就会对权值进行初始化。</p>
<p>初始化过程是在Conv2d的基类_ConvNd中进行的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&gt; class Conv2d(_ConvNd):</div><div class="line">&gt; --&gt; 在_ConvNd 中:</div><div class="line">&gt; --&gt; self.reset_parameters()</div><div class="line">&gt; ---&gt; def reset_parameters(self)</div><div class="line">&gt; ---&gt; self.weight.data.uniform_(-stdv, stdv)</div><div class="line">&gt;</div></pre></td></tr></table></figure>
</blockquote>
<p>&gt;</p>
<blockquote>
<p>可以看出这里是均匀分布，其中-stdv与kernel的size有关。</p>
<p>补充：在Pytorch1.0版本中，这里改用了kaiming<em>uniform</em>()进行初始化。</p>
</blockquote>
<p>2、按需定义初始化方法，例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">if isinstance(m, nn.Conv2d):</div><div class="line">	n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels 		</div><div class="line">	m.weight.data.normal_(0, math.sqrt(2. / n))</div></pre></td></tr></table></figure>
<h2 id="2-3-模型Finetune"><a href="#2-3-模型Finetune" class="headerlink" title="2.3 模型Finetune"></a>2.3 模型Finetune</h2><p>实际应用中，通常采用一个已经训练模型的权值参数作为我们模型的初始化参数，也称之为finetune—迁移学习。迁移学习中的 Finetune 技术，本质上就是让我们新构建的模型，拥有一个较好的权值初始值。</p>
<p>finetune 权值初始化三步曲，finetune 就相当于给模型进行初始化，其流程共用三步:<br>第一步:保存模型，拥有一个预训练模型;<br>第二步:加载模型，把预训练模型中的权值取出来;<br>第三步:初始化，将权值对应的“放”到新模型中</p>
<h3 id="2-3-1-权值初始化"><a href="#2-3-1-权值初始化" class="headerlink" title="2.3.1 权值初始化"></a>2.3.1 权值初始化</h3><p>在进行 finetune 之前我们需要拥有一个模型或者是模型参数，因此需要了解如何保存 模型。官方文档中介绍了两种保存模型的方法，一种是保存整个模型，另外一种是仅保存 模型参数(官方推荐用这种方法)，这里采用官方推荐的方法。 </p>
<h4 id="1、保存模型参数"><a href="#1、保存模型参数" class="headerlink" title="1、保存模型参数"></a>1、保存模型参数</h4><p>若拥有模型参数，可跳过这一步。<br>假设创建了一个 net = Net()，并且经过训练，通过以下方式保存:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torch.save(net.state_dict(), &apos;net_params.pkl&apos;)</div></pre></td></tr></table></figure>
<h4 id="2、加载模型"><a href="#2、加载模型" class="headerlink" title="2、加载模型"></a>2、加载模型</h4><p>进行三步曲中的第二步，加载模型，这里只是加载模型的参数: </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pretrained_dict = torch.load(&apos;net_params.pkl&apos;)</div></pre></td></tr></table></figure>
<h4 id="3、初始化"><a href="#3、初始化" class="headerlink" title="3、初始化"></a>3、初始化</h4><p>进行三步曲中的第三步，将取到的权值，对应的放到新模型中: 首先我们创建新模型，并且获取新模型的参数字典 net_state_dict: </p>
<p>net=Net()# 创建net</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">net_state_dict = net.state_dict() # 获取已创建 net 的 state_dict</div></pre></td></tr></table></figure>
<p>接着将 pretrained_dict 里不属于 net_state_dict 的键<strong>剔除掉</strong>: </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pretrained_dict_1 = &#123;k: v for k, v in pretrained_dict.items() if k in net_state_dict&#125;</div></pre></td></tr></table></figure>
<p>然后，用预训练模型的参数字典 对 新模型的参数字典 net_state_dict 进行更新: </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">net_state_dict.update(pretrained_dict_1)</div></pre></td></tr></table></figure>
<p>最后，将更新了参数的字典 “放”回到网络中: </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">net.load_state_dict(net_state_dict)</div></pre></td></tr></table></figure>
<p>采用 finetune 的训练过程中，<strong>有时候希望前面层的学习率低一些，改变不要太大，而 后面的全连接层的学习率相对大一些</strong>。这时就需要对不同的层设置不同的学习率，下面就 介绍如何为不同层配置不同的学习率。 </p>
<h3 id="2-3-2-不同层设置不同学习率"><a href="#2-3-2-不同层设置不同学习率" class="headerlink" title="2.3.2 不同层设置不同学习率"></a>2.3.2 不同层设置不同学习率</h3><p>在利用 pre-trained model 的参数做初始化之后，我们可能想让 fc 层更新相对快一些，而希望前面的权值更新小一些，这就可以通过为不同的层设置不同的学习率来达到此目的。</p>
<p>为不同层设置不同的学习率，主要通过优化器对多个参数组进行设置不同的参数。所以，只需要将原始的参数组，划分成两个，甚至更多的参数组，然后分别进行设置学习率。</p>
<p>这里将原始参数“切分”成 fc3 层参数和其余参数，为 fc3 层设置更大的学习率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 返回的是 parameters 的 内存地址 </span></div><div class="line"><span class="comment"># 将fc3层的参数从原始参数net.parameters中剥离出来。</span></div><div class="line"><span class="comment"># base_params是剥离了fc3层参数的其他参数。</span></div><div class="line">ignored_params = list(map(id, net.fc3.parameters())) </div><div class="line">base_params = filter(<span class="keyword">lambda</span> p: id(p) <span class="keyword">not</span> <span class="keyword">in</span> 	ignored_params, net.parameters())</div><div class="line"><span class="comment"># 然后优化器中为fc3的单独设置学习率</span></div><div class="line">optimizer = optim.SGD([</div><div class="line">  &#123;<span class="string">'params'</span>: base_params&#125;,</div><div class="line">	&#123;<span class="string">'params'</span>: net.fc3.parameters(), <span class="string">'lr'</span>: <span class="number">0.001</span>*<span class="number">10</span>&#125;], <span class="number">0.001</span>, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">1e-4</span>)</div><div class="line">])</div></pre></td></tr></table></figure>
<p>完整代码在<code>/Code/2_model/2_finetune.py</code></p>
<h1 id="第三章-损失函数和优化器"><a href="#第三章-损失函数和优化器" class="headerlink" title="第三章 损失函数和优化器"></a>第三章 损失函数和优化器</h1><p>Pytorch中十七个损失函数，十个优化器和六个学习率调整方法。</p>
<h2 id="3-1-十七个损失函数"><a href="#3-1-十七个损失函数" class="headerlink" title="3.1 十七个损失函数"></a>3.1 十七个损失函数</h2><p>我们所说的优化，即优化网络权值使得损失函数值变小。但是，损失函数值变小是否能代表模型的分类/回归精度变高呢?那么多种损失函数，应该如何选择呢?请来了解PyTorch 中给出的十七种损失函数吧。</p>
<h3 id="3-1-1-L1loss"><a href="#3-1-1-L1loss" class="headerlink" title="3.1.1 L1loss"></a>3.1.1 L1loss</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">class torch.nn.L1Loss(size_average=None, reduce=None)</div></pre></td></tr></table></figure>
<p>官方文档中仍有 reduction=’elementwise_mean’参数，但代码实现中已经删除该参数</p>
<p><strong>功能：</strong></p>
<p>计算output和target之差的绝对值，可选返回同维度的tensor或一个标量。</p>
<p><strong>公式：</strong></p>
<p><img src="/2019/06/14/深度学习笔记/Pytorch笔记/完整的一次任务/pic/image-20190616085944731.png" alt="image-20190616085944731"></p>
<p><strong>参数:</strong> </p>
<p>reduce(bool)- 返回值是否为标量，默认为 True<br>size_average(bool)- 当 reduce=True 时有效。为 True 时，返回的 loss 为平均值;为 False 时，返回的各样本的 loss 之和。<br><strong>实例:</strong> </p>
<p><code>/Code/3_optimizer/3_1_lossFunction/1_L1Loss.py</code></p>
<h1 id="第四章-监控模型-可视化"><a href="#第四章-监控模型-可视化" class="headerlink" title="第四章 监控模型-可视化"></a>第四章 监控模型-可视化</h1><h2 id="4-1-TensorBoardX"><a href="#4-1-TensorBoardX" class="headerlink" title="4.1 TensorBoardX"></a>4.1 TensorBoardX</h2><p>流行的有两种方法，本文重点介绍第二种。</p>
<p>1、构建Logger类</p>
<p>Logger 类中“包”了 tf.summary.FileWriter ，截至目前(2018.10.17)，只有三种操作，分别是 scalar_summary(), image_summary(), histo_summary()。</p>
<p>优点:轻便，可满足大部分需求</p>
<p>参考github：<a href="https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/04-utils/tensorboard" target="_blank" rel="noopener">https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/04-utils/tensorboard</a></p>
<p>2、借助TensorBoardX包</p>
<p>TensorBoardX 包的功能就比较全，截至目前(2018.10.17)，支持除<br>beholder 之外的所有 tensorboard 的记录类型。</p>
<p>github：<a href="https://github.com/lanpa/tensorboardX" target="_blank" rel="noopener">https://github.com/lanpa/tensorboardX</a></p>
<p>API文档：<a href="https://tensorboard-pytorch.readthedocs.io/en/latest/tutorial_zh.html" target="_blank" rel="noopener">https://tensorboard-pytorch.readthedocs.io/en/latest/tutorial_zh.html</a></p>
<p><strong>代码实现:</strong></p>
<p>tensorboardX 提供 13 个函数，可以记录标量、图像、语音、文字等等，功能十分丰富。<br>本节将对这些函数进行介绍，所用代码为 tensorboardX 的官方 demo.py，放在：</p>
<p><code>/Code/4_viewer/1_tensorboardX_demo.py</code></p>
<p>运行该文件，再打开一个terminal，进入/Result/，执行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tensorboard --logdir=runs</div></pre></td></tr></table></figure>
<p>然后浏览器打开</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">localhost:6006</div></pre></td></tr></table></figure>
<p>可以看到显示界面如下：</p>
<h2 id="4-2-TensorBoardX的函数"><a href="#4-2-TensorBoardX的函数" class="headerlink" title="4.2 TensorBoardX的函数"></a>4.2 TensorBoardX的函数</h2><h3 id="4-2-1-add-scalar"><a href="#4-2-1-add-scalar" class="headerlink" title="4.2.1 add_scalar()"></a>4.2.1 add_scalar()</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">add_scalar(tag, scalar_value, global_step=None, walltime=None)</div></pre></td></tr></table></figure>
<p><strong>功能：</strong></p>
<p>在一个图表中记录一个标量的变化，常用于Loss和Accuracy曲线的记录。</p>
<p><strong>参数:</strong> </p>
<p>tag(string)- 该图的标签，类似于 polt.title。 </p>
<p>scalar_value(float or string/blobname)- 用于存储的值，曲线图的 y 坐标 </p>
<p>global_step(int)- 曲线图的 x 坐标<br> walltime(float)- 为 event 文件的文件名设置时间，默认为 time.time() 运行 demo 中的: </p>
<p>用 github 首页 demo 运行这一行:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">writer.add_scalar(&apos;data/scalar1&apos;, dummy_s1[0], n_iter)</div></pre></td></tr></table></figure>
<p> 可以得到下图: </p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/32/">32</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Schwimmer</p>
              <div class="site-description motion-element" itemprop="description">Record and Think!</div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">314</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">17</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">55</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          

          

          
          

          
            
          
          

        </div>
      </div>

      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Schwimmer</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.5.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> v7.2.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/utils.js?v=7.2.0"></script>

  <script src="/js/motion.js?v=7.2.0"></script>



  
  


  <script src="/js/schemes/muse.js?v=7.2.0"></script>




  

  


  <script src="/js/next-boot.js?v=7.2.0"></script>


  

  

  

  
  
<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://https-schwimmer-github-io.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->







  




  

  

  

  

  

  

  

  

  

  

  

  

  

  

</body>
</html>
