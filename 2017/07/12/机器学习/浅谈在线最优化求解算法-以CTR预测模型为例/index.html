<!doctype html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="google-site-verification" content="Mx7Ikp0IpBtTbSpHDTBV0_CMJA-E8CLn8NRIrwyq5m4" />






<meta name="baidu-site-verification" content="ZBTsWx4NdC" />






  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Lobster Two:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="最优化问题,CTR," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="[TOC] 1、最优化求解问题通常，我们需要求解的最优化问题有如下三类： （1）无约束优化问题：  X=\arg \underset{X}{min}f(X)含义是求解X，令目标函数$f(X)$最小。 对于这类问题，在$f(X)$ 是凸函数的前提下，通常做法就是对$f(X)$ 求导，并令$\frac {\partial} {\partial X} f(X) =0$ ，求解可以得到最优值。   凸函数">
<meta name="keywords" content="最优化问题,CTR">
<meta property="og:type" content="article">
<meta property="og:title" content="浅谈在线最优化求解算法-以CTR预测模型为例">
<meta property="og:url" content="https://schwimmer.github.io/2017/07/12/机器学习/浅谈在线最优化求解算法-以CTR预测模型为例/index.html">
<meta property="og:site_name" content="Schwimmer&#39;s Blog">
<meta property="og:description" content="[TOC] 1、最优化求解问题通常，我们需要求解的最优化问题有如下三类： （1）无约束优化问题：  X=\arg \underset{X}{min}f(X)含义是求解X，令目标函数$f(X)$最小。 对于这类问题，在$f(X)$ 是凸函数的前提下，通常做法就是对$f(X)$ 求导，并令$\frac {\partial} {\partial X} f(X) =0$ ，求解可以得到最优值。   凸函数">
<meta property="og:image" content="https://schwimmer.github.io/2017/07/12/机器学习/浅谈在线最优化求解算法-以CTR预测模型为例/凸函数.png">
<meta property="og:image" content="https://schwimmer.github.io/2017/07/12/机器学习/浅谈在线最优化求解算法-以CTR预测模型为例/求导的推导.jpg">
<meta property="og:image" content="https://schwimmer.github.io/2017/07/12/机器学习/浅谈在线最优化求解算法-以CTR预测模型为例/最大似然估计求偏导.png">
<meta property="og:image" content="https://schwimmer.github.io/2017/07/12/机器学习/浅谈在线最优化求解算法-以CTR预测模型为例/过拟合1.png">
<meta property="og:image" content="https://schwimmer.github.io/2017/07/12/机器学习/浅谈在线最优化求解算法-以CTR预测模型为例/正则化解空间.png">
<meta property="og:image" content="https://schwimmer.github.io/2017/07/12/机器学习/浅谈在线最优化求解算法-以CTR预测模型为例/FTRL伪代码.png">
<meta property="og:image" content="https://schwimmer.github.io/2017/07/12/机器学习/浅谈在线最优化求解算法-以CTR预测模型为例/roc3.png">
<meta property="og:image" content="https://schwimmer.github.io/2017/07/12/机器学习/浅谈在线最优化求解算法-以CTR预测模型为例/roc1.png">
<meta property="og:image" content="https://schwimmer.github.io/2017/07/12/机器学习/浅谈在线最优化求解算法-以CTR预测模型为例/roc2.png">
<meta property="og:image" content="https://schwimmer.github.io/2017/07/12/机器学习/浅谈在线最优化求解算法-以CTR预测模型为例/roc4.png">
<meta property="og:updated_time" content="2018-01-30T06:55:44.960Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="浅谈在线最优化求解算法-以CTR预测模型为例">
<meta name="twitter:description" content="[TOC] 1、最优化求解问题通常，我们需要求解的最优化问题有如下三类： （1）无约束优化问题：  X=\arg \underset{X}{min}f(X)含义是求解X，令目标函数$f(X)$最小。 对于这类问题，在$f(X)$ 是凸函数的前提下，通常做法就是对$f(X)$ 求导，并令$\frac {\partial} {\partial X} f(X) =0$ ，求解可以得到最优值。   凸函数">
<meta name="twitter:image" content="https://schwimmer.github.io/2017/07/12/机器学习/浅谈在线最优化求解算法-以CTR预测模型为例/凸函数.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"always","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://schwimmer.github.io/2017/07/12/机器学习/浅谈在线最优化求解算法-以CTR预测模型为例/"/>





  <title> 浅谈在线最优化求解算法-以CTR预测模型为例 | Schwimmer's Blog </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  














  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Schwimmer's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description"></h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocapitalize="off" autocomplete="off" autocorrect="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://schwimmer.github.io/2017/07/12/机器学习/浅谈在线最优化求解算法-以CTR预测模型为例/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Schwimmer">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Schwimmer's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
            
            
              
                浅谈在线最优化求解算法-以CTR预测模型为例
              
            
          </h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-07-12T11:49:53+08:00">
                2017-07-12
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
            <!--noindex-->
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/07/12/机器学习/浅谈在线最优化求解算法-以CTR预测模型为例/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count hc-comment-count" data-xid="2017/07/12/机器学习/浅谈在线最优化求解算法-以CTR预测模型为例/" itemprop="commentsCount"></span>
                </a>
              </span>
              <!--/noindex-->
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>[TOC]</p>
<h1 id="1、最优化求解问题"><a href="#1、最优化求解问题" class="headerlink" title="1、最优化求解问题"></a>1、最优化求解问题</h1><p>通常，我们需要求解的最优化问题有如下三类：</p>
<p><strong>（1）无约束优化问题</strong>：</p>
<script type="math/tex; mode=display">
X=\arg \underset{X}{min}f(X)</script><p>含义是求解X，令目标函数$f(X)$最小。</p>
<p>对于这类问题，在$f(X)$ 是凸函数的前提下，通常做法就是对$f(X)$ 求导，并令$\frac {\partial} {\partial X} f(X) =0$ ，求解可以得到最优值。</p>
<blockquote>
<p> <strong>凸函数</strong></p>
<p> 如果$f(x)$是定义在N维向量空间上的实变量函数，对于在$f(x)$的定义域C上的任意两个点$x_1$和$x_2$，以及任意[0,1]之间的值t都有：</p>
<script type="math/tex; mode=display">
 f(tX_1 + (1-t)X_2) \leq tf(X_1)+(1-t)f(X_2)\\
 \forall X_1,X_2 \in C,\ \ 0 \leq t \leq 1</script><p> 则称$f(x)$是凸函数。一个函数是凸函数是其存在最优解的充要条件。</p>
<p> 此外，如果$f(x)$满足</p>
<script type="math/tex; mode=display">
 f(tX_1 + (1-t)X_2)< tf(X_1)+(1-t)f(X_2)\\
 \forall X_1,X_2 \in C,\ \ 0 \leq t \leq 1</script><p> 则$f(x)$为严格凸函数。如下图所示，左边是严格凸函数，右边是凸函数</p>
<p> <img src="/2017/07/12/机器学习/浅谈在线最优化求解算法-以CTR预测模型为例/凸函数.png" alt="凸函数"></p>
</blockquote>
<p><strong>（2）有等式约束的最优化问题</strong>：</p>
<script type="math/tex; mode=display">
X=\arg \underset{X}{min}f(X)\\
s.t. h_k(X)=0;k=1,2,...,n</script><p>含义是在n个等式约束$h_k(X)$ 的条件下求解X，另目标函数$f(X)$最小。</p>
<p>针对有等式的最优化问题，采用<strong>拉格朗日乘数法</strong>进行求解，通过拉格朗日系数$A=[a_1,a_2,…,a_n]^T$ 把等式约束和目标函数组合成一个式子</p>
<script type="math/tex; mode=display">
X=\arg \underset{X}{min}[f(X)+ A^TH(X)]</script><p>相当于转化成无约束最优化求解问题，解决方法是分别对X，A求偏导并令其等于0。</p>
<p><strong>（3）不等式约束的优化问题求解</strong> ：</p>
<script type="math/tex; mode=display">
X=\arg \underset{X}{min}f(X)\\
s.t. h_k(X)=0;k=1,2,...,n\\
g_l(X)\leq 0;l=1,2,...,m</script><p>对于不等式约束，通过KKT条件求解。将所有的约束和目标函数写为一个式子</p>
<script type="math/tex; mode=display">
L(X,A,B)=f(X)+A^TH(X)+B^TG(X)</script><p>KKT条件是说最优值必须满足以下条件：</p>
<script type="math/tex; mode=display">
\frac \partial {\partial X} L(X,A,B)=0\\
H(X)=0\\
B^TG(X)=0</script><p>KKT条件是求解最优值的必要条件，要使其成为充要条件，还需要f(x)为凸函数。</p>
<h1 id="2、批量最优化求解算法"><a href="#2、批量最优化求解算法" class="headerlink" title="2、批量最优化求解算法"></a>2、批量最优化求解算法</h1><p>一些定义：</p>
<p>$i=1,2,…,N$表示向量维度</p>
<p>$j=1,2,…,M$表示样本个数</p>
<p>$t=1,2,…$表示迭代次数</p>
<h2 id="2-1-批量和随机求解"><a href="#2-1-批量和随机求解" class="headerlink" title="2.1 批量和随机求解"></a>2.1 批量和随机求解</h2><p>我们面对的最优化问题都是无约束的最优化问题（有约束的也可以转成无约束的），因此通常可以将其描述为</p>
<script type="math/tex; mode=display">
W=\arg \underset{W}{min}\   l(W,Z)\\
Z=\{ (X_j,y_j) | j=1,2,...,M  \}\\
y_j=h(W,X_j)
\tag {2-1-1}</script><p>就是<strong>在已知训练集的情况下，求使得目标函数最小的权重矩阵</strong>。其中，$Z$是训练集，$\mathbf{X}$是特征向量，$X_j$是其中一个样本，$Y$是预测值，$y_j$是其中一个样本对应的预测值。一共有M个样本。$h(W,X_j)$ 是特征向量到预测值的<strong>映射函数</strong>，$ l(W,Z)$ 最优化求解的目标函数，也称为<strong>损失函数</strong>，$W$ 为特征权重，也就是在损失函数中需要求解的参数。</p>
<blockquote>
<p> 损失函数一般包括损失项和正则项</p>
</blockquote>
<p>常用的损失函数有：</p>
<p>（1）<strong>平方损失函数</strong>（线性回归）</p>
<p>最小二乘法（Ordinary Least Squares）是常用的一种平方损失函数，最小二乘的基本原理是：最优拟合直线应该是使各点到回归直线的距离和最小的直线，即平方和最小。</p>
<p>线性回归的映射函数为：</p>
<script type="math/tex; mode=display">
h(W,X_j)=W^TX_j</script><p>损失函数可以表示为</p>
<script type="math/tex; mode=display">
l(W,Z)=\sum_{j=1}^M (y_j-W^TX_j)^2</script><p>（2）<strong>Logistics损失函数</strong>（逻辑回归）</p>
<p>逻辑回归的映射函数为：</p>
<script type="math/tex; mode=display">
h(W,X_j)=\frac 1 {1+e^{-W^TX_j}}</script><blockquote>
<p>logistic函数的优点是：</p>
<p>1、他的输入范围是$-\infty \rightarrow  + \infty $ ，<strong>输出范围是(0,1)，正好满足概率分布为（0，1）的要求</strong>。我们用概率去描述分类器，自然比单纯的某个阈值要方便很多； </p>
<p>2、是一个单调上升的函数，具有良好的连续性，<strong>不存在不连续点</strong>。</p>
</blockquote>
<p>由于该函数服从伯努利分布（0-1分布），通过最大似然估计，对于每一维的权重W，损失函数可以表示为</p>
<script type="math/tex; mode=display">
l(W,Z)=(Y-h_W(\mathbf X))X</script><blockquote>
<p><strong>推导过程</strong></p>
<p>令</p>
<script type="math/tex; mode=display">
h_W(X) = \frac 1 {1+e^{-W^T\mathbf X}}</script><p>该函数服从伯努利分布（一次点击要么成功，要么失败，通过训练集可以知道不同特征组合下成功和失败的概率）</p>
<script type="math/tex; mode=display">
P(Y=1 | \mathbf X;W) = h_W(\mathbf X)\\
P(Y=0 | \mathbf X;W) = 1-h_W(\mathbf X)</script><p>则概率分布函数为</p>
<script type="math/tex; mode=display">
P(Y|\mathbf X;W) = (h_W(\mathbf X))^Y*(1-h_W(\mathbf X))^{1-Y}</script><p>（<strong>也就是说，我们有样本，通过样本能知道概率分布，那么我们需要知道得到这个概率分布的最有可能的参数W。即我们通过样本知道一些特征组合下的点击率，现在需要求概率函数中的系数。</strong>）</p>
<p>我们假设样本数据相互独立，所以它们的联合分布可以表示为各边际分布的乘积，用似然函数表示为：</p>
<script type="math/tex; mode=display">
\begin{aligned} 
L(W)=P(Y|\mathbf X;W) &= (h_W(\mathbf X))^Y(1-h_W(\mathbf X))^{1-Y}\\
&=\prod_{j=1}^M(h_W(X_j))^{y_j}(1-h_W(X_j))^{1-y_j}
\end{aligned}
\tag {2-1-2}</script><p>从而，损失函数的求解，可以转化为求最有可能导致这样概率分布的W，也就是求L(W)的最大值。最简单的方法就是对W求偏导，并令导数为零。</p>
<p>在多数情况下，直接对变量进行求导反而会使得计算式子更加的复杂，此时可以借用对数函数。由于对数函数是单调增函数，因此与（2-1-2）具有相同的最大值，上式变为</p>
<script type="math/tex; mode=display">
\begin{aligned} 
l(W) &= Log\ L(W)\\
&=\sum_{j=1}^M(y_jln\ h(X_j)+(1-y_j)ln\ (1-h(X_j)))
\end{aligned}</script><p>对其求关于W的偏导</p>
<p>首先求logistic函数的导数，得（最后一个X是对$W^TX$的求导）</p>
<script type="math/tex; mode=display">
h_W^{'}(\mathbf X) = h_W(\mathbf X)(1-h_W(\mathbf X))X</script><blockquote>
<p><strong>推导过程如下</strong></p>
<p><img src="/2017/07/12/机器学习/浅谈在线最优化求解算法-以CTR预测模型为例/求导的推导.jpg" alt="求导的推导"></p>
</blockquote>
<p>为了求解方便，将l(W)转为（其实1/M没用，完全可以去掉，不懂为何要加上）</p>
<script type="math/tex; mode=display">
J(W) = -\frac {1}{M} l(W)</script><p>则就变成求J(W)的最小值。求偏导的过程如下：</p>
<p><img src="/2017/07/12/机器学习/浅谈在线最优化求解算法-以CTR预测模型为例/最大似然估计求偏导.png" alt="最大似然估计求偏导"></p>
<p>最后得到目标函数（损失函数）为：</p>
<script type="math/tex; mode=display">
\frac {\partial }{\partial W}J(W) =-\frac{1}{M} (Y-h_W(\mathbf X))X</script></blockquote>
<p>对于损失函数的求解，一个典型的方法就是梯度下降法，由于损失函数是凸函数，因此沿着梯度下降的方向找到最小点。</p>
<p>假设样本总数为n，<strong>批量梯度下降</strong>是：</p>
<script type="math/tex; mode=display">
Repeat\ until\ convergence \{ \\
W^{(t+1)} := W^t - \eta^t\triangledown  _{W}l(W^{t},Z) \\
\}\\
 \tag{1-2}</script><p>而<strong>随机梯度下降（SGD）</strong>是：</p>
<script type="math/tex; mode=display">
Repeat\ until\ convergence \{ \\
      for\ j=1\ to\ M, \{ \\
          W^{(t+1)} := W^t - \eta^t\triangledown  _{W}l(W^{t},Z_j) \\
\}</script><p>两者的区别是：</p>
<p>前者每次更新$W$都需要遍历一次整个样本集合；而后者在遍历样本集合的时候，每个样本都能改变$W$ ，有更快的收敛速度 。由于SGD针对观测到的随机一条数据进行权重的更新，很适合进行增量计算，实现梯度下降的online模式。</p>
<h2 id="2-2-正则化"><a href="#2-2-正则化" class="headerlink" title="2.2 正则化"></a>2.2 正则化</h2><p>正则化的主要目的是防止过拟合。对于损失函数构成的模型，可能会出现有些权重很大，有些权重很小的情况，导致过拟合，使得模型的复杂度提高，泛化能力较差（对未知数据的预测能力）。</p>
<p> <img src="/2017/07/12/机器学习/浅谈在线最优化求解算法-以CTR预测模型为例/过拟合1.png" alt="过拟合1"></p>
<p>而正则化就是对损失函数中权重的限制，限制其模不要太大：</p>
<script type="math/tex; mode=display">
W=\arg \underset{W}{min}\   l(W,Z)\\
s.t. \Psi(W)<\delta</script><p>其中，$\Psi(W)$称为正则化因子，是一个关于W求模的函数，常用的正则化因子有L1和L2正则化。</p>
<script type="math/tex; mode=display">
L1\ Regularization \ \ \ \ \ \ \ \ \Psi(W)=||W||_1=\sum_{i=1}^N|w_i|\\
L2 \ Regularization\ \ \ \ \ \ \ \Psi(W)=||W||_2^2=\sum_{i=1}^N(w_i)^2=W^TW</script><p>L1和L2的主要区别有两个：</p>
<p>（1）L1在0处不可导，而L2可导。</p>
<p>（2）L1通常能产生更稀疏的模型，也就是W的更多维度是0。这些为0的权重就代表了不是很重要的维度，所以能起到特征选择的目的。</p>
<p>（3）L2能限制特征权重各个维度的模不要太大，解决过拟合。</p>
<blockquote>
<p><img src="/2017/07/12/机器学习/浅谈在线最优化求解算法-以CTR预测模型为例/正则化解空间.png" alt="正则化解空间"><br> 其中，左图的圆形区域是L2正则化的单位圆，右图的方形区域是L1正则化的单位圆。<br><strong>单位圆</strong></p>
<p>使$||X||_p=1$的图形，当p=1和2时，单位圆分别为$|x|+|y|=1$和$x^2+y^2=1$。</p>
</blockquote>
<p>但是在SGD中，由于每次W的更新并不是沿着全局梯度进行下降，而是沿着某个样本产生的梯度方向进行下降，这样即使采用L1的方式也很难产生稀疏解。因此在接下来的在线最优化求解算法中，稀疏性是一个主要的追求目标。</p>
<p>参考：</p>
<p><a href="http://www.mamicode.com/info-detail-517504.html【正则化方法：L1和L2" target="_blank" rel="external">http://www.mamicode.com/info-detail-517504.html【正则化方法：L1和L2</a> regularization、数据集扩增、dropout】</p>
<p><a href="http://blog.csdn.net/zouxy09/article/details/24971995/【机器学习中的范数规则化之（一）L0、L1与L2范数】" target="_blank" rel="external">http://blog.csdn.net/zouxy09/article/details/24971995/【机器学习中的范数规则化之（一）L0、L1与L2范数】</a></p>
<h1 id="3、在线最优化求解算法"><a href="#3、在线最优化求解算法" class="headerlink" title="3、在线最优化求解算法"></a>3、在线最优化求解算法</h1><h2 id="3-1-截断梯度法TG"><a href="#3-1-截断梯度法TG" class="headerlink" title="3.1 截断梯度法TG"></a>3.1 截断梯度法TG</h2><p>为了使特征权重W有更多的0，最简单的方法就是设一个阈值，当W的某个维度值小于这个阈值的时候置为0，这个称为<strong>简单截断法</strong>。但实际中W的某个系数比较小可能是由于该维度训练不足引起，所以这么做会导致这部分特征的丢失。于是又改进为<strong>截断梯度法Truncated Gradient</strong>。</p>
<h3 id="3-1-1-简单截断法"><a href="#3-1-1-简单截断法" class="headerlink" title="3.1.1 简单截断法"></a>3.1.1 简单截断法</h3><p>以$k$为窗口，当$t/k$不为整数时，采用标准的SGD；否则，采用如下的权重更新方式：</p>
<script type="math/tex; mode=display">
W^{t+1}=T_0(W^t - \eta^tG^t,\theta) \\
T_0(v_i,\theta) = \begin{Bmatrix}
0\ if\ |v_i|\leqslant \theta\\ 
v_i\ otherwise
\end{Bmatrix}</script><p>其中，$G^t=\triangledown  _{W}l(W^{t},Z^{t})$ 代表第t次迭代中损失函数的梯度，$\eta^{t}$ 是学习率，通常将其设置为 $1/\sqrt{t}$ 的函数。可以看出，简单截断法的思路是，如果某个维度的权重变化小于设定的$\theta$ ，则直接置为0。</p>
<h3 id="3-1-2-截断梯度法"><a href="#3-1-2-截断梯度法" class="headerlink" title="3.1.2 截断梯度法"></a>3.1.2 截断梯度法</h3><p>在前一种方法上的改进。加入了L1正则化项$\eta^{t}\lambda sgn(W^{t})$ 。</p>
<script type="math/tex; mode=display">
W^{t+1}=W^t-\eta ^tG^t-\eta^t\lambda sgn(W^t)</script><p>其中$sgn(v)$是符号函数。由于每次仅根据一个样本进行更新，因此也不再使用区分样本的下表$j$。</p>
<p>采用类似的方式表示为：</p>
<script type="math/tex; mode=display">
W^{t+1}=T_1(W^t - \eta^tG^t,\eta^t\lambda^t,\theta) \\
T_1(v_i,\alpha,\theta) = \begin{Bmatrix}
\begin{aligned}
& max(0,v_i-\alpha)\ if\ v_i\in [0,\theta]\\ 
& min(0,v_i+\alpha)\ if\ v_i\in [-\theta,0]\\
& v_i\ otherwise
\end{aligned}
\end{Bmatrix}</script><p>其中，$\lambda^{t} \in \mathbb{R}$且$\lambda^{t}\geqslant0 $ 。同样以k为窗口，每k步进行一次截断。当t/k不为整数时，$\lambda^{t}=0$， 否则，$\lambda^{t}=k\lambda$。可以看出，$\lambda$和$\theta$决定了权重的稀疏程度，这两个值越大越稀疏。</p>
<h2 id="3-2-前向后向切分FOBOS"><a href="#3-2-前向后向切分FOBOS" class="headerlink" title="3.2 前向后向切分FOBOS"></a>3.2 前向后向切分FOBOS</h2><h3 id="3-2-1-FOBOS算法原理"><a href="#3-2-1-FOBOS算法原理" class="headerlink" title="3.2.1 FOBOS算法原理"></a>3.2.1 FOBOS算法原理</h3><p>在FOBOS（Forward-backward Splitting）中，将权重的更新分为两个步骤：</p>
<script type="math/tex; mode=display">
W^{t+\frac{1}{2}} = W^t-\eta^tG^{t}\\
W^{t+1}=\arg \underset{W}{min} \{\frac {1} {2} ||W- W^{t+\frac{1}{2}}||_2^2+\eta^{t+\frac {1}{2}}\Psi (W)\}
\tag {3-2-1}</script><p>前一个步骤还是标准的梯度下降，后一个步骤可以理解为对梯度下降的结果进行微调，其中第一项是L2正则化，表示不能离损失迭代结果太远，第二项$\Psi (W)$是正则化项。</p>
<p>将上面两个式子合并，有</p>
<script type="math/tex; mode=display">
W^{t+1}=\arg \underset{W}{min} \{\frac {1} {2} ||W- W^{t}-\eta^{t}G^{t}||_2^2+\eta^{t+\frac {1}{2}}\Psi (W)\}</script><p>令</p>
<script type="math/tex; mode=display">
F(W)=\frac {1} {2} ||W- W^{t}-\eta^{t}G^{t}||_2^2+\eta^{t+\frac {1}{2}}\Psi (W)</script><p>如果$W^{t+1}$存在一个最优解，<strong><em>那么可以推断0向量一定属于$F(W)$的一维次梯度集合</em>。</strong></p>
<script type="math/tex; mode=display">
0 \in \partial F(W)=W-W^{t}+\eta^{t}G^{t}+\eta^{t+\frac 1 2}\partial \Psi(W)</script><blockquote>
<p><strong>次导数和次梯度</strong></p>
<p>参考SubGradient.pdf</p>
<p>次导数是一个区间，一维次梯度就是次导数</p>
</blockquote>
<p>由于$W^{t+1}=\arg \underset{x}{min} F(W)$，则有：</p>
<script type="math/tex; mode=display">
0=\left \{ W-W^{t} - \eta^{t}G^{t}+\eta^{t+\frac {1}{2}}\partial\Psi(W) \right \}|_{W=W^{t+1}}</script><p>便可以得到另一种更新权重的方式</p>
<script type="math/tex; mode=display">
W^{t+1}=W^{t}+ \eta^{t}G^{t}-\eta^{t+\frac {1}{2}}\partial\Psi(W^{t+1})</script><p>从上式可以看到权重的更新不仅与迭代前的状态有关，也与迭代后的$W^{t+1}$有关。</p>
<h3 id="3-2-2-L1-FOBOS"><a href="#3-2-2-L1-FOBOS" class="headerlink" title="3.2.2 L1-FOBOS"></a>3.2.2 L1-FOBOS</h3><p>在L1正则化下，有$\Psi (W)=\lambda||w||_1$ 。对于（2-3-1），</p>
<script type="math/tex; mode=display">
W^{t+1}=\arg \underset{W}{min} \{\frac {1} {2} ||W- W^{t+\frac{1}{2}}||_2^2+\eta^{t+\frac {1}{2}}\Psi (W)\}</script><p>用向量V来表示$W^{t+\frac 1 2}$ ，用标量$\tilde{\lambda} \in \mathbb{R}$来表示$\eta^{t+\frac 1 2}\lambda$ ，将公式展开，并改写为</p>
<script type="math/tex; mode=display">
W^{t+1}=\arg \underset{W}{min}\sum_{i=1}^N (\frac {1} {2}(w_i-v_i)^2+ \tilde{\lambda}|w_i|)
\tag {3-2-2}</script><p>可以看到，在求和公式中的每一项都是大于0的，所以公式（3-2-2）可以拆解成对特征权重W的每一维度单独求解</p>
<script type="math/tex; mode=display">
w_i^{t+1}=\arg \underset{w_i}{min}(\frac {1} {2}(w_i-v_i)^2+ \tilde{\lambda}|w_i|)
\tag {3-2-3}</script><p>假设$w_i^<em>$是一个维度上的最优解，通过反证法证明$w_i^</em>v_i\geq0$（证明略）。再分$v_i\geq0$和$v_i&lt;0$来讨论。</p>
<p><strong>（1）当$v_i\geq0$时</strong>，</p>
<p>由于$w_i^<em>v_i\geq0$，所以$w_i^</em> \geq0$ 。相当于给（2-3-3）增加了一个不等式约束条件：</p>
<script type="math/tex; mode=display">
w_i^{t+1}=\arg \underset{w_i}{min}(\frac {1} {2}(w_i-v_i)^2+ \tilde{\lambda}|w_i|)\\
s.t. -w_i\leq 0</script><p>通过拉格朗日乘子求解这个含不等式的约束问题。</p>
<p>引入拉格朗日系数$\beta \geq 0$ ，由KKT条件，有</p>
<script type="math/tex; mode=display">
\frac \partial {\partial w_i}\left ( \frac {1} {2}(w_i-v_i)^2+ \tilde{\lambda}|w_i|-\beta w_i \right )|_{w_i=w_i^*}=0 \\
\beta w_i^*=0</script><p>根据上面的求导可得</p>
<script type="math/tex; mode=display">
w_i^*=v_i-\tilde{\lambda}+\beta</script><p>再分为两种情况</p>
<p>① 当$w_i^<em> &gt; 0$ 时，由于$\beta w_i^</em>=0$ 所以$\beta=0$，此时有$w_i^*=v_i-\tilde{\lambda}$ ，从而$v_i-\tilde{\lambda} &gt; 0$ 。</p>
<p>② 当$w_i^* = 0$ 时，有$v_i-\tilde{\lambda}+\beta=0$ 。由于$\beta \geq 0$ ，所以$v_i-\tilde{\lambda} \leq 0$  。</p>
<p>可以得出，当$v_i\geq0$ 时，</p>
<script type="math/tex; mode=display">
w_i^* = max(0, v_i-\tilde{\lambda})</script><p><strong>（2）当$v_i&lt;0$时</strong>，</p>
<p>采用同样的分析方法，得到</p>
<script type="math/tex; mode=display">
w_i^* =- max(0, -v_i-\tilde{\lambda})</script><p>综上，可得FOBOS在L1正则化条件下，特征权重各个维度的更新方式为：</p>
<script type="math/tex; mode=display">
\begin{aligned} 
w_i^{t+1} &= sgn(v_i)max(0,|v_i|-\tilde{\lambda})\\
& = sgn(w_i^{t}-\eta^{t}g_i^{t})max \left \{ 0, |w_i^{t}-\eta^{t}g_i^{t}|-\eta^{t+ \frac {1} {2}} \lambda \right \}
 \end{aligned}
 \tag{3-2-4}</script><p>其中，$g_i^{t}$就是梯度在维度i上的取值。</p>
<p><strong>从公式（3-2-4）可以看出，L1-FOBOS每次更新W的时候，对W的每个维度都会进行判定，当$|w_i^{t}-\eta^{t}g_i^{t}|-\eta^{t+ \frac {1} {2}} \lambda&lt;0$的时候对齐进行截断，即权重置为0。</strong></p>
<p>换一种写法，</p>
<script type="math/tex; mode=display">
|w_i^{t}-\eta^{t}g_i^{t}|<\eta^{t+ \frac {1} {2}} \lambda
\tag {3-2-5}</script><p>可以看出截断的意义是，<strong>当一条样本产生的梯度不足以令对应维度上的权重值发生足够大的变大（$\eta^{t+ \frac {1} {2}} \lambda$ ），则认为在本次更新过程中该维度不重要，令其权重为0</strong>。</p>
<p>若对L1-FOBOS进行适当的变换，可以发现，L1-FOBOS就是TG在特定条件下的特殊形式。</p>
<h2 id="3-3-RDA"><a href="#3-3-RDA" class="headerlink" title="3.3 RDA"></a>3.3 RDA</h2><h3 id="3-3-1-RDA算法原理"><a href="#3-3-1-RDA算法原理" class="headerlink" title="3.3.1 RDA算法原理"></a>3.3.1 RDA算法原理</h3><p>TG和FOBOS都是建立在SGD的基础之上，属于梯度下降类型的方法，这类型方法的优点就是精度比较高，并且 TG、 FOBOS 也都能在稀疏性上得到提升。 但是有些其它类型的算法，例如 RDA，是从另一个方面来求解 Online Optimization 并且更有效地提升了特征权重的稀疏性。 </p>
<p>正则对偶平均（ RDA, Regularized Dual Averaging） 是微软十年的研究成果， RDA 是 Simple Dual Averaging Scheme 的一个扩展， 由 Lin Xiao 发表于 2010 年 。</p>
<p>在 RDA 中， 特征权重的更新策略为： </p>
<script type="math/tex; mode=display">
W^{t+1} = \arg \underset{W}{min} \left \{ \frac 1 t \sum_{r=1}^t \left \langle G^r,W \right \rangle  +\Psi(W)+\frac {\beta^{t}}{t}h(W) \right \}
\tag {3-3-1}</script><p>本质上，公式（3-3-1）包括了3个部分：</p>
<p>（1）线性函数$\frac 1 t \sum_{r=1}^t \left \langle G^r,W \right \rangle$ 包含了之前所有梯度（或次梯度）的平均值（dual average），$G^r$ 是梯度；</p>
<p>（2）$\Psi(W)$ 为正则项；</p>
<p>（3）额外正则项$\frac {\beta^{t}}{t}h(W)$。其中$h(W)$是一个辅助的严格凸函数。${\beta^{t}|t\geq 1}$ 是一个非负且非自减序列。</p>
<h3 id="3-3-2-L1-RDA"><a href="#3-3-2-L1-RDA" class="headerlink" title="3.3.2 L1-RDA"></a>3.3.2 L1-RDA</h3><p>在L1正则化下，有$\Psi (W)=\lambda||w||_1$，并且由于$h(W)$是一个关于W的严格凸函数，就令$h(W)=\frac {1} {2} ||W||_2^2 $ 。此外，将${\beta^{t}|t\geq 1}$定义为$\beta^{t}=\gamma \sqrt t $ 。再代入（2-4-1），有</p>
<script type="math/tex; mode=display">
W^{t+1} = \arg \underset{W}{min} \left \{ \frac 1 t \sum_{r=1}^t \left \langle G^r,W \right \rangle  +\lambda||w||_1+\frac {\gamma} {2\sqrt t}||W||_2^2 \right \}
\tag {3-3-2}</script><p>分解到每一个权重的维度上</p>
<script type="math/tex; mode=display">
w_i^{t+1} = \arg \underset{w_i}{min} \left \{ \bar{g_i}^{t}w_i +\lambda|w_i|+\frac {\gamma} {2\sqrt t}w_i^2 \right \}
\tag {3-3-3}</script><p>这里$\lambda &gt;0,\ \frac {\gamma} {\sqrt t}&gt;0,\  \bar{g<em>i}^{t} = \frac 1 t \sum</em>{r=1}^t g_i^{(r)}$ 。公式（2-4-3）就是一个无约束的非平滑最优化问题（因为第二项$\lambda|w_i|$ 在0处不可导）。所以用次导数求解。</p>
<p>假设$w_i^<em>$ 是其最优解，并且定义$\xi \in \partial  |w_i|$为$|w_i|$ 在$w_i^</em>$ 的次导数，则有</p>
<script type="math/tex; mode=display">
\partial |w_i^*| =  \left\{\begin{matrix}
-1<\xi<1  & if w_i^*=0\\ 
1 & if w_i^*>0\\ 
-1 & if w_i^*<0
\end{matrix}\right.</script><p>对公式（3-3-3）求次导数，并令其为0，则有</p>
<script type="math/tex; mode=display">
\bar{g_i}^{t} + \lambda\xi + \frac {\gamma} {\sqrt t} w_i = 0</script><p>由于$\lambda &gt;0$，再分情况讨论（略），可以得到L1-RDA特征权重的各个维度更新的方式为：</p>
<script type="math/tex; mode=display">
w_i^{t+1}=\begin{Bmatrix}
0 & if |\bar{g_i}^{t}|<\lambda\\ 
-\frac {\sqrt t}{\gamma}\left (\bar{g_i}^{t}-\lambda sgn(\bar{g_i}^{t})  \right ) & otherwise
\end{Bmatrix}
\tag {3-3-4}</script><p><strong>这里可以看出，当某个维度上累积梯度平均值的绝对值小于阈值$\lambda$ 时，产生截断</strong>。</p>
<h3 id="3-3-3-L1-RDA和L1-FOBOS的比较"><a href="#3-3-3-L1-RDA和L1-FOBOS的比较" class="headerlink" title="3.3.3 L1-RDA和L1-FOBOS的比较"></a>3.3.3 L1-RDA和L1-FOBOS的比较</h3><p>在L1-FOBOS中，进行截断的条件是</p>
<script type="math/tex; mode=display">
|w_i^{t}-\eta^{t}g_i^{t}|<\eta^{t+ \frac {1} {2}} \lambda</script><p>通常会定义$\eta$为与$\frac 1 {\sqrt t}$ 正相关的函数$\eta=\Theta \left ( \frac {1} {\sqrt t} \right )$ 。因此L1-FOBOS的<strong>截断阈值为$\Theta \left ( \frac {1} {\sqrt t} \right )\lambda$  ，</strong>随着**t的增加，这个阈值会逐渐降低。</p>
<p>相比较而言，L1-RDA的<strong>截断阈值是$\lambda$ </strong>。是一个常数，并不随着t变化，因此相对于L1-FOBOS更简单粗暴。这种性质使得L1-RDA更容易产生稀疏性。此外， RDA 中判定截断的对象是梯度的累加平均值$\bar{g_i}^{t} $ ， 不同于 TG或L1-FOBOS 中针对单次梯度计算的结果进行判定，避免了由于某些维度由于训练不足导致截断的问题。 并且通过调节一个参数$\lambda$，很容易在精度和稀疏性上进行权衡 。</p>
<h2 id="3-4-FTRL"><a href="#3-4-FTRL" class="headerlink" title="3.4 FTRL"></a>3.4 FTRL</h2><p>有实验证明， <strong>L1-FOBOS 这一类基于梯度下降的方法有比较高的精度，但是 L1-RDA 却能在损失一定精度的情况下产生更好的稀疏性。 FTRL则是结合了两者的优点</strong>。</p>
<h3 id="3-4-1-L1-FOBOS和L1-RDA在形式上的统一"><a href="#3-4-1-L1-FOBOS和L1-RDA在形式上的统一" class="headerlink" title="3.4.1 L1-FOBOS和L1-RDA在形式上的统一"></a>3.4.1 L1-FOBOS和L1-RDA在形式上的统一</h3><p>之前提到，L1-FOBOS可以表示为（这里令$\eta^{t+\frac 1 2}=\eta^t=\Theta(\frac 1 {\sqrt t})$  是一个随t变化的非增正序列） </p>
<script type="math/tex; mode=display">
W^{t+1}=\arg \underset{W}{min} \{\frac {1} {2} ||W- W^t-\eta^tG^t||^2+\eta^{t}\lambda||w||_1\}</script><p>将其按W的维度分解为N个独立的最优化步骤</p>
<script type="math/tex; mode=display">
\underset{w_i}{minimize} \left \{  \frac 1 2 (w_i-w_i^t+\eta^tg_i^t)^2+\eta^t\lambda|w_i| \right \}\\
=\underset{w_i}{minimize}\left \{  \frac 1 2 (w_i-w_i^t)^2 + \frac 1 2(\eta^tg_i^t)^2+w_i\eta^tg_i^t- w_i^t\eta^tg_i^t+    \eta^t\lambda|w_i| \right \}\\</script><p>同时除以$\eta^t$ ，得到</p>
<script type="math/tex; mode=display">
\underset{w_i}{minimize}\left \{ w_ig_i^t+\lambda|w_i|+\frac 1 {2\eta^t}(w_i-w_i^t)^2 + [\frac {\eta^t}{2}(g_i^t)2+w_i^tg_i^t] \right \}</script><p>由于$\frac {\eta^t}{2}(g_i^t)2+w_i^tg_i^t$ 与变量$w_i$ 无关，因此上式可以等价于</p>
<script type="math/tex; mode=display">
\underset{w_i}{minimize}\left \{ w_ig_i^t+\lambda|w_i|+\frac 1 {2\eta^t}(w_i-w_i^t)^2 +  \right \}</script><p>再将这N个独立的合并，则L1-FOBOS可以写成</p>
<script type="math/tex; mode=display">
W^{t+1} = \arg \underset{W}{min} \left \{ G^t\cdot W+\lambda||W||_1+\frac 1 {2\eta^t}||W-W^t||_2^2 \right \}</script><p>而对于L1-RDA的公式（3-3-2）</p>
<script type="math/tex; mode=display">
W^{t+1} = \arg \underset{W}{min} \left \{ \frac 1 t \sum_{r=1}^t \left \langle G^r,W \right \rangle  +\lambda||w||_1+\frac {\gamma} {2\sqrt t}||W||_2^2 \right \} \\</script><p>同时乘以t，得到</p>
<script type="math/tex; mode=display">
\begin{aligned} 
W^{t+1} & = \arg \underset{W}{min} \left \{ \sum_{r=1}^t  G^r \cdot W  +t\lambda||w||_1+\frac {1} {2\eta^t}||W-0||_2^2 \right \}\\
& =\arg \underset{W}{min} \left \{  G^{1:t} \cdot W  +t\lambda||w||_1+\frac {1} {2\eta^t}||W-0||_2^2 \right \}
\end{aligned}</script><p>如果令$\sigma ^s = \frac 1 {\eta^s}-\frac 1 {\eta^{s-1}}$  ，则$\sigma^{1:t} = \frac 1 {\eta^t}$ 。L1-FOBOS和L1-RDA的公式可以写成</p>
<script type="math/tex; mode=display">
W^{t+1} = \arg \underset{W}{min} \left \{ G^t\cdot W+\lambda||W||_1+\frac 1 2\sigma^{1:t}||W-W^t||_2^2 \right \}\\
W^{t+1} =\arg \underset{W}{min} \left \{  G^{1:t} \cdot W  +t\lambda||W||_1+\frac 1 2\sigma^{1:t}||W-0||_2^2 \right \}
\tag {3-4-1}</script><p>比较这两个公式，可以看出L1-FOBOS和L1-RDA的区别在于：</p>
<p>（1）前者对梯度只考虑当前的状态，而后者的梯度是累加的形式；</p>
<p>（2）前者的第三项限制了W的变化不能离已经迭代过的解太远，后者限制W不能离0太远。</p>
<h3 id="3-4-2-FTRL算法原理"><a href="#3-4-2-FTRL算法原理" class="headerlink" title="3.4.2 FTRL算法原理"></a>3.4.2 FTRL算法原理</h3><p>FTRL综合考虑了L1-FOBOS和L1-RDA中对正则项和W限制的区别，其特征权重的更新公式为</p>
<script type="math/tex; mode=display">
W^{t+1} =\arg \underset{W}{min} \left \{  G^{1:t} \cdot W  +\lambda_1||W||_1+\lambda_2||W||_2^2+\frac 1 2\sum_{s=1}^t \sigma^s ||W-W^s||_2^2 \right \}
\tag {3-4-2}</script><p>其中L2的正则项在论文中并没有出现，但是2013年的FTRL工程化实现的论文却使用。事实上该项的引入并不影响FRTL<br>的稀疏性， 后面的推导过程会显示这一点。 L2正则项的引入仅仅相当于对最优化过程多了一个约束，使得结果求解结果更加“平滑”。 </p>
<p>对（3-4-2）进行变换，将其的最后一项展开</p>
<script type="math/tex; mode=display">
W^{t+1} =\arg \underset{W}{min} \left \{  (G^{1:t}-\sum_{s=1}^t\sigma^sW^s) \cdot W  +\lambda_1||W||_1+\frac 1 2 (\lambda_2+\sum_{s=1}^t\sigma^s)||W||_2^2+\frac 1 2\sum_{s=1}^t \sigma^s ||W^s||_2^2 \right \}</script><p>其中，由于$\frac 1 2\sum_{s=1}^t \sigma^s ||W^s||_2^2$ 相对于W是常数项，再令</p>
<script type="math/tex; mode=display">
Z^{t} =G^{1:t}-\sum_{s=1}^t\sigma^sW^s
\tag {3-4-3}</script><p>上式等价于</p>
<script type="math/tex; mode=display">
W^{t+1} =\arg \underset{W}{min} \left \{  Z^t \cdot W  +\lambda_1||W||_1+\frac 1 2 (\lambda_2+\sum_{s=1}^t\sigma^s)||W||_2^2 \right \}</script><p>再针对每个维度将其拆解成N个独立的标量最小化问题</p>
<script type="math/tex; mode=display">
 \underset{w_i}{minimize} \left \{  z_i^tw_i  +\lambda_1|w_i|+\frac 1 2 (\lambda_2+\sum_{s=1}^t\sigma^s)w_i^2 \right \}</script><p>到这里，遇到了与L1-RDA的（3-3-3）类似的优化问题，用相同的分析方法可以得到</p>
<script type="math/tex; mode=display">
w_i^{t+1}=\left\{\begin{matrix}
0 & if\ |z_i^t|<\lambda_1\\ 
-\left ( \lambda_2+\sum_{s=1}^t\sigma^s \right )^{-1}\ \left ( z_i^t-\lambda_1 sgn(z_i^t) \right ) & otherwise
\end{matrix} \right.
\tag {3-4-4}</script><p>可以看出，引入L2并没有对FTRL结果的稀疏性产生影响。</p>
<h3 id="3-4-3-学习率"><a href="#3-4-3-学习率" class="headerlink" title="3.4.3 学习率"></a>3.4.3 学习率</h3><p>前面的推导中，学习率的选择和计算没有被提及。事实上在FTRL中，每个维度的学习率都是单独考虑的。</p>
<p>考虑特征维度的变化率：如果特征 1 比特征 2 的变化更快，那么在维度 1 上的学习率应该下降得更快。我们很容易就可以想到可以用某个维度上梯度分量来反映这种变化率。在FTRL 中，维度 i上的学习率是这样计算的<strong>（原作者没有推导过程）</strong>：</p>
<script type="math/tex; mode=display">
\eta_i^t=\frac {\alpha}{\beta + \sqrt {\sum_{s=1}^t (g_i^s)^2}}</script><p>由于$\sum_{s=1}^t\sigma^s=\frac 1 {\eta^t}$ ，因此（3-4-4）就变成</p>
<script type="math/tex; mode=display">
w_i^{t+1}=\left\{\begin{matrix}
0 & if\ |z_i^t|<\lambda_1\\ 
-\left ( \lambda_2 + \frac {\beta + \sqrt {\sum_{s=1}^t (g_i^s)^2}}{\alpha}  \right )^{-1}\ \left ( z_i^t-\lambda_1 sgn(z_i^t) \right ) & otherwise
\end{matrix} \right.
\tag {3-4-5}</script><p>这里的$\alpha, \beta$ 都是要输入的参数。</p>
<h3 id="2-5-4-伪代码解读"><a href="#2-5-4-伪代码解读" class="headerlink" title="2.5.4 伪代码解读"></a>2.5.4 伪代码解读</h3><p> <img src="/2017/07/12/机器学习/浅谈在线最优化求解算法-以CTR预测模型为例/FTRL伪代码.png" alt="FTRL伪代码"></p>
<p>首先设置各个参数的初始值，包括</p>
<ul>
<li>更新学习率的$\alpha,\beta$。</li>
<li>L1和L2正则化的参数$\lambda_1,\ \lambda_2$ </li>
<li>更新权重时用到的$z_i$ 一维数组（数组的长度是特征项个数），初始值是0</li>
<li>存放梯度累加的$n_i$ 一维数组（数组的长度是特征项个数），初始值是0</li>
</ul>
<p>算法步骤中：</p>
<p>（1）第一阶段，计算第t次迭代的预测值</p>
<p><strong>S1</strong>：用给定的初始值计算权重$w_{t,i}$，并计算出预测值$p_t$ 。见①</p>
<p>（2）第二阶段，更新第t+1次的权重，对当前样本不为0的每个特征项都要进行一次更新。在第i个特征项中，</p>
<p><strong>S1</strong>：采用logloss计算损失函数的梯度$g_{t+1}$，见②</p>
<p><strong>S2</strong>：可以看出①里面还需要计算$n_i$  和$z_i$ 在第t+1次的值。</p>
<p>对于$z_i$，根据公式（2-5-3）</p>
<script type="math/tex; mode=display">
Z^{t} =G^{1:t}-\sum_{s=1}^t\sigma^sW^s</script><p>可以看出z的更新可以通过下式计算</p>
<script type="math/tex; mode=display">
\begin {aligned}
Z^{t+1}& =G^{1:t+1}-\sum_{s=1}^{t+1}\sigma^sW^s\\
&=G^{1:t}-\sum_{s=1}^t\sigma^sW^s + G^{t+1} - \sigma^{t+1}W^{t+1}\\
&=Z^t + G^{t+1} - \sigma^{t+1}W^{t+1}
 \end{aligned}
 \tag {3-4-6}</script><p>则需要计算$\sigma^{t+1}$ 的值。而根据上文的推导</p>
<script type="math/tex; mode=display">
\sigma ^s = \frac 1 {\eta^s}-\frac 1 {\eta^{s-1}}</script><p>又</p>
<script type="math/tex; mode=display">
\eta_i^t=\frac {\alpha}{\beta + \sqrt {\sum_{s=1}^t (g_i^s)^2}}</script><p>则</p>
<script type="math/tex; mode=display">
\begin {aligned}
\sigma ^{t+1}& = \frac 1 {\eta^{t+1}}-\frac 1 {\eta^t}\\
&=\frac {\beta + \sqrt {\sum_{s=1}^{t+1} (g_i^s)^2}}{\alpha}-\frac {\beta + \sqrt {\sum_{s=1}^t (g_i^s)^2}}{\alpha}\\
&=\frac 1 \alpha \left ( \sqrt {\sum_{s=1}^{t+1} (g_i^s)^2}- \sqrt {\sum_{s=1}^t (g_i^s)^2}\right )
 \end{aligned}</script><p>由于用$n_i$ 记录$g_i$ 的累加和，上式可以变成</p>
<script type="math/tex; mode=display">
\sigma ^{t+1} = \sqrt {n^t+(g^{t+1})^2}-\sqrt {n^t}
\tag {3-4-7}</script><p>见③。再根据公式（3-4-6），计算$z_i$ 的值，见④。</p>
<p><strong>S3</strong>：对于$n_i$ ，根据公式（3-4-7），</p>
<script type="math/tex; mode=display">
n^{t+1} = n^t +(g^{t+1})^2</script><p>见⑤。</p>
<h3 id="2-5-5-实现代码"><a href="#2-5-5-实现代码" class="headerlink" title="2.5.5 实现代码"></a>2.5.5 实现代码</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span></span>(x: <span class="type">Array</span>[<span class="type">Int</span>]): <span class="type">Double</span> = &#123;</div><div class="line">  <span class="keyword">var</span> wTx = <span class="number">0.0</span></div><div class="line"></div><div class="line">  x foreach &#123; x =&gt;</div><div class="line">    <span class="keyword">val</span> sign = <span class="keyword">if</span> (z(x) &lt; <span class="number">0</span>) <span class="number">-1.0</span> <span class="keyword">else</span> <span class="number">1.0</span></div><div class="line"></div><div class="line">    <span class="keyword">if</span> (sign * z(x) &lt;= <span class="type">L1</span>)</div><div class="line">      w(x) = <span class="number">0.0</span></div><div class="line">    <span class="keyword">else</span></div><div class="line">      w(x) = (sign * <span class="type">L1</span> - z(x)) / ((beta + math.sqrt(n(x))) / alpha + <span class="type">L2</span>)</div><div class="line"></div><div class="line">    wTx = wTx + w(x)</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">return</span> <span class="number">1.0</span> / (<span class="number">1.0</span> + math.exp(-math.max(math.min(wTx, <span class="number">35.0</span>), <span class="number">-35.0</span>)))</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(x: <span class="type">Array</span>[<span class="type">Int</span>], p: <span class="type">Double</span>, y: <span class="type">Int</span>): <span class="type">Unit</span> = &#123;</div><div class="line">  <span class="keyword">val</span> g = p - y</div><div class="line"></div><div class="line">  x foreach &#123; x =&gt;</div><div class="line">    <span class="keyword">val</span> sigma = (math.sqrt(n(x) + g * g) - math.sqrt(n(x))) / alpha</div><div class="line">    z(x) = z(x) + g - sigma * w(x)</div><div class="line">    n(x) = n(x) + g * g</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>从代码可以看出，在更新权重时，SGD和FTRL的区别在于：</p>
<p><del>SGD在遍历每个样本的时候，都会更新所有维度的权重，而FTRL在遍历每个样本的时候只会更新样本对应维度的权重。从而可以节省训练的时间</del></p>
<p>并不是节省时间。SGD也可以用于在线学习，过拟合的限制上没有FTRL好。参数太多，会导致模型复杂度上升，容易过拟合。</p>
<h3 id="3-4-6-实验及结论"><a href="#3-4-6-实验及结论" class="headerlink" title="3.4.6 实验及结论"></a>3.4.6 实验及结论</h3><p>1、</p>
<p>训练：16-10-22的前7天数据</p>
<p>预测：16-10-22当天数据</p>
<p>维度：1048576</p>
<p>参数：默认</p>
<p> <img src="/2017/07/12/机器学习/浅谈在线最优化求解算法-以CTR预测模型为例/roc3.png" alt="roc3"></p>
<p>logloss：</p>
<p>线上方法：0.274321867859</p>
<p>FRTL：0.0326626593411</p>
<p>2、</p>
<p>训练：16-10-22的前7天数据</p>
<p>预测：16-10-22当天数据，其中每9条用于在线学习，预测第10条</p>
<p>维度：1048576</p>
<p>参数：默认</p>
<p>训练时间：11:54-12:18</p>
<p> <img src="/2017/07/12/机器学习/浅谈在线最优化求解算法-以CTR预测模型为例/roc1.png" alt="roc1"></p>
<p>logloss：</p>
<p>线上方法：0.275704770725</p>
<p>FRTL：0.032281346379</p>
<p>3、</p>
<p>训练集：16-10-18的前10天数据</p>
<p>预测：16-10-18当天数据，其中每9条用于在线学习，预测第10条</p>
<p>维度：4194304</p>
<p>参数：默认</p>
<p>训练时间：13:50-14:19</p>
<p> <img src="/2017/07/12/机器学习/浅谈在线最优化求解算法-以CTR预测模型为例/roc2.png" alt="roc2"></p>
<p>logloss</p>
<p>线上方法：0.073087783303</p>
<p>FTRL：0.022967801811</p>
<p>4、</p>
<p>训练集：16-10-18的前10天数据</p>
<p>预测：16-10-18当天数据，其中每9条用于在线学习，预测第10条</p>
<p>维度：4194304</p>
<p>参数：训练的特征项改为1-10</p>
<p>训练时间：17:07-17:45</p>
<p> <img src="/2017/07/12/机器学习/浅谈在线最优化求解算法-以CTR预测模型为例/roc4.png" alt="roc4"></p>
<p>logloss</p>
<p>线上方法：0.073087783303</p>
<p>FTRL：0.0221369813697</p>
<p>特征权重不为0的维度有11301个</p>
<h2 id="观点"><a href="#观点" class="headerlink" title="观点"></a>观点</h2><p>FTRL在线训练时间长了效果往往会下降，因为学习率会逐渐降低，必须要offline结合online。</p>
<h1 id="主要参考资料"><a href="#主要参考资料" class="headerlink" title="主要参考资料"></a>主要参考资料</h1><p>【在线最优化求解(Online Optimization)-冯扬】</p>
<p>【逻辑回归从入门到精通-腾讯柳超】</p>
<p>【FTRL的理论论文】Factorization machines with follow-the-regularized-leader for CTR prediction in display advertising  <a href="http://www0.cs.ucl.ac.uk/staff/w.zhang/rtb-papers/fm-ftrl.pdf" target="_blank" rel="external">http://www0.cs.ucl.ac.uk/staff/w.zhang/rtb-papers/fm-ftrl.pdf</a></p>
<p>【FTRL的工程实现论文】<a href="https://www.eecs.tufts.edu/~dsculley/papers/ad-click-prediction.pdf" target="_blank" rel="external">https://www.eecs.tufts.edu/~dsculley/papers/ad-click-prediction.pdf</a> </p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/最优化问题/" rel="tag"># 最优化问题</a>
          
            <a href="/tags/CTR/" rel="tag"># CTR</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/07/12/算法与数据结构/查找树/" rel="next" title="查找树">
                <i class="fa fa-chevron-left"></i> 查找树
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/07/12/机器学习/集成学习/" rel="prev" title="集成学习">
                集成学习 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="hypercomments_widget"></div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Schwimmer" />
          <p class="site-author-name" itemprop="name">Schwimmer</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
           
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">43</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">10</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">41</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1、最优化求解问题"><span class="nav-number">1.</span> <span class="nav-text">1、最优化求解问题</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2、批量最优化求解算法"><span class="nav-number">2.</span> <span class="nav-text">2、批量最优化求解算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-批量和随机求解"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 批量和随机求解</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-正则化"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 正则化</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3、在线最优化求解算法"><span class="nav-number">3.</span> <span class="nav-text">3、在线最优化求解算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-截断梯度法TG"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 截断梯度法TG</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-1-简单截断法"><span class="nav-number">3.1.1.</span> <span class="nav-text">3.1.1 简单截断法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-2-截断梯度法"><span class="nav-number">3.1.2.</span> <span class="nav-text">3.1.2 截断梯度法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-前向后向切分FOBOS"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 前向后向切分FOBOS</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-1-FOBOS算法原理"><span class="nav-number">3.2.1.</span> <span class="nav-text">3.2.1 FOBOS算法原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-2-L1-FOBOS"><span class="nav-number">3.2.2.</span> <span class="nav-text">3.2.2 L1-FOBOS</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-3-RDA"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 RDA</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-1-RDA算法原理"><span class="nav-number">3.3.1.</span> <span class="nav-text">3.3.1 RDA算法原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-2-L1-RDA"><span class="nav-number">3.3.2.</span> <span class="nav-text">3.3.2 L1-RDA</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-3-L1-RDA和L1-FOBOS的比较"><span class="nav-number">3.3.3.</span> <span class="nav-text">3.3.3 L1-RDA和L1-FOBOS的比较</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-4-FTRL"><span class="nav-number">3.4.</span> <span class="nav-text">3.4 FTRL</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-1-L1-FOBOS和L1-RDA在形式上的统一"><span class="nav-number">3.4.1.</span> <span class="nav-text">3.4.1 L1-FOBOS和L1-RDA在形式上的统一</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-2-FTRL算法原理"><span class="nav-number">3.4.2.</span> <span class="nav-text">3.4.2 FTRL算法原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-3-学习率"><span class="nav-number">3.4.3.</span> <span class="nav-text">3.4.3 学习率</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-4-伪代码解读"><span class="nav-number">3.4.4.</span> <span class="nav-text">2.5.4 伪代码解读</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-5-实现代码"><span class="nav-number">3.4.5.</span> <span class="nav-text">2.5.5 实现代码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-6-实验及结论"><span class="nav-number">3.4.6.</span> <span class="nav-text">3.4.6 实验及结论</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#观点"><span class="nav-number">3.5.</span> <span class="nav-text">观点</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#主要参考资料"><span class="nav-number">4.</span> <span class="nav-text">主要参考资料</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Schwimmer</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>

<div>
<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

本站总访问量 <span id="busuanzi_value_site_pv"></span> 次&nbsp&nbsp&nbsp
本站访客数<span id="busuanzi_value_site_uv"></span>人次
</div>




        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  






  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.0"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  


  




	

		<script type="text/javascript">
		_hcwp = window._hcwp || [];

		_hcwp.push({widget:"Bloggerstream", widget_id: 100710, selector:".hc-comment-count", label: "{\%COUNT%\}" });

		
		_hcwp.push({widget:"Stream", widget_id: 100710, xid: "2017/07/12/机器学习/浅谈在线最优化求解算法-以CTR预测模型为例/"});
		

		(function() {
		if("HC_LOAD_INIT" in window)return;
		HC_LOAD_INIT = true;
		var lang = (navigator.language || navigator.systemLanguage || navigator.userLanguage || "en").substr(0, 2).toLowerCase();
		var hcc = document.createElement("script"); hcc.type = "text/javascript"; hcc.async = true;
		hcc.src = ("https:" == document.location.protocol ? "https" : "http")+"://w.hypercomments.com/widget/hc/100710/"+lang+"/widget.js";
		var s = document.getElementsByTagName("script")[0];
		s.parentNode.insertBefore(hcc, s.nextSibling);
		})();
		</script>

	












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length == 0) {
      search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.popup').toggle();
    }
    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';
      $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = $( "entry", xmlResponse ).map(function() {
            return {
              title: $( "title", this ).text(),
              content: $("content",this).text(),
              url: $( "url" , this).text()
            };
          }).get();
          var $input = document.getElementById(search_id);
          var $resultContent = document.getElementById(content_id);
          $input.addEventListener('input', function(){
            var matchcounts = 0;
            var str='<ul class=\"search-result-list\">';
            var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
            $resultContent.innerHTML = "";
            if (this.value.trim().length > 1) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var content_index = [];
                var data_title = data.title.trim().toLowerCase();
                var data_content = data.content.trim().replace(/<[^>]+>/g,"").toLowerCase();
                var data_url = decodeURIComponent(data.url);
                var index_title = -1;
                var index_content = -1;
                var first_occur = -1;
                // only match artiles with not empty titles and contents
                if(data_title != '') {
                  keywords.forEach(function(keyword, i) {
                    index_title = data_title.indexOf(keyword);
                    index_content = data_content.indexOf(keyword);
                    if( index_title >= 0 || index_content >= 0 ){
                      isMatch = true;
                      if (i == 0) {
                        first_occur = index_content;
                      }
                    }

                  });
                }
                // show search results
                if (isMatch) {
                  matchcounts += 1;
                  str += "<li><a href='"+ data_url +"' class='search-result-title'>"+ data_title +"</a>";
                  var content = data.content.trim().replace(/<[^>]+>/g,"");
                  if (first_occur >= 0) {
                    // cut out 100 characters
                    var start = first_occur - 20;
                    var end = first_occur + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if(start == 0){
                      end = 50;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    var match_content = content.substring(start, end);
                    // highlight all keywords
                    keywords.forEach(function(keyword){
                      var regS = new RegExp(keyword, "gi");
                      match_content = match_content.replace(regS, "<b class=\"search-keyword\">"+keyword+"</b>");
                    });

                    str += "<p class=\"search-result\">" + match_content +"...</p>"
                  }
                  str += "</li>";
                }
              })};
            str += "</ul>";
            if (matchcounts == 0) { str = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>' }
            if (keywords == "") { str = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>' }
            $resultContent.innerHTML = str;
          });
          proceedsearch();
        }
      });}

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched == false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(function(e){
      $('.popup').hide();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    });
    $('.popup').click(function(e){
      e.stopPropagation();
    });
  </script>





  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

</body>
</html>
