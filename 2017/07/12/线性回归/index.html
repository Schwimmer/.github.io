<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Schwimmer&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="线性回归首先定义线性回归的函数方程 $$h=\theta_0+\theta_1x_1+\theta_2x2=\sum{i=1}^n\theta_ix_i=\theta^Tx$$ 这里的$\theta$是权重，等式的最右边中$\theta$和$x$都是向量，n是输入变量的个数（不包括$x_0$）。 为了使$h(x)$接近$y$，定义损失函数为$$J(\theta)=\frac 1 2 \sum{i=">
<meta property="og:type" content="article">
<meta property="og:title" content="Schwimmer&#39;s Blog">
<meta property="og:url" content="http://Schwimmer.github.io/2017/07/12/线性回归/index.html">
<meta property="og:site_name" content="Schwimmer&#39;s Blog">
<meta property="og:description" content="线性回归首先定义线性回归的函数方程 $$h=\theta_0+\theta_1x_1+\theta_2x2=\sum{i=1}^n\theta_ix_i=\theta^Tx$$ 这里的$\theta$是权重，等式的最右边中$\theta$和$x$都是向量，n是输入变量的个数（不包括$x_0$）。 为了使$h(x)$接近$y$，定义损失函数为$$J(\theta)=\frac 1 2 \sum{i=">
<meta property="og:image" content="http://schwimmer.github.io/2017/07/12/线性回归/pic/feature%20scaling.jpg">
<meta property="og:image" content="http://images.cnitblog.com/blog/575572/201401/261254436426608.png">
<meta property="og:image" content="http://images.cnitblog.com/blog/575572/201401/261257015324349.png">
<meta property="og:updated_time" content="2017-07-12T03:44:44.011Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Schwimmer&#39;s Blog">
<meta name="twitter:description" content="线性回归首先定义线性回归的函数方程 $$h=\theta_0+\theta_1x_1+\theta_2x2=\sum{i=1}^n\theta_ix_i=\theta^Tx$$ 这里的$\theta$是权重，等式的最右边中$\theta$和$x$都是向量，n是输入变量的个数（不包括$x_0$）。 为了使$h(x)$接近$y$，定义损失函数为$$J(\theta)=\frac 1 2 \sum{i=">
<meta name="twitter:image" content="http://schwimmer.github.io/2017/07/12/线性回归/pic/feature%20scaling.jpg">
  
    <link rel="alternate" href="/atom.xml" title="Schwimmer&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Schwimmer&#39;s Blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://Schwimmer.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-线性回归" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/07/12/线性回归/" class="article-date">
  <time datetime="2017-07-12T03:44:44.011Z" itemprop="datePublished">2017-07-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><p>首先定义线性回归的函数方程</p>
<p>$$<br>h=\theta_0+\theta_1x_1+\theta_2x<em>2=\sum</em>{i=1}^n\theta_ix_i=\theta^Tx<br>$$</p>
<p>这里的$\theta$是<strong>权重</strong>，等式的最右边中$\theta$和$x$都是向量，n是输入变量的个数（不包括$x_0$）。</p>
<p>为了使$h(x)$接近$y$，定义损失函数为<br>$$<br>J(\theta)=\frac 1 2 \sum<em>{i=1}^m(h</em>\theta(x^{(i)})-y^{(i)})^2<br>$$<br>其中m是样本个数。找到使公式最小的$\theta$的取值。</p>
<h4 id="【插入知识点】-损失函数"><a href="#【插入知识点】-损失函数" class="headerlink" title="【插入知识点】- 损失函数"></a>【插入知识点】- <a href="http://www.360doc.com/content/16/0327/06/1317564_545544873.shtml" target="_blank" rel="external">损失函数</a></h4><p>损失函数（loss function)是估计预测值和实际值的不一致程度。通常使用$L(Y, f(x))$表示。损失函数是 <strong>经验风险函数</strong> 的核心部分，也是 <strong>结构风险函数</strong>重要组成部分。结构风险函数包括<strong>经验风险项</strong>和<strong>正则项</strong>：<br>$$<br>\theta^<em>=arg\min \limits<em>{\theta} \frac 1 N \sum</em>{i=1}^NL(y_i,f(x_i,\theta))+\lambda\Phi(\theta)<br>$$<br>其中，前面的均值函数表示经验风险函数，L代表损失函数，后面的$\Phi$是正则化项。整个式子的含义是：<em>*找到使目标函数最小时的$\theta$值</em></em>。常用的损失函数有：</p>
<h5 id="1、平方损失函数（线性回归）"><a href="#1、平方损失函数（线性回归）" class="headerlink" title="1、平方损失函数（线性回归）"></a>1、平方损失函数（线性回归）</h5><p>最小二乘法（Ordinary Least Squares）是常用的一种平方损失函数，最小二乘的基本原理是：最优拟合直线应该是使各点到回归直线的距离和最小的直线，即平方和最小。<br>平方损失的标准形式：<br>$$<br>L(Y, f(X)) = (Y-f(X))^2<br>$$<br>当样本个数为n时，损失函数变为：<br>$$<br>L(y,f(x)) = \sum_{i=1}^n(y-f(x_i))^2<br>$$<br>其中$y-f(x)$表示残差，整个式子的目的就是最小化<strong>残差的平方和</strong>（residual sum of squares，RSS）。</p>
<p>在实际中，通常用均方误差（MSE）作为衡量指标：<br>$$<br>MSE = \frac{1}{n} \sum_{i=1} ^{n} (\tilde{Y_i} - Y_i )^2<br>$$</p>
<h5 id="2、对数损失函数（逻辑回归）"><a href="#2、对数损失函数（逻辑回归）" class="headerlink" title="2、对数损失函数（逻辑回归）"></a>2、对数损失函数（逻辑回归）</h5><p>逻辑回归假设样本服从<strong>伯努利分布（0-1分布）</strong>，因此不用平方损失函数。</p>
<p>线性回归是以<strong>高斯分布</strong>（正态分布）为误差分析模型，逻辑回归采用的是<strong>伯努利分布</strong>分析误差。而高斯分布、伯努利分布、贝塔分布、迪特里特分布，都属于指数分布。</p>
<p><strong>【插入知识点】</strong> - 离散型随机变量分布</p>
<p>1、0-1分布 随机变量X只能取0和1两个值，分布律是<br>$$<br>P{X=k} = p^k(1-p)^k, k=0,1, (0&lt;P&lt;1)<br>$$</p>
<p>2、伯努利分布、二项分布</p>
<p>实验E只有两个结果，$A$和$\bar A$ 。就称为伯努利实验。n重伯努利实验必须要相互独立（放回抽样，大数据下可以不放回抽样）。</p>
<p>以随机变量X表示n重伯努利实验中事件A发生的次数，求X的分布律。</p>
<p>X所有可能的取值为0,1,2,…n。由于每次实验都是独立的，因此事件A在指定的k次实验中发生，在其他n-k次实验中不发生的概率为：<br>$$<br>p^k(1-p)^{n-k}<br>$$<br>这种指定的方式一共有$C_n^k$种（概率的计算后，因为取样有很多种不同的取法，所以还要再乘以$C_n^k$ ）<br>$$<br>20 *0.2^10.8^{19} = 0.058<br>$$</p>
<p>$$<br>19 <em> 0.2^2 </em> 0.8^{18} =<br>$$</p>
<h1 id="1、Gradient-Descent"><a href="#1、Gradient-Descent" class="headerlink" title="1、Gradient Descent"></a>1、Gradient Descent</h1><p>我们要选择$\theta$来最小化$J(\theta)$，因此给定一个$\theta$的初值，通过梯度下降来优化（1-1）。<br>$$<br>\theta_j:=\theta_j-\alpha\frac {\partial} {\partial\theta_j} J(\theta) \tag{1-1}<br>$$</p>
<p>每次会更新所有的$j=0,…,n$。其中，$\alpha$是学习率。</p>
<p>为了实现算法，我们需要解决偏导的问题。假设我们只有一个训练样本$(x,y)$ ，我们可以先忽略$J(\theta)$的累加。推导过程为：</p>
<p>$$<br>\frac {\partial} {\partial\theta_j} J(\theta) = \frac {\partial} {\partial\theta<em>j} \frac 1 2 (h</em>\theta(x)-y)^2 \<br>= (h_\theta(x)-y) \cdot \frac {\partial} {\partial\theta<em>j}  (h</em>\theta(x)-y) \<br>= (h_\theta(x)-y) \cdot \frac {\partial} {\partial\theta<em>j}  (\sum</em>{i=0}^n \theta_ix<em>i-y) \<br>= (h</em>\theta(x)-y) \cdot x_j<br>$$</p>
<p>这里的i是指这一个训练样本的每个分量。</p>
<p>假设样本总数为m，<strong>批量梯度下降</strong>是：<br>$$<br>Repeat\ until\ convergence { \<br>\theta_j := \theta<em>j - \alpha \sum</em>{i=1}^m (h_\theta(x^{(i)})-y^{(i)})\cdot x_j^{(i)} \tag{1-2}\ \ \ (for\ eveny\ j) \<br>}<br>$$<br>而<strong>随机梯度下降</strong>是：<br>$$<br>Repeat\ until\ convergence { \<br>      for\ i=1\ to\ m, { \<br>          \theta_j := \theta<em>j - \alpha (h</em>\theta(x^{(i)})-y^{(i)})\cdot x_j^{(i)} \tag{1-3}\ \ \ (for\ eveny\ j) \<br>}<br>$$<br>两者的区别是：</p>
<p>前者，每次更新$\theta$都需要遍历一次整个样本集合；而后者，在遍历样本集合的时候，每个样本都能改变$\theta$ ，有更快的收敛速度 。</p>
<blockquote>
<p>使用梯度下降要注意feature scaling（数据规范化），能减少寻找最优解的时间。</p>
</blockquote>
<p> <img src="pic\feature scaling.jpg" alt="feature scaling"></p>
<blockquote>
<p>$\alpha$的选择</p>
<p>1、对于足够小的$\alpha$， $J(\theta)$会单调减少</p>
<p><img src="http://images.cnitblog.com/blog/575572/201401/261254436426608.png" alt="img"></p>
<p>2、如果$\alpha$过小，梯度下降会很慢；</p>
<p>3、如果$\alpha$很大，$J(\theta)$可能会不收敛。</p>
<p><img src="http://images.cnitblog.com/blog/575572/201401/261257015324349.png" alt="img"></p>
<p>如何选择α，如下:</p>
<p>…, 0.001, 0.01, 0.1, 1, …, 或者 …, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, ….</p>
</blockquote>
<h1 id="2、Normal-Equation"><a href="#2、Normal-Equation" class="headerlink" title="2、Normal Equation"></a>2、Normal Equation</h1><h2 id="2-1-矩阵的求导"><a href="#2-1-矩阵的求导" class="headerlink" title="2.1 矩阵的求导"></a>2.1 矩阵的求导</h2><p>梯度下降是最小化$J$的一种途径。</p>
<p>矩阵求导。对于一个函数$f: \mathbb{R}^{m \times n} \mapsto \mathbb{R}$，我们定义函数$f$关于$A$的梯度$\triangledown_Af(A)$为：<br>$$<br>\triangledown<em>Af(A)= \begin{bmatrix}<br>\frac {\partial f} {\partial A</em>{11}}       &amp; \cdots &amp; \frac {\partial f} {\partial A<em>{1n}}      \<br> \vdots &amp; \ddots &amp; \vdots \<br>  \frac {\partial f} {\partial A</em>{m1}}      &amp; \cdots &amp; \frac {\partial f} {\partial A<em>{mn}}      \<br>\end{bmatrix}<br>$$<br>假设$A=\begin{bmatrix}A</em>{11} &amp; A<em>{12}\ A</em>{21} &amp; A<em>{22}\end{bmatrix}$ ，函数$f: \mathbb{R}^{2 \times 2} \mapsto \mathbb{R}$ 为<br>$$<br>f(A)=\frac{3}{2}A</em>{11}+5A<em>{12}^2 + A</em>{21}A_{22}<br>$$<br>则<br>$$<br>\triangledown<em>Af(A)= \begin{bmatrix}<br>\frac {3} {2}       &amp; 10A</em>{12}      \</p>
<p>  A<em>{22}      &amp; A</em>{21}      \<br>\end{bmatrix}<br>$$<br>对于一个n<em>n的矩阵，我们引入<em>*trace</em></em>操作符，$trA$定义为对角线之和<br>$$<br>trA = \sum<em>{i=1}^nA</em>{ii}<br>$$</p>
<h2 id="2-2-最小均方的推导"><a href="#2-2-最小均方的推导" class="headerlink" title="2.2 最小均方的推导"></a>2.2 最小均方的推导</h2><p>得出<br>$$<br>\theta = (X^TX)^{-1}X^Ty<br>$$</p>
<blockquote>
<p>优势：不管特征的scale。因为是纯粹的矩阵算法，没有迭代。</p>
<p>劣势：需要大量矩阵计算。NG建议维数&lt;10000时用normal。</p>
</blockquote>
<p>其中，X可逆的条件是X的列向量不相关。所以在特征选取时不要选线性相关的特征。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://Schwimmer.github.io/2017/07/12/线性回归/" data-id="cj50gpfsh00013cskcxdjhbx2" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2017/07/12/hello-world/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Hello World</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">July 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/07/12/线性回归/">(no title)</a>
          </li>
        
          <li>
            <a href="/2017/07/12/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 Schwimmer<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>